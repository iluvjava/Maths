


---
### **Intro**

Yurri Nesterov in his Lectures in Convex Optimizations, had proposed a lower bound on the convergence rate of iterates and objective values, of first order, generic method, and the class of [convex](CVX%20Analysis/Convex%20Functions%20CENTRAL%20HUB.md), [Lipschitz Smooth](Global%20Lipschitz%20Gradient,%20Strong%20Smoothness,%20Equivalence%20and%20Implications.md) functions. I go over his proof, in my own words, hopefully simiplying, and emphasizes some aspects of the claims. Let's start by define a class of algorithm called the GA1st. 

---
### **GA1st**

We introduce a class of algorithm. 
#### **Def | Class of GA1st**
>We are in $\mathbb R^n$ for now. Given $x^{(0)} \in \mathbb R^n$ An iterative algorithm generates sequence of $(x^{(n)})$ in the space. All classes in GA1st satisfies that 
> $$
> \begin{aligned}
>     x^{(k + 1)} \in \{x^{(0)}\} + \text{span}\left\{\nabla f(x^{(i)})\right\}_{i = 1}^{k - 1}. 
> \end{aligned}
> $$

**Observations**

the best method of class, the method that produce the iterates that has the minimum would be characterized by the following scheme: 

$$
\begin{aligned}
    x^{(k + 1)} = \argmin_{x} \left\lbrace
        f(x) \left| 
            x \in \{x^{(0)}\} + \text{span}\left\{\nabla f(x^{(i)})\right\}_{i = 1}^{k - 1}
        \right.
    \right\rbrace, 
\end{aligned}
$$

The class of GA1st method, would includes algorithm such as 
1. Conjugate Gradient,
2. Quasi-Gradient, 
3. Gradient Descent and Gradient descent with Momentum. 

**Remarks**

GA1st means, Generic Algorithm First Order. 


#### **Thm | A Lower Convexity Bounds of GA1st in Lipschitz Smooth Convex Function**

> There exists a function that is convex and Lipschitz Smooth in $\mathbb R^n$ such that for all algorithm from GA1st, for all $x^{(0)}\in \mathbb R^n$, and $k$ such that $1 \le k \le 1/2(n - 1)$ we have the lower bound for the optimality gap for the function values and its iterates: 
>
> $$
> \begin{aligned}
>   f(x^{(k)}) - f^* \ge 
> \frac{3L \Vert x - x^*\Vert^2}{32(k + 1)^2}, \quad \Vert x^{(k)} - x^*\Vert \ge 1/8 \Vert x^{(0)} - x^*\Vert. 
> \end{aligned}
> $$

We will Proof it in later sections before developing an instance of function that is Lipschitz smooth and convex. That function will assists us with establish the lower bounds for the convergence rates of the algorithm. 

---
### **Observations and Preparations for the proof**

The following content is essential for the proof for the above theorem introduced in the previous section. 

#### **Claim 1 | Shifts Inveriance of GA1st**
> Any $(x^{(k)})_{k \in \mathbb N}$, with $x^{(0)} \in \mathbb R^n$, has $(x^{(k)} - x^{(0)})_{n \in \mathbb Rn}$ generated by unique another algorithm from the class of GA1st, initialized with $x^{(0)} = \mathbf 0$. Therefore, for every algorithm, it suffice to assume that $x^{(0)} = \mathbf 0$. 

**Proof**

Let $x^{(k)}$ be generated by an algorithm from GA1st, with any $x^{(0)} \in \mathbb R^n$, then according to the definition of class GA1st, for all $k \in \mathbb N$, we have $x^{(k)} - x^{(0)} \in \text{span}\{\nabla f(x^{(i)})\}_{i = 1}^{k - 1} = \{\mathbf 0\} + \text{span}\{\nabla f(x^{(i)})\}_{i = 1}^{k - 1}$. Hence, the shifted sequence is generated by another algorithm from class GA1st such that its initial value is $x^{(0)} = \mathbf 0$. 


### **Def | A Function Based on TST Matrices**
> Define $f_k$ to be a Lipschitz smooth function with smoothness constant $L$, has gradient, and in $\mathbb R^n$. The function is given by $f(x):= L/4(1/2 \langle P_k x, AP_k x\rangle - x_1)$, where $A \in \mathbb R^{n \times n}$ is $\text{tridiag}(-1, 2, -1)$, and $P_k = [\e_1\; \e_2\; \cdots \; \e_k \; \mathbf 0 \; \cdots \mathbf 0]$. 

**Observations**

We make observations aboud the following entities
1. $P_kA$. 
2. $\nabla f_k, \nabla ^2 f_k$. 
3. The minimizers and the minimum of $f_k$. 
4. Some other miscellaneous facts
5. An equality between $f_k, f_p$ for $n\ge p\ge k$. 

**The Matrix**

The matrix $P_kA$ is a $\mathbb R^{n\times n}$ matrix such that the upper $k\times k$ matrix is a Tridiagonal Toeplitz matrix (TST) with $-1$ on its sub and super diagonal, and $2$ on its diagonal. The non-zero eigenvalue of $P_k A$ would equal to the eigenvalues of it's top left $k \times k$ diagonal matrix. For any TST matrix, its eigen values are given by $\lambda_k = 2 - 2\cos\left(\frac{k \pi}{n + 1}\right)$. Meaning that the spectrum of matrix $P_k A$ is within $[0, 4]$, a Positive Semi-Definite matrix. 

**The Gradient and Hessians**

Observe that $\nabla f_k (x) = L/4(P_k^TAP_kx - \e_1)$, and the Hessian $\nabla f_k(x) = L/4P_k^TAP_k$. Observe that the Hessian would be positive semi-definition with $L$ being the upper bound for its spectrum. Finally, we denote $A_k = P_k A$ for notational convenience for the future. 

**The minimizers and the Minimum of the Function**

$f$ is a quadratic function with minimizers: 

$$
\begin{aligned}
    \bar x^{[k]} = \frac{1}{k + 1}
    \begin{bmatrix}
        k & k -1 & k - 2 & \cdots & 1 & 0 &  0 & \cdots & 0 
    \end{bmatrix} \in \mathbb R^n, 
\end{aligned}
$$

compactly we have $\bar x^{[k]}_i = (1 - i/(k + 1))$ for all $1 \le i \le k$, and $\bar x^{[k]} = 0$ for all $k + 1 \le i \le n$. We observe the fact that this minimizer is not the unique minimizer for $f_k$, and further more, $\bar x^{[k]} \in \text{span}\{e^{(i)}\}_{i = 1}^k$. The reader should verify that this is indeed one of the minimizers for $f_k$. Next, by the definition of $f_k$, the minimum $f_k^*$ would be 

$$
\begin{aligned}
    f_k^* &= \frac{L}{4}\left(
        \frac{1}{2} 
        \left\langle 
            \bar x^{[k]}, A_k \bar x^{[k]}
        \right\rangle - \bar x^{[k]}
    \right)
    \\
    & \textcolor{gray}{
        \nabla f_k(\bar x^{[k]}) = A_k \bar x^{[k]} - \e_1 = \mathbf 0 \implies 
        A_k \bar x^{[k]} = \e_1
        }
    \\
    &= \frac{L}{4}\left(
            \frac{1}{2} 
            \left\langle 
                \bar x^{[k]}, \e_1
            \right\rangle - \bar x^{[k]}
        \right)
    \\
    &= \frac{L}{4}
    \left(
        \frac{1}{2}\bar x^{[k]} - \bar x_1^{[k]}
    \right) = \frac{-L}{8}\bar x_1^{[k]} = \frac{-L}{8} \frac{k}{k + 1}. 
\end{aligned}
$$

This fact is listed as \[2.1.17\] in Nesterov's Lecture in Convex Optimizations Textbook. 

**The sum of Squared Integers**

The following identity is a fact 

$$
\begin{aligned}
    \sum_{i = 1}^{k} i^2 = \frac{(k (k + 1)(2k + 1))}{6} \le \frac{(k + 1)^3}{3}, 
\end{aligned}
$$

and therefore, the norm of the minimizer can be bounded as 

$$
\begin{aligned}
    \left\Vert
        \bar x^{[k]}
    \right\Vert^2 = 
    \frac{1}{(k + 1)^2} \sum_{i = 1}^{k} i^2 
    \le 
    \frac{1}{(k + 1)^2} \frac{(k + 1)^3}{3} = \frac{k + 1}{3}. 
\end{aligned}
$$
The above is listed as \[2.1.19\] in Nesterov book, Lectures on Convex Optimizations. 


**The Equivalences Between This Class of Functions**

Observe that, $f_k(x) = f_p(P_kx)$ for all $x\in \mathbb R^n$, and $n\ge p \ge k$. 


#### **Lemma 2.15 | The Linear Space Containing the kth Iterates of any GA1st**
> With $x^{(0)} = \mathbf 0$, $p \le n$, we have $\{x^{(k)}\}_{k = 0}^p$, generated by any algorithm from GA1st satisfies the condition $x^{(k)} \in \text{span}\{\nabla f_p(x^{(k)})\}_{i = 0}^{k - 1} = \text{span}\{\e_i\}_{i = 1}^{k}$ for all $1\le k \le p$.  

**Proof**

$\nabla f_p(\mathbf 0) = A_p \mathbf 0 - \e_1\in \text{span}(\e_1)$, hence the base case is satisfied. Inductively we have 

$$
\begin{aligned}
    x^{(k + 1)} &\in
    \text{span}\left\lbrace
        \e_i
    \right\rbrace_{i = 1}^{k - 1}
    \\
    A_p x^{(k + 1)} 
    &\in A_p \text{span}\{\e_i\}^{k - 1}_{i = 1}
    \\
    A_px^{(k + 1)} &\in \text{span}\{\e_i\}^{k}_{i = 1} \textcolor{gray}{
        \quad \triangleright[{[1]}]
    }
    \\
    \nabla f_p\left(
        x^{(k +1)}
    \right) & \in \text{span}\{\e_i\}^{k}_{i = 1}. 
    \quad \textcolor{gray}{\triangleright \text{by def of }\nabla f_p. }
\end{aligned}
$$

at `[[1]]`, we used the fact that $A_p$ has block $p \times p$ tridiagonal matrix with $-1, 2, -1$, and $k\le p$, hence, the lower diagonal of the matrix will shift the elements of the vector in $\text{span}\{\e_i\}_{i = 1}^{k -1}$, bring it into the space of $\text{span}\{\e_i\}_{i = 1}^{k}$. 


#### **Corollary 2.11 | Lower Bound for the Objective value of $f_p$**
> 
