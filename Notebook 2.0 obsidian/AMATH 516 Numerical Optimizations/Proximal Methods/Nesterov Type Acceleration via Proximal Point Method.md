- [Proximal Point Method, Convex](Proximal%20Point%20Method,%20Convex.md)


---
### **Intro**

We use the descent inequality and Lyapunov frameworks proposed in [Proximal Point Method, Convex](Proximal%20Point%20Method,%20Convex.md) to derive the method to derive a specific form of the accelerated gradient. 
We will also prove the convergence rate of the method and deduce the optimal step sizes. 
Before we start, we set up the stage by setting up some quantities that are useful for our derivations later. 

- $f:\R^n \mapsto \overline \R$ is a convex function. 

In this file, we: 
- Introduce the similar triangle form of the Nesterov accelerations method. 
- Derive the convergence rate of the Nesterov accelerations method and optimal stepsizes using the PPM formulations and the Lyaponouv function. Recall results [Nesterov Lower Convergence Bounds for Lipschitz Smooth Functions](../../MATH%20602%20Nesterov%20Acceleration/Convergence%20Rate%20Lower%20Bnd%20for%20Lip%20Functions.md), and we show method proposed in this file can achieve that convergence rate. 

This file has the same goal as the file [Nesterov Acceleration Sequence Method](../../MATH%20602%20Nesterov%20Acceleration/Nesterov%20Original%20Conception%20of%20Momentum%20Method.md). 
For discussion in this file, we reference works by Ahn and Sra ^[K. Ahn and S. Sra, “Understanding nesterov’s acceleration via proximal point method.” arXiv, Jun. 02, 2022. doi: 10.48550/arXiv.2005.08304.
]
The newer derivations using PPM understanding of Nesterov Triangular Form is magnituded easier to understand and follow, and it provides insights into the matter. 

---
### **The Classic, Original Nesterov Accelerations via PPM**
In this section, we discuss the relations between the classic Nesterov accelerated momentum and the PPM method. 
Our goal is to derive the Nesterove accelerations method, derive its convergence rate, and the sequence of stepsizes that achieves the optimal convergence rate. 

#### **Algorithm | PPM Formulation of Nesterov Accelerations**
> Let $x_0\in \R^n$, $y_0 = x_0$, run 
> $$
> \begin{aligned}
>     x_{t + 1} &= \argmin{x \in \R^n} 
>     \left\lbrace
>         l_f(x; y_t) + \frac{1}{\eta_{t + 1}}\Vert x - x_t\Vert^2
>     \right\rbrace & \text{([4.8a])}
>     \\
>     y_{t + 1} &= \argmin{x \in \R^n}
>     \left\lbrace
>         \hat l_f(x; y_t) + \frac{1}{2\eta_{t + 1}} \Vert x - x_{t+ 1}\Vert^2
>     \right\rbrace, & \text{([4.8b])}
> \end{aligned}
> $$
> with $l_f(x; y_t) = f(y_t) + \langle \nabla f(y_t), x - y_t\rangle$, $\hat l_f (x; y_t) = l(x; y_t) + \frac{L}{2}\Vert x - y_t\Vert^2$. 


#### **Algorithm | Nesterov Triangular Form I**
> The nesterov triangular has recurrenced based on $(x_t, y_t, z_t)$, with $x_0 = y_0 = z_0$. 
> $$
> \begin{aligned}
>     y_t &= \frac{L^{-1}}{L^{-1} + \eta_t} + 
>     \frac{\eta_t}{L^{-1} + \eta_t} z_t , 
>     & \text{([4.9a])}
>     \\
>     x_{t + 1} &= x_t - \eta_{t + 1} \nabla f(y_t), 
>     & \text{([4.9b])}
>     \\
>     z_{t + 1} &= y_t - L^{-1}\nabla f(y_t). 
>     & \text{([4.9c])}
> \end{aligned}
> $$


#### **Fact | Equivalences Between the two Forms**
> The PPM formulations above, and the Nesterov Triangular form I, are equivalent formulations of Nesterov Accelerations Algorithms. 

**Remarks**

The proof is complicated and it should be dealt with in separate files. 

#### **Thm 1 | INEQ1 AGM**
> If we execute algorithm: Nesterov Triangular Form I, for convex and differentiable $f: \R^n \mapsto \overline \R$ with a $L$-Lipschitz gradient, Let $(\eta_t)_{t \in \N}$ be strictly positive,then we can obtain following bound on the Lyponouv function: 
> $$
> \begin{aligned}
>     & \eta_{t + 1} f(z_{t + 1} - f(x_*)) + \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2 
>     - \Vert x_* - x_t\Vert^2 
>     \\
>     &\le 
>     \left(
>         \frac{\eta_{t + 1}^2}{2} - \frac{\eta_{t + 1}}{2L} 
>     \right)\Vert \nabla f(y_t)\Vert^2 + 
>     L \eta_{t} \eta_{t + 1} \langle \nabla f(y_t), z_t - y_t\rangle \quad \forall t \in \N, 
> \end{aligned}
> $$
> Where $x_* \in \argmin{x} f(x)$ and we assume it exists, and $(x_t, y_t, z_t)$ are generated by the Nesterov Triangular Form I. 

**Proof**

Let $\phi_t(x) = \eta_{t + 1}(f(y_t) + \langle \nabla f(y_t), x - y_t\rangle)$, which is a convex function. 
By convexity and $L$-Lipschitz smoothness of $f$, we have the inequalities: 

$$
\begin{aligned}
    \eta_{t + 1} l_f(x; y_t) = 
    \phi_t(x) 
    &\le \eta_{t + 1} f(x) 
    \le 
    \phi_t(x)  + \frac{L}{2} \Vert x - y_t\Vert^2. 
\end{aligned}
$$

By convexity of $\phi_t$, the PPM Descent Inequalities are

$$
\begin{aligned}
    \phi_t(x_{t + 1}) - \phi_t(x_*) + 
    \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2 
    - \frac{1}{2}\Vert x_* - x_t\Vert^2
    &\le 
    -\frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 \quad \forall x^* \in \R^n. 
\end{aligned}\tag{$\star$}
$$

The above inequality can be viewed as a consequence of ([4.8a]). 
By the definition of $\phi_t(x)$, we have that

$$
\begin{aligned}
    \phi_t(x_{t + 1}) 
    &= \eta_{t + 1}
    \left(
        f(y_t) + \langle \nabla f(y_t), z_{t + 1} - y_t\rangle 
        + 
        \langle \nabla f(y_t), x_{t + 1} - z_{t + 1}\rangle
    \right)
    \\
    & \quad \text{By Smoothness: }
    \left\lbrace
        \begin{aligned}
            f(z_{t + 1}) - \langle \nabla f(y_t), z_{t + 1} - y_{t} \rangle - f(y_t)
            &\le 
            \frac{L}{2}\Vert z_{t + 1} - y_t \Vert^2, 
            \\
            -(\langle \nabla f(y_t), z_{t + 1} - y_t \rangle + f(y_t) + f(y_t))
            &\le 
            \frac{L}{2}\Vert z_{t + 1} - y_t \Vert^2 - f(z_{t + 1}). 
        \end{aligned}
    \right.
    \\
    \implies 
    \phi(x_{t + 1}) & \ge 
    \eta_{t + 1} 
    \left(
        f(z_{t + 1}) - \frac{L}{2} \Vert z_{t + 1} - y_t\Vert^2
        + 
        \langle \nabla f(y_t), x_{t + 1} - z_{t + 1}\rangle
    \right) \leftarrow ([\star]). 
\end{aligned}
$$

That is a lower bound and an upper bound would be $\phi_t(x) \le \eta_{t + 1}f(x) \;\forall x$, refer to it as ([$\star *$]). 
Substiting ([$\star$]), ([$\star *$]) into ($\star$) yields: 

$$
\begin{aligned}
    & 
    \eta_{t + 1}(f(z_{t + 1}) - f(x_*)) 
    + \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2
    - \frac{1}{2}\Vert x_* - x_t\Vert^2
    \\
    &\le 
    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 + 
    \eta_{t + 1}
    \left(
        \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
        + 
        \langle \nabla f(y_t), z_{t + 1} - x_{t + 1}\rangle
    \right). \leftarrow ([\star\star])
\end{aligned}
$$

$f(z_{t + 1})$ comes from RHS of ([$\star$]). 
Other terms in RHS of ([$\star$]) is put as negative as it moves to the RHS of ([$\star\star$]). 
To investigate futher, we split the inner product term on RHS of $([\star\star])$. 
So, 

$$
\begin{aligned}
    \langle \nabla f(y_t), z_{t + 1} - x_{t + 1}\rangle 
    &= 
    \langle 
    \nabla f(y_t), (z_{t + 1} - y_t)+ (y_t - x_t)
    + (x_t - x_{t + 1})
    \rangle. 
    \\
    \text{from 4.9}: &
    \left\lbrace
    \begin{aligned}
        & 
        x_{t + 1} - x_t = -\eta_{t + 1} \nabla f(y_t)  
        \\
        &
        z_{t + 1} - y_t = L^{-1} \nabla f(y_t) 
    \end{aligned}
    \right\rbrace \leftarrow ([\star **]), 
    \\
    &= 
    \langle \nabla f(y_t), 
    -L^{-1} \nabla f(y_t) + (y_t - x_t) + \eta_{t + 1} \nabla f(y_t) 
    \rangle
    \\
    &= 
    - L^{- 1} \Vert f(y_t) \Vert^2 + 
    \langle \nabla f(y_t), y_t - x_t\rangle + 
    \eta_{t + 1} \Vert \nabla f(y_t)\Vert^2. 
\end{aligned}
$$

Substitute above back to RHS of $([\star\star])$, we obtain the inequality: 

$$
\begin{aligned}
    & 
    \eta_{t + 1}(f(z_{t + 1}) - f(x_*)) 
    + \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2
    - \frac{1}{2}\Vert x_* - x_t\Vert^2
    \\
    &\le 
    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 + 
    \eta_{t + 1}
    \left(
        \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
        + 
        \langle \nabla f(y_t), z_{t + 1} - x_{t + 1}\rangle
    \right)
    \\
    &= 
    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 + 
    \eta_{t + 1}
    \left(
        \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
        + 
       - L^{- 1} \Vert f(y_t) \Vert^2 + 
    \langle \nabla f(y_t), y_t - x_t\rangle + 
    \eta_{t + 1} \Vert \nabla f(y_t)\Vert^2
    \right)
    \\
    &= 
    \left(
        \frac{-\eta_{t + 1}^2}{2} + \frac{\eta_{t + 1}}{2L} - 
        \frac{\eta_{t + 1}}{L} + \eta_{t + 1}^2 
    \right)\Vert \nabla f(y_t)\Vert^2 
    + 
    \eta_{t + 1} \langle \nabla f(y_t), y_t - x_t\rangle
    \\
    &= 
    \left(
        \frac{\eta_{t + 1}^2}{2} - \frac{\eta_{t + 1}}{2L}
    \right) 
    \Vert \nabla f(y_t)\Vert^2  + 
    \eta_{t + 1} \langle \nabla f(y_t), y_t - x_t\rangle. 
\end{aligned}
$$

Use ([4.9a]), we have update relation $y_t - x_t = L \eta_t(z_t - y_t)$ (**Steps are skipped here**). 
Which simplifies the above futher 

$$
\begin{aligned}
    &= 
    \left(
        \frac{\eta_{t + 1}^2}{2} - \frac{\eta_{t + 1}}{2L}
    \right) 
    \Vert \nabla f(y_t)\Vert^2  + 
    \eta_{t + 1} \langle \nabla f(y_t), L\eta_t(z_t - y_t)\rangle
    \\
    &=
    \left(
        \frac{\eta_{t + 1}^2}{2} - \frac{\eta_{t + 1}}{2L}
    \right) 
    \Vert \nabla f(y_t)\Vert^2  + 
    L \eta_{t + 1} \eta_t\langle \nabla f(y_t), (z_t - y_t)\rangle, 
\end{aligned}
$$

And this completes the proof of AGM INEQ1. 


**Remark**

We didnt' use the condition that $x_*$ is a minimizer, this inequality is true for all $x_*$. 
It is unclear why it's stated as such in the original paper. 
It could be the artifacts of something else, and this provide potential directions for generalizations of the proof as well. 


#### **Thm 2 | INEQ2**
> With the exact same assumption used in Thm 1 for deriving INEQ1, we have additionally the inequality: 
>
> $$
> \begin{aligned}
>     f(z_{t + 1}) - f(z_t) &\le 
>     - \frac{1}{2L} \Vert \nabla f(y_t)\Vert^2
>     + 
>     \langle \nabla f(y_t), y_t - z_t\rangle, 
> \end{aligned}
> $$
> for all $t \in \N$. 

**Proof**

Start by considering 

$$
\begin{aligned}
    f(z_{t + 1}) - f(z_t) &= f(z_{t + 1}) - f(y_t) + f(y_t) - f(z_t). 
\end{aligned}\tag{$[\star]$}
$$

By update $z_{t + 1} = y_t - L^{-1}\nabla f(y_t)$, and smoothness of $f$, we have inequalities: 

$$
\begin{aligned}
    0 \le f(z_{t + 1}) - f(y_t) - \langle \nabla f(y_t), z_{t +1} - y_t\rangle
    &\le 
    \frac{L}{2}\Vert z_{t + 1} - y\Vert^2
    \\
    \langle \nabla f(y_t), z_{t +1} - y_t\rangle 
    \le f(z_{t + 1}) - f(y_t) 
    &\le  \langle \nabla f(y_t), z_{t +1} - y_t\rangle + \frac{L}{2}\Vert z_{t + 1} - y\Vert^2
    \\
    \text{ substitue: } z_{t + 1} - y_t 
    &= - L^{-1} \Vert z_{t + 1} - y\Vert^2, \text{ we get: }
    \\
    -L^{-1} \Vert \nabla f(y_t)\Vert^2 
    \le 
    f(z_{t + 1}) - f(y_t) &\le 
    \underbrace{
        - L^{-1} \Vert \nabla f(y_t)\Vert^2 + 
        \frac{1}{2L} \Vert \nabla f(y_t)\Vert^2
    }_{
        = - \frac{1}{2L}\Vert \nabla f(y_t)\Vert^2
    }. 
\end{aligned}
$$

Substituting the above back into ([$\star$]), we have inquality

$$
\begin{aligned}
    f(z_{t + 1}) - f(z_t) &\le - \frac{1}{2L} \Vert \nabla f(y_t)\Vert^2
    + f(y_t) - f(z_t), 
\end{aligned}
$$

because $f$ convex we have $f(z_t) - f(y_t) \ge \langle \nabla f(y_t), z_t - y_t\rangle$, giving us 
$$
\begin{aligned}
    f(z_{t + 1}) - f(z_t) &\le - \frac{1}{2L} \Vert \nabla f(y_t)\Vert^2
    + 
    \langle \nabla f(y_t), z_t - y_t\rangle. 
\end{aligned}
$$

and this is AGM INEQ2. 


---
### **The Convergence Theorem**

The eastalishment of Thm 1, 2, which are INEQ1 AGM, INEQ2 AGM, allows us to select for a specific stepsizes sequence $\eta_t$ such that the method of Nesterov Similar Triangle Form I has convergence of $f(z_t)$. 
This is stated by the following theorem: 

#### **Thm | Optimal Convergence Rate of The Nesterov Triangular Form I**
> 



