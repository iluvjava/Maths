- [Smooth Gradient Descend The Basics](../Classics%20Algorithms/Smooth%20Gradient%20Descend%20The%20Basics.md)
- [Morau Envelope and Proximal Operators](Moreau%20Envelope%20and%20Convex%20Proximal%20Mapping.md)

---
### **Intro**

Let the ambient space be $\R^n$ and let $f: \R^n \rightarrow \overline {\R}$
Our attention is on the method of proximal point. 
Let $x_0 \in \text{dom} f$, given a strictly positive sequence $(\eta_t)_{t \ge 1}$, consider a sequence $(x_t)_{t \ge0}$ generated by 

$$
\begin{aligned}
    x_{t + 1} = \prox_{\eta_{t + 1} f}(x_t):= 
    \argmin{x\in \R^n}
    \left\lbrace
        f(x) + \frac{1}{\eta_{t + 1}}\Vert x - x_t\Vert^2
    \right\rbrace. 
\end{aligned}
$$

For notational convenience, for all $y \in \R^n$ we define model function 

$$
\begin{aligned}
    (\forall x \in \R^n)\quad 
    \mathcal M_f^{1/\eta}(x; y) := f(x) + \frac{\eta}{2}\Vert x - y\Vert^2. 
\end{aligned}
$$

The following lemma gives the results of proximal inequality which is relevant to proving the convergence of PPM method. 

#### **Lemma 1 | Proximal inequality**
> Let $y \in \R^n$. 
> Suppose that the PPM generates sequence $(x_t)_{ t \ge 0}$ on $F$. 
> If $F: \R^n \rightarrow \overline \R$ is $\mu \ge 0$ strongly convex, 
> then for all $x \in \R^n, t\ge 0$:
> $$
> \begin{aligned}
>     0 &\le 
>     F(x) - F(x_{t + 1}) + \frac{\eta_{t + 1}}{2}\left(
>         \Vert x - x_t\Vert^2 - \Vert x_{t + 1} - x_t \Vert^2
>     \right) - \frac{\eta_{t + 1} + \mu}{2}\Vert x - x_{t + 1}\Vert^2
>     \\
>     &= 
>     F(x) - F(x_{t + 1}) 
>     + 
>     \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle
>     - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
>     \\
>     &\le 
>     F(x) - F(x_{t + 1}) 
>     + 
>     \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle. 
> \end{aligned}
> $$

**Proof**

Consider the $\mu + \eta_{t + 1}$ strong convexity property of $\mathcal M^{\eta_{t + 1}}_F(\cdot; y)$ and the fact that $x_{t + 1}$ is the minimizer of $\mathcal M_F^{\eta_{t + 1}}(\cdot; y)$ which gives the quadratic growth condition: 

$$
\begin{aligned}
    0 &\le 
    \mathcal M(x; x_t) - \mathcal M(x_{t + 1}; x_t) 
    - \frac{\eta_{t + 1} + \mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= F(x) - F(x_{t + 1}) + \frac{\eta_{t + 1}}{2}\left(
        \Vert x - x_t\Vert^2 - \Vert x_{t + 1} - x_t \Vert^2
        - \Vert x - x_{t + 1}\Vert^2
    \right)
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= F(x) - F(x_{t + 1}) + \frac{\eta_{t + 1}}{2}\left(
        \Vert x - x_t\Vert^2 - \Vert x_{t + 1} - x_t \Vert^2
    \right) - \frac{\eta_{t + 1} + \mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= F(x) - F(x_{t + 1}) + \frac{\eta_{t + 1}}{2}\left(
        \Vert x - x_{t + 1} + x_{t + 1} - x_t\Vert^2 - \Vert x_{t + 1} - x_t \Vert^2
        - \Vert x - x_{t + 1}\Vert^2
    \right) 
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= 
    F(x) - F(x_{t + 1}) 
    + \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &\le 
    F(x) - F(x_{t + 1}) 
    + \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle. 
\end{aligned}
$$

**Remarks**

The inequality is equivalent to $F$ being $\mu \ge 0$ strongly convex. 
For a discussion on the proximal mapping in the convex settings, visit: [Moreau Envelope and Convex Proximal Mapping](../Proximal%20Operator/Moreau%20Envelope%20and%20Convex%20Proximal%20Mapping.md). 


#### **Lemma 2 | Monotone decrease of function value**
> Let iterates $(x_t)_{t \ge 1}$ be generated by the PPM method on $F: \R^n \rightarrow \R$ that is convex then for all $t \ge 0$ it has 
> $$
> \begin{aligned}
>     0 &\le F(x_t) - F(x_{t + 1}) - \eta_{t + 1}\Vert x_{t + 1} - x_t\Vert^2. 
> \end{aligned}
> $$
> When the function is bounded below by $F^+$, this gives 
> $$
> \begin{aligned}
>     \eta_{t + 1} \Vert x_{t + 1} - x_t\Vert^2 
>     &\le 
>     F(x_t) - F(x_{t + 1}) \le F(x_t) - F^+. 
> \end{aligned}
> $$

**Proof**

The proof is direct from the proximal inequality. 
Substituting $x = x_t$ then it has: 

$$
\begin{aligned}
    0 &\le 
    F(x_t) - F(x_{t + 1}) + \frac{\eta_{t + 1}}{2}\left(
    \Vert x_t - x_t\Vert^2 - \Vert x_{t + 1} - x_t \Vert^2
    \right) - \frac{\eta_{t + 1}}{2}\Vert x_t - x_{t + 1}\Vert^2
    \\
    &\le 
    F(x_t) - F(x_{t + 1}) + \frac{\eta_{t + 1}}{2}\left(
     - \Vert x_{t + 1} - x_t \Vert^2
    \right) - \frac{\eta_{t + 1}}{2}\Vert x_t - x_{t + 1}\Vert^2
    \\
    &\le F(x_t) - F(x_{t + 1}) - \eta_{t + 1}\Vert x_{t + 1} - x_t\Vert^2. 
\end{aligned}
$$

$\blacksquare$

---
### **Convergence in the convex case**

We show the convergence of PPM in the convex case. 


#### **Theorem 1.1 | A Lyapunov quantity for PPM under convexity**
> Suppose that $F:\R^n \rightarrow \overline \R$ is a convex function. 
> Let $(x_t)_{t \ge 0}$ be a sequence generated by PPM with $F$, and relaxation sequence $(\eta_t)_{t \ge 1}$ such that $\infty > \eta_t > 0$ for all $t \ge 0$. 
> Then, the iterates satisfies for all $x \in \R^n$: 
> $$
> \begin{aligned}
>     \left(
>         \sum_{i = 1}^{t + 1} 
>         \eta_i^{-1}
>     \right)(F(x_{t + 1}) - F(x)) + \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
>     &\le \frac{1}{2}(\Vert x - x_0\Vert^2 - \Vert x_0 - x_1\Vert^2). 
> \end{aligned}
> $$
> If $x^+$ exists as a minimizer of $F$, this gives a convergence rate of $\mathcal O\left(\left(\sum_{i = 1}^{N}\eta_i^{-1}\right)^{-1}\right)$ for the sequence $F(x_k) - F(x^+)$. 


**Proof**


Let's assume that the sequence $(\sigma_t)_{t \ge 1}, (\eta_t)_{t \ge 1}$ are sequence such that $\sigma_t \ge 0$ is always non-negative and $\eta_t > 0$ for all $t \ge 1$. 
The following intermediate results are presented in advanced before their proofs. 

**(I)**. 
The algorithm has $F(x_{t + 1}) - F(x_t) \le 0$ for all $t \ge 0$ from **Lemma 2** because of the convexity assumption on $F$. 

**(II)**. 
For all $t \ge 0$: 
$$
\begin{aligned}
    F(x_{t + 1}) - F(x)
    &\ge (\sigma_{t + 1} + 1)(F(x_{t + 1}) - F(x)) - \sigma_{t + 1} (F(x_t) - F(x)). 
\end{aligned}
$$

**(III)**. If $(\forall t \ge 1)\;\sigma_{t + 1}/\eta_{t + 1} = \sigma_t/\eta_t + \eta_t^{-1}$ for all $t \ge 1$. 
Then it has that $\eta_{t + 1}^{-1}\sigma_{t + 1} = \sigma_1 \eta_1^{-1} + \sum_{i = 1}^{t}\eta_i^{-1}$. 
Let the base case be: $\sigma_1 = 0$, then it simplifies. 

**(IV)**. 
From the base case we also have for all $x_0 \in \R^n, x \in \R^n$ that 
$$
\begin{aligned}
    \eta_1^{-1}(F(x_1) - F(x)) + \frac{1}{2}\Vert x - x_1\Vert^2
    &\le 
    \frac{1}{2}\Vert x - x_0\Vert^2 - \frac{1}{2}\Vert x_0 - x_1\Vert^2. 
\end{aligned}
$$


Then consider for all $t \ge 0$, from the proximal point inequality: 

$$
\begin{aligned}
    0 &\ge 
    F(x_{t + 1}) - F(x) + 
    \frac{\eta_{t + 1}}{2}
    \left(
        \Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2
    \right)
    +
    \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    \\
    &\hspace{-0.2em}\underset{\text{(II)}}{\ge}
    (\sigma_{t + 1} + 1)(F(x_{t + 1}) - F(x)) - \sigma_{t + 1} (F(x_t) - F(x))
    + 
    \frac{\eta_{t + 1}}{2}
    \left(
        \Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2
    \right)
    \\
    &\quad 
        + 
        \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    \\
    &\ge 
    (\sigma_{t + 1} + 1)(F(x_{t + 1}) - F(x)) - \sigma_{t + 1} (F(x_t) - F(x))
    + 
    \frac{\eta_{t + 1}}{2}
    \left(
        \Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2
    \right)
    \\
    &= 
    \eta_{t + 1}
    \left(
        \left(
            \frac{\sigma_{t + 1}}{\eta_{t + 1}} + \frac{1}{\eta_{t + 1}}
        \right)(F(x_{t + 1}) - F(x))
        + \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
        - 
        \left(
            \frac{\sigma_{t + 1}}{\eta_{t + 1}}(F(x_t) - F(x))
            + \frac{1}{2}\Vert x - x_t\Vert^2
        \right)
    \right)
    \\
    \iff 
    0 
    &\ge  
    \left(
        \frac{\sigma_{t + 1}}{\eta_{t + 1}} + \frac{1}{\eta_{t + 1}}
    \right)(F(x_{t + 1}) - F(x))
    + \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
    - 
    \left(
        \left(
            \frac{\sigma_{t}}{\eta_{t}} + \frac{1}{\eta_t}
        \right)
        (F(x_t) - F(x))
        + \frac{1}{2}\Vert x - x_t\Vert^2
    \right). 
\end{aligned}
$$

Since this is true for all $t \ge 0$ telescoping the series makes: 

$$
\begin{aligned}
    0 &\ge 
    \left(
        \frac{\sigma_{t + 1}}{\eta_{t + 1}} + \frac{1}{\eta_{t + 1}}
    \right)(F(x_{t + 1}) - F(x))
    + \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
    -
    \left(
        \left(
            \frac{\sigma_{1}}{\eta_{1}} + \frac{1}{\eta_1}
        \right)
        (F(x_1) - F(x))
        + \frac{1}{2}\Vert x - x_1\Vert^2
    \right)
    \\
    &\hspace{-.3em}\underset{\text{(III)}}{=}
    \left(
        \sum_{i = 1}^{t + 1}\eta_i^{-1}
    \right)
    (F(x_{t + 1}) - F(x))
    + 
    \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
    - 
    \left(
        \frac{1}{\eta_1}
        (F(x_1) - F(x))
        + \frac{1}{2}\Vert x - x_1\Vert^2
    \right)
    \\
    &\hspace{-0.3em}\underset{\text{(IV)}}{\ge} 
    \left(
        \sum_{i = 1}^{t + 1}\eta_i^{-1}
    \right)
    (F(x_{t + 1}) - F(x))
    + 
    \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
    - 
    \frac{1}{2}
    \left(
        \Vert x - x_0\Vert^2 - \Vert x_0 - x_1\Vert^2
    \right). 
\end{aligned}
$$

And this is pretty much the convergence claim of the algorithm.


**Proof of (II)**. 
The proof is direct when $\sigma_{t + 1} \ge 0$ for all $t \ge 1$. 
Consider 

$$
\begin{aligned}
    F(x_{t + 1}) - F(x) &= 
    (\sigma_{t + 1} + 1 - \sigma_{t + 1})(F(x_{t + 1})- F(x))
    \\
    &= 
    (\sigma_{t + 1} + 1)(F(x_{t + 1}) - F(x))
    - \sigma_{t + 1}(F(x_{t + 1}) - F(x))
    \\
    &= 
    (\sigma_{t + 1} + 1)(F(x_{t + 1}) - F(x))
    - \sigma_{t + 1} (F(x_{t + 1}) - F(x_k) + F(x_k) - F(x))
    \\
    &\underset{\text{(I)}}{\ge} 
    (\sigma_{t + 1} + 1)(F(x_{t + 1}) - F(x))
    - \sigma_{t + 1} (F(x_k) - F(x))

\end{aligned}
$$

**Proof of (III)**. 
The proof is direct from the recursive relations. 
Consider for all $t \ge 1$ we have the equation: 
$$
\begin{aligned}
    \eta_{t + 1}^{-1}\sigma_{t + 1} - \sigma_t \eta_t^{-1} 
    & = \eta_t^{-1}
    \\
    \implies
    \sum_{i = 1}^{t} \eta_{i + 1}^{-1} \sigma_{i + 1} - \sigma_i \eta_i^{-1}
    &= \sum_{i = 1}^{t} \eta_i^{-1}
    \\
    \iff 
    \eta_{t + 1}^{-1} \sigma_{t + 1} - \sigma_1\eta_1^{-1}
    &= 
    \sum_{i = 1}^{t} \eta_i ^{-1}. 
\end{aligned}
$$

**Proof of (IV)**. 
This is true if we put in $t = 0$ for the proximal inequality, which gives us the inequality of the first step in the PPM method. 

$$
\begin{aligned}
    0 &\le F(x) - F(x_1) + \frac{\eta_1}{2}(\Vert x - x_1\Vert^2 - \Vert x_1 - x_0\Vert^2) 
    - \frac{\eta_1}{2}\Vert x - x_1\Vert^2
    \\
    \iff 
    0 &\le 
    \eta_1^{-1}(F(x) - F(x_1)) + \frac{1}{2}(\Vert x - x_1\Vert^2 - \Vert x_1 - x_0\Vert^2) 
    - \frac{1}{2}\Vert x - x_1\Vert^2
    \\
    &= 
    \left(
        - \eta_1^{-1}(F(x_1) - F(x)) - \frac{1}{2}\Vert x - x_1\Vert^2
    \right)
    + 
    \frac{1}{2}(\Vert x - x_1\Vert^2 - \Vert x_1 - x_0\Vert^2) . 
\end{aligned}
$$

$\blacksquare$

**Remarks**

For a proof with a different mood but with the same vibe, visit [Convex Proximal Point Method, Part III](Convex%20Proximal%20Point%20Method,%20Part%20III.md). 
This proof here is strictly better because it's constructed on Lemma 1, which only used that $M_F^{1/\eta}(\cdot;y)$ has quadratic growth. 
This condition is strictly weaker than $F$ being convex. 
By relaxing some of the parameters, a similar convergence result can be claimed for functions that are not necessarily convex. 

The convergence we showed here is sufficient but not necessary. 
The sequence $(\eta_t)_{t \ge 1}$ is the quadratic growth constant for the model function $\mathcal M_F$. 

---
### **In connection between PPM and smooth gradient descent**

This section clarifies the fact that, smooth gradient descent's convergence proof is not different to the convergence proof of the proximal point method. 
Only a few more steps and it can prove the convergence of smooth gradient descent. 
See [L-Smoothness as an Implication of Globally Lipschitz Gradient Under Convexity](../Properties%20of%20Functions/Convex%20Function%20with%20Global%20Lipschitz%20Gradient.md) for comprehensive introduction of convex function with global Lipschitz smooth gradient. 


#### **Algorithm | Smooth gradient descent**
> Let $F: \R^n \rightarrow \overline{\mathbb{R}}$ be that is $L$ differentiable. 
> Then the gradient gradient descent algorithm generates iterates $(x_t)_{t \ge 0}$ by the procedure: 
> $$
> \begin{aligned}
>     x_{t + 1} = \argmin{x \in \R^n} \left\lbrace
>         \langle \nabla f(x_t)\rangle
>         + \frac{\eta_{t + 1}}{2}\Vert x - x_t\Vert^2
>     \right\rbrace
>     &= x_t - \eta_{t + 1}^{-1} \nabla F(x_t). 
> \end{aligned}
> $$

#### **Lemma 2.0 | Smooth gradient descent inequality**
> Let the function $F: \R^n \rightarrow \R$ be a $L$ smooth function. 
> Suppose that the iterates $(x_t)_{t \ge 0}$ were generated by the smooth gradient descent algorithm. 
> Then, the iterates satisfy for all $x \in \R^n$ and $t \ge 0$ the inequality: 
> $$
> \begin{aligned}
>     0 &\le 
>     F(x) - F(x_{t + 1}) + \frac{\eta_{t + 1}}{2}\left(
>         \Vert x - x_t\Vert^2 - \Vert x - x_{t + 1}\Vert^2
>     \right)
>     + 
>     \frac{L - \eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2. 
> \end{aligned}
> $$
> In addition, when $x = x_t$ it yields inequality: 
> $$
> \begin{aligned}
>     F(x_{t + 1}) - F(x_t) \le (L/2 - \eta_{t + 1})\Vert x_t - x_{t + 1}\Vert^2. 
> \end{aligned}
> $$

**Proof**. 

Gradient descent is an approximation of PPM method. 
Define the linear under approximation function: 
$$
\begin{aligned}
    \widetilde F(x; y) := 
    F(y) + \langle \nabla f(y), x - y\rangle.
\end{aligned}
$$

It follows that for all $x, y\in \R^n$: 

$$
\begin{aligned}
    \widetilde F(x; y) + \frac{L}{2}\Vert x - y\Vert^2
    \ge 
    F(x) \ge \widetilde F(x; y). 
\end{aligned}
$$

This inequality will assist with deriving the results for **(a)**. 
Starting with the quadratic growth conditions on the minimizer of the model function it has for all $t \in \Z_+, \forall x \in \R^n$: 

$$
\begin{aligned}
    0 &\le 
    \mathcal M_{\widetilde F}^{1/\eta_{t + 1}}(x; x_t)
    - \mathcal M_{\widetilde F}^{1/\eta_{t + 1}}(x_{t + 1}; x_t)
    - \frac{\eta_{t + 1}}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &=
    \widetilde{F}(x; x_t) + \frac{\eta_{t + 1}}{2}\Vert x - x_t\Vert^2
    - \widetilde{F}(x_{t + 1}, x_t) 
    - \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    - \frac{\eta_{t + 1}}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &\le 
    F(x) + \frac{\eta_{t + 1}}{2}\Vert x - x_t\Vert^2
    - \widetilde{F}(x_{t + 1}, x_t) 
    - \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    - \frac{\eta_{t + 1}}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &=
    F(x) + \frac{\eta_{t + 1}}{2}\Vert x - x_t\Vert^2
    - \left(
        \widetilde{F}(x_{t + 1}, x_t) 
        + \frac{L}{2}\Vert x_{t + 1} - x_t\Vert^2
    \right)
    + \frac{L - \eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    - \frac{\eta_{t + 1}}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &\le 
    F(x) - F(x_{t + 1})
    + \frac{L - \eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    + \frac{\eta_{t + 1}}{2}
    \left(
        \Vert x - x_t\Vert^2 -
        \Vert x - x_{t + 1}\Vert^2
    \right). 
\end{aligned}
$$

$\blacksquare$


#### **Theorem 2.1 | Smooth gradient descent Lyapunov quantity**
> Let $F:\R^n \rightarrow \overline \R$ be a convex function that is $L$ Lipschitz smooth. 
> Suppose that PPM is being approximated iteratively by: 
> $$
> \begin{aligned}
>     x_{k + 1} = \argmin{x \in \R^n}\left\lbrace
>         \langle \nabla F, x\rangle + \frac{\eta_{t + 1}}{2}\Vert x - x_t\Vert^2
>     \right\rbrace = x_t - \eta_{t + 1}^{-1}\nabla f(x_t)
> \end{aligned}
> $$
> using $(\eta_t)_{t \ge 1}$. 
> For all $x \in \R^n$, and for all $t \ge 1$ define: 
> $$
> \begin{aligned}
>     \sigma_t &= \sum_{i = 1}^{t}\eta_i^{-1}, 
>     \Phi_t = \sigma_t \left(
>         F(x_t) - F(x)
>     \right) + \frac{1}{2}\Vert x_t - x\Vert^2. 
> \end{aligned}
> $$
> Then, the sequence $\Phi_t$ satisfies for all $t \ge 1$
> $$
> \begin{aligned}
>     0 &\le 
>     \Phi_t - \Phi_{t + 1}
>     + 
>     \left(
>         \sigma_t \left(\frac{L}{2} - \eta_{t + 1}\right) + 
>         \frac{1}{2}\left(
>             \frac{L}{\eta_{t + 1}} - 1
>         \right)
>     \right)\Vert x_{t + 1} - x_t\Vert^2. 
> \end{aligned}
> $$

**Proof**

The proof contains the following key intermediate steps which will be proved at the end. 

**(a)**: The function $F$ is convex and $L$ Lipschitz smooth hence we can invokve lemma 2.0 and it has for all $\R^n$, $t \ge 1$: 

$$
\begin{aligned}
    \eta_{t + 1}^{-1} (F(x_{t + 1}) - F(x)) &= 
    (\sigma_{t + 1} - \sigma_{t})(F(x_{t + 1}) - F(x))
    \\
    &= \sigma_{t + 1}(F(x_{t + 1}) - F(x)) - \sigma_t(F(x_{t + 1}) - F(x_t)) - \sigma_t(F(x_t) - F(x))
    \\
    &\ge 
    \sigma_{t + 1}(F(x_{t + 1}) - F(x)) - (L/2 - \eta_{t + 1})\Vert x_t - x_{t + 1}\Vert^2 - \sigma_t(F(x_t) - F(x)). 
\end{aligned}
$$

The overall proof starts with Lemma 2.0, which has for all $t \ge 1, x \in \R^n$: 

$$
\begin{aligned}
    0 &\le 
    \eta_{t + 1}^{-1}(F(x) - F(x_{t + 1})) + \frac{1}{2}\left(
        \Vert x - x_t\Vert^2 - \Vert x - x_{t + 1}\Vert^2
    \right) + \frac{L\eta_{t + 1}^{-1} - 1}{2}\Vert x_{t + 1} - x_t\Vert^2
    \\
    &\underset{\text{(a)}}{\le} 
    - \sigma_{t + 1}(F(x_{t + 1}) - F(x)) + 
    \sigma_t\left(\frac{L}{2} - \eta_{t + 1}\right) \Vert x_t - x_{t + 1}\Vert^2
    + \sigma_t(F(x_t) - F(x))
    \\
        & \quad 
        + \frac{1}{2}(\Vert x - x_t\Vert^2 - \Vert x - x_{t + 1}\Vert^2) + 
        \frac{1}{2}
        \left(\frac{L}{\eta_{t + 1}} - 1\right)
        \Vert x_{t +1} - x_t\Vert^2
    \\
    & = 
    \sigma_t \left(
        F(x_t) - F(x)
    \right) + \frac{1}{2}\Vert x - x_t\Vert^2
    - 
    \left(
        \sigma_{t + 1}
        \left(
            F(x_{t + 1}) - F(x)
        \right) + \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
    \right)
    \\
        &\quad 
        + \left(
            \sigma_t\left(\frac{L}{2} - \eta_{t + 1}\right)
            + 
            \frac{1}{2}
            \left(\frac{L}{\eta_{t + 1}} - 1\right)
        \right)\Vert x_{t + 1} - x_t\Vert^2. 
    \\
    &= 
    \Phi_t - \Phi_{t + 1}
    + \left(
        \sigma_t\left(\frac{L}{2} - \eta_{t + 1}\right)
        + 
        \frac{1}{2}
        \left(\frac{L}{\eta_{t + 1}} - 1\right)
    \right)\Vert x_{t + 1} - x_t\Vert^2. 
\end{aligned}
$$
 
$\blacksquare$

**Remarks**

The same Lyapunov function were use in the convergence of the same smooth gradient descent algorithm in [Smooth Gradient Descend The Basics](../Classics%20Algorithms/Smooth%20Gradient%20Descend%20The%20Basics.md). 


#### **Theorem 2.2 | Rate of convergence and step sizes for smooth gradient**
> Let $F: \R^n \rightarrow \overline \R$ be convex and $L$ Lipschitz smooth. 
> Then the smooth gradient descent scheme defined by $x_{t + 1} = x_t - \eta_{t + 1}^{-1}\nabla F(x_t)$ given initial $x_0$ and step size sequence $(\eta_t)_{t \ge 1}$. 
> Define $\sigma_t = \sum_{i = 1}^{t}\eta_i^{-1}$. 
> If the sequence $(\eta_{t})_{t \ge 1}$ satisfies any of the following schedules: 
> 1. $\eta_t \ge L$ for all $t \in \Z_+$. 
> 2. $\eta_1 \ge L$ and it has $\eta_{t + 1}^{-1}\le (L/2 - 2/\sigma_t)^{-1}$ for all $t \in \N$. 
> 
> Then, the method has for all $x \in \R^n$ the convergence rate 
> $$
> \begin{aligned}
>     \left(\sum_{i = 1}^{t + 1}\eta_i^{-1}\right)
>     \left(
>         F(x_{t + 1}) -  F(x) 
>     \right)
>         + \frac{1}{2}\Vert x_{t + 1} - x\Vert^2
>     &\le 
>     \frac{1}{2}\Vert x - x_0\Vert^2 + \frac{1}{2}\left(
>         \frac{L}{\eta_1} - 1
>     \right)\Vert x_1 - x_0\Vert^2. 
> \end{aligned}
> $$

**Proof**

Because the function is differentiable and $L$ Lipschitz smooth, from Lemma 2.0, we set $t = 0$ and it gives the inequality: 

$$
\begin{aligned}
    0 &\le 
    F(x) - F(x_{1}) + \frac{\eta_{1}}{2}\left(
        \Vert x - x_0\Vert^2 - \Vert x - x_{1}\Vert^2
    \right)
    + 
    \frac{L - \eta_{1}}{2}\Vert x_{1} - x_0\Vert^2. 
\end{aligned}\tag{a}
$$

If there exists a sequence $(\eta_t)_{t \ge 1}$ which makes: 

$$
\begin{aligned}
    \left(
    \sigma_t \left(\frac{L}{2} - \eta_{t + 1}\right) + 
    \frac{1}{2}\left(
        \frac{L}{\eta_{t + 1}} - 1
    \right)
    \right) &\le 0
\end{aligned}
$$

Then, it implies for all $t \ge 1$: 

$$
\begin{aligned}
    0 &\le 
    \Phi_t - \Phi_{t + 1}
    + 
    \left(
        \sigma_t \left(\frac{L}{2} - \eta_{t + 1}\right) 
    \right) + 
    \frac{1}{2}\left(
        \frac{L}{\eta_{t + 1}} - 1
    \right)
    \Vert x_{t + 1} - x_t\Vert^2
    \\
    &\le 
    \Phi_t - \Phi_{t + 1}. 
\end{aligned}
$$

Since this is true for all $t \ge 1$, this gives: $\Phi_{t + 1} \le \Phi_1$ unrolling recursively it has: 

$$
\begin{aligned}
    \Phi_{t + 1} &= \sigma_{t + 1}\left(
        F(x_{t + 1}) -  F(x) 
    \right)
     + \frac{1}{2}\Vert x_{t + 1} - x\Vert^2
    \\
    &\le 
    \eta_1^{-1}(F(x_1) - F(x))
     + \frac{1}{2}\Vert x_1 - x\Vert^2
    \\
    &\underset{\text{(a)}}{\le}
    \eta_1^{-1}\left(
        \frac{\eta_{1}}{2}\left(
        \Vert x - x_0\Vert^2 - \Vert x - x_{1}\Vert^2
        \right)
        + 
        \frac{L - \eta_{1}}{2}\Vert x_{1} - x_0\Vert^2
    \right)
    + 
    \frac{1}{2}\Vert x_1 - x\Vert^2
    \\
    &= 
    \eta_1^{-1}\left(
        \frac{\eta_{1}}{2} \Vert x - x_0\Vert^2 
        + 
        \frac{L - \eta_{1}}{2}\Vert x_{1} - x_0\Vert^2
    \right)
    \\
    &= 
    \frac{1}{2}\Vert x - x_0\Vert^2 + \frac{1}{2}\left(
        \frac{L}{\eta_1} - 1
    \right)\Vert x_1 - x_0\Vert^2. 
\end{aligned}
$$

Solving the inequality will yield the desired constraints on $(\eta_t)_{t \ge 1}$. 

**To see (1.)**, if $\eta_t \ge L$, then for all $t \ge \Z_+$, it has $L/2 - \eta_{t + 1} \le 0$ and $L/\eta_{t + 1}\le 1$ hence $L/\eta_{t + 1} - 1 \le 0$. 
Therefore, the quantity over all coefficient is $\le 0$. 

**To see (2.)**, we solve the inequality for $\eta_{t + 1}$ for all $t \in \N$.
Since $\sigma_0 = 0$ from the setting of Theorem 2.1, it has for $t = 0$ $(1/2)(L/\eta_{t + 1} - 1) \le 0$ which yields: $\eta_{t + 1} \ge L$. 
Otherwise, we consider when $t \ge 1$. 
From there we solve the inequality. 
Start by considering: 

$$
\begin{aligned}
    \left(
        \sigma_t \left(\frac{L}{2} - \eta_{t + 1}\right) + 
        \frac{1}{2}\left(
            \frac{L}{\eta_{t + 1}} - 1
        \right)
    \right) &\le 0
    \\
    \iff 
    \frac{L}{2} - \eta_{t + 1} + 
    \frac{L}{2\eta_{t + 1}\sigma_t} - \sigma_t^{-1} &\le 0
    \\
    \iff 
    \frac{L\eta_{t + 1}}{2} - \eta_{t + 1}^2 + 
    \frac{L}{2\sigma_t} - \eta_{t + 1}\sigma_t^{-1} &\le 0
    \\
    (L/2 - \sigma_t^{-1})\eta_{t + 1} - \eta_{t + 1}^2 + (1/2)\sigma_t^{-1}
    &\le 0. 
\end{aligned}
$$

Solving it yields solutions to the roots of the quadratic. 
We are interested in the non-negative solution for gradient descent which gives:

$$
\begin{aligned}
    |\eta_{t + 1}| = 
    \frac{1}{2}\left|
        \frac{L}{2} - \sigma_t^{-1} \pm 
        \sqrt{
            \left(
            \frac{L}{2} - \sigma_t^{-1}
            \right)^2 + 2 \sigma_t
        }
    \right|
    &\ge 
    \frac{1}{2}\left(
        \frac{L}{2} - \sigma_t^{-1} + 
        \frac{L}{2} - \sigma_t^{-1}
    \right) = \frac{L}{2} - \frac{2}{\sigma_t}. 
\end{aligned}
$$

Observe that it's always the case where it has a positive, and negative solution. 
Since the stepsize is always positive, we want the positive root of the quadratic inequality, and a lower bound would be a sufficient condition to assert the convergence claim for the Lypunov. 



---
### **PPM in the Strongly Convex Settings**

Strong convexity strengthen the convergence results of PPM. 
It leads to faster convergence rate for a same schedule of the step size sequences $(\eta_{t})_{t \ge 1}$. 
We will prove a new convergence rate by including the strong convexity constant in this section. 
The proof will differ from the general convex case. 

#### **Theorem | PPM Convergence under strong convexity**
> Suppose $F: \R^n \rightarrow \overline \R$ is $\mu > 0$ convex. 
> Let $(x_k)_{k \ge0}$ be a sequence generated by PPM method applied on $F$. 
> Then for $x^* \in \arg\min_{x\in \R^n}\;F(x) \neq \emptyset$ it has $F(x_k) - F(x^*) \le \mathcal O\left(\prod_{i = 1}^k(1 + \mu\eta_{t + 1})^{-1}\right)$. 

**Proof**

Observe that from strong convexity, $F$ always has a unique minimizer $x^*$. 
Define $F^+ = F(x^*)$ to be the minimizer. 
Because $F$ is strongly convex it has quadratic Error Bound condition which gives the inequality 

$$
\begin{aligned}
    \frac{1}{2\mu}\Vert x_{k + 1} - x_k\Vert^2 
    &\ge 
    \dist(\partial F(x_{k + 1}), \mathbf 0)^2
    \ge 
    F(x_{k + 1}) - F^+. 
\end{aligned}
$$

$F$ is convex, therefore $\mathcal M_F^{1/\eta_{t + 1}}(\cdot;y)$ is $\eta_{t + 1} + \mu$ strongly convex for all $t \in \Z_0$. 
It has quadratic growth condition over the minimizer $x_{t + 1}$ admiting inequality from Lemma 1: 

$$
\begin{aligned}
    0 &\le 
    F(x) - F(x_{t + 1}) 
+ 
\frac{\eta_{t + 1}}{2}\left(
    \Vert x - x_k\Vert^2 - \Vert x_{k + 1} - x_k\Vert^2
    - \Vert x - x_{k + 1}\Vert^2
    \right)
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2.
\end{aligned}
$$

Set $x = x_t$ it has for all $t \in \Z_+$: 
$$
\begin{aligned}
    0&\le F(x_t) - F(x_{t + 1}) + 
    (-\eta_{t + 1} - \mu/2)\Vert x_t - x_{t + 1}\Vert^2
    \\
    \iff 
    F(x_t) - F(x_{t + 1}) 
    &\ge (\eta_{t + 1} + \mu/2)\Vert x_t - x_{t + 1}\Vert^2 
    \\
    &\ge 
    (\eta_{t + 1} + \mu/2)(2\mu)
    (F(x_{t + 1}) - F^+)
    \\
    \iff 
    F(x_t) - F^+ + F^+ - F(x_{t + 1}) 
    &\ge
    (\eta_{t + 1} + \mu/2)(2\mu)
    (F(x_{t + 1}) - F^+)
    \\
    \iff F(x_t) - F^+
    &\ge 
    (1 + (\eta_{t + 1} + \mu/2)(2\mu))(F(x_{t + 1}) - F^+)
    \\
    \iff 
    (1 + (2\mu\eta_{t + 1} + \mu^2))^{-1}(F(x_t) - F^+)
    &\ge F(x_{t + 1}) - F^+. 
\end{aligned}
$$

Unrolling the recurrence it yields 

$$
\begin{aligned}
    F(x_{t + 1}) - F^+ 
    &\le 
    \left(
        \prod_{i = 1}^{t + 1}
        \left(
            1 + (2\mu \eta_{i} + \mu^2)
        \right)
    \right)^{-1}(F(x_0) - F^+). 
\end{aligned}
$$

Easy to see that $2\mu\eta_i + \mu^2 \ge \mu \eta_i$, therefore the inverted big product can be upper bounded by $\left(\prod_{i = 1}^{t} (1 + \eta_i \mu)\right)^{-1}$, which is more simplified. 

---
### **Convergence under inexact proximal oracles**

The key here is the characterize the error and restate the Proximal Inequality. 



---
### **Rockafellar's PPM Analysis**

Read [Convex Proximal Point Method, Part II](Convex%20Proximal%20Point%20Method,%20Part%20II.md) for more. 
