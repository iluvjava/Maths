- [Smooth Gradient Descend The Basics](Smooth%20Gradient%20Descend%20The%20Basics.md)
- [Morau Envelope and Proximal Operators](Moreau%20Envelope%20and%20Convex%20Proximal%20Mapping.md)

---
### **Intro**

Let the ambient space be $\R^n$ and let $f: \R^n \rightarrow \overline {\R}$
Our attention is on the method of proximal point. 
Let $x_0 \in \text{dom} f$, given a strictly positive sequence $(\eta_t)_{t \ge 1}$, consider a sequence $(x_t)_{t \ge0}$ generated by 

$$
\begin{aligned}
    x_{t + 1} = \prox_{\eta_{t + 1} f}(x_t):= 
    \argmin{x\in \R^n}
    \left\lbrace
        f(x) + \frac{1}{\eta_{t + 1}}\Vert x - x_t\Vert^2
    \right\rbrace. 
\end{aligned}
$$

For notational convenience, for all $y \in \R^n$ we define model function 

$$
\begin{aligned}
    (\forall x \in \R^n)\quad 
    \mathcal M_f^{1/\eta}(x; y) := f(x) + \frac{\eta}{2}\Vert x - y\Vert^2. 
\end{aligned}
$$

The following lemma gives the results of proximal inequality which is relevant to proving the convergence of PPM method. 

#### **Lemma 1 | Proximal inequality**
> Let $y \in \R^n$. 
> Suppose that the PPM generates sequence $(x_t)_{ t \ge 0}$ on $F$. 
> If $F: \R^n \rightarrow \overline \R$ is $\mu \ge 0$ strongly convex, 
> then for all $x \in \R^n$ it has 
> $$
> \begin{aligned}
>     0 &\le 
>     F(x) - F(x_{t + 1}) 
>     + 
>     \frac{\eta_{t + 1}}{2}\left(
>         \Vert x - x_k\Vert^2 - \Vert x_{k + 1} - x_k\Vert^2
>         - \Vert x - x_{k + 1}\Vert^2
>     \right)
>     - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
>     \\
>     &= 
>     F(x) - F(x_{t + 1}) 
>     + 
>     \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle
>     - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
>     \\
>     &\le 
>     F(x) - F(x_{t + 1}) 
>     + 
>     \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle. 
> \end{aligned}
> $$

**Proof**

Consider the $\mu + \eta_{t + 1}$ strong convexity property of $\mathcal M^{\eta_{t + 1}}_F(\cdot; y)$ and the fact that $x_{t + 1}$ is the minimizer of $\mathcal M_F^{\eta_{t + 1}}(\cdot; y)$ which gives the quadratic growth condition: 

$$
\begin{aligned}
    0 &\le 
    \mathcal M(x; x_t) - \mathcal M(x_{t + 1}, x_t) 
    - \frac{\eta_{t + 1} + \mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= F(x) - F(x_t) + \frac{\eta_{t + 1}}{2}\left(
        \Vert x - x_t\Vert^2 - \Vert x_{t + 1} - x_t \Vert^2
        - \Vert x - x_{t + 1}\Vert^2
    \right)
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= F(x) - F(x_t) + \frac{\eta_{t + 1}}{2}\left(
        \Vert x - x_{t + 1} + x_{t + 1} - x_t\Vert^2 - \Vert x_{t + 1} - x_t \Vert^2
        - \Vert x - x_{t + 1}\Vert^2
    \right) 
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= 
    F(x) - F(x_t) 
    + \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &\le 
    F(x) - F(x_t) 
    + \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle. 
\end{aligned}
$$


#### **Lemma 2 | Convergence rate of a sequence**
> Let $(\sigma_t)_{t \ge 0}$, and $(\Delta_t)_{t \ge 0}$ be two sequences with the following list of conditions: 
> - (a): $\sigma_t > 0\; \forall t \ge 0$, the sequence $\sigma_t$ is strictly positive. 
> - (b): $\Delta_{t + 1} - \Delta_t \le 0$, i.e: $\Delta_t$ monotonically decreases. 
> - (c): $(\sigma_{t + 1} - \sigma_t)\Delta_t \le 0$, which we call the descent lemma. 
> 
> Then the following scenarios are true. 
> 1. For all $t \ge 0$, $(\sigma_{t + 1} - \sigma_t)\Delta_t = \sigma_{t + 1} \Delta_{t + 1} - \sigma_t \Delta_t - \sigma_{t + 1}(\Delta_{t + 1} - \Delta_t)$. 
> 2. If (a), (b) are true. Then it has $\sigma_{t + 1}\Delta_{t + 1} - \sigma_t \Delta_t \le (\sigma_{t + 1} - \sigma_t)\Delta_t$. 
> 3. If (a), (b), (c) are true, then it has: $\sigma_{t + 1}\Delta_{t + 1} - \sigma_t \Delta_{t} \le 0$. 

**Proof**

The proof of (1.), (2.) is direct. 
Consider 

$$
\begin{aligned}
    \sigma_{t + 1}\Delta_{t + 1} - \sigma_t \Delta_t
    &= 
    \sigma_{t + 1}(\Delta_{t + 1} - \Delta_t) - \sigma_t \Delta_t + \sigma_{t + 1} \Delta_t
    \\
    &= \sigma_{t + 1}(\Delta_{t + 1} - \Delta_t) 
    + (\sigma_{t + 1} - \sigma_t) \Delta_t
    \\
    \iff 
    (\sigma_{t + 1} - \sigma_t)\Delta_t 
    &= 
    \sigma_{t + 1}\Delta_{t + 1} - \sigma_t \Delta_t - \sigma_{t + 1}(\Delta_{t + 1} - \Delta_t)
    \\
    & \ge  \sigma_{t + 1}\Delta_{t + 1} - \sigma_t \Delta_t. 
\end{aligned}
$$

To prove (c), because (c) is true, then it bounds $\sigma_{t + 1}\Delta_{t + 1} - \sigma_t \Delta_t \le 0$. 




---
### **Convergence in the convex case**

We show the convergence of PPM in the convex case. 
Let $\sigma_t$ be a monotone increasing sequence. 
The following idea is important to showing the convergence of PPM method in the convex settings. 
It shows the way of proving convergence using the method of Lyapunov function. 
Consider the following: 

$$
\begin{aligned}
    0 
    &\ge 
    \sigma_{t + 1}(\Delta_{t + 1} - \Delta_{t})
    + (\sigma_{t + 1} - \sigma_t)\Delta_t
    \\
    &= 
    \sigma_{t + 1}\Delta_{t + 1} - \sigma_{t + 1}\Delta_t
    + \sigma_{t + 1}\Delta_t - \sigma_t \Delta_t
    \\
    &=
    \sigma_{t + 1}\Delta_{t + 1}  - \sigma_t \Delta_t. 
\end{aligned}
$$


Performing a telescoping sum, it shows that $\Delta_t \le \mathcal O(\sigma_{t + 1}^{-1})$. 
Following a similar idea, we are going to prove the convergence rate of proximal point method applied to a convex function: $f: \R^n \rightarrow \overline \R$. 


#### **Theorem 1.1 | A Lyapunov quantity for PPM under convexity**
> Suppose that $F:\R^n \rightarrow \overline \R$ is a convex function. 
> Let $(x_t)_{t \ge 0}$ be a sequence generated by PPM with $F$, and relaxation sequence $(\eta_t)_{t \ge 1}$. 
> For all $x \in \R^n$, $t \ge 0$ define: 
> $$
> \begin{aligned}
>     \Phi_{t + 1} := \left(
>         \sum_{i = 1}^{t + 1}\eta_{i}^{-1}
>     \right)\left(
>         F(x_{t + 1}) - F(x) + 
>         \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
>     \right). 
> \end{aligned}
> $$
> Define $\Phi_0 = F(x_0) - F(x) + (1/2)\Vert x_0 - x\Vert^2$. 
> Then $\Phi_t$ decreases monotonically, i.e: for all $t \ge 1$ it has $\Phi_{t + 1} - \Phi_t \le 0$. 

**Proof**

The proof admits the following key intermediate steps: 

1. (**Step I**): $F(x_t)$ is a monotone decreasing sequence. 

2. (**Step II:**): 
For any positive sequence $(\sigma_t)_{t \ge 0}$, the following inequality is true for all $x \in \R^n$ and $t \ge 0$: 
$$
\begin{aligned}
    (\sigma_{t + 1} - \sigma_t)(F(x_{t + 1}) - F(x)) 
    &= 
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    - 
    \sigma_t(F(x_{t + 1}) - F(x_t))
    - \sigma_t(F(x_t) - F(x))
    \\
    &\ge 
    \sigma_{t + 1} (F(x_{t + 1}) - F(x))
    - \sigma_t(F(x_t) - F(x)). 
\end{aligned}
$$


We will go back to show **(Step I), (Step II)**. 
The monotone property of $\Phi_t$ is now achievable from these intermediate results. 
Define $\sigma_{t + 1} = \sigma_t + \eta_{t}^{-1}$ for all $t \ge 1$ and $\sigma_0 = 0$. 
Then the sequence $(\eta_t)_{t \ge 1}$ is strictly positive which makes $\sigma_t$ a monotone increasing strictly positive sequence because it has $\sigma_t = \sum_{i = 1}^{t} \eta_i^{-1} > 0$. 

At this point, we are ready to show the results. 
For all $x \in \R^n, t \in \Z_+$, it has from the proximal inequality: 
$$
{\small
\begin{aligned}
    0 &\ge 
    F(x_{t + 1}) - F(x) + \frac{\eta_{t + 1}}{2}
    \left(
        \Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2
    \right)
    +
    \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    \\
    \iff 
    0 &\ge 
    \eta_{t + 1}^{-1}(F(x_{t + 1}) - F(x))
    +
    (1/2)(\Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2)
    + 
    (1/2)\Vert x_{t + 1} - x_t\Vert^2
    \\
    &= 
    (\sigma_{t + 1} - \sigma_t)(F(x_{t + 1}) - F(x))
    +
    (1/2)(\Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2)
    + 
    (1/2)\Vert x_{t + 1} - x_t\Vert^2
    \\
    &\ge
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    - \sigma_t(F(x_t) - F(x))
    +
    (1/2)(\Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2)
    + 
    (1/2)\Vert x_{t + 1} - x_t\Vert^2
    \\
    &\ge
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    - \sigma_t(F(x_t) - F(x))
    +
    (1/2)(\Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2)
    \\
    &= \sigma_{t + 1}\left(
        F(x_{t + 1}) - F(x) + \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
    \right) - 
    \sigma_{t}\left(
        F(x_{t}) - F(x) + \frac{1}{2}\Vert x - x_{t}\Vert^2
    \right). 
    \\
    &= \Phi_{t + 1} - \Phi_t. 
\end{aligned}
}
$$


It remains to show **(Step I), (Step II)**. 

**Proof of (Step I)**. 
Setting $x = x_{t}$ in the inequality from **(Step I)** it yields: 

$$
\begin{aligned}
    0 &\ge 
    F(x_{t + 1}) - F(x_{t}) + \frac{\eta_{t + 1}}{2}\Vert x_{t} - x_{t + 1}\Vert^2 
    + \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    \\
    &= F(x_{t + 1}) - F(x_t). 
\end{aligned}
$$


**Proof of (Step II)**. 
Let $\sigma_{t + 1}$ be any number, it follows that: 

$$
{\small
\begin{aligned}
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    &= 
    (\sigma_{t + 1} - \sigma_t) (F(x_{t + 1}) - F(x)) 
    + \sigma_t(F(x_{t + 1}) - F(x))
    \\
    &= 
    (\sigma_{t + 1} - \sigma_t) (F(x_{t + 1}) - F(x)) 
    + \sigma_t(F(x_{t + 1}) - F(x_t) + F(x_t) - F(x))
    \\
    &= 
    (\sigma_{t + 1} - \sigma_t) (F(x_{t + 1}) - F(x)) 
    + \sigma_t(F(x_{t + 1}) - F(x_t)) 
    + \sigma_t(F(x_t) - F(x))
    \\
    \iff 
    (\sigma_{t + 1} - \sigma_t)(F(x_{t + 1}) - F(x)) &= 
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    - 
    \sigma_t(F(x_{t + 1}) - F(x_t))
     - \sigma_t(F(x_t) - F(x)). 
\end{aligned}
}
$$

#### **Theorem 1.2 | PPM convergence rate**
> Let $(x_t)_{t \ge 0}$ be generated by the PPM on function $F: \R^n \rightarrow \overline \R$ which is convex. 
> Assuming that a minimizer $x^+$ exists for $F$ and the minimum is $F^+$. 
> Then it has the following convergence rate: 
> $$
> \begin{aligned}
>     F(x_{N + 1}) - F^+ +
>     \frac{1}{2}\Vert x^+ - x_{N + 1}\Vert^2
>     &\le 
>     \left(
>         \sum_{i = 0}^{N}
>         \eta_{i + 1}^{-1}
>     \right)^{-1}
>     \left(
>         F(x_0) - F^+ + \frac{1}{2}\Vert x^+ - x_0\Vert^2
>     \right). 
> \end{aligned}
> $$

**Proof**

From theorem 1.1 it has for all $t \in \Z^+: \Phi_{t + 1} - \Phi_t \le 0$. 
Hence, telescoping it for all $t = 1, \ldots, N$ it has 

$$
\begin{aligned}
    0 &\ge 
    \sum_{i = 0}^{N}
    \Phi_{t + 1} - \Phi_t
    = \Phi_{N + 1} - \Phi_0
    \\
    &= 
    \left(
        \sum_{i = 1}^{N + 1}\eta_{i}^{-1}
    \right)
    \left(
        F(x_{N + 1}) - F^+ +
        \frac{1}{2}\Vert x^+ - x_{N + 1}\Vert^2
    \right) - 
    \left(
        F(x_0) - F(x^+) + \frac{1}{2}\Vert x_0 - x^+\Vert^2
    \right). 
\end{aligned}
$$

Re-arranging the above inequality yields the convergence results of the PPM in the convex settings. 


**Remarks**

For a proof with a different mood but with the same vibe, visit [Convex Proximal Point Method, Part III](Convex%20Proximal%20Point%20Method,%20Part%20III.md). 
This proof here is strictly better because it's constructed on Lemma 1, which only used that $M_F^{1/\eta}(\cdot;y)$ has quadratic growth. 
This condition is strictly weaker than $F$ being convex. 
By relaxing some of the parameters, a similar convergence result can be claimed for functions that are not necessarily convex. 

The convergence we showed here is sufficient but not necessary. 
The sequence $(\eta_t)_{t \ge 1}$ is the quadratic growth constant for the model function $\mathcal M_F$. 
In the spacial case of convexity, it coincided with the regularization parameters for PPM. 

---
### **In connection between PPM and smooth gradient descent**

This section clarifies the fact that, smooth gradient descent's convergence proof is not different to the convergence proof of the proximal point method. 
Only a few more steps and it can prove the convergence of smooth gradient descent. 
See [L-Smoothness as an Implication of Globally Lipschitz Gradient Under Convexity](../Global%20Lipschitz%20Gradient,%20Strong%20Smoothness,%20Equivalence%20and%20Implications.md) for comprehensive introduction of convex function with global Lipschitz smooth gradient. 

#### **Theorem 2.1 | Smooth gradient descent Lyapunov quantity**
> Let $F:\R^n \rightarrow \overline \R$ be a convex function that is $L$ Lipschitz smooth. 
> Suppose that PPM is being approximated iteratively by: 
> $$
> \begin{aligned}
>     x_{k + 1} = \argmin{x \in \R^n}\left\lbrace
>         \langle \nabla F, x\rangle + \frac{\eta_{t + 1}}{2}\Vert x - x_t\Vert^2
>     \right\rbrace = x_t - \eta_{t + 1}^{-1}\nabla f(x_t)
> \end{aligned}
> $$
> using $(\eta_t)_{t \ge 1}$. 
> For all $x \in \R^n$, define $\Phi_0 = F(x_0) - F(x) + (1/2)\Vert x_0 - x\Vert^2, \sigma_0 = 0$, and for all $t \in \Z_+$ define: 
> $$
> \begin{aligned}
>     \sigma_t &= \sum_{i = 1}^{t}\eta_i^{-1}, 
>     \\
>     \Phi_t &= \sigma_t \left(
>         F(x_t) - F(x) + \frac{1}{2}\Vert x_t - x\Vert^2
>     \right). 
> \end{aligned}
> $$
> Then, the sequence $\Phi_t$ satisfies for all $t \ge 0$
> $$
> \begin{aligned}
>     0 &\le 
>     \Phi_t - \Phi_{t + 1}
>     + 
>     \left(
>         \sigma_t \left(\frac{L}{2} - \eta_{t + 1}\right) + 
>         \frac{1}{2}\left(
>             \frac{L}{\eta_{t + 1}} - 1
>         \right)
>     \right)\Vert x_{t + 1} - x_t\Vert^2. 
> \end{aligned}
> $$

**Proof**

The proof contains the following key intermediate steps which will be proved at at the end. 
1. **Step I**: Using the proximal inequality and smoothness inequality, it can be shown that for all $x \in \R^n$: 

$$
\begin{aligned}
    0 &\le 
    F(x) - F(x_{t + 1}) + \frac{\eta_{t + 1}}{2}\left(
        \Vert x - x_t\Vert^2 - \Vert x - x_{t + 1}\Vert^2
    \right)
    + 
    \frac{L - \eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2. 
\end{aligned}
$$
2. **Step II**: Follows from the previous step we can derive 
$$
\begin{aligned}
    F(x_{t + 1}) - F(x_t) \le (L/2 - \eta_{t + 1})\Vert x_t - x_{t + 1}\Vert^2. 
\end{aligned}
$$ 

Now, using the results of (**Step I**), (**Step II**), we can derive stated inequality. 
For all $\R^n$, considers the following equality: 

$$
\begin{aligned}
    \eta_{t + 1}^{-1} (F(x_{t + 1}) - F(x)) &= 
    (\sigma_{t + 1} - \sigma_{t})(F(x_{t + 1}) - F(x))
    \\
    &= \sigma_{t + 1}(F(x_{t + 1}) - F(x)) - \sigma_t(F(x_{t + 1}) - F(x_t)) - \sigma_t(F(x_t) - F(x)). 
\end{aligned}
$$

Substituting the inequaity from **(Step I)**, it follows that: 

$$
\begin{aligned}
    0 &\le 
    - \sigma_{t + 1}(F(x_{t + 1}) - F(x)) + \sigma_t(F(x_{t + 1}) - F(x_t)) + \sigma_t(F(x_t) - F(x))
    \\
        & \quad 
        + \frac{1}{2}(\Vert x - x_t\Vert^2 - \Vert x - x_{t + 1}\Vert^2) + 
        \frac{1}{2}
        \left(\frac{L}{\eta_{t + 1}} - 1\right)
        \Vert x_{t +1} - x_t\Vert^2
    \\
    &\le 
    - \sigma_{t + 1}(F(x_{t + 1}) - F(x)) + 
    \sigma_t\left(\frac{L}{2} - \eta_{t + 1}\right) \Vert x_t - x_{t + 1}\Vert^2
    + \sigma_t(F(x_t) - F(x))
    \\
        & \quad 
        + \frac{1}{2}(\Vert x - x_t\Vert^2 - \Vert x - x_{t + 1}\Vert^2) + 
        \frac{1}{2}
        \left(\frac{L}{\eta_{t + 1}} - 1\right)
        \Vert x_{t +1} - x_t\Vert^2
    \\
    & = 
    \sigma_t \left(
        F(x_t) - F(x) + \frac{1}{2}\Vert x - x_t\Vert^2
    \right)
    - \sigma_{t + 1}
    \left(
        F(x_{t + 1}) - F(x) + \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
    \right)
    \\
        &\quad 
        + \left(
            \sigma_t\left(\frac{L}{2} - \eta_{t + 1}\right)
            + 
            \frac{1}{2}
            \left(\frac{L}{\eta_{t + 1}} - 1\right)
        \right)\Vert x_{t + 1} - x_t\Vert^2. 
\end{aligned}
$$

It remains to show **(Step I)**, **(Step II)**. 

**Proof of (Step I)**. 
Gradient descent is an approximation of PPM method. 
Define the linear under approximation function: 
$$
\begin{aligned}
    \widetilde F(x; y) := 
    F(y) + \langle \nabla f(y), x - y\rangle.
\end{aligned}
$$

It follows that for all $x, y\in \R^n$: 

$$
\begin{aligned}
    \widetilde F(x; y) + \frac{L}{2}\Vert x - y\Vert^2
    \ge 
    F(x) \ge \widetilde F(x; y). 
\end{aligned}
$$

This inequality will assist with deriving the results for **(Step I)**. 
Starting with the quadratic growth conditions on the minimizer of the model function it has for all $t \in \Z_+, \forall x \in \R^n$: 

$$
\begin{aligned}
    0 &\le 
    \mathcal M_{\widetilde F}^{1/\eta_{t + 1}}(x; x_t)
    - \mathcal M_{\widetilde F}^{1/\eta_{t + 1}}(x_{t + 1}; x_t)
    - \frac{\eta_{t + 1}}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &=
    \widetilde{F}(x; x_t) + \frac{\eta_{t + 1}}{2}\Vert x - x_t\Vert^2
    - \widetilde{F}(x_{t + 1}, x_t) 
    - \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    - \frac{\eta_{t + 1}}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &\le 
    F(x) + \frac{\eta_{t + 1}}{2}\Vert x - x_t\Vert^2
    - \widetilde{F}(x_{t + 1}, x_t) 
    - \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    - \frac{\eta_{t + 1}}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &=
    F(x) + \frac{\eta_{t + 1}}{2}\Vert x - x_t\Vert^2
    - \left(
        \widetilde{F}(x_{t + 1}, x_t) 
        + \frac{L}{2}\Vert x_{t + 1} - x_t\Vert^2
    \right)
    + \frac{L - \eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    - \frac{\eta_{t + 1}}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &\le 
    F(x) - F(x_{t + 1})
    + \frac{L - \eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    + \frac{\eta_{t + 1}}{2}
    \left(
        \Vert x - x_t\Vert^2 -
        \Vert x - x_{t + 1}\Vert^2
    \right). 
\end{aligned}
$$

**To show (Step II)**, set $x = x_k$ on the above inequality which will then produce the result. 

**Remarks**

The same Lyapunov function were use in the convergence of the same smooth gradient descent algorithm in [Smooth Gradient Descend The Basics](../Smooth%20Gradient%20Descend%20The%20Basics.md). 





#### **Theorem 2.2 | Rate of convergence and step sizes for smooth gradient**
> Let $F: \R^n \rightarrow \overline \R$ be convex and $L$ Lipschitz smooth. 
> Then the smooth gradient descent scheme defined by $x_{t + 1} = x_t - \eta_{t + 1}^{-1}\nabla F(x_t)$ given initial $x_0$ and step size sequence $(\eta_t)_{t \ge 1}$. 
> Define $\sigma_t = \sum_{i = 1}^{t}\eta_i^{-1}$. 
> If the sequence $(\eta_{t})_{t \ge 1}$ satisfies any of the following schedules: 
> 1. $\eta_t \ge L$ for all $t \in \Z_+$. 
> 2. $\eta_1 \ge L$ and it has $\eta_{t + 1}^{-1}\le (L/2 - 2/\sigma_t)^{-1}$ for all $t \in \N$. 
> Then, the method has for all $x \in \R^n$ the convergence rate 
> $$
> \begin{aligned}
>    F(x_{t + 1}) - F(x) + \frac{1}{2}\Vert x_{t + 1} - x\Vert^2 
>    &\le \left(
>     \sum_{i = 1}^{t} \eta_i^{-1}
>    \right)^{-1}
>    \left(
>         F(x_0) - F(x) + \frac{1}{2}\Vert x_t - x\Vert^2
>    \right). 
> \end{aligned}
> $$

**Proof**

The assumption of the theorem is identical to the settings of Theorem 2.1. 
To assert the convergence, it's sufficient to consider the sequence $(\eta_t)_{t \ge 1}$ which makes: 

$$
\begin{aligned}
    \left(
    \sigma_t \left(\frac{L}{2} - \eta_{t + 1}\right) + 
    \frac{1}{2}\left(
        \frac{L}{\eta_{t + 1}} - 1
    \right)
    \right) &\le 0
\end{aligned}
$$

which implies: 

$$
\begin{aligned}
    0 &\le 
    \Phi_t - \Phi_{t + 1}
    + 
    \left(
        \sigma_t \left(\frac{L}{2} - \eta_{t + 1}\right) + 
        \frac{1}{2}\left(
            \frac{L}{\eta_{t + 1}} - 1
        \right)
    \right)\Vert x_{t + 1} - x_t\Vert^2
    \\
    &\le 
    \Phi_t - \Phi_{t + 1}
    \\
    \iff 
    \Phi_{t + 1} - \Phi_t &\le 0. 
\end{aligned}
$$

Solving the inequality will yield the desired constraints on $(\eta_t)_{t \ge 1}$. 

**To see (1.)**, if $\eta_t \ge L$, then for all $t \ge \Z_+$, it has $L/2 - \eta_{t + 1} \le 0$ and $L/\eta_{t + 1}\le 1$ hence $L/\eta_{t + 1} - 1 \le 0$. 
Therefore, the quantity over all coefficient is $\le 0$. 

**To see (2.)**, we solve the inequality for $\eta_{t + 1}$ for all $t \in \N$.
Since $\sigma_0 = 0$ from the setting of Theorem 2.1, it has for $t = 0$ $(1/2)(L/\eta_{t + 1} - 1) \le 0$ which yilds: $\eta_{t + 1} \ge L$. 
Otherwise, we consider when $t \ge 1$. 
From there we solve the inequality. 
Start by considering: 

$$
\begin{aligned}
    \left(
        \sigma_t \left(\frac{L}{2} - \eta_{t + 1}\right) + 
        \frac{1}{2}\left(
            \frac{L}{\eta_{t + 1}} - 1
        \right)
    \right) &\le 0
    \\
    \iff 
    \frac{L}{2} - \eta_{t + 1} + 
    \frac{L}{2\eta_{t + 1}\sigma_t} - \sigma_t^{-1} &\le 0
    \\
    \iff 
    \frac{L\eta_{t + 1}}{2} - \eta_{t + 1}^2 + 
    \frac{L}{2\sigma_t} - \eta_{t + 1}\sigma_t^{-1} &\le 0
    \\
    (L/2 - \sigma_t^{-1})\eta_{t + 1} - \eta_{t + 1}^2 + (1/2)\sigma_t^{-1}
    &\le 0
\end{aligned}
$$

Solving it yields solutions to the roots of the quadratic, which has: 

$$
\begin{aligned}
    \eta_{t + 1} = 
    \frac{1}{2}\left(
        \frac{L}{2} - \sigma_t^{-1} \pm 
        \sqrt{
            \left(
            \frac{L}{2} - \sigma_t^{-1}
            \right)^2 + 2 \sigma_t
        }
    \right). 
\end{aligned}
$$

Observe that it's always the case where it has a positive, and negative solution. 
Since the stepsize is always positive, we want the positive root of the quadratic inequality to give the lower bound of $\eta_{t + 1}$. 
Therefore, it implies that: 

$$
\begin{aligned}
    \eta_{t + 1} &=
    \frac{1}{2}\left(
        \frac{L}{2} - \sigma_t^{-1} + 
        \sqrt{
            \left(
            \frac{L}{2} - \sigma_t^{-1}
            \right)^2 + 2 \sigma_t
        }
    \right)
    \\
    &\ge 
    \frac{1}{2}\left(
        \frac{L}{2} - \sigma_t^{-1} + 
        \frac{L}{2} - \sigma_t^{-1}
    \right) = \frac{L}{2} - \frac{2}{\sigma_t}. 
\end{aligned}
$$

The larger $\sigma_t$, the larger stepsize $\eta_{t +1}^{-1}$ can be made. 


---
### **PPM in the Strongly Convex Settings**

Strong convexity strengthen the convergence results of PPM. 
It leads to faster convergence rate for a same schedule of the step size sequences $(\eta_{t})_{t \ge 1}$. 
We will prove a new convergence rate by including the strong convexity constant in this section. 
The proof will differ from the general convex case. 

#### **Theorem | PPM Convergence under strong convexity**
> Suppose $F: \R^n \rightarrow \overline \R$ is $\mu > 0$ convex. 
> Let $(x_k)_{k \ge0}$ be a sequence generated by PPM method applied on $F$. 
> Then for $x^* \in \arg\min_{x\in \R^n}\;F(x) \neq \emptyset$ it has $F(x_k) - F(x^*) \le \mathcal O\left(\prod_{i = 1}^k(1 + \mu\eta_{t + 1})^{-1}\right)$. 

**Proof**

Observe that from strong convexity, $F$ always has a unique minimizer $x^*$. 
Define $F^+ = F(x^*)$ to be the minimizer. 
Because $F$ is strongly convex it has quadratic Error Bound condition which gives the inequality 

$$
\begin{aligned}
    \frac{1}{2\mu}\Vert x_{k + 1} - x_k\Vert^2 
    &\ge 
    \dist(\partial F(x_{k + 1}), \mathbf 0) 
    \ge 
    F(x_{k + 1}) - F^+. 
\end{aligned}
$$

$F$ is convex, therefore $\mathcal M_F^{1/\eta_{t + 1}}(\cdot;y)$ is $\eta_{t + 1} + \mu$ strongly convex for all $t \in \Z_0$. 
It has quadratic growth condition over the minimizer $x_{t + 1}$ admiting inequality from Lemma 1: 

$$
\begin{aligned}
    0 &\le 
    F(x) - F(x_{t + 1}) 
+ 
\frac{\eta_{t + 1}}{2}\left(
    \Vert x - x_k\Vert^2 - \Vert x_{k + 1} - x_k\Vert^2
    - \Vert x - x_{k + 1}\Vert^2
    \right)
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2.
\end{aligned}
$$

Set $x = x_t$ it has for all $t \in \Z_+$: 
$$
\begin{aligned}
    0&\le F(x_t) - F(x_{t + 1}) + 
    (-\eta_{t + 1} - \mu/2)\Vert x_t - x_{t + 1}\Vert^2
    \\
    \iff 
    F(x_t) - F(x_{t + 1}) 
    &\ge (\eta_{t + 1} + \mu/2)\Vert x_t - x_{t + 1}\Vert^2 
    \\
    &\ge 
    (\eta_{t + 1} + \mu/2)(2\mu)
    (F(x_{t + 1}) - F^+)
    \\
    \iff 
    F(x_t) - F^+ + F^+ - F(x_{t + 1}) 
    &\ge
    (\eta_{t + 1} + \mu/2)(2\mu)
    (F(x_{t + 1}) - F^+)
    \\
    \iff F(x_t) - F^+
    &\ge 
    (1 + (\eta_{t + 1} + \mu/2)(2\mu))(F(x_{t + 1}) - F^+)
    \\
    \iff 
    (1 + (2\mu\eta_{t + 1} + \mu^2))^{-1}F(x_t) - F^+
    &\ge F(x_{t + 1}) - F^+. 
\end{aligned}
$$

Unrolling the recurrence it yields 

$$
\begin{aligned}
    F(x_{t + 1}) - F^+ 
    &\le 
    \left(
        \prod_{i = 1}^{t + 1}
        \left(
            1 + (2\mu \eta_{i} + \mu^2)
        \right)
    \right)^{-1}(F(x_0) - F^+). 
\end{aligned}
$$

Easy to see that $2\mu\eta_i + \mu^2 \ge \mu \eta_i$, therefore the inverted big product can be upper bounded by $\left(\prod_{i = 1}^{t} (1 + \eta_i \mu)\right)^{-1}$, which is more simplified. 

---
### **Bregman Proximal point, extension to Non-Euclidean cases**



---
### **Convergence under inexact proximal oracles**

The key here is the characterize the error and restate the Proximal Inequality. 



---
### **Rockafellar's PPM Analysis**

Read [Convex Proximal Point Method, Part II](Convex%20Proximal%20Point%20Method,%20Part%20II.md) for more. 
