- [Smooth Gradient Descend The Basics](Smooth%20Gradient%20Descend%20The%20Basics.md)
- [Morau Envelope and Proximal Operators](Moreau%20Envelope%20and%20Convex%20Proximal%20Mapping.md)

---
### **Intro**

Let the ambient space be $\R^n$ and let $f: \R^n \rightarrow \overline {\R}$
Our attention is on the method of proximal point. 
Let $x_0 \in \text{dom} f$, given a strictly positive sequence $(\eta_t)_{t \ge 1}$, consider a sequence $(x_t)_{t \ge0}$ generated by 

$$
\begin{aligned}
    x_{t + 1} = \prox_{\eta_{t + 1} f}(x_t):= 
    \argmin{x\in \R^n}
    \left\lbrace
        f(x) + \frac{1}{\eta_{t + 1}}\Vert x - x_t\Vert^2
    \right\rbrace. 
\end{aligned}
$$

For notational convenience, for all $y \in \R^n$ we define model function 

$$
\begin{aligned}
    (\forall x \in \R^n)\quad 
    \mathcal M_f^{\eta}(x; y) := f(x) + \frac{\eta}{2}\Vert x - y\Vert^2. 
\end{aligned}
$$

The following lemma gives the results of proximal inequality which is relevant to proving the convergence of PPM method. 

#### **Lemma 1 | Proximal inequality**
> Let $y \in \R^n$. 
> Suppose that the PPM generates sequence $(x_t)_{ t \ge 0}$ on $F$. 
> If $F: \R^n \rightarrow \overline \R$ is $\mu \ge 0$ strongly convex, 
> then for all $x \in \R^n$ it has 
> $$
> \begin{aligned}
>     0 &\le 
>     F(x) - F(x_{t + 1}) 
>     + 
>     \frac{\eta_{t + 1}}{2}\left(
>         \Vert x - x_k\Vert^2 - \Vert x_{k + 1} - x_k\Vert^2
>         - \Vert x - x_{k + 1}\Vert^2
>     \right)
>     - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
>     \\
>     &= 
>     F(x) - F(x_{t + 1}) 
>     + 
>     \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle
>     - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
>     \\
>     &\le 
>     F(x) - F(x_{t + 1}) 
>     + 
>     \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle. 
> \end{aligned}
> $$

**Proof**

Consider the $\mu + \eta_{t + 1}$ strong convexity property of $\mathcal M^{\eta_{t + 1}}_F(\cdot; y)$ and the fact that $x_{t + 1}$ is the minimizer of $\mathcal M_F^{\eta_{t + 1}}(\cdot; y)$ which gives the quadratic growth condition: 

$$
\begin{aligned}
    0 &\le 
    \mathcal M(x; x_t) - \mathcal M(x_{t + 1}, x_t) 
    - \frac{\eta_{t + 1} + \mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= F(x) - F(x_t) + \frac{\eta_{t + 1}}{2}\left(
        \Vert x - x_t\Vert^2 - \Vert x_{t + 1} - x_t \Vert^2
        - \Vert x - x_{t + 1}\Vert^2
    \right)
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= F(x) - F(x_t) + \frac{\eta_{t + 1}}{2}\left(
        \Vert x - x_{t + 1} + x_{t + 1} - x_t\Vert^2 - \Vert x_{t + 1} - x_t \Vert^2
        - \Vert x - x_{t + 1}\Vert^2
    \right) 
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &= 
    F(x) - F(x_t) 
    + \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle
    - \frac{\mu}{2}\Vert x - x_{t + 1}\Vert^2
    \\
    &\le 
    F(x) - F(x_t) 
    + \eta_{t + 1}\langle x - x_{t + 1}, x_{t + 1} - x_t\rangle. 
\end{aligned}
$$


---
### **Convergence in the convex case**

We show the convergence of PPM in the convex case. 
Let $\sigma_t$ be a monotone increasing sequence. 
The following idea is important to showing the convergence of PPM method in the convex settings. 
It shows the way of proving convergence using the method of Lyapunov function. 
Consider the following: 

$$
\begin{aligned}
    0 
    &\ge 
    \sigma_{t + 1}(\Delta_{t + 1} - \Delta_{t})
    + (\sigma_{t + 1} - \sigma_t)\Delta_t
    \\
    &= 
    \sigma_{t + 1}\Delta_{t + 1} - \sigma_{t + 1}\Delta_t
    + \sigma_{t + 1}\Delta_t - \sigma_t \Delta_t
    \\
    &=
    \sigma_{t + 1}\Delta_{t + 1}  - \sigma_t \Delta_t. 
\end{aligned}
$$


Performing a telescoping sum, it shows that $\Delta_t \le \mathcal O(\sigma_{t + 1}^{-1})$. 
Following a similar idea, we are going to prove the convergence rate of proximal point method applied to a convex function: $f: \R^n \rightarrow \overline \R$. 


#### **Theorem 1.1 | A Lyapunov quantity for PPM under convexity**
> Suppose that $F:\R^n \rightarrow \overline \R$ is a convex function. 
> Let $(x_t)_{t \ge 0}$ be a sequence generated by PPM with $F$, and relaxation sequence $(\eta_t)_{t \ge 1}$. 
> For all $x \in \R^n$, $t \ge 0$ define: 
> $$
> \begin{aligned}
>     \Phi_t := \left(
>         \sum_{i = 1}^{t + 1}\eta_{i}^{-1}
>     \right)\left(
>         F(x_{t + 1}) - F(x) + 
>         \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
>     \right). 
> \end{aligned}
> $$
> Define $\Phi_0 = F(x_0) - F(x) + (1/2)\Vert x_0 - x\Vert^2$. 
> Then $\Phi_t$ decreases monotonically, i.e: for all $t \ge 1$ it has $\Phi_{t + 1} - \Phi_t \le 0$. 

**Proof**

The proof admits the following key intermediate steps: 

1. (**Step I**): $F(x_t)$ is a monotone decreasing sequence. 

2. (**Step II:**): 
For any positive sequence $(\sigma_t)_{t \ge 0}$, the following inequality is true for all $x \in \R^n$ and $t \ge 0$: 
$$
\begin{aligned}
    (\sigma_{t + 1} - \sigma_t)(F(x_{t + 1}) - F(x)) 
    &= 
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    - 
    \sigma_t(F(x_{t + 1}) - F(x_t))
    - \sigma_t(F(x_t) - F(x))
    \\
    &\ge 
    \sigma_{t + 1} (F(x_{t + 1}) - F(x))
    - \sigma_t(F(x_t) - F(x)). 
\end{aligned}
$$


We will go back to show **(Step I), (Step II)**. 
The monotone property of $\Phi_t$ is now achievable from these intermediate results. 
Define $\sigma_{t + 1} = \sigma_t + \eta_{t}^{-1}$ for all $t \ge 1$ and $\sigma_0 = 0$. 
Then the sequence $(\eta_t)_{t \ge 1}$ is strictly positive which makes $\sigma_t$ a monotone increasing strictly positive sequence because it has $\sigma_t = \sum_{i = 1}^{t} \eta_i^{-1} > 0$. 

At this point, we are ready to show the results. 
For all $x \in \R^n, t \in \Z_+$, it has from the proximal inequality: 
$$
\begin{aligned}
    0 &\ge 
    F(x_{t + 1}) - F(x) + \frac{\eta_{t + 1}}{2}
    \left(
        \Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2
    \right)
    +
    \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    \\
    \iff 
    0 &\ge 
    \eta_{t + 1}^{-1}(F(x_{t + 1}) - F(x))
    +
    (1/2)(\Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2)
    + 
    (1/2)\Vert x_{t + 1} - x_t\Vert^2
    \\
    &= 
    (\sigma_{t + 1} - \sigma_t)(F(x_{t + 1}) - F(x))
    +
    (1/2)(\Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2)
    + 
    (1/2)\Vert x_{t + 1} - x_t\Vert^2
    \\
    &= 
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    - 
    (1/2)(\Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2)
    + 
    (1/2)\Vert x_{t + 1} - x_t\Vert^2
    \\
    &\ge
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    - \sigma_t(F(x_t) - F(x))
    +
    (1/2)(\Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2)
    + 
    (1/2)\Vert x_{t + 1} - x_t\Vert^2
    \\
    &\ge
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    - \sigma_t(F(x_t) - F(x))
    +
    (1/2)(\Vert x - x_{t + 1}\Vert^2 - \Vert x - x_t\Vert^2)
    \\
    &= \sigma_{t + 1}\left(
        F(x_{t + 1}) - F(x) + \frac{1}{2}\Vert x - x_{t + 1}\Vert^2
    \right) - 
    \sigma_{t}\left(
        F(x_{t}) - F(x) + \frac{1}{2}\Vert x - x_{t}\Vert^2
    \right). 
    \\
    &= \Phi_{t + 1} - \Phi_t. 
\end{aligned}
$$


It remains to show **(Step I), (Step II)**. 

**Proof of Step I**. 
Setting $x = x_{t}$ in the inequality from **(Step I)** it yields: 

$$
\begin{aligned}
    0 &\ge 
    F(x_{t + 1}) - F(x_{t}) + \frac{\eta_{t + 1}}{2}\Vert x_{t} - x_{t + 1}\Vert^2 
    + \frac{\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert^2
    \\
    &= F(x_{t + 1}) - F(x_t). 
\end{aligned}
$$


**Proof of (Step II)**. 
Let $\sigma_{t + 1}$ be any number, it follows that: 

$$
\begin{aligned}
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    &= 
    (\sigma_{t + 1} - \sigma_t) (F(x_{t + 1}) - F(x)) 
    + \sigma_t(F(x_{t + 1}) - F(x))
    \\
    &= 
    (\sigma_{t + 1} - \sigma_t) (F(x_{t + 1}) - F(x)) 
    + \sigma_t(F(x_{t + 1}) - F(x_t) + F(x_t) - F(x))
    \\
    &= 
    (\sigma_{t + 1} - \sigma_t) (F(x_{t + 1}) - F(x)) 
    + \sigma_t(F(x_{t + 1}) - F(x_t)) 
    + \sigma_t(F(x_t) - F(x))
    \\
    \iff 
    (\sigma_{t + 1} - \sigma_t)(F(x_{t + 1}) - F(x)) &= 
    \sigma_{t + 1} (F(x_{t + 1}) - F(x)) 
    - 
    \sigma_t(F(x_{t + 1}) - F(x_t))
     - \sigma_t(F(x_t) - F(x)). 
\end{aligned}
$$


#### **Theorem 1.2 | PPM convergence rate**
> Let $(x_t)_{t \ge 0}$ be generated by the PPM on function $F: \R^n \rightarrow \overline \R$ which is convex. 
> Assuming that a minimizer $x^+$ exists for $F$ and the minimum is $F^+$. 
> Then it has the following convergence rate: 
> $$
> \begin{aligned}
>     F(x_{N + 1}) - F^+ +
>     \frac{1}{2}\Vert x^+ - x_{N + 1}\Vert^2
>     &\le 
>     \left(
>         \sum_{i = 0}^{N}
>         \eta_{i + 1}^{-1}
>     \right)^{-1}
>     \left(
>         F(x_0) - F^+ + \frac{1}{2}\Vert x^+ - x_0\Vert^2
>     \right). 
> \end{aligned}
> $$

**Proof**

From theorem 1.1 it has for all $t \in \Z^+: \Phi_{t + 1} - \Phi_t \le 0$. 
Hence, telescoping it for all $t = 1, \ldots, N$ it has 

$$
\begin{aligned}
    0 &\ge 
    \sum_{i = 0}^{N}
    \Phi_{t + 1} - \Phi_t
    = \Phi_{N + 1} - \Phi_0
    \\
    &= 
    \left(
        \sum_{i = 1}^{N + 1}\eta_{i}^{-1}
    \right)
    \left(
        F(x_{N + 1}) - F^+ +
        \frac{1}{2}\Vert x^+ - x_{N + 1}\Vert^2
    \right) - 
    \left(
        F(x_0) - F(x^+) + \frac{1}{2}\Vert x_0 - x^+\Vert^2
    \right). 
\end{aligned}
$$

Re-arranging the above inequality yields the convergence results of the PPM in the convex settings. 

---
### **Analysis of Proximal Convex Lower Bounding Function**

Let $f$ convex differentiable and its gradient be $L$-Lipschitz smooth, then the proximal method assists with the derivation of gradient descent method. 
Use $l_f(x; \bar x)$ to be a linearization of function $f$ at point $\bar x$: 

$$
\begin{aligned}
    l_f(x ; \bar x) &= f(\bar x) + \langle \nabla f(x), x - \bar x\rangle \le f(x) 
    \quad \forall x \in X. 
\end{aligned}
$$

We consider the following PPM method with undetermined step size scheme: 

$$
\begin{aligned}
    x_{t +1} = \argmin{x\in X} \left\lbrace
        l_f(x; x_t) + \frac{1}{2\eta_{t + 1}}\Vert x - x_t\Vert^2
    \right\rbrace = x_t - \eta_{t + 1} \nabla f(x_t). 
\end{aligned}
$$

Our goal is to apply the Lyapunov function derived for PPM, to the approximation above. 
The reader should observe additional error introduced by the above approximation and how it's controlled by smoothness assumption of the gradient of $f$. 

**Remark**

$l_f(x; \bar x)$, can be any l.s.c., closed, convex lower bound function that is favorable for the proximal operator. 
Proximal gradient is one fine example where $l_f(x; \bar x)$ is not necessarily a smooth function. 
We illustrate how this property is used in the below proof for a claim. 


#### **Theorem 2.1 | Generic Descent Lemma via PPM of Lower Bounding Function and Upper Smoothness**
> Let $f$ be a function that has minimizer: $x_*$.
> Let $l_f(x; x_t)$ be a convex, l.s.c, proper lower bound function. 
> Let the lower bounding $\phi$ function satisfies inequality: 
> $$
> \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) + \frac{L\eta_{t + 1}}{2}\Vert x - x_t\Vert^2 \quad \forall x \in X, 
> $$ 
> Assume an algorithm the makes: 
> $$
> \begin{aligned}
>     x_{t +1} = \argmin{x\in X} \left\lbrace
>         l_f(x; x_t) + \frac{1}{2\eta_{t + 1}}\Vert x - x_t\Vert^2
>     \right\rbrace, 
> \end{aligned}
> $$
> Then the iterates satisfies: 
> $$
> \begin{aligned}
>     \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) + \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2
>     - \frac{1}{2}\Vert x_* - x_t\Vert^2
>     & \le 
>     \left(
>         \frac{L \eta_{n + 1}}{2} - \frac{1}{2}
>     \right)\Vert x_{t + 1} - x_t\Vert^2.
> \end{aligned}
> $$
> This is a result analogous to Lemma-1. 
> Furthermore, if $\exists \epsilon > 0: \eta_i\in (\epsilon, 2L^{-1} - \epsilon)\;\forall i \in \N$ then $f(x_T) -f(x_*)$ at a rate of $\frac{L - \epsilon^{-1}}{TL\epsilon}$ where $x_*$ is the minimizer of $f$. 

**Proof**

By $\phi_t(x)$ be a convex function and Lemma 1, Moreau Envelope Inequality produces $\forall u$: 

$$
{\small
\begin{aligned}
    \phi_t(x_{t + 1}) - \phi_t(u) - 
    \frac{1}{2}\Vert x_t - u\Vert^2 
    + 
    \frac{1}{2}\Vert u - x_{t + 1}\Vert^2 
    &\le 
    - \frac{1}{2}\Vert x_{t + 1} - x_t \Vert^2
    \\
    \left(
        \phi_t(x_{t + 1}) + \frac{\eta_{t + 1}L}{2}\Vert x_{t+1} -x_t\Vert^2
    \right)
    - \phi_t(u) - \frac{1}{2}\Vert x_t - u\Vert^2 + \frac{1}{2}\Vert u - x_{t + 1} \Vert^2
    &\le 
    \left(
        \frac{\eta_{t + 1}L}{2} - \frac{1}{2}
    \right)\Vert x_{t+1} - x_t\Vert^2. 
\end{aligned}\tag{$\star$}
}
$$

Using the assumption of the lower bounding function of $f$, the hypothesis allows for 
$$
\begin{aligned}
    \left(
        \phi_t(x_{t + 1}) + 
        \frac{\eta_{t + 1}L}{2}\Vert x_{t+1} -x_t\Vert^2
    \right) &\ge \eta_{t + 1}f(x_{t + 1}), 
    \\
    - \phi_t(u) &\ge  - \eta_{t + 1}f(u), 
\end{aligned}
$$

substituting the above inequality into the RHS of $(\star)$, we obtain $\forall u \in X$: 

$$
\begin{aligned}
    \left(
        \eta_{t + 1} f(x_{t + 1}) - \eta_{t + 1}f(u)
    \right) 
    - \frac{1}{2}\Vert x_t - u\Vert^2 + 
    \frac{1}{2}\Vert u - x_{t + 1}\Vert^2 
    &\le 
    \left(
        \frac{\eta_{t + 1}L}{2} - \frac{1}{2}
    \right)
    \Vert x_{t + 1} - x_t \Vert^2. 
\end{aligned}
$$

Since this is true for all $x_t$, we claim that $\Phi_t = (\sum_{i = 1}^t \eta_i)(f(x_t) - f(x_*)) + \frac{1}{2}\Vert x_* - x_t\Vert^2$ is strictly non-increasing when $\eta_{t + 1}\le L^{-1}$. 
Surprisingly, if $\eta_i \in (0, 2L^{-1})$, $\Phi_{t}$ still converge. 
For simplicity, we make $\sigma_t = \sum_{i = 1}^{t}\eta_i$. 
It starts with considerations that $(L\eta_{t + 1}/2 - 1) < 0$, so that 

$$
\begin{aligned}
    f(x_{t + 1}) - f(x_t) &\le 
    \left(\frac{L\eta_{t + 1}}{2} - 1\right)\Vert x_{t + 1} - x_t\Vert^2
    \\
    f(x_T) - f(x_0)
    &\le 
    \underbrace{
    \left(
        \frac{L\sigma_T}{2} - T
    \right)
    }_{< 0}
    \sum_{t = 0}^{T - 1}\Vert x_{t + 1} - x_t\Vert^2
    \\
    \implies 
    \sum_{t = 0}^{T -1}\Vert x_t - x_{t + 1}\Vert^2
    &\le 
    \left(
        \frac{L}{2}\sigma_T  - T
    \right)^{-1} 
    (f(x_T) - f(x_0))
\end{aligned}
$$

Continue on the RHS of $\Phi_{t + 1} - \Phi_t$ so 
$$
\begin{aligned}
    \sum_{t = 0}^{T - 1}\Phi_{t + 1} - \Phi_t 
    &\le 
    \left(
        \frac{L}{2}\sigma_T - \frac{T}{2}
    \right)
    \sum_{t = 0}^{T - 1}\Vert x_{t + 1} - x_t\Vert^2
    \\
    \Phi_T - \Phi_0 &\le 
    \left(
        \frac{\frac{L}{2}\sigma_T - \frac{T}{2}}{
            \frac{L}{2}\sigma_T - T
        }
    \right)
    (f(x_T) - f(x_0))
    \\
    &= 
    \left(
        \frac{L\sigma_T - T}{L\sigma_T - 2T}
    \right)
    (f(x_T) - f(x_0)), 
\end{aligned}
$$
implies
$$
\begin{aligned}
    \sigma_T (f(x_T) - f(y)) + \frac{1}{2}\Vert y - x_t\Vert^2
    - \frac{1}{2}\Vert y - x_0 \Vert^2 
    &\le 
    \left(
        \frac{L\sigma_T - T}{L\sigma_T - 2T}
    \right)
    (f(x_T) - f(x_0))
    \\
    \iff
    f(x_T) - f(y) + 
    \frac{1}{2\sigma_T}(\Vert y - x_t\Vert^2 - \Vert y - x_0\Vert^2)
    &\le 
    \left(
        \frac{L- T\sigma_T^{-1}}{2T - L\sigma_T}
    \right)
    (f(x_0) - f(x_T)), 
\end{aligned}
$$
therefore, we obtain the bound: 
$$
\begin{align}
    f(x_T) - f(y) &\le 
    \left(
        \frac{L- T\sigma_T^{-1}}{2T - L\sigma_T}
    \right)
    (f(x_0) - f(x_T))
    - 
    \frac{1}{2\sigma_T}(\Vert y - x_t\Vert^2 - \Vert y - x_0\Vert^2)
\end{align}
$$
In the case where $\sup_{i\in \N} \eta_i \le 2L^{-1} - \epsilon$, and $\inf_{i\in \N}\eta_i \ge \epsilon$ with $\epsilon > 0$. 
Then we have 
$$
\begin{aligned}
    \frac{L -T\sigma_T^{-1}}{2T - L\sigma_T}
    &\le 
    \frac{L - \epsilon^{-1}}{2T - LT(2L^{-1} - \epsilon)}
    \\
    &= 
    \frac{L - \epsilon^{-1}}{2T - T(2 - L\epsilon)}
    \\
    &= 
    \frac{L - \epsilon^{-1}}{TL\epsilon}. 
\end{aligned}
$$
With $y = x_*$, by $f(x_t)$ strictly monotone decreasing, we get the claimed convergence rate by 
$$
\begin{aligned}
    f(x_T) - f(x_*) 
    &\le 
    \frac{L - \epsilon^{-1}}{TL\epsilon}(f(x_0) - f(x_T)) 
    \le 
    \frac{L - \epsilon^{-1}}{TL\epsilon}(f(x_0) - f(x_*)) 
\end{aligned}
$$


**Remark**

If, $f$ is differentiable, and $\phi_t(x) = l_f(x, x_t) = f(x_t) + \langle \nabla f(x_t), x - x_t\rangle$. 
Then, the upper $L$-Lipschitz gradient smooth condition and convexity of $f$ are backed into the inequality: 

$$
\eta_{t + 1}l_f(x, x_t) \le 
\eta_{t + 1} f(x) \le \eta_{t + 1}l_f(x, x_t) + \frac{\eta_{t + 1}L}{2}\Vert x - x_t\Vert^2
\quad \forall x \in X. 
$$

we empahsize that it's the upper smoothness wrt to 2-norm. 
In implementation of algorithms, we only need to make sure that: 

$$
\eta_{t + 1}l_f(x_{t + 1}, x_t) \le 
\eta_{t + 1} f(x_{t + 1}) \le \eta_{t + 1}l_f(x_{t + 1}, x_t) 
+ 
\frac{\eta_{t + 1}L_t}{2}\Vert x_{t + 1} - x_t\Vert^2
\quad \forall t \in \N. 
$$

for the appropriate $L_t$, this is called: "line search". 
This is strictly weaker than what is used to prove the theorem. 
There are some potential for generalization and creativity by considering various kind of convex $\phi_t(x)$. 
One can imagine maximizing $\eta_{t + 1}L_t$ by appropriate choice of $x_{t + 1}$ to obtain faster convergence rate. 


**Questions**

Would a similar analysis work for strongly convex functions, or it's going to be a different story since $\Phi_t$, the Lyapunov function would change accordingly? 
I don't have any good ideas but to use gradient descent on the envelope interpretation to asist the analysis in the strongly convex case. 

#### **Thm 2.2 | Stepsize and the Convergence of the Gradient Descent Method**
> Choose stepsize $0 < \eta_t \le L^{-1}$, then the method of PPM on convex lower bounding function (which is gradient descent) has convergence rate $\mathcal O\left(\sum_{i = 1}^{T}\eta_t^{-1}\right)$. 

**Proof**

Set $\eta_{t + 1} \in (0, L^{-1}]$, then we can conclude the same conclusion as theorem 1. 
Because the inequality in claim 1 are less than zero, and that makes the exact same inequalities as proposition 2, making the results of theorem 1 follows naturally. 

**Remarks**

In practice, the step size is usually constant, or it's determined by some type line search algorithm. 

#### **Example | Bounded Bregman Divergence**
> In the case where $l_f(x, \bar x) = f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle$ is the linearization of $f$ at the point $\bar x$, then the the following ineualities are be equivalent, for any norm $\Vert \cdot\Vert$. 
> 1. $l_f(x, \bar x) \le f(x) \le l_f(x, \bar x) + (L/2)\Vert x - \bar x\Vert^2$. 
> 2. $0 \le D_f(x, \bar x) \le (L/2)\Vert y - x\Vert^2$. 

**Proof**

Directly by considering 

$$
\begin{aligned}
    & l_f(x, \bar x) \le f(x) \le l_f(x, \bar x) + (L/2)\Vert x - \bar x\Vert^2
    \\
    & f(x) - D_f(x, \bar x) \le f(x) \le f(x) - D_f(x, \bar x) + (L/2)\Vert x - \bar x\Vert^2 
    \\
    & -D_f(x, \bar x) \le 0 \le -D_f(x, \bar x) + (L/2)\Vert y - x\Vert^2 
    \\
    & 0 \le D_f(x, \bar x) \le (L/2)\Vert y - x\Vert^2. 
\end{aligned}
$$




#### **Corollary-2.3 | Proximal Gradient Descent**
> Consider additive composite $f = g + h$ where $g$ is convex and $h$ is convex smooth with $L$-Lipschitz gradient. 
> Consider $l_f(x; \bar x) = g(x) + h(\bar x) + \langle \nabla h(\bar x), x - \bar x\rangle$. 
> Then proximal point method $x_{t + 1} \in \text{prox}[l_f(\cdot, \bar x)](x_t)$ produces the proximal gradient method and the above convergence analysis.   

**Proof**

The proximal point on $l_f(x, \bar x)$, is proximal gradient, that part is obvious. 
See [Proximal Gradient, Forward Backwards Envelope](Proximal%20Gradient,%20Forward%20Backwards%20Envelope.md) for more information.
Next, $l_f(\cdot, \bar x)$ is convex for all $\bar x \in X$, and by convexity of $g$, it is a lower bounding function for $f$. 
The function $h$ is smooth, and therefore the upper bound
$$
    f(\cdot) \le l_f(\cdot | x_t) + \frac{L}{2}\Vert\cdot - x_t\Vert^2 
$$
applies. 
The inequality in Theorem 2 is satisfied hence the results of theorem 2 applies. 


#### **Theorem 2.4 | Recovery of the fundamental lemma of proximal gradient (CONTAINS MAJOR MISTAKES)**
> With $f = h + g$ where $g$ is $L$-Lipschitz smooth, $h$ convex, consider $\phi(u) = \eta(h(u) + g(x) + \langle \nabla g(x), u - x \rangle)$ and $\eta^{-1} \ge L$ satisfies $\eta\phi \le \eta f \le \eta f + \frac{1}{2}\Vert \cdot - x\Vert^2$. 
> Let $x^+ = \prox_{\phi}(x)$ then $\forall \;u\in \R^n, \eta \le L^{-1}$: 
> $$
> \begin{aligned}
>    f(u) - f(x^+) 
>     + \frac{1}{2\eta} \Vert u - x\Vert^2 
>     - \frac{1}{2\eta} \Vert x^+ - u\Vert^2 
>     \ge 
>     D_g(u, x) + \frac{1}{2\eta}\Vert x^+ - x\Vert^2. 
> \end{aligned}
> $$

**Proof**
<!-- 
$$
\begin{aligned}
    & \phi(u) + \frac{1}{2}\Vert u - x\Vert^2 - \phi(x^+) - \frac{1}{2}\Vert x^+ - x\Vert^2 
    \ge \frac{1}{2}\Vert x^+ - u\Vert^2
    \\
    \implies &
    \eta\underbrace{
        \left(
            h(u) + g(x) + \langle \nabla g(x), u - x\rangle 
        \right)
    }_{= \phi(u)} 
    - (\eta f + (1/2)\Vert x^+ - x\Vert^2)
    \\
    &\quad  
    + \frac{1}{2} \Vert u - x\Vert^2 
    \ge 
    \frac{1}{2}\Vert x^+ - u\Vert^2 
    \\
    \iff & 
    f(u) + \left(
        g(x) - g(u) + \langle \nabla g(x), u -x\rangle 
    \right)
    - f(x^+) - \frac{1}{2\eta}\Vert x - x^+\Vert^2
    \\
    & \quad 
    + 
    \frac{1}{2}\Vert u - x\Vert^2 
    \ge 
    \frac{1}{2\eta}\Vert x^+ - u\Vert^2 
    \\
    \iff 
    & f(u) - f(x^+) - D_g(u, x)
    + \frac{1}{2\eta} \Vert u - x\Vert^2 - \frac{1}{2\eta}\Vert x^+ - x\Vert^2
    \ge 
    \frac{1}{2\eta} \Vert x^+ - u\Vert^2. 
\end{aligned}
$$

One of the consequence of the above inequaity are

$$
\begin{aligned}
    \implies 
    f(u) - f(x^+) - D_g(u, x) 
    + 
    \frac{1}{2\eta}\Vert u - x\Vert^2 
    - 
    \frac{1}{2\eta}\Vert x^+ - u\Vert^2
    &\ge 
    0 \quad \forall u
    \\
    \iff 

    f(u) - f(x^+) - D_g(u, x) + \eta^{-1}
    \left(
        \frac{1}{2}\Vert x^+ - x\Vert^2 
        + 
        \langle u - x^+, x^+ - x\rangle
    \right)
    &\ge 0 \quad \forall u. 
\end{aligned}
$$

All we did is moving the term $-1/(2\eta)\Vert x^+ - x\Vert^2$ to the RHS since it's a non-negative quantity, it's larger than zero, which explains the $\ge 0$ for the above inequalites. 
 -->



**Remarks**

At the end we obtained the fundamental proximal gradient lemma in Amir Beck's writings. 
This bound is tighter. 
This can also be used for deriving the convergence of the gradient descent method. 
For more information on this lemma, visits: [V-FISTA](V-FISTA.md), [Proximal Gradient, Forward Backwards Envelope](Proximal%20Gradient,%20Forward%20Backwards%20Envelope.md), [Proximal Gradient Convergence Rate](AMATH%20516%20Numerical%20Optimizations/Classics%20Algorithms/Proximal%20Gradient%20Convergence%20Rate.md), and [A Better Proof for FISTA Convergence](A%20Better%20Proof%20for%20FISTA%20Convergence.md). 


#### **Corollary 2.5 | Another proximal gradient lemma via Moreau Envelope**
> Suppose that $F = f + g$ with $f, g$ convex and $f$ $L$-Lipschitz smooth. 
> For any $\bar x$, define proximal gradient point 
> $$
> \begin{aligned}
>   x^+ = \argmin{x}   \left\lbrace
>       g(x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
>       \frac{L}{2}\Vert x - \bar x\Vert^2
>   \right\rbrace. 
> \end{aligned}
> $$
> Then it has $\forall x$
> $$~~~~
> \begin{aligned}
>     F(x^+) + \frac{L\Vert x^+ - x\Vert^2}{2} &\le 
>     F(x) + \frac{L\Vert x - \bar x\Vert^2}{2}
> \end{aligned}
> $$

**Proof**

This is a consequence of previous theorem. 
To use the previous theorem, set $g:= f, f:= F, h:= g$, and $\bar x = x, \eta = L^{-1}$, then the previous theorem becomes 

$$
\begin{aligned}
    (\forall u)
    \quad F(u) - F(x^+) 
    - D_f(u, \bar x) + 
    \frac{L}{2}\Vert u - \bar x\Vert^2 - \frac{L}{2}\Vert x^+ - u\Vert^2 
    &\ge 0, 
    \\
    (\forall x, u)\; D_f(u, x)\ge 0
    \implies 
    (\forall u)\; 
    F(u) - F(x^+)
    + \frac{L}{2}\Vert u - \bar x\Vert^2 
    - \frac{L}{2}\Vert x^+ - u\Vert^2 &\ge 0. 
\end{aligned}
$$

This is proved. 

**Remark**

This is a different proximal gradient lemma used in the proof of Chambolle and Dossal's variants of the accelerated proximal gradient method. 



---
### **The PPM with Strongly Convex Objective**

It may be counter intuitive to write things in this order, but we will see the necessity since the convergence proof for PPM for strongly convex objective function involves more theorems. 
The proof below will be similar to proving the convergence rate of gradient descent on a strongly convex and Lipchitz smooth function. 
To elucidate recall from [Proximal Point Method, Interpretations](Proximal%20Point%20Method,%20Interpretations.md) that, the proximal point method with a constant stepsize is equivalent to gradient descent on the Moreau Envelope of the function. 
The only difference here is that our PPM analysis is based on varying stepsizes. 

#### **Theorem 3.1 | PPM Convergence for Strongly Convex Function**
> Let $f$ be $\beta$-strongly convex. 
> Then the proximal point method has a linear convergence rate with a ratio of $(1 -\eta_{t + 1}\beta)$. 

**Proof**

With $\beta$-strong convexity of $f$, the proximal operator $P_{t + 1}=(I + \eta_{t + 1}\partial f)^{-1}$ is a contraction with constant $(1 + \beta_{t + 1})^{-1}$ where $\beta_{t+ 1} = \eta_{t + 1}\beta$ for all $t \in \N$. 
We can conclude that $I - P_{t + 1}$ is $1 - (1 + \beta)^{-1}$ strongly monotone and Lipschitz smooth by

$$
\begin{aligned}
    0 &\le \langle P_{t + 1} x - P_{t + 1} y, x - y\rangle 
    \\
    &\le \Vert P_{t + 1}x - P_{t + 1} y\Vert \Vert x - y\Vert \leftarrow \text{ Cauchy }
    \\
    &\le (1 + \beta_{t + 1})^{-1} \Vert x - y \Vert^2 \leftarrow \text{ Contraction} 
    \\
    \implies 
    - (1 + \beta_{t + 1})^{-1} \Vert x - y\Vert^2 
    &\le 
    \langle -(P_{t + 1} x - P_{t + 1}y), x - y\rangle \le 0
    \\
    (1 - (1 + \beta_{t + 1})^{-1}) \Vert x - y\Vert^2 
    &\le 
    \langle (x - y) - (P_{t + 1}x - P_{t + 1}y), x - y\rangle 
    \le \Vert x - y\Vert^2 
    \\
    (1 - (1 + \beta_{t + 1})^{-1}) \Vert x - y\Vert^2 
    &\le 
    \langle [I - P_{t + 1}]x - [I - P_{t + 1}]y, x - y\rangle 
    \le \Vert x - y\Vert^2. 
\end{aligned}
$$

Recall from [[Moreau Envelope and Convex Proximal Mapping]] that for any convex $g$ the gradient on the Moreau Envelope of $g$ has

$$
\begin{aligned}
    \underbrace{\nabla \left[x \mapsto \min_{u}\left\lbrace
        g(u) + \frac{1}{2}\Vert u - x\Vert^2
    \right\rbrace\right](x)}_{
        \nabla \env_{g}(x)
    } 
    &= 
    x - (I + \partial g)^{-1}x, 
\end{aligned}
$$

with $g = \eta_{t + 1}f$ then the above is 

$$
\begin{aligned}
   \nabla \env_{\eta_{t + 1}f}(x) = 
   x - (I + \eta_{t + 1}\partial f)^{-1}x = [I - P_t] x, 
\end{aligned}
$$

naming $\env{\eta_{t+ 1}f}$ as $F_{t + 1}$ for short, we have $\forall t \in \Z_+: \nabla F_{t + 1} = [I - P_{t + 1}]x$ being a strongly monotone operator, which we previously derived. 
Therefore, $F_{t + 1}$ is a strongly convex function and Lipschitz smooth function with constants: $(1 - (1 + \beta_{t + 1})^{-1}), 1$. 
Now, we make use of the Lipschitz Smoothness and strong convexity to yield: 

$$
\begin{aligned}
    F_{t + 1}(x_{t + 1}) - F_{t + 1}(x_t) &= 
    F_{t + 1}(P_{t + 1} x_t)  - F_{t + 1}(x_t)
    \\
    &= F_{t + 1}(x_t - (x_t - P_{t + 1}x_t)) - F_{t + 1}(x_t)
    \\
    &= 
    F_{t + 1}(x_t - \nabla F_{t + 1}(x_t)) - F_{t + 1}(x_t)
    \\
    &\le - \frac{1}{2}\Vert \nabla F_{t + 1}(x_t)\Vert^2 \leftarrow \text{ By $1$-Lipschitz Smooth}. 
\end{aligned}
$$

Next, by strongly convexity of $F_{t + 1}$, let $x_* = \argmin{x}f(x) = \argmin{x}F_{t + 1}(x)$ (it's one of the implications stated in [Strong Convexity, Equivalences and Implications](Strong%20Convexity,%20Equivalences%20and%20Implications.md)) we would have for all $t \in \N$: 
$$
\begin{aligned}
    \frac{1}{2} \Vert \nabla F_{t + 1}(x_t)\Vert^2 &\ge 
    (1 - (1 + \beta_{t + 1})^{-1})(
        F_{t + 1}(x_t) - F_{t + 1}(x_*)
    ) \leftarrow \text{ Result of Strong convexity. }
    \\
    \implies 
    F_{t + 1}(x_{t + 1}) - F_{t + 1}(x_t)
    &\le 
    -(1 - (1 + \beta_{t + 1})^{-1}) (F_{t + 1}(x_t) - F_{t + 1}(x_*))
    \\
    F_{t + 1}(x_{t + 1}) - F_{t + 1}(x_*)
    &\le 
    F_{t + 1}(x_t) - F_{t + 1}(x_*) 
    -(1 - (1 + \beta_{t + 1})^{-1}) (F_{t + 1}(x_t) - F_{t + 1}(x_*))
    \\
    &= 
    (1 + \beta_{t + 1})^{-1} (F_{t + 1}(x_t) - F_{t + 1}(x_*))
    \\
    \iff 
    F_{t + 1}(x_{t + 1}) - F_{t + 1}(x_*) 
    &\le (1 + \beta_{t + 1})^{-1}(F_{t + 1}(x_t) - F_{t + 1}(x_*))
\end{aligned}
$$

This establishes the descent of optimality gap for one step of the iteration. 
Amazingly, unrollowing the above would yield: 

$$
\begin{aligned}
    F_{t + 1}(x_{t + 1}) - F_{t+1}(x_*) &\le 
    \left(
        \prod_{j = 0}^{t} (1 + \beta_{t + 1})^{-1} 
    \right)(F_{1}(x_0) - F_1(x_*)). 
\end{aligned}\tag{$[*]$}
$$

Recall the property of a Moreau Envelope and by the definition of $F_{t + 1}$ satisfies for all $x$: 

$$
\begin{aligned}
    \eta_{t + 1} f(x)
    &\le 
    F_{t + 1}(x) = \eta_{t + 1} f(P_{t + 1} x) + \frac{1}{2}\Vert P_{t + 1}x - x \Vert^2 
    \le 
    \eta_{t + 1} f(x). 
\end{aligned}
$$

Continuing (\[*\]), we can simplify with the above and $F_0 = f$ so 

$$
\begin{aligned}
    f(x_{t + 1}) - f(x_*) &\le 
    \eta_{t + 1}^{-1} 
    \left(
        \prod_{j = 0}^{t} (1 + \beta_{t + 1})^{-1} 
    \right)(F_1(x_0) - f(x_*)), 
\end{aligned}
$$

where $\beta_{t+ 1} = \eta_{t + 1}\beta$ for all $t \in \Z_+$. 


**Comments**

From a theoretical point of view, this proof is not good and it's kinda cheating since it made use of the gradient descent interpretation and theory of monotone operator, while at the same time, reusing the same old convergence proof of gradient descent instead of innovating the old ideas. 

We try an alternative approach the addresses the above comment. 
The alternative proof doesn't use the Lipschitz smoothness of the Moreau envelope. 

**Alternative Proof**

With $\beta$-strongly convex $f$, let $\phi = \eta_{t + 1}f$, let $E_\phi(x)$ be $\min_u \{\eta_{t + 1} f(u) + (1/2)\Vert u - x\Vert^2\}$ which is the Moreau envelope of $\phi$, let $P_\phi := [I + \eta_{t + 1}\partial f]^{-1}$. 
For any $x_0$ let $x_{t + 1} = P_\phi(x_t)$, with $x_* \in \argmin{x} \phi(x)$, we realize these consequences:
1. $I - P_\phi$ is a $1 - (1 + \eta_{t + 1}\beta)^{-1}$ strongly monotone operator;
2. $\nabla E_\phi = I - P_\phi$, meaning that $E_\phi$ is $1 - (1 + \eta_{t + 1}\beta)^{-1}$ strongly convex;
3. $P_\phi$ is a contractive mapping with constant $(1 + \beta\eta_{t + 1})^{-1}$. 

Using strong convexity of the Moreau envelope it has 

$$
\begin{aligned}
    \frac{1}{2}\Vert \nabla E_\phi(x_t)\Vert^2 
    &\ge 
    (1 - (1 + \beta\eta_{t + 1})^{-1}) (E_\phi(x_t) - E_\phi(x_*))
    \\
    &\ge 
    (1 - (1 + \beta\eta_{t + 1})^{-1})\left(
        \frac{1}{2}(1 - (1 + \beta\eta_{t + 1})^{-1})
    \right)\Vert x - x_*\Vert^2
    \\
    &= 
    \frac{1}{2}\left(
        1 - (1 + \eta_{t + 1}\beta)^{-1}
    \right)^2 \Vert x_t - x_*\Vert^2
\end{aligned}
$$

invoke PPM descent inequality on the function $\phi$, it has 

$$
{\small
\begin{aligned}
    E_\phi(x_{t}) - \phi(x_*) - \frac{1}{2}\Vert x_t - x_*\Vert^2 
    &\le 
    -\frac{1 + \eta_{t + 1}\beta}{2}\Vert x_{t + 1} - x_*\Vert^2
    \\
    \iff 
    \phi(x_{t + 1}) - \phi(x_*)
    &\le 
    -\frac{1}{2}(1 + \eta_{t + 1}\beta)\Vert x_{t + 1} - x_*\Vert^2
    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 
    + \frac{1}{2}\Vert x_t - x_*\Vert^2
    \\
    \implies 
    \phi(x_{t + 1}) - \phi(x_*)
    &\le 
    -\frac{1}{2}(1 + \eta_{t + 1}\beta)\Vert x_{t + 1} - x_*\Vert^2
    - \frac{1}{2}(1 - (1 + \eta_{t + 1}\beta)^{-1})^2\Vert x_t - x_*\Vert^2
    + \frac{1}{2}\Vert x_t - x_*\Vert^2
    \\
    \iff
    \phi(x_{t + 1}) - \phi(x_*)
    &\le 
    -\frac{1}{2}(1 + \eta_{t + 1}\beta)\Vert x_{t + 1} - x_*\Vert^2
    + \frac{1}{2}\left(
        1 - (1 - (1 + \eta_{t + 1}\beta)^{-1})^2
    \right)
    \Vert x_t - x_*\Vert^2
    \\
    \implies
    \phi(x_{t + 1}) - \phi(x_*)
    &\le 
    -\frac{1}{2}(1 + \eta_{t + 1}\beta)\Vert x_{t + 1} - x_*\Vert^2
    \\
    & \quad 
    + \frac{1}{2}\left(
        1 - (1 - (1 + \eta_{t + 1}\beta)^{-1})^2
    \right)
    \left(
        \prod_{i = 1}^{t}
            (1 + \eta_i \beta)^{-1}
    \right)\Vert x_0 - x_*\Vert^2. 
\end{aligned}
}
$$

On the last inequality, we used the property of $P_\phi$ being a $(1 + \eta_{t + 1}\beta)^{-1}$ contractive mapping. 
Using the definition of $\phi$, we have 

$$
\begin{aligned}
    f(x_{t + 1}) - f(x_*)
    &\le 
    -\frac{1}{2\eta_{t + 1}}(1 + \eta_{t + 1}\beta)\Vert x_{t + 1} - x_*\Vert^2
    \\
    & \quad 
    + \frac{1}{2\eta_{t + 1}}\left(
        1 - (1 - (1 + \eta_{t + 1}\beta)^{-1})^2
    \right)
    \left(
        \prod_{i = 1}^{t}
            (1 + \eta_i \beta)^{-1}
    \right)\Vert x_0 - x_*\Vert^2, 
\end{aligned}
$$

and the convergence is asymptotically $\mathcal O\left(\prod_{i = 1}^{t} (1 + \eta_i\beta)^{-1}\right)$. 

**Comment:** 

The convergence rate is a bit too good to be true, considering upper bounding $(1/2)\Vert x_0 - x_*\Vert^2 \le (\eta_{t + 1}\beta)(f(x_t) - f(x_*))$ applied to the middle of the inequality to derive step-wise Lyapunov function, then we are in trouble. 

---
### **Analysis of Prox Convex Lower Bounding Function (Strongly Convex)**

In this section, we consider lower bounding functions that are strongly convex, in which case we hope to make use of proximal point method with strongly convex objective that has a constant $\mu$. 

#### **Theorem 4.1 | Approximated PPM under strong convexity**

---
### **Rockafellar's PPM Analysis**

Read [Convex Proximal Point Method, Part II](Convex%20Proximal%20Point%20Method,%20Part%20II.md) for more. 
