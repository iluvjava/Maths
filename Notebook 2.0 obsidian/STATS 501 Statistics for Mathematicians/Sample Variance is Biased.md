- [[Variance, Covariance]]
- [Sufficient Statistics](Sufficient%20Statistics.md)
- [Maximal Likelihood Estimator](Maximal%20Likelihood%20Estimator.md)

---
### **Intro**

Remember the Bessel's correction used for all sample variance where we divides by $n-1$ instead of $n$? Well, see [here](https://proofwiki.org/wiki/Bias_of_Sample_Variance) for an elementary demonstration for this phenomena. The sample variance is an estimaor for the variance of the original distribution. Biases are a properties for empirical estimator. In this case the sample variance $\hat \sigma^2$ is generated by $n$ i.i.d draws of samples $\{x_i\}_{i=1}^n$, the bias is computed via $\mathbb  E[\hat \sigma^2 - \sigma^2]$, since the true parameter is a constant, we just have $\mathbb E[\hat \sigma^2] - \sigma^2$. Next we demonstrate the bias for the variance estimator. 

**Demonstrations**

Define vector $\vec X = [X_1, \cdots, X_n]^T$ be a vector of the observe i.i.d sequence from the distribution. Let $\bar X$ be the sample mean. Define random variable vector $\vec C = \vec X - \bar X \mathbf 1$, observe that $\hat \sigma^2 = \mathbb E (1/n)\Vert \vec C\Vert_2^2$. In addition, define $\vec A = \vec X - \mu$ to be the deviation vector and $\vec B  = \bar X \mathbf 1 - \mu$, to be the sample mean deviation vector, which is collinear to the vector $\mathbf 1$. Directly from the variance estimator 

$$
\begin{aligned}
    \langle \vec A, \vec B\rangle
    &= 
    \langle
        \vec X - \mu \mathbf 1, 
        (\bar X - \mu) \mathbf 1
    \rangle
    \\
    &= 
    \langle \vec X, (\bar X - \mu)\mathbf 1\rangle
        - 
    \langle \mu \mathbf 1, (\bar X - \mu) \mathbf 1\rangle
    \\
    &= (\bar X - \mu)\langle \vec X, \mathbf 1\rangle - 
    \mu(\bar X - \mu)\langle \mathbf 1, \mathbf 1\rangle
    \\
    &= (\bar X - \mu) n \bar X - n \mu (\bar X - \mu)
    \\
    &= (\bar X - \mu)(n \bar X - n \mu)
    \\
    &= n(\bar X - \mu), 
\end{aligned}
$$

take note you might need your glasses to read whether it's a bar or an arror on top of $X$. With the expression for the inner product of the vector we have 

$$
\begin{aligned}
     \frac{1}{n}\Vert \vec C\Vert_2^2 &= 
    \frac{1}{n}\Vert A\Vert_2^2 + \frac{1}{n}\Vert B\Vert_2^2 
    - \frac{2}{n}\langle \vec A, \vec B\rangle
    \\
    &= \frac{1}{n}\sum_{i = 1}^{n} (X_i - \mu)^2 + 
    (\bar X - \mu)^2  -(2/n)n(\bar X - \mu) 
    \\
    &= \left(
        \frac{1}{n}\sum_{i = 1}^{n}(X_i - \mu)^2
    \right) - (\bar X - \mu)^2, 
\end{aligned}
$$

takingthe exected value we have on the LHS $\mathbb E \hat \sigma^2$, and on the RHS: 

$$
\begin{aligned}
    \frac{1}{n}\mathbb{E}\left[
        \sum_{i = 1}^{n} (X_i - \mu)^2
    \right] - 
    (\bar X - \mu)^2
    &= \frac{1}{n}\sum_{i = 1}^{n} \mathbb{E}\left[ 
            (X_i - \mu)^2
    \right] - \underbrace{\mathbb{E}\left[(\bar X - \mu)^2\right]}_{= \sigma^2/n}
    \\
    &= 
    \frac{1}{n}\sum_{i = 1}^{n} \mathbb{E}\left[ 
            (X_i - \mu)^2
    \right] - \sigma^2
    \\
    &= \frac{1}{n} \sum_{ i =1}^{n}\sigma^2 - \sigma^2=(1 - 1/n)\sigma^2 \neq \sigma^2, 
\end{aligned}
$$

and therefore, since the expected value of the estimator is not the same as the original parameter, the sample variance estimator is a biased estimator. At the end, we used the definition for the variance with central limit theorem to show that $\mathbb{E}\left[(\bar X - \mu)^2\right] = \sigma^2 / n$, adding to our assumptions, we have $n$ to be a singificantly large number for the central limit theorem, and the variance for the distribution where i.i.d $X_i$ are from must have finite amount of variance to it. 


**Remarks**

The above math only demonstrated the fact that, there exists a bias for the sample variance estimation without the Bessel's correction, however it remains to clear out the WHY part of the question, as to why this is the case, and what type of deeper intuitions one can seek in this phenomena. 

