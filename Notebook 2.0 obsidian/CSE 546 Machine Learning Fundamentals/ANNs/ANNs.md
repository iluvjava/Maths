[Auto Diff](../Auto%20Diff.md)

---
### **Intro**

Artificial neural network is a is a specific models that looks cool and has a lot of parameters. 
They were inspired by real neurons and they were never the same thing as the neurons in human brain. 
For mathematician, it's mostly safe to assume that it's a function mapping between $\mathbb R^n,\mathbb  R^m$. 
More precisely, and this is mixed with my own personal opinion: 

> Artificial Neural is one of the representation of some numerical algorithm with automatically differentiable parameters. 

There are many differentiable numerical algorithms and Artificial Neural Networks (ANNs) is one of them. 

#### **Elementary ANNs Computations Components**
1. Transformation by a function. 
2. Basic `+, -, /, *` with other numerical value generated by the algorithm.  
3. If statements that change the path of the computations. 
	1. Taking the maximum of minimum of a vectors, or choose any specific element from the vector according to some rules. 
	2. Dropping out certain computations based on some rules. 
4. Concatenating vectors. 

These operations can be combined and given names to form modules of high level numerical computations. 


---
### **Dual for ANNs**

It the atomic differentiable operations on numbers. 

#### **Def | Dual Number**
> A dual is a number in $\mathbb R^2$ that follows a specific computational rules. 
> These rules helps with differentiation. 
> They works with $(+, *)$ like in $\mathbb R$ and their inverses, additionally functional composition with a differentiable functions. 
> Let $(a, \epsilon), (b, \delta)$ be a hyper dual, We list the rules. 
> 1. $(a, \epsilon) * (b, \delta) = (ab, a\delta+b\epsilon)$. 
> 2. $(a, \epsilon) \pm (b, \delta) = (a \pm b, \delta \pm \epsilon)$. 
> 3. $(a, \epsilon)/(b, \delta) = (a/b, \epsilon/b - a/\delta^2)$.
> 
> The second component of the number is the differentiation. In general, let $f: \mathbb R \mapsto \mathbb R$ be a function such that it's differentiable at $a$, we have: $f((a, \epsilon)) = (f(a), f'(a)\epsilon)$. 
> A real number $x \in \mathbb R$ is itself a hyper dual and it's $(x, 1)$. 

**Notes**

It's like complex numbers with imaginary parts. 
A single hyper dual is the atomic differentiable number. 

#### **Examples | Directional Derivative via Differentiable Hyper Dual**
> Let $f(x) = \Vert x\Vert^2/2$ with $x \in \mathbb H^n$, the product space of $n$ hyper dual. 
> Then we can compute the directional derivative using the dual number. 

#### **Demonstrations | Directional Derivative**

Let's represent $x_i \in \mathbb H$ as $(x_i, \epsilon_i)$. 

$$
\begin{aligned}
    \Vert x\Vert^2/2 
    &= \sum_{i = 1}^{n} (x_i, \epsilon_i)^2
    \\
    &= \sum_{i = 1}^{n}
        \left(
            x_i, \epsilon_i \left[x\mapsto x^2/2\right]'(x_i)
        \right)
    \\
    &= \sum_{i = 1}^{n}(x_i, x_i\epsilon_i) 
    \\
    &= (f(x), \nabla f(x)). 
\end{aligned}
$$

Computations on a initial hyper dual gives the directional derivative the computation sequence. 

**Remarks**

To compute partial derivative wrt to $x_j$, we use dual $(x_j, 1)$ and $(x_i, 0)$ for all $i \neq j$. 




---
### **Neurons and Computational Architecture**

Neurons in neural networks is a concepualization of the underlying computations and it's almost never how deep learning library are implemented. 
Deep learning libraries usually convert the computations to matrix/tensor computations for memory efficiency. 
Neural networks has 3 type of neurons. 
Trainable parameters are a set of specific differentiable parameter. 

#### **Definition | Neurons**
> Neuron is an figurative representation of a computational process. 

**Notes**

Mathematician can visualize it as a vertex on a graph. 

#### **Classifications | Neurons**
1. Input neurons. Commonly, they withold a single numerical value in the form of a `float32` representing one sepcific number in the sample. 
2. Hidden neurons. 
   1. It has multiple channels for inputs. It has one channel for output. 
   2. It sums up all the inputs and add a bias term to it. 
   3. It has a single bias term to offset the sum of all the inputs. The bias term is a **trainable parameter**. 
   4. It passes the sum added with the bias into the **activation function** to produce an output. 
3. Output neurons. 
   1. It sums up inputs from multiple channel and passe it to a loss function to produce a single output. 

Mathematically, we can define $\text{nrn}(b): \mathbb R^n \mapsto \mathbb R$ to represent a single hidden neuron. 
Let $f: \mathbb R \mapsto \mathbb R$ be the activation function. 
Then the hidden neuron admits representation in the form

$$
\begin{aligned}
    \text{nrn}(x|b) = f\left(
        \left(
            \sum_{i = 1}^{n} x_i
        \right) + b
    \right). 
\end{aligned}
$$

The output neuron will have $f$ being some other type of functions. 
Usually output neurons have the identity function instead. 

#### **Definition | Connecting Arc Between Neurons**

Assuming our neural network only consists of $n_1, n_2$ and a single connection going from $n_1$ to $n_2$

A connection $(n_1, n_2)$ is an arc linking between two neurons: $n_1, n_2$. 
It has a trainable parameter $w \in \mathbb R$. 
Let $x \in \mathbb R$ represents the output of neuron $n_1$, let $w$ be a trainable parameter associated with an arc, then the arc takes the output from $n_1$, multiply it by weight $w$, and feed it as the intput into $n_2$. 
It admit representation: 

$$
\begin{aligned}
    n_2 = \text{nrn}(wx | c). 
\end{aligned}
$$

#### **Demonstration | Computational Model of a Single Neuron**

An ANN is a directed graph $G(V, A)$ and the edges are weights connecting between pair of neuron and the vertices are neurons. 
Then the output on neuron $v \in V$ has representation: 

$$
\begin{aligned}
    f\left(
        \left(
            \sum_{u\in \mathcal N^+(v)}^{}
            w_{v, u} x_u
        \right) + b_v
    \right). 
\end{aligned}
$$

Here, $\mathcal N^+(v)$ is the set of all arcs coming into the node $v$. 
Defined as $\mathcal N^+(v) := \{(u, v) \in E| u \in V \}$. 
$x_u$ represents the numerical output of the neuron $u$. 


**Remarks**

There are more than just weights that connects between different neurons. 
There are other modules and utility functions inside of a neuron networks. 
But each of those modules can be modeled as a node on the graph with arcs coming in and out. 
Additionally, neurons and connections doesn't represent the most elementary atomic computational process.

#### **Opinion | Alterantive Representations**
> Alternatively, neuron can use to represent any computation process that takes in some input, and outputs, and arcs between the neurons defines composition beteween these computations. 

---
### **Example | Multi-Layer Perceptron**

This is the most basic type of neural network. 

#### **Definition | MLP**
> Let $\mathcal L_i$, $i = 0, \cdots , (n + 1)$ be an ordered family of sets that contain neurons. 
> $\mathcal L_0$ contains all the input neurons. 
> $\mathcal  L_{n + 1}$ contains all the output neurons. 
> $\mathcal L_i$ are non-empty set and at least contains one neuron. 
> The connecting arcs only connects between neurons from $\mathcal L_{i}$ to $\mathcal L_{i + 1}$ for $i = 0, \cdots, n$. 

**Observations**

Between two adjacent layers, the neurons are fully connected. 
The directed connections between two sets of neurons can be represented as a matrix. 
The outputs, denoted as $x^{(i + 1)}$ of all neurons $\mathcal L_{i +1}$ can be represented as 

$$
\begin{aligned}
    
\end{aligned}
$$
