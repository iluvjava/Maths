[Auto Diff](../Auto%20Diff.md)

---
### **Intro**

<mark style="background: #FFB86CA6;">This is an introduction for people who already know about Neural Networks</mark>. 
Artificial neural network is a is a specific models that looks cool and has a lot of parameters. 
They were inspired by real neurons and they were never the same thing as the neurons in human brain. 
For mathematician, it's mostly safe to assume that it's a function mapping between $\mathbb R^n,\mathbb  R^m$. 
More precisely, and this is mixed with my own personal opinion: 

#### **Opinion | What is Artificial Neural Network**
> Artificial Neural is one of the representation of some numerical algorithm with automatically differentiable parameters. 

There are many differentiable numerical algorithms and Artificial Neural Networks (ANNs) are a subset of them. 

#### **Atomic ANNs Computations**

An atomic operator is operator applied to a single real number. 
These operations are associative and independent. 

1. Transformation by a function, $\mathbb R \mapsto \mathbb R$. 
2. Basic `+, -, /, *` with other numerical value generated by the algorithm.  
3. If statements that change the path of the computations. 
	1. Taking the maximum of minimum of a vectors, or choose any specific element from the vector according to some rules. 
	2. Dropping out certain computations based on some rules. 
4. Concatenating vectors. 

These operations can be combined and given names to form components that represent higher level of numerical computations. 

---
### **Notations**

$[\cdot](\cdot)$ represent a anonymous function. 
The square bracket encloses expression for functions, or an expression of form $x \mapsto ???$ or simply $\cdot \mapsto ???$ if there is only one variable involved. 
The bracket is the input variable of the function which can also be a number. 


---
### **Dual Numbers as the Atomic Foundations for ANNs**

A dual number can represent atomic operations a numerical algorithm that uses real numbers and as inputs and outputs while keep track of the numerical value of derivative wrt the dual number. 

#### **Def | Dual Number**
> A dual is a number in $\mathbb R^2$ that follows a specific computational rules. 
> These rules helps with differentiation. 
> They works with $(+, *)$ like in $\mathbb R$ and their inverses, additionally functional composition with a differentiable functions. 
> Let $(a, \epsilon), (b, \delta)$ be a dual, We list the rules. 
> 1. $(a, \epsilon) * (b, \delta) = (ab, a\delta+b\epsilon)$. 
> 2. $(a, \epsilon) \pm (b, \delta) = (a \pm b, \delta \pm \epsilon)$. 
> 3. $(a, \epsilon)/(b, \delta) = (a/b, \epsilon/b - a/\delta^2)$.
> 
> The second component of the number is the differentiation. In general, let $f: \mathbb R \mapsto \mathbb R$ be a function such that it's differentiable at $a$, we have: $f((a, \epsilon)) = (f(a), f'(a)\epsilon)$. 
> A real number $x \in \mathbb R$ is itself a dual and it's $(x, 1)$. 

**Notes**

A differentiable dual number is $(x, 1)$, and a constant that we are not interested in taking the derivative is $(x, 0)$. 

**Remarks**

Using these rules to compute numerical operations inside of a computer by representing it as a type in programming language, it can enable automatic differentiation without the autodiff algorithm. 
It's less efficient because we can only take derivatie wrt to a single variable each time (or equivalently, taking directional derivative, which we will demonstrate in the comming parts) and we have to repeat computations for all parameters when we want gradient wrt to all parameters. 
But hey, coordinate descent everyone? 

#### **Examples | Directional Derivative via Differentiable Dual**

> Let $f(x) = \Vert x\Vert^2/2$ with $x \in \mathbb H^n$, the product space of $n$ dual. 
> Then we can compute the directional derivative using the dual number. 

#### **Demonstrations**

Let's represent $x_i \in \mathbb H$ as $(x_i, \epsilon_i)$. 

$$
\begin{aligned}
    \Vert x\Vert^2/2 
    &= \sum_{i = 1}^{n} (x_i, \epsilon_i)^2
    \\
    &= \sum_{i = 1}^{n}
        \left(
            (x_i)^2/2, \epsilon_i \left[x\mapsto x^2/2\right]'(x_i)
        \right)
    \\
    &= \sum_{i = 1}^{n}(x_i^2/2, x_i\epsilon_i) 
    \\
    &= (f(x), \langle \nabla f, (\epsilon_1, \cdots, \epsilon_n)\rangle). 
\end{aligned}
$$

Computations on a initial dual gives the directional derivative the computation sequence. 

**Remarks**

To compute partial derivative wrt to $x_j$, we use dual $(x_j, 1)$ and $(x_i, 0)$ for all $i \neq j$. 


#### **Example | Basic Single Variable Chain Rule Tests**
> Compute $f(h(x)g(x)): \mathbb R \mapsto \mathbb R$ using generic dual number $(x, \epsilon)$. 

**Demonstrations**

$$
\begin{aligned}
    {[\cdot\mapsto f(\cdot)]} 
    \circ
    [\cdot\mapsto h(\cdot)g(\cdot)]((x, \epsilon))
    &= 
    [\cdot\mapsto f(\cdot)]((h(x), h'(x)\epsilon)* (g(x), g'(x)\epsilon))
    \\
    &=
    [\cdot\mapsto f(\cdot)]((h(x)g(x), \epsilon(h(x)g'(x)+ h'(x)g(x))))
    \\
    &= 
    [\cdot\mapsto f(\cdot)](([hg](x), [hg'+ h'g](x)\epsilon))
    \\
    &= (f([hg](x)), f'([hg](x))[hg'+ h'g](x)\epsilon)
    \\
    &=  ([f\circ[hg]](x), \epsilon[f'\circ[hg]](x)\; [hg'+ h'g](x)\epsilon). 
\end{aligned}
$$

And as you can see, this will get devilish quite fast. 

**Remarks**

In this case, because we are computing symbolically, we have to stack up the dual number. 
In actual numerical computations, the hyper dual are computed just like a number and the expression won't be this long. 
However, we will "lose" how we get to that number because we are not "hoarding" the operations. 




---
### **A More Abstract and Robust Representation for Computations in Neural Networks**

We construct the foundations of conceptualizing the computational process of Neural Network. 

#### **Def | A Component**
> A component is a function $f(x; p|w): \mathbb R^m \mapsto \mathbb R^n$. 
> $x$ is the inputs and $w$ represent trainable parameters, usually in the form of a multi-dimensional array. 
> And $p$ represents parameters that are not trainable parameters. 


#### **Def | A Connection**
> Let $f:\mathbb R^n \mapsto \mathbb R^m, g: \mathbb R^m \mapsto \mathbb R^k$ be two components, then a connection between is a $\mathbb R^m \mapsto \mathbb R^m$ function $h(x; p | w)$ with trainable parameters $w$, and parameter $p$. 

**Observation**

A connection is a special type of component. 

**Remarks**

The flexibility of this definition is much greater. 
The function doesn't have to be deterministic. 

#### **Proposition | Representing Artificial Neural Network**
> Let $G(V, A)$ it be a graph. 
> Let $v\in V$ be component, let $e \in A$ be connection. 
> Then it can represent a specific instance of artificial a neural network with a fixed inputs, and parameters. 


#### **Example | Fixed Artificial Neural Network**
> A neural network with a fixed input size and output size can be a component. 

#### **Example | Linear Dense Layer**
> Let $m, n \in \mathbb N$, let $A \in \mathbb R^{n\times m}$, $b \in \mathbb R^n$, then a Dense layer is a $\mathbb R^m \mapsto \mathbb R^n$ functions with a list of activation function $\sigma_i$ for $i = 1, \cdots, n$. 
> Let $x \in \mathbb R^m$ be the input, then a dense layer is a component. We define its computation: 
> $$
> \begin{aligned}
>     \text{DnsLyr}(x ; m, n | (A, b), \{\sigma_i\}_{i=1}^n) = 
>     \left[
>          z \mapsto \bigoplus_{i = 1}^n\sigma_i(z_i)
>     \right]
>     \left(
>         Ax + b
>     \right). 
> \end{aligned}
> $$
> Where, inside of $[\cdot]$, we denote the definition of a anonymous function. 

**Observations**

$\sigma_i$, are the parameters and $A, b$ are the trainable parameters. 


#### **Example | Multi-Layer Perceptron (Stacking Dnese Linear Layers)**
> Let $l_1, l_2, \cdots, l_N$ be integers. 
> We define the Multi-Layer Perceptron to be a composition of dense layer mapping from $l_{i}$ to $l_{i + 1}$ for $i = 1, \cdots, N - 1$. 
> Let $\sigma_{i, j}$ represent the activation function for the $j$ th output in the $i$ th layer. 
> Then a Multi-Layer Perceptron (MLP) is a component admit representation
> $$
> \begin{aligned}
>     & \text{MLP}\left(x ; l_1, \cdots, l_n | \{(A_i, b_i)\}_{i=1}^N\right): \mathbb R^{l_1} \mapsto \mathbb R^{l_N}
>     \\
>     :=&
>     \left[
>     \bigodot_{i = 1}^n \text{DnsLyr}
>     \left(
>         (\cdot) ; l_i, l_{i + 1} \left| (A_i, b_i), \{\sigma_{j, i}\}_{j=1}^{l_i} \right.
>     \right)
>     \right](x). 
> \end{aligned}
> $$
> Where $\bigodot$ is functional composition and it represents $\bigodot_{i=1}^n f_i = f_n\circ\cdots\circ f_1(x)$, and $(\cdot)$ represents the input of the anonymous function, in this case it's the dense layer. 

**Observations**

We haven't add the output layer with the loss function yet. 
This neural network has head and body and doesn't have a tail. 

