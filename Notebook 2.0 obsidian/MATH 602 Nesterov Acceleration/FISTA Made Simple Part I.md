- [A Better Proof for FISTA Convergence](../AMATH%20516%20Numerical%20Optimizations/Proximal%20Methods/A%20Better%20Proof%20for%20FISTA%20Convergence.md)

---
### **Intro**

We improve upon the previous proof of FISTA and make it simpler. 
In fact, we make it so simple it's borderline crazy. 
We assume the following optimization problem: 
$$
\begin{aligned}
    \min_{x \in \R^n}\{F(x):= f(x) + g(x)\}. 
\end{aligned}
$$

Unless specified, the following are assumed throughout. 

#### **Assumptions Set 1**
1. $f$ is a differentiable function with $L$ Lipschitz gradient, and it's convex. 
2. $g$ is a convex function. 
3. $\argmin{x}{F(x)} \neq \emptyset$. 

This is a description of the three points algorithm. 


#### **Definition | proximal gradient operator**
> Let $F = f + g$ where $f$ is differentiable, $L$ Lipschitz smooth. 
> $g$ is convex. 
> Define the proximal gradient operator: 
> $$
> \begin{aligned}
>     T_Lx = \argmin{z \in \R^n }
>     \left\lbrace 
>         g(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{L}{2}\Vert z - x\Vert^2
>     \right\rbrace. 
> \end{aligned}
> $$

This is a single-valued operator. 

Proved in [Proximal Gradient Inequality Part I](../AMATH%20516%20Numerical%20Optimizations/Proximal%20Methods/Proximal%20Gradient%20Inequality%20Part%20I.md) is the following theorem: 

#### **Theorem | Proximal gradient inequality**
> Assume that $F = f + g$ where $f$ is $L$ Lipschitz smooth and $\mu \ge 0$ convex. 
> Then, for $y \in \R^n, \bar y = T_Ly$ it has the following: 
> $$
> \begin{aligned}
>     (\forall x \in \R^n)\quad 
>     0 &\le 
>     F(x) - F(\bar y) - \langle L(y - \bar y), x - y\rangle
>     - \frac{\mu}{2}\Vert x - y\Vert^2
>     - \frac{L}{2}\Vert y - \bar y\Vert^2. 
>     \\
>     &= F(x) - F(Ty) - \frac{L}{2}\Vert x - Ty\Vert^2  + \frac{L - \mu}{2}\Vert x - y\Vert^2. 
> \end{aligned}
> $$

#### **Corollary | Function Descent**
> Assume that $F = f + g$ where $f$ is $L$ Lipschitz smooth and $\mu \ge 0$ convex. 
Then, for $y \in \R^n, \bar y = T_Ly$ it has the following: 
> $$
> \begin{aligned}
>     0 &\le 
>     F(y) - F(Ty) - \frac{L}{2}\Vert y - Ty\Vert^2. 
> \end{aligned}
> $$


---
### **Convergence rate accelerated gradient algorithm**

Let's take a look at this basic convergence rate for the Accelerated Proximal Gradient method Proposed in the previous section. 


#### **Algorithm | Accelerated proximal gradient**
> Let $(\alpha_k)_{k \ge 0}$ be a sequence in $\R$. 
> Initialize with $(x_0, v_0)$. 
> For $k = 1, \ldots$ updates: 
> $$
> \begin{aligned}
>     y_{k} &= \alpha_{k} v_{k - 1} + (1 - \alpha_{k}) x_{k - 1}. 
>     \\
>     x_k &= T_Ly_k, 
>     \\
>     v_k &= x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}).
> \end{aligned}
> $$


#### **Theorem | convergence rate of accelerated proximal gradient**
> Let $(\alpha_k)_{k \ge 0}$ be a sequence in $\R$ such that $\alpha_k\in (0, 1)$ for all $k \ge 1$, and $\alpha_0 \in (0, 1]$. 
> Define $(\rho_k)_{k \ge 0}$ to be such that $\rho_k = \alpha_{k + 1}^2\alpha_k^{-2}(1 - \alpha_{k + 1})^{-1}$. 
> Let $(x_k, v_k, y_k)_{k \ge0}$ be the sequence generated by Accelerated Proximal Gradient algorithm. 
> Let $x^+$ be a minimizer of $F$, then it has for all $k \ge 1$: 
> $$
> {\small\begin{aligned}
>     F(x_k) - F(x^+) + 
>     \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
>     \le 
>     \left(
>         \prod_{i = 0}^{k - 1}(1 - \alpha_{i + 1})\max(\rho_{i}, 1)
>     \right)
>     \left(
>         F(x_{0}) - F(x^+)
>         + \frac{L\alpha_{0}^2}{2} \Vert x^+ - v_{0}\Vert^2 
>     \right). 
> \end{aligned}}
> $$
> In addition, if $v_0 = x_0 = T_Lx_{-1} \in \text{dom}\; F$, $\alpha_0 = 1$ were used as initial condition, then the inequality simplifies to 
> $$
> \begin{aligned}
>     F(x_k) - F(x^+) + \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
>     &\le 
>     \left(
>         \prod_{i = 0}^{k - 1}(1 - \alpha_{i + 1})\max(\rho_{i}, 1)
>     \right) \frac{L\alpha_0^2}{2}\Vert x^+ - x_{-1}\Vert^2. 
> \end{aligned}
> $$

**Proof**

Suppose $x^+$ exists as a minimizer of $F$. 
Define $z_k = \alpha_k x^+ + (1 - \alpha_k)x_{k - 1}$. 
It can be verified that: 
$$
\begin{aligned}
    z_k - x_k &= \alpha_k(x^+ - v_k),
    \\
    z_k - y_k &= \alpha_k(x^+ - v_{k - 1}). 
\end{aligned}
$$

To verify, consider the definition of the algorithm: 

$$
\begin{aligned}
    z_k - x_k &= 
    \alpha_k x^+ + (1 - \alpha_k)x_{k - 1} - x_k
    \\
    &= \alpha_kx^+ + (x_{k - 1} - x_k) - \alpha_kx_{k - 1}
    \\
    &= \alpha_k x^+ - \alpha_k v_k, 
    \\
    z_k - y_k &= 
    \alpha_k x^+ + (1 - \alpha_k)x_{k - 1} - y_k
    \\
    &= \alpha_k x^+ + ((1 - \alpha_k)x_{k - 1} - y_k)
    \\
    &= \alpha_kx^+ - \alpha_k v_{k - 1}. 
\end{aligned}
$$

Suppose that the sequence satisfy $\rho_{k - 1}(1 - \alpha_k) = \alpha_k^2/\alpha_{k - 1}^2$ for all $k \ge 1$, and $\alpha_0 \in (0, 1)$. 
Using the proximal gradient inequality and convexity of $F$ it has for all $k \ge 1$

$$
{\small
\begin{aligned}
    0 
    &\le F(z_k) 
    - F(x_k) - \frac{L}{2}\Vert z_k - x_k\Vert^2 + 
    \frac{L}{2}\Vert z_k - y_k\Vert^2
    \\
    &\le 
    \alpha_k F(x^+) + (1 - \alpha_k) F(x_{k - 1}) - F(x_k)
    + \frac{L\alpha_k^2}{2}
    \left(
        \Vert x^+ - v_{k - 1}\Vert^2 - \Vert x^+ - v_k\Vert^2
    \right)
    \\
    &= 
    (\alpha_k - 1)F(x^+) + F(x^+) + (1 - \alpha_k) F(x_{k - 1}) - F(x_k)
    + \frac{L\alpha_k^2}{2} \Vert x^+ - v_{k - 1}\Vert^2 
    - \frac{L\alpha_k^2}{2} \Vert x^+ - v_k\Vert^2 
    \\
    &= 
    (1 - \alpha_k)(F(x_{k - 1}) - F(x^+))
    + F(x^+) - F(x_k)
    + \frac{L\alpha_k^2}{2} \Vert x^+ - v_{k - 1}\Vert^2 
    - \frac{L\alpha_k^2}{2} \Vert x^+ - v_k\Vert^2 
    \\
    &= 
    \left(
        (1 - \alpha_k)(F(x_{k - 1}) - F(x^+))
        + \frac{L\alpha_k^2}{2} \Vert x^+ - v_{k - 1}\Vert^2 
    \right)
    -
    \left(
        F(x_k) - F(x^+) + 
        \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \right)
    \\
    &= 
    \left(
        \frac{\alpha_k^2}{\alpha_{k - 1}^2 \rho_{k - 1}}(F(x_{k - 1}) - F(x^+))
        + \frac{L\alpha_k^2}{2} \Vert x^+ - v_{k - 1}\Vert^2 
    \right)
    -
    \left(
        F(x_k) - F(x^+) + 
        \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \right)
    \\
    &= \frac{\alpha_k^2}{\alpha_{k - 1}^2\rho_{k - 1}}
    \left(
        (F(x_{k - 1}) - F(x^+))
        + \frac{L\alpha_{k - 1}^2\rho_{k - 1}}{2} \Vert x^+ - v_{k - 1}\Vert^2 
    \right)
    - 
    \left(
        F(x_k) - F(x^+) + 
        \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \right)
    \\
    &\le 
    \frac{\alpha_k^2}{\alpha_{k - 1}^2\rho_{k - 1}}
    \left(
        F(x_{k - 1}) - F(x^+)
        + \frac{L\alpha_{k - 1}^2\max(\rho_{k - 1}, 1)}{2} \Vert x^+ - v_{k - 1}\Vert^2 
    \right)
    - 
    \left(
        F(x_k) - F(x^+) + 
        \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \right)
    \\
    &\le 
    \left(
        \frac{\alpha_k^2}{\alpha_{k - 1}^2\rho_{k - 1}}
    \right)\max(\rho_{k - 1}, 1)
    \left(
        F(x_{k - 1}) - F(x^+)
        + \frac{L\alpha_{k - 1}^2}{2} \Vert x^+ - v_{k - 1}\Vert^2 
    \right)
    - 
    \left(
        F(x_k) - F(x^+) + 
        \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \right)
    \\
    &= 
    \left(
        1 - \alpha_k
    \right)\max(\rho_{k - 1}, 1)
    \left(
        F(x_{k - 1}) - F(x^+)
        + \frac{L\alpha_{k - 1}^2}{2} \Vert x^+ - v_{k - 1}\Vert^2 
    \right)
    - 
    \left(
        F(x_k) - F(x^+) + 
        \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \right)
    \\
    &\le 
    (1 - \alpha_k)\max(\rho_{k - 1}, 1)(1 - \alpha_{k - 1})\max(\rho_{k - 2}, 1)
    \left(
        F(x_{k - 2}) - F(x^+)
        + \frac{L\alpha_{k - 2}^2}{2} \Vert x^+ - v_{k - 2}\Vert^2 
    \right)
    \\
    &\quad 
    - 
    \left(
        F(x_k) - F(x^+) + 
        \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \right)
    \\
    & \cdots
    \\
    &\le 
    \left(
        \prod_{i = 0}^{k - 1}(1 - \alpha_{i + 1})\max(\rho_{i}, 1)
    \right)
    \left(
        F(x_{0}) - F(x^+)
        + \frac{L\alpha_{0}^2}{2} \Vert x^+ - v_{0}\Vert^2 
    \right)
    - 
    \left(
        F(x_k) - F(x^+) + 
        \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \right). 
\end{aligned}
}
$$

Let $x_0 = v_0 = T_Lx_{-1}$ using the proximal gradient again then it has 

$$
\begin{aligned}
    0 &\le F(x^+) - F(T_Lx_{-1}) - \frac{L}{2}\Vert x^+ - T_L x_{-1} \Vert^2
    + \frac{L}{2}\Vert x^+ - x_{-1}\Vert^2
    \\
    &= F(x^+) - F(x_0) - \frac{L}{2}\Vert x^+ - v_0 \Vert^2
    + \frac{L}{2}\Vert x^+ - x_{-1}\Vert^2. 
\end{aligned}
$$

Because $\alpha_0 = 1$, it simplifies the previous inequality and it is now: 

$$
\begin{aligned}
    0 &\le 
    \left(
        \prod_{i = 0}^{k - 1}(1 - \alpha_{i + 1})\max(\rho_{i}, 1)
    \right)
    \frac{L}{2}\Vert x^+ - x_{-1}\Vert^2
    - 
    \left(
        F(x_k) - F(x^+) + 
        \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \right). 
\end{aligned}
$$

$\blacksquare$

**Remarks**

This proof is inspired by the approach in the Catalyst Paper. It's our belief that it can be improved further. 
It doesn't use any inner product making which is fascinating. 

Observe further that, if $x_0 \not \in \text{dom}\;F$, then the inequality has $\infty$ on the RHS. 
This is not a huge deal and the next corollary will deal with this. 




#### **Discussion**

Let's talk about convergence of quantities other than the objective function $F$. 
With the assumption that $g \equiv 0$, it has $F = f$. 
Using the Lipschitz continuity of the gradient it has the inequality: 

$$
\begin{aligned}
    \alpha_k^2\left(
        f(x_0) - f^+
    \right) \ge 
    f(x) - \inf_{x} f(x) &\ge \frac{1}{2L}\Vert \nabla f(x)\Vert_\star^2. 
\end{aligned}
$$

There are more convergence claims in the literatures. 
They will hopefully come soon. 

---
### **Convergence of Gradient Mapping**

Some extra assumption is required to derive the convergence rate for the FISTA algorithm proposed in the previous section. 
With some additional assumption, we can show that the convergence of the gradient mapping which we defined it to be: 

$$
\begin{aligned}
    G_L x &= L(x - T_L x). 
\end{aligned}
$$

#### **Theorem | Convergence of the gradient mapping (RESULTS SEEM INCORRECT)**
> Suppose that $F = f + g$ satisfies Assumption Set 1. 
> $L$ is Lipschitz smooth constant for $f$. 
> Let $(\alpha_k)_{k \ge 1}$ be a sequence such that $1 - \alpha_{k + 1} = \alpha_{k + 1}^2\alpha_k^{-2}\rho_k^{-1}\; \forall k \ge 0$. 
> Let $\alpha_0 = 1$.
> Then the iterates generated by the Accelerated proximal Gradient Algorithm initialized using $v_0 = x_0 = T_{L}x_{-1}$ where $x_{-1}$ will satisfy for all minimizer $x^+$ that: 
> $$
> \begin{aligned}
>    &\beta_k := \prod_{i = 0}^{k - 1}(1 - \alpha_{i + 1})\max(1, \rho_i), \beta_0 := 1, 
>    \\
>    & \Vert \mathcal G_L(y_k)\Vert \le
>    \sqrt{\beta_k}L\left(
>        1 + 
>        \min\left(1, \rho^{-1/2}_{k - 1}\right)
>    \right)\Vert x^+ - x_{-1}\Vert. 
> \end{aligned}
> $$

**Proof**


We will summarize the conditions used to derive the results first and then prove them later. 

(**Condition I**): For all $k \ge 1$, $\mathcal G_L(y_k) = L\alpha_k(v_k - v_{k - 1})$, from the definition of the algorithm. 
(**Condition II**): It has for all $k \ge 0$ that $\alpha_k/\sqrt{\beta_k} \Vert x^+ - v_k\Vert \le \Vert x^+ - x_{-1}\Vert$.
(**Condition III**): From the definition of $\beta_k$, it's not hard to show $\beta_{k + 1}(1 - \alpha_{k + 1})^{-1} = \max(1, \rho_k)\beta_k$ for all $k\ge 0$. 

The proof now follows. 
It has for all $k \ge 1$: 
$$
\begin{aligned}
    \Vert \mathcal G_L y_k\Vert
    &= 
    L\alpha_k\Vert v_{k} - v_{k - 1}\Vert
    \\
    &\le 
    L\alpha_k \left(
        \Vert v_{k} - x^+\Vert + \Vert v_{k - 1} - x^+\Vert
    \right)
    \\
    &= L\alpha_k\left(
        \frac{\sqrt{\beta_{k}}}{\alpha_{k}} + 
        \frac{\sqrt{\beta_{k - 1}}}{\alpha_{k - 1}}
    \right)\Vert x^+ - x_{-1}\Vert
    \\
    &= 
    L\left(
        \sqrt{\beta_{k}} + 
        \frac{\alpha_k\sqrt{\beta_{k - 1}}}{\alpha_{k - 1}}
    \right)\Vert x^+ - x_{-1}\Vert
    \\
    &= 
    L\left(
        \sqrt{\beta_{k}} + 
        \sqrt{\beta_{k - 1}\rho_{k - 1}(1 - \alpha_k)}
    \right)\Vert x^+ - x_{-1}\Vert
    \\
    &= L\left(
        \sqrt{\beta_{k}} + 
        \sqrt{\beta_{k - 1}\max(\rho_{k - 1}, 1)^{-1}\max(\rho_{k - 1}, 1)\rho_{k - 1}(1 - \alpha_k)}
    \right)\Vert x^+ - x_{-1}\Vert
    \\
    &= 
    L\left(
        \sqrt{\beta_{k}} + 
        \sqrt{\beta_{k}\rho_{k - 1}\max(\rho_{k - 1}, 1)^{-1}}
    \right)\Vert x^+ - x_{-1}\Vert
    \\
    &=
    \sqrt{\beta_k}L\left(
        1 + 
        \sqrt{\frac{\rho_{k - 1}}{\max(\rho_{k - 1}, 1)}}
    \right)\Vert x^+ - x_{-1}\Vert
    \\
    &= 
    \sqrt{\beta_k}L\left(
        1 + 
        \min\left(1, \rho^{-1/2}_{k - 1}\right)
    \right)\Vert x^+ - x_{-1}\Vert. 
\end{aligned}
$$

**Proving (Condition I)**. 
From the update of $y_k$, it has 
$$
\begin{aligned}
    y_k &= \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}
    v_{k - 1} = \alpha_k^{-1}(y_k - (1 - \alpha_k)x_{k - 1}). 
\end{aligned}
$$

Hence, it has for all $k \ge 0$

$$
\begin{aligned}
    v_k - v_{k - 1} &= 
    (x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1})) - \alpha_k^{-1}(y_k - (1 - \alpha_k)x_{k - 1})
    \\
    &= 
    x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1})
    - \alpha_k^{-1}y_k + (\alpha_k^{-1} - 1)x_{k - 1}
    \\
    &= \alpha_k^{-1}(x_k - x_{k - 1}) - \alpha_k^{-1}y_k + \alpha_k^{-1} x_{k - 1}
    \\
    &= \alpha_k^{-1}x_k - \alpha_k^{-1} y_k 
    = \alpha^{-1}_k(x_k - y_k) = \alpha_k^{-1}(T_L y_k - y_k)
    \\
    &= -\alpha_k^{-1}L^{-1} \mathcal G_L(y_k). 
\end{aligned}
$$


**Proving (Condition II)**. 
Let's consider the base case where $k = 0$. 
The proximal inequality has 
$$
\begin{aligned}
    0 &\le F(x^+) - F(T_Lx_{-1}) - \frac{L}{2}\Vert x^+ - T_Lx_{-1}\Vert^2 + \frac{L}{2}\Vert x^+ - x_{-1}\Vert^2
    \\
    &\le \frac{L}{2}
    \left(
        -\Vert x^+ - T_L x_{-1}\Vert^2 + \Vert x^+ - x_{-1}\Vert^2
    \right)
    \\
    &= \left(
        -\Vert x^+ - v_0\Vert^2 + \Vert x^+ - x_{-1}\Vert^2
    \right). 
\end{aligned}
$$

Using the fact that $\alpha_0 = \beta_0 = 1$. 
The inequality $\alpha_k/\sqrt{\beta_k}\Vert x^+ - v_k\Vert \le \Vert x^+ - x_{-1}\Vert$ holds for $k = 0$.
From the convergence rate of the accelerated proximal gradient algorithm, it has for all $k \ge 1$ giving 
$$
\begin{aligned}
    0 &\le \frac{L\beta_k }{2}\Vert x^+ - x_{-1}\Vert^2 
    - F(x_k) + F(x^+) - \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \\
    &\le 
    \frac{L\beta_k }{2}\Vert x^+ - x_{-1}\Vert^2 
    - \frac{L\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
    \\
    &= \frac{\alpha_k^2L}{2}\left(
        \frac{\beta_k}{\alpha_k^2}
        \Vert x^+ - x_{-1}\Vert^2 
        - \Vert x^+ - v_k\Vert^2
    \right)
    \\
    \iff 
    0 &\le 
    \Vert x^+ - x_{-1}\Vert - \frac{\alpha_k}{\sqrt{\beta_k}}\Vert x^+ - v_k\Vert. 
\end{aligned}
$$



**Remarks**

Convergence of the gradient mapping is still, entirely based on the original proof of the convergence in function value. 
It didn't show the convergence when the objective function doesn't necessarily have a minimizer.


