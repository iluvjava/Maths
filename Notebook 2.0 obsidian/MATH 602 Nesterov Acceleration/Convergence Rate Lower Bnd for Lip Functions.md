---
alias: Nesterov Lower Convergence Bounds for Lipschitz Smooth Functions

---
### **Intro**

Yurri Nesterov in his "Lectures in Convex Optimizations", had proposed a lower bound on the convergence rate of iterates and objective values, of first order, generic method, and the class of [convex](CVX%20Analysis/Convex%20Functions%20CENTRAL%20HUB.md), [Lipschitz Smooth](../AMATH%20516%20Numerical%20Optimizations/Properties%20of%20Functions/Convex%20Function%20with%20Global%20Lipschitz%20Gradient.md) functions. I go over his proof, in excruciating details. The goal is to uncover some the devils in the details and demystify his myth. 

---
### **GA1st Class of Algorithm**

We introduce a class of algorithm. 

#### **Def | Class of GA1st**
> We are in $\mathbb R^n$ for now. Given $x^{(0)} \in \mathbb R^n$, an iterative algorithm generates sequence of $\left(x^{(n)}\right)_{n \in \mathbb N}$ in the space. All $\mathcal A \in \text{ GA}^1$ satisfy that 
> $$
> \begin{aligned}
>         x^{(j + 1)}:= \mathcal A_f^{j + 1}x^{(0)} \in \left\{x^{(0)}\right\} + 
>     \text{span}\left\{\nabla f\left(x^{(i)}\right)\right\}_{i = 1}^{j - 1} \quad \forall f, \forall 1\le j \le k -1
> \end{aligned}
> $$

**Examples**

The class of GA1st method, would includes algorithm such as 
1. Conjugate Gradient,
2. Quasi-Newton, 
3. Gradient Descent and Gradient descent with Momentum. 


**Observations**



**Remarks**

GA1st means, Generic Algorithm First Order. 


#### **Thm 2.1.7 | A Lower Convexity Bounds of GA1st in Lipschitz Smooth Convex Function**

> For any $1\le k \le 1/2(n - 1)$, for all $x^{(0)}\in \mathbb R^n$, there exists a Lipschitz smooth convex function in $\mathbb R^n$ such that for all algorithm from GA1st, we have the lower bound for the optimality gap for the function values and its iterates: 
>
> $$
> \begin{aligned}
>   f\left(x^{(k)}\right) - f^* \ge 
>   \frac{3L \Vert x - x^*\Vert^2}{32(k + 1)^2}, \quad \Vert x^{(k)} - x^*\Vert^2 \ge \frac{1}{8} \Vert x^{(0)} - x^*\Vert^2. 
> \end{aligned}
> $$
> Where $x^*$ is the minimizer of $f$, so that $f(x^*) = \inf_{x}f(x)$. 

**Observations**

The theorem says, for all $1\le k \le 1/2(n + 1)$ and initial guess $x^{(0)}$ first, before quantifying a function the gives the lower bound. For a different value of $k$, the functions that provides us the above a lower bound, they could be different. More precisely let $\mathcal F^{1, 1}_L$ denotes $L$-Lipschitz smooth, differentiable function, we have 

$$
\begin{aligned}
    \forall\; 1\le k \le \frac{n + 1}{2}, x^{(0)} \in \mathbb R^n\; 
    \exists f \in \mathcal F^{1, 1}_L \text{ s.t: }
    \min_{A\in \text{GA}^1} 
    \left\lbrace
        f\left(A_f^k x^{(0)}\right)
    \right\rbrace - f^* 
    &\ge 
    \frac{3L \Vert x^{(0)} - x^*\Vert^2}{32(k + 1)^2}
    \\
    \forall\; 1\le k \le \frac{n + 1}{2}, x^{(0)} \in \mathbb R^n
    \quad
    \max_{f\in \mathcal F_L^{1,1}}
    \min_{A\in \text{GA}^1} 
    \left\lbrace
        f\left(A_f^k x^{(0)}\right) - f^*  
    \right\rbrace 
    &\ge 
    \frac{3L \Vert x^{(0)} - x^*\Vert^2}{32(k + 1)^2}
    \\
    \forall
    x^{(0)} \in \mathbb R^n \quad 
    \min_{1 \le k \le 1/2 (n + 1)}
    \max_{f\in \mathcal F_L^{1, 1}}
    \min_{ A \in GA^1}
    \left\lbrace
        f\left(A_f^kx^{(0)}\right) - f^*
    \right\rbrace
    & \ge 
    \frac{3L \Vert x^{(0)} - x^*\Vert^2}{32(1/2(n + 1))^2}, 
\end{aligned}
$$

when phrased in English, it's saying that 
> for all initial guess and the first $1/2(n + 1)$ iterations, there is a always a Lipschitz smooth convex function with gradient oracle that is bad enough such that for all first order algorithm GA1st, its kth iterates won't achieve optimality in function value for a certain amount, as shown by the monstrosity on the RHS.

We will prove it in later sections before developing an instance of function that is Lipschitz smooth and convex. That function will assist us to establish the lower bounds of the convergence rates for all algorithm of GA1st class, at certain iteration. 

---
### **Observations and Preparations for the proof**

The following content is essential for the proof for the above theorem introduced in the previous section. 

#### **Claim 1 | Shifts Invariance of GA1st**
> Any $(x^{(k)})_{k \in \mathbb N}$, with $x^{(0)} \in \mathbb R^n$, has $(x^{(k)} - x^{(0)})_{n \in \mathbb Rn}$ generated by unique another algorithm from the class of GA1st, initialized with $x^{(0)} = \mathbf 0$. Therefore, for every algorithm, it suffice to assume that $x^{(0)} = \mathbf 0$. 

**Proof**

Let $x^{(k)}$ be generated by an algorithm from GA1st, with any $x^{(0)} \in \mathbb R^n$, then according to the definition of class GA1st, for all $k \in \mathbb N$, we have $x^{(k)} - x^{(0)} \in \text{span}\{\nabla f(x^{(i)})\}_{i = 1}^{k - 1} = \{\mathbf 0\} + \text{span}\{\nabla f(x^{(i)})\}_{i = 1}^{k - 1}$. Hence, the shifted sequence is generated by another algorithm from class GA1st such that its initial value is $x^{(0)} = \mathbf 0$. 

### **Def | A Function Based on TST Matrices**
> Define $f_k$ to be a Lipschitz smooth function with smoothness constant $L$, has gradient, and in $\mathbb R^n$. The function is given by $f(x):= L/4(1/2 \langle P_k x, AP_k x\rangle - x_1)$, where $A \in \mathbb R^{n \times n}$ is $\text{tridiag}(-1, 2, -1)$, and $P_k = [\e_1\; \e_2\; \cdots \; \e_k \; \mathbf 0 \; \cdots \mathbf 0]$. 

**Observations**

We make observations about the following entities
1. $P_kA$. 
2. $\nabla f_k, \nabla ^2 f_k$. 
3. The minimizers and the minimum of $f_k$. 
4. Some other miscellaneous facts.
5. An equality between $f_k, f_p$ for $n\ge p\ge k$. 

**The Matrix**

The matrix $P_kA$ is a $\mathbb R^{n\times n}$ matrix such that the upper $k\times k$ matrix is a Tridiagonal Teoplitz matrix (TST) with $-1$ on its sub and super diagonal, and $2$ on its diagonal. The non-zero eigenvalue of $P_k A$ would be a subset of the eigenvalues for the top left $k \times k$ corner matrix. For any TST matrix, its eigenvalues are given by $\lambda_k = 2 - 2\cos\left(\frac{k \pi}{n + 1}\right)$ (See [The TST Matrix](../AMATH%20585%20Numerical%20Analysis%20of%20BVP/The%20TST%20Matrix.md) for more coverage and proofs). Meaning that the spectrum of matrix $P_k A$ is within $[0, 4]$, a Positive Semi-Definite matrix. 

**The Gradient and Hessians**

Observe that $\nabla f_k (x) = L/4(P_k^TAP_kx - \e_1)$, and the Hessian $\nabla f_k(x) = L/4P_k^TAP_k$. Observe that the Hessian would be positive semi-definition with $L$ being the upper bound for its spectrum. Finally, we denote $A_k = P_k A$ for notational convenience for the future. 

**The minimizers and the Minimum of the Function**

$f$ is a quadratic function with minimizers: 

$$
\begin{aligned}
    \bar x^{[k]} = \frac{1}{k + 1}
    \begin{bmatrix}
        k & k -1 & k - 2 & \cdots & 1 & 0 &  0 & \cdots & 0 
    \end{bmatrix} \in \mathbb R^n, 
\end{aligned}
$$

compactly we have $\bar x^{[k]}_i = (1 - i/(k + 1))$ for all $1 \le i \le k$, and $\bar x^{[k]} = 0$ for all $k + 1 \le i \le n$. We observe the fact that this minimizer is not the unique minimizer for $f_k$, and further more, $\bar x^{[k]} \in \text{span}\{e^{(i)}\}_{i = 1}^k$. The reader should verify that this is indeed one of the minimizers for $f_k$. Next, by the definition of $f_k$, the minimum $f_k^*$ would be 

$$
\begin{aligned}
    f_k^* &= \frac{L}{4}\left(
        \frac{1}{2} 
        \left\langle 
            \bar x^{[k]}, A_k \bar x^{[k]}
        \right\rangle - \bar x^{[k]}
    \right)
    \\
    & \textcolor{gray}{
        \nabla f_k(\bar x^{[k]}) = A_k \bar x^{[k]} - \e_1 = \mathbf 0 \implies 
        A_k \bar x^{[k]} = \e_1
        }
    \\
    &= \frac{L}{4}\left(
            \frac{1}{2} 
            \left\langle 
                \bar x^{[k]}, \e_1
            \right\rangle - \bar x^{[k]}
        \right)
    \\
    &= \frac{L}{4}
    \left(
        \frac{1}{2}\bar x^{[k]} - \bar x_1^{[k]}
    \right) = \frac{-L}{8}\bar x_1^{[k]} = \frac{-L}{8} \frac{k}{k + 1}. 
\end{aligned}
$$

This fact is listed as \[2.1.17\] in Nesterov's Lecture in Convex Optimizations Textbook. 

**The sum of Squared Integers**

The following identity is a fact 

$$
\begin{aligned}
    \sum_{i = 1}^{k} i^2 = \frac{(k (k + 1)(2k + 1))}{6} \le \frac{(k + 1)^3}{3}, 
\end{aligned}
$$

and therefore, the norm of the minimizer can be bounded as 

$$
\begin{aligned}
    \left\Vert
        \bar x^{[k]}
    \right\Vert^2 = 
    \frac{1}{(k + 1)^2} \sum_{i = 1}^{k} i^2 
    \le 
    \frac{1}{(k + 1)^2} \frac{(k + 1)^3}{3} = \frac{k + 1}{3}. 
\end{aligned}
$$
The above is listed as \[2.1.19\] in Nesterov book, Lectures on Convex Optimizations. 


**The Equivalences Between This Class of Functions**

Observe that, $f_k(x) = f_p(P_kx)$ for all $x\in \mathbb R^n$, and $n\ge p \ge k$. 


#### **Lemma 2.1.5 | The Linear Space Containing the kth Iterates of any GA1st**
> With $x^{(0)} = \mathbf 0$, $p \le n$, we have $\{x^{(k)}\}_{k = 0}^p$, generated by any algorithm from GA1st satisfies the condition $x^{(k)} \in \text{span}\{\nabla f_p(x^{(k)})\}_{i = 0}^{k - 1} = \text{span}\{\e_i\}_{i = 1}^{k}$ for all $1\le k \le p$.  In Brief, for all $p \le n$, and $0\le k \le p$, and algorithm $\mathcal A_f \in \text{GA}^1$ we have $\mathcal A_f^1 \mathbf 0 \in \text{span}\{\e_i\}_{i = 1}^{k}$

**Proof**

$\nabla f_p(\mathbf 0) = A_p \mathbf 0 - \e_1\in \text{span}(\e_1)$, hence the base case is satisfied. Inductively we have 

$$
\begin{aligned}
    x^{(k + 1)} &\in
    \text{span}\left\lbrace
        \e_i
    \right\rbrace_{i = 1}^{k - 1}
    \\
    A_p x^{(k + 1)} 
    &\in A_p \text{span}\{\e_i\}^{k - 1}_{i = 1}
    \\
    A_px^{(k + 1)} &\in \text{span}\{\e_i\}^{k}_{i = 1} \textcolor{gray}{
        \quad \triangleright[{[1]}]
    }
    \\
    \nabla f_p\left(
        x^{(k +1)}
    \right) & \in \text{span}\{\e_i\}^{k}_{i = 1}. 
    \quad \textcolor{gray}{\triangleright \text{ by def of }\nabla f_p. }
\end{aligned}
$$

at `[[1]]`, we used the fact that $A_p$ has block $p \times p$ tridiagonal matrix with $-1, 2, -1$, and $k\le p$, hence, the lower diagonal of the matrix will shift the elements of the vector in $\text{span}\{\e_i\}_{i = 1}^{k -1}$, bring it into the space of $\text{span}\{\e_i\}_{i = 1}^{k}$. 



#### **Corollary 2.1.1 | Lower Bound for the Objective value of $f_p$ for Iterates Generated by GA1st**
> For any sequence $\{x^{(k)}\}_{k = 0}^p$, with $x^{(0)} = \mathbf 0$, generated by GA1st, we have $f_p(x^{(k)}) \ge f^*_k$. 

**Proof**

$$
\begin{aligned}
    f_p(x^{(k)}) &= f_p(P_k x^{(k)}) = f_k (x^{(k)}) \ge f_k^*, 
\end{aligned}
$$

where the second equality comes from the fact that $k \le p$. Others relations are by the definition of $f_p, P_k, f^*_k$.


---
### **Proof of Thm 2.1.7**

Fix any $1\le k \le 1/2(n - 1)$, and initial value $x^{(0)} = \mathbf 0$, The function we propose is $f(x^{(k)}) = f_{2k + 1}(x^{(k)})$. This function IS DIFFERENT depending what iteration number we want for a lower bound! For example, if I fix $k = 3$ with $n = 7$, then my function that provides a lower bound at iteration $k=3$ is $f_{7}$. 

As long as the iterations $k$ satisfies $1\le k \le 1/2(n - 1)$ the function $f$ would stay consistent for all algorithms of the class $GA^1$ for all iterations including and before $k$, because using the assumption that $x^{(0)} = \mathbf 0$ we have for all $\mathcal A_f\in \text{GA}^1$, for all $l \le k$

$$
\begin{aligned}
    f\left(
        \mathcal A_f^l \mathbf 0
    \right) = f_{2k + 1}\left(
        A^l_f \mathbf 0
    \right) = f_k\left(
        A^l_f \mathbf 0
    \right). 
\end{aligned}
$$

the first equality is by the definition of $f$, the second equality is using corollary 2.1.1 and $l \le k$. Fix iteration counter $1\le k \le 1/2(n - 1)$, we show that the function $f = f_{2k + 1}$ provides a lower bounds for all algorithm $\mathcal A_f \in \text{GA}^1$. Observe that the value of kth iterates for any $\mathcal A_f$ has 

$$
\begin{aligned}
    \min_{\mathcal A_f \in \text{GA}^1} \left\lbrace
        f(\mathcal A_f^k \mathbf 0)
    \right\rbrace &= 
    \min_{\mathcal A_f \in \text{GA}^1} \left\lbrace
        f_{2k+1}(\mathcal A_f^k \mathbf 0)
    \right\rbrace 
    \\
    &= \min_{\mathcal A_f \in \text{GA}^1} \left\lbrace
        f_{k}(\mathcal A_f^k \mathbf 0)
    \right\rbrace \quad 
    \textcolor{gray}{\triangleright \text{corollary 2.1.1, lemma 2.1.5}}
    \\
    & \ge f_k^*\ge f^*_{2k+1}. 
\end{aligned}
$$

therefore we have for a fixed $1\le k \le 1/2(n - 1)$ and any algorithm $\widetilde{\mathcal A}_f \in \text{GA}^1$ the optimality gap at the kth iterations for function $f = f_{2k+1}$ being 

$$
\begin{aligned}
    \frac{f_{2k + 1}\left(\widetilde{\mathcal A}_f^k x^{(0)}\right) - f^*_{2k+1}}{
        \Vert x^{(0)} - x^*\Vert^2 
    }
    & \ge 
    \frac{
        \min_{\mathcal A_f\in GA^{1}}
        \left\{f_{2k + 1}\left(x^{(k)}\right) \right\}
        - f^*_{2k+1}
    }
    {
        \Vert x^{(0)} - x^*\Vert^2
    } 
    \\
    & \ge
    \frac{f_k^* - f_{2k+1}^*}{
        \Vert x^*\Vert^2
    }\quad 
    \textcolor{gray}{\triangleright \text{shift invariance of }GA^{1}}
    \\
    &\ge 
    \frac{\frac{L}{8}\left(
        \frac{k}{k + 1} - \frac{2k + 1}{2k+ 2}
    \right)}{\frac{1}{3}(2k + 2)} \quad 
    \textcolor{gray}{\triangleright \text{ minimum of } f_k}
    \\
    &= \frac{
        \frac{L}{8}(2k - 2k + 1)
    }{\frac{1}{3}(2 k + 2)^2}
    \\
    &= \frac{3/8 L}{4(k + 1)^2}
    \\
    \implies 
    f_{2k + 1}\left(x^{(k)}\right) - f_{2k+1}^*
    &\ge 
    \frac{3/8L \Vert x^{(0)} - x^*\Vert^2}{4(k + 1)^2}. 
\end{aligned}
$$

First parts of the claim has been proved. For the second part we need a lot of algebra. Start by considering that for any fixed $1 \le k \le 1/2(n - 1)$, given any $\mathcal A_f \in \text{GA}^1$ we have $x^{(k)} = \mathcal A_f^k \mathbf 0$, then a lower bound can be found for iterate: 

$$
\begin{aligned}
    \Vert x^{(k)} - x^*\Vert^2 &\ge 
    \min_x 
    \left\lbrace
        \left. \Vert x - x^*\Vert^2 \right| 
        x \in \text{span}(\e_1)^k_{i = 1}
    \right\rbrace
    \\
    &= \sum_{i = k + 1}^{2k + 1}
    \left(
        \bar {x}_i^{[2k + 1]}
    \right)^2
    \\
    &=  \sum_{i = k + 1}^{2k + 1}
    \left(
        1 - \frac{i}{2k + 1} 
    \right)^2 \quad 
    \textcolor{gray}{\triangleright \text{$i$ element for the minimizer of }f_{2k+1}}
    \\
    &= 
    (k + 1) - \left(
        \sum_{ i =k + 1}^{2k + 1}
        \frac{i}{k + 1}
    \right) + 
    \frac{1}{4(k^2 + 1)}
    \sum_{i = k + 1}^{2k + 1}
    i^2
    \\
    & \ge
    (k + 1) - \frac{1}{k + 1}\left(
        \sum_{i =k + 1}^{2k + 1}
        i
    \right) + 
    \frac{1}{4(k + 1)^2}
    \sum_{i = k + 1}^{2k + 1}
    i^2, 
    \\
    & \quad 
    {\small
    \begin{aligned}
        \text{by: }\sum_{i = k + 1}^{2k + 1} i^2 
        &= 
        \frac{1}{6}(
            (2k + 1)(2k + 2)(4k + 3) - k(k + 1)(2k + 1)
        )
        \\
        &= 1/6(k + 1)(2k + 1)(7k + 6), 
    \end{aligned}
    }
    \\
    & = 
    (k + 1) - 
    \frac{1}{k + 1}\frac{(3k + 2)(k + 1)}{2}
    + 
    \frac{1/6(k + 1)(2k + 1)(7k + 6)}{4(k + 1)^2}
    \\
    &= \frac{k}{2} + 
    \frac{(2k + 1)(7k + 6)}{24(k + 1)}
    \\
    &= \frac{2k^2 + 7k + 6}{24(k + 1)}
    \\
    &= \frac{2k^2 + 7k + 6}{8}\frac{1}{3}(2k + 2)
    \\
    &\ge 
    \frac{2k^2 + 7k + 6}{8}
    \left\Vert
        x^{(0)} - {\bar x}^{[2k + 1]}
    \right\Vert^2
    \\
    & \quad\quad 
    {\small
        \begin{aligned}
            \text{by: }
            2k^2 + 7k + 6 & = 2(k^2 + 7/4 k + 3)
            \\
            &= 2(k^2 + 2k - 1/4k + 1 + 2)
            \\
            &= 2((k + 1)^2 + 1/4k + 2)
        \end{aligned}
    }
    \\
    &\ge 
    \frac{1}{8}
    \left\Vert
        x^{(0)} - {\bar x}^{[2k + 1]}
    \right\Vert^2. 
\end{aligned}
$$
and hence proving the second claim in theorem 2.1.7. 


**Remarks**

I think the theorem can be extended to Hilbert space in a similar manner. However, the lower bound is not asymptotically true if we were to keep using the same type of functions for deriving the lower bound. 
