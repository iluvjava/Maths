- [A Better Proof for FISTA Convergence](../AMATH%20516%20Numerical%20Optimizations/Proximal%20Methods/A%20Better%20Proof%20for%20FISTA%20Convergence.md)
- [Ryu's Proof of AG](Ryu's%20Proof%20of%20AG.md)

----
### **Intro**

In this section, we present a sleek proof of the Nesterov acceleration method using the PPM interpretation developed in the previous materials. 
The proof is similar in form as stated in [Ryu's Proof of AG](Ryu's%20Proof%20of%20AG.md), and it's reverse engineered from materials presented in [A Better Proof for FISTA Convergence](../AMATH%20516%20Numerical%20Optimizations/Proximal%20Methods/A%20Better%20Proof%20for%20FISTA%20Convergence.md). 


#### **Assumptions**

Recally previously that 
1. $h = f + g$,
2. $f, g$ are convex, 
3. $f$ is $L$-Lipschitz smooth. 
4. $\mathcal T_L = (I + \partial g)^{-1}\circ [I - L^{-1}\nabla f]$, the proximal gradient operator. 
5. $\mathcal G_L = L (I - \mathcal T_L)$, the gradient mapping. 
6. $\bar x_+ \in \argmin{x} h(x)$, the minimizer exists for $h$.  


#### **Algorithm**

The algorithm is the S-CVX PPM form with the assumption that $\tilde \eta_{t + 1} = \eta_t + L^{-1}$, by writing $\tilde \eta_{t + 1}$ as $\tilde \eta_t$ instead, we can write the S-CVX PPM from iterates to be: 

$$
\begin{aligned}
    y_k &= (1 - (L\tilde \eta_k)^{-1}) z_k - (L\tilde \eta_k)^{-1} x_k, 
    \\
    x_{k + 1} &= x_k + \tilde \eta_k \mathcal G_L(y_k), 
    \\
    z_{t + 1} &= y_k - L^{-1} \mathcal G_L(y_k). 
\end{aligned}
$$


#### **Notations**
Use the notation $\Delta_k = h(z_{k + 1}) - h(\bar x)$ to describe the optialty gap. 

#### **The prximal gradient lemma**
Recall that previously we have the inequality that for all $x, z$ we have 

$$
\begin{aligned}
    h(z) 
    &\ge h(\mathcal T_L x) + 
    \langle \mathcal G_L x, z - x\rangle + \frac{L}{2}\Vert x - \mathcal T_L x\Vert^2
    \\
    &= 
    h(\mathcal T_L x) + \langle \mathcal G_L x, z - x\rangle + \frac{1}{2L}\Vert \mathcal G_L x\Vert^2
\end{aligned}
$$


---
### **The convergence rate of the algorithm**

Convergence rate proof is better exposed by brute force. 
There is very little beauty in the argument of the convergence proof unfortunately. 
The entire argument is based on the proximal gradient lemma, which is pivotal. 

#### **Claim | The optimal convergence rate of the algorithm**
> With the assumptions, if there is a sequence $\sigma_t$ such that $\sigma_k - \sigma_{k-1} \le L \tilde \eta_k, \sigma_t = L\tilde \eta_k^2$, producing a convergence rate of $\mathcal O(\sigma_k^{-1})$ for $\Delta_k$. 

**Proof**

We label several of the important relations to state the convergence result first, and then we establish the proof for these realtions. 
For all $k \in \N$ it yields the following inequalities: 

$$
\begin{aligned}
    \Delta_k &\le
    \langle \mathcal G_L y_k, \bar x - y_k \rangle + \frac{1}{2L} \Vert \mathcal G_L y_k\Vert^2, 
    & 
    ([1])
    \\
    \Delta_k - \Delta_{k -1} &\le 
    - \langle \mathcal G_L y_k, z_k - y_k\rangle - \frac{1}{2L}\Vert \mathcal G_L y_k\Vert^2.
    & 
    ([2])
\end{aligned}
$$

If we assume that $\sigma_k - \sigma_{k - 1} \le \epsilon_k, \epsilon_k \ge 0$ then the following special inequality on the optimality gap: 

$$
\begin{aligned}
    \sigma_k\Delta_k - \sigma_{k -1}\Delta_{k -1}
    &\le 
    (\sigma_k - \epsilon_k)(\Delta_k - \Delta_{k -1}) + \epsilon_k \Delta_k. 
\end{aligned}\tag{[3]}
$$

And we have the equality: 

$$
\begin{aligned}
    \frac{1}{2}
    (
        \Vert x_{k + 1} - x_+\Vert^2 - \Vert x_k - x_+\Vert^2
    )
    &= 
    - \langle \tilde \eta_k, \mathcal G_L, x_k - x_+\rangle
    + \frac{\tilde \eta_k^2}{2}\Vert \mathcal G_L(y_k)\Vert^2. 
\end{aligned}\tag{[4]}
$$

Next, we show that these inequalities can construct the convergence rate without too much gory details of algebra. 
Using the relations between the iterates generated by the algorithm, a combinations of $(\sigma_k - \epsilon_k)([2]) + \epsilon_k([1]) \le 0$ simplifies to 

$$
\begin{aligned}
    (\sigma_k - \epsilon_k)(\Delta_k - \Delta_{k - 1}) + \epsilon_k \Delta_k 
    + 
    \left(
        \langle 
            \mathcal G_L y_k, (\sigma_k - \epsilon_k)z_k - \sigma_k y_k + \epsilon_k x_+
        \rangle
        + 
        \frac{\sigma_k}{2L}\Vert \mathcal G_L y_k\Vert^2
    \right) &\le 0
\end{aligned}\tag{[5]}
$$

Using the algorithm, recall that the iterats satisfies the relations: 

$$
\begin{aligned}
    y_k &= (1 - (L \tilde \eta_k)^{-1})z_k + (L\tilde \eta_k)^{-1}x_k
    \\
    L\tilde \eta_k y_k &= 
    (L \tilde \eta_k - 1)z_k + x_k
    \\
    -x_k &= 
    (L\tilde \eta_k - 1)z_k - L \tilde \eta_k y_k, 
\end{aligned}
$$

if we use the relations that $L\tilde \eta_k = \epsilon_k^{-1}\sigma_k$, it transform the above equality into: 

$$
\begin{aligned}
    -x_k &= (\epsilon_k^{-1}\sigma_k - 1)z_k - \epsilon_k^{-1}\sigma_k y_k
\end{aligned}
$$

allowing us to transform the cross product in $([5])$, so it simplifies to 

$$
\begin{aligned}
    (\sigma_k - \epsilon_k)(\Delta_k - \Delta_{k - 1}) + \epsilon_k \Delta_k 
    + 
    \left(
        \epsilon_k 
        \langle \mathcal G_L y_k, x_+ - x_k\rangle
        + 
        \frac{\sigma_k}{2L}\Vert \mathcal G_L y_k\Vert^2
    \right) &\le 0. 
\end{aligned}\tag{[5.1]}
$$

Comparing ([4]) with the above, set $\epsilon_k = \tilde \eta_k, L \tilde \eta_k^2 = \sigma_t$, previous relation $L\tilde \eta_k = \epsilon_k^{-1} \sigma_k$ remains true.
This equates parts in ([5.1]), producing: 

$$
\begin{aligned}
    \epsilon_k \langle  \mathcal G_Ly_k, x_+ - x_k\rangle + 
    \frac{\sigma_k}{2L}\Vert \mathcal G_L y_k\Vert^2
    = 
    \tilde \eta 
    \langle  \mathcal G_Ly_k, x_+ - x_k\rangle
     + 
    \frac{\tilde \eta_k^2}{2}\Vert \mathcal G_L y_k\Vert^2, 
\end{aligned}
$$

Therefore, substituting ([4]) into ([5.1]) yields inequality: 

$$
\begin{aligned}
    (\sigma_k - \epsilon_k)(\Delta_k - \Delta_{k - 1}) + \epsilon_k \Delta_k 
    + 
    \frac{1}{2}
    (
        \Vert x_{k + 1} - x_+\Vert^2 - \Vert x_k - x_+\Vert^2
    )
    &\le 0
    \\
    \underset{([3])}{\implies}
    \sigma_k \Delta_k - \sigma_{k - 1}\Delta_{k - 1}
    + 
    \frac{1}{2}
    (
        \Vert x_{k + 1} - x_+\Vert^2 - \Vert x_k - x_+\Vert^2
    )
    &\le 
    0
    \\
    \iff 
    \sigma_k \Delta_k + \frac{1}{2}\Vert x_{k + 1} - x_+\Vert^2
    - 
    \left(
        \sigma_{k - 1}\Delta_{k - 1}
        + 
        \frac{1}{2}\Vert x_k - x_+\Vert^2
    \right)
    &\le 
    0
\end{aligned}
$$

By the end, it makes Lyapunov quantity $\Phi_k = \sigma_k \Delta_k + (1/2)\Vert x_{k + 1} - x_+\Vert^2$. 
Hence the above inequality is $\Phi_k - \Phi_{k - 1}$, telescoping it then yields the inequaity: 

$$
\begin{aligned}

\end{aligned}
$$


**Proof for ([1], [2])**

**Proof for ([3])**

**Proof for ([4])**

**Proof for ([5])**
