### **Intro**

Kido, we have some very important shit to go over. 
Here is a list of things we are learning so that they can be included into the thesis proposal. 

---
### **Chapters in Books**

There are a list of good books that we need to read. 

#### **Nesterov's Book**

Nesterov's writing is unique in the way it covers the topic of Optimal Method (common knows as Nesterov accelerations). 
It's also unique because of a computation perspective for algorithms, it's manifested as ample amount of derivations for the behaviors of algorithms, complexity and theoretical lower bounds. 
We will hightlight content in bold if I hadn't read it carefully or the content is new and/or it gives interesting perspective on things that I already know. 

For Nesterov's Lecture on Convex Optimization, I think these chapters requires absolute mastery. 
- Chapter 2 | Smooth Convex Optimization 
  - Minimization of smooth functions
    - Lower complexity bounds for Lipschitz smooth funciton
    - Strongly convex function
    - Lower complexity bounds for strongly convex function
    - The Gradient Method
  - Optimal Methods
    - Estimating Sequences
    - Decreasing Norm of the Gradient
    - Convex Sets 
    - [ ] **The Gradient Mapping**
    - [ ] **Minimization of Simple Sets**
  - Minimization Problems with Smooth Components
    - The Minmax Problem
    - [ ] **The Gradient Mapping**
    - [ ] **Minimization Method for the Minmax Problem**
    - [ ] **Optimization with Functional Constraints**
    - [ ] **The Method for Constrained Minimization**
- Chapter 3 | Nonsmooth convex optimization 
  - General Convex Functions
    - Motivation and Definitions 
    - Operations with Convex Functions
    - Continuity and Differentiability
    - Separation Theorem
    - Subgradients
    - [ ] **Computing Subgradients**
    -  Optimality Conditions
    - [ ] **Minmax Theorem**
    - [ ] **Basic Elements of Primal Dual Methods **
  - Methods of Nonsmooth Minimization 
    - [ ] **General Lower Complexity Bounds**
    - [ ] **Estimating Quality of Approximate Solution **
    - [ ] The Subgradient Method 
    - [ ] Minimization of Functional Constraints 
    - [ ] Approximating the Optimal Lagrange Multpliers 
    - Strongly Convex Functions
    - [ ] Complexity Bounds in Finite Dimensions 
    - [ ] Cutting Plane Schemes
  - Methods with Complete Data
    - [ ] Nonsmooth Model of the Objective Functions 
    - [ ] Kelley's Method 
    - [ ] The Level Method 
    - [ ] Constrained Minimization


---
### **Papers**

#### **Nesterov**
- Linear convergence of First order method for non-strongly convex optimization ^[I. Necoara, Yu. Nesterov, and F. Glineur, “Linear convergence of first order methods for non-strongly convex optimization,” _Math. Program._, vol. 175, no. 1, pp. 69–107, May 2019, doi: [10.1007/s10107-018-1232-1](https://doi.org/10.1007/s10107-018-1232-1).]
- Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems ^[Y. Nesterov, “Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems,” _SIAM Journal on Optimization_, vol. 22, no. 2, pp. 341–362, 2012, doi: [10.1137/100802001](https://doi.org/10.1137/100802001).]

#### **Sasha**

- Relax and Split for Nonconvex Inverse Problem ^[P. Zheng and A. Aravkin, “Relax-and-split method for nonsmooth nonconvex problems.”]. 


#### **Siam Review**

#### **Theory Papers**

#### **Applications and Modeling**

- Statistical Problems. 
- Forestry and machine learning. 

