Lagrangian has information for the optimality conditions for the Dual and the Primal Problems. 
[[Lagrangian and Dualization Techniques]]: For dualizing primal problems with concrete steps. 
[[Dualization Examples]]: Actual Dualizations on real world problems. 
For further intution about the 2 Player game formulations under the general context of smooth constrained variational probem, consult: 
[[Lagrange Multiplier]]

---
### **Intro**

The Lagrangian appeared during the dualization process of the primal can be exploited. 

By connecting the primal and the dual via the Lagrangian, a newton based solver on the Lagrangian will be able to locate the optimality conditions, which is a saddle point of the Lagrangian. 

**What the thing we are exploiting**? 

> Karush-Kuhn-Tucker conditions (KKT)


---
### **Saddle Points of the Lagrangian**

> With strong duality, the saddle point of the Lagrangian is the optimal solution, and because of convexity of both convex feasible region and objective function, the saddle point will be unique. 

Keep in mind that the Lagrangian is the same from the calculus class, therefore using imagination, we can know that: 

* If the objective function is not convex, but the region is convex, then it's possible to have multiple critical points for the Lagrangian.



---
### **Exploits: Inequality constraints and Convex Functions**

We are interested in primal dual of the form: 

$$
\min_x\left\lbrace
    g(x) + \delta_+(b - Ax)
\right\rbrace = \max_z\left\lbrace
        z^Tb - \delta_-(z) + \delta_0(z^TA - c^T)
    \right\rbrace
$$

Assumption: 
* $g(x)$ is convex. 

Topics: 

* The Lagrangian of the system and the saddle point of the Lagrangian.
* The complementary Slackness to relax the optimality conditions a bit. 
* This thing is called the KTT conditions. 

$$
\begin{aligned}
    \mathcal{L}(x, z) &= 
        g(x) + z^T(b - Ax) - \delta_-(z)
    \\
    \implies & 
    \begin{cases}
        \bar{\partial}_x \mathcal{L}(x, z) = 
        \bar{\partial}_x[g(x)] - A^Tz &\ni \mathbf{0}
        \\
        \bar{\partial}_z \mathcal{L}(x, z) = 
        (b - Ax) - \bar{\partial}[\delta_-(z)] &\ni \mathbf{0}
    \end{cases}
    \\
    \implies &
    \begin{cases}
        \bar{\partial}_x \mathcal{L}(x, z) = 
        A^Tz  &\in \bar{\partial}_x[g(x)] 
        \\
        \bar{\partial}_z \mathcal{L}(x, z) = 
        (b - Ax) &\in \bar{\partial}_z[\delta_-(z)]
    \end{cases}
    \\
    \text{Recall: }& 
    \\
    \bar{\partial}_z [\delta_-(z)] 
    &= 
    \begin{cases}
        (-\infty, 0] & z_i \ge 0
        \\
        0 & z_i < 0
    \end{cases} \quad \wedge \quad z \le \mathbf{0}
    \\
    \implies &
    \begin{cases}
        A^Tz &\in \bar{\partial}_x[g(x)]
        \\
        z\circ (b - Ax) &= \mathbf{0}
    \end{cases}
\end{aligned}
$$

Note: Sometimes, in the formulations for the KTT, people swap the sign for $z$, so it's $z \ge \mathbf{0}$ instead.

**Complementary Slackness**

The term $z\circ (b - Ax)$ is the complementary slackness term. Here it's expressed as exact equality, but in reality we will need to relax the conditions if we want some kinda fast, algorithmic approach to the problem. 

Adding an relaxation conditions we have this alternative for KKT: 

> $$
> \text{KTT}: 
> \begin{cases}
>         A^Tz &\in \bar{\partial}_x[g(x)]
>         \\
>         z\circ (b - Ax) &\le \mu
>         \\
>         \mu, -z, b - Ax &\ge \mathbf{0}
> \end{cases}
> $$

**Algorithmic Exploit: Interior Points**


> It's a running, Hessian Based Optimizer, it takes in the Jacobian of the KTT conditions, and then solve it, it's complicated and deserves it's own sections. here is a just a breakdown for what is needed to run the Interior Points Solver. 

[[Interior Points, Remastered]] is an example for the Interior Points solver, but for L2 norm objective function. 

The Interior Points method can be summarized easily but the actual implementations of it deserves its own pages.

1. Relax the conditions of the KTT, so that newton's algorithm can work on it. 
2. Get the Hessian of the Objective function for the Newton's method
3. Feasibility search
4. Relax the boundary conditions, and update them accordingly, keeping the update inside the feasible region. 





