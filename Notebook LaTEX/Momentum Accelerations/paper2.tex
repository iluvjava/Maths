\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont The Proximal Point interpretation of Nesterov accelerated proximal gradient}}

\author{
    Hongda Li
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~ and~Xianfu Wang
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    Nesterov accelreated gradient method has been in the spotlight for the past decades due its wide spread applications and theories of optimal convergence. 
    Decades later it still opens up new interpretations. 
    Our work suggests a proximal point interpretation of accelerated gradient method for the method of accelerated proximal gradient method as a major extension to the interpretation proposed by Ahn and Sra \cite{ahn_understanding_2022}. 
    The proofs had been streamlined, extended and new error terms are added to allow a larger set of stepsize sequence for the proximal point interpreation. 
    Additionally, we conduct numerical experiment for a line search method that dynamically adjust the strong convexity index $\mu$ and Lipschitz constant of the gradient in for algorithm  using the proximal point understanding of accelerated gradient. 
    
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

\section{Introduction}
    Recent works from Ahn and Sra \cite{ahn_understanding_2022} and Nesterov \cite{nesterov_lectures_2018} inspired content in this section.
    Ahn and Sra explored the interpretation of Nesterov acceleration as the proximal point method (PPM) applied to an upper surrogate function, and then a lower surrogate function. 
    Inspired by the interpretation, we generalize the framework to the case of $h = f + g$ with $f$ Lipschitz smooth and $g$ convex whose proximal mapping can be evaluated exactly for algorithm implementations. 
    In addition, we derive a relaxed step size conditions for the algorithm and their corresponding convergence rate. 
    As a major extension, our work describes the method of proximal gradient using the PPM interpretations. 
    We also compactify the proof using Lypunov analysis, which allows us to show the convergence still holds even if there is error in the evaluations of the proximal gradient operator. 
    We derive a major variant of the Nesterov acceleation algorithm
    \cite{chambolle_convergence_2015}, \cite{beck_fast_2009-1}, \cite[Chapter 12]{ryu_large-scale_2022}
    of the accelerated gradient (AG) and show that they all falls within the same framework. 

    \par
    Some of the earlier examples for the extension of the Nesterov acceleration method make use of the classical analysis introduced by Nestrov. 
    It uses the Nesterov acceleration sequence. 
    See \cite{guler_new_1992} for an extension of the Nesterov accelerated gradient method to the proximal point method for convex programming. 
    However the classical analysis found in \cite[chapter 2]{nesterov_lectures_2018} involves the assumption of a specific kind of Lypunov function and a specific format of the Nesterov's estimating sequence to accomodate the proof. 
    In Ahn's work however, the complexities are packaged into the proximal point interpretation of accelerated gradient. 
    It uses a lemma from Moreau envelope for the Lypunov analysis.
    The interpration allows choices of undetermined parameters to encompass several variants of the Nesterov accelerated gradient algorithm. 
    Ahn and Sra's approach is inspired by works from Defazio \cite{defazio_curved_2019}, Zeyuan and Lorenzo \cite{allen-zhu_linear_2016}. 
    Instead of a mirror descent step on the dual, they propose an alternative without the gradient of the dual. 
    
    \par
    Numerous notable variations of Nesterov accelerated gradient exists. \cite[(6.1.19)]{nesterov_lectures_2018} described a variant of accelerated gradient restricted to a convex domain $Q$ using Bregman Divergence. 
    Beck and Toubolle \cite{beck_fast_2009} introduced a variant the problem type of smooth plus non-smooth, known as FISTA. 
    For a variant of accelerated gradient where the iterates converge (weakly in Hilbert space), see Chambolle and Dossal \cite{chambolle_convergence_2015}. 
    Extension such as the Harpen acceleration for the resolvent operator of a maximally monotone opreator in general is outside of the scope since it not all maximal monotone operator corresponds to a subgradient operator. 
    \par
    A wide varieties of interpretation for the Nesterov accelerated gradient exist in the literatures. 
    Consult \cite{su_differential_2015} for a dynamical system interpretation of Nesterov acceleration. 
    The dynamical system interpretation of the algorithm leads to the valuable insight that restarting the accerlated gradient algorithm would lead to faster convergence rate for the class of strongly convex function. 
    In work by \cite{allen-zhu_linear_2016}, they interpreted the idea of Nesterov acceleration as a combinations of gradient descent and mirror descent. 

    % \par
    % The paper is organized as follow: 
    % \begin{enumerate}
    %     \item Section 
    % \end{enumerate}
    
\section{Preliminaries}\label{sec:preliminaries}
    In this section we introduce the a descent lemma for Proximal Point Method (PPM) in the convex case. 
    We define the proximal gradient mapping $\mathcal T_L$, and gradient mapping $\mathcal G_L$ for function satisfying 
    \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
    {assumption \ref*{ass:smooth-nonsmooth-sum-lipschitz-grad}}. 
    A lower bound function is identified using the gradient mapping operator and proved in
    \hyperref[lemma:grad_map_linearization]{Lemma \ref*{lemma:grad_map_linearization}}, 
    this is a key component of the proximal point interpretation of the accelerated gradient method. 
    \begin{assumption}\label{ass:smooth-nonsmooth-sum-lipschitz-grad}
        Let $h = f + g$ where $f, g$ are convex and $f$ is Lipschitz-Smooth with constant $L$. 
    \end{assumption}
    \begin{definition}[Strongly convex functions]
        A function $f: X \mapsto \RR$ is $\beta$-strongly convex
        with $\beta\geq 0$ if $f - \beta \frac{\Vert \cdot\Vert^2}{2}$ is convex.
    \end{definition}

    \begin{definition}[The gradient mapping]
        \label{def:gradient_mapping}
        Suppose $h = f + g$ satisfies 
        \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
        {Assumption \ref*{ass:smooth-nonsmooth-sum-lipschitz-grad}}. 
        Define the proximal gradient operator
        $$
            \mathcal T_L(x) := \hprox_{L^{-1}g}(x - L^{-1}\nabla f(x)),
        $$
        and the gradient mapping operator
        $$
            \mathcal G_L(x) = L(x - \mathcal T_L(x)). 
        $$
    \end{definition}
    \begin{remark}
        The name ``gradient mapping" comes from \cite[(2.2.54)]{nesterov_lectures_2018}, however, Nesterov was referring to only the case when $g$ is an indicator function of a convex set in his writing. 
        Of course, in Amir Beck \cite[10.3.2]{beck_first-order_nodate}, it has the exact same definition for gradient mapping as the above. 
    \end{remark}

    \begin{lemma}[Gradient mapping approximates subgradient]
    \label{lemma:grad-map-approx-subgrad}\; \\
        Suppose $h = f + g$ satisfies 
        \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
        {Assumption \ref{ass:smooth-nonsmooth-sum-lipschitz-grad}}, 
        let $\mathcal T_L, \mathcal G_L$ be given by 
        \hyperref[def:gradient_mapping]{Definition \ref*{def:gradient_mapping}}.
        Then for all $x$, the gradient mapping verifies
        \begin{align*}
            x^+ &= \mathcal T_L(x), 
            \\
            \mathcal G_L(x) := L(x - x^+) &\in  \nabla f(x) + \partial g(x^+). 
        \end{align*}
        Equivalently, $\exists v \in \partial g(x^+)$ such that $G_L(x) = \nabla f(x) + v = L(x - x^+)$. 
    \end{lemma}
    \begin{proof}
        Using the resolvent definition of the proximal gradient operator and the fact that the single-valuedness in the convex settings, $x^+$ has relations: 
        \begin{align*}
            x^+ &= [I + L^{-1}\partial g]^{-1}\circ [I - L^{-1}\nabla f](x)
            \\
            [I + L^{-1}\partial g](x^+) &\ni [I - L^{-1}\nabla f](x)
            \\
            x^+ + L^{-1}\partial g(x^+) &\ni x - L^{-1}\nabla f(x)
            \\
            x^+ - x + L^{-1}\partial g(x^+) &\ni L^{-1}\nabla f(x)
            \\
            L(x^+ - x) + \partial g(x^+) &\ni - \nabla f(x)
            \\
            L(x - x^+) &\in \nabla f(x) + \partial g(x^+)
            \\
            \mathcal G_L(x) &\in \nabla f(x) + \partial g(x^+). 
        \end{align*}
    \end{proof}

    \begin{lemma}[Linearized gradient mapping lower bound]
    \label{lemma:grad_map_linearization}\; \\
        Suppose that $h = f + g$ satisfies 
        \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
        {Assumption \ref*{ass:smooth-nonsmooth-sum-lipschitz-grad}}, 
        further assume that $f$ is strongly convex with index $\mu \ge 0$. 
        Let $x^+ = \mathcal T_L(x)$ as given in 
        \hyperref[def:gradient_mapping]{Definition \ref*{def:gradient_mapping}}. 
        Then for all $z \in \RR$, it satisfies
        \begin{align*}
            h(z) &\ge 
            h(x^+) + 
            \langle \mathcal G_L (x), z - x\rangle 
            + 
            \frac{L}{2}\Vert x - x^+\Vert^2 + \frac{\mu}{2}
            \Vert z - x\Vert^2. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Using the $L$-smoothness of $f$ and convexity of $g, f$, it has inequalities
        \begin{align*}
            &f(x^+) \le 
            f(x) + \langle \nabla f(x), x^+ - x\rangle
            + \frac{L}{2}\Vert x - x^+\Vert^2, 
            \\
            &
            \frac{\mu}{2}\Vert z - x\Vert^2+ 
            f(x) + \langle \nabla f(x), z - x\rangle 
            \le f(z), 
            \\
            &g(x^+) \le 
            g(z) + \langle v, x^+ - z\rangle\quad 
            \forall v \in \partial g(x^+)
        \end{align*}
        For all $v \in \partial g (x^+)$, apply the above by considering the following sequence of relations
        \begin{align*}
            h(x^+) &= f(x^+) + g(x^+)
            \\&
            \begin{aligned}
                &\le 
                \left(
                    f(x) + \langle \nabla f(x), x^+ - x\rangle
                    + \frac{L}{2}\Vert x - x^+\Vert^2
                \right)
                \\
                &\qquad  
                + (g(z) + \langle v, x^+ - z\rangle)
            \end{aligned}
            \\&
            \begin{aligned}
                &\le 
                \left(
                    f(z) - \langle \nabla f(x), z - x\rangle
                    - \frac{\mu}{2}\Vert z - x\Vert^2
                    + \langle \nabla f(x), x^+ - x\rangle
                    + 
                    \frac{L}{2}\Vert x - x^+\Vert^2
                \right)
                \\
                &\qquad 
                +
                (g(z) + \langle v, x^+ - z\rangle)
            \end{aligned}
            \\&
            \begin{aligned}
                &= 
                (f(z) + h(z)) 
                \\
                &\qquad 
                + \left(
                    \langle \nabla f(x), x - z\rangle + 
                    \langle \nabla f(x), x^+ - x\rangle + 
                    \langle v, x^+ - z\rangle
                \right) 
                \\ 
                &\qquad 
                - \frac{\mu}{2}\Vert z - x\Vert^2
                + \frac{L}{2}\Vert x - x^+\Vert^2
            \end{aligned}
            \\& 
            \begin{aligned}
                &= h(z) + 
                \left(
                    \langle \nabla f(x), x - x^+ + x^+ - z\rangle + 
                    \langle \nabla f(x), x^+ - x\rangle + 
                    \langle v, x^+ - z\rangle
                \right)
                \\
                &\qquad 
                - \frac{\mu}{2}\Vert z - x\Vert^2
                + \frac{L}{2}\Vert x - x^+\Vert^2
            \end{aligned}
            \\& 
            \begin{aligned}
                &= h(z) + 
                \langle \nabla f(x) + v, x^+ - z\rangle 
                - \frac{\mu}{2}\Vert z - x\Vert^2
                + \frac{L}{2}\Vert x - x^+\Vert^2
            \end{aligned}
        \end{align*}
        Showed in  
        \hyperref[lemma:grad-map-approx-subgrad]{Lemma \ref*{lemma:grad-map-approx-subgrad}}, 
        we have $\mathcal G_L(x) \in \nabla f(x) + \partial g(x^+)$, choose $v \in \partial g(x^+)$ such that $G_L(x) = \nabla f(x) + v = L(x - x^+)$, so it yields
        \begin{align*}
            h(x^+) & 
            \le  
            h(z) + \langle L(x - x^+), x^+ - x + x - z\rangle 
            - \frac{\mu}{2}\Vert z - x\Vert^2
            + \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &= h(z) + 
            \underbrace{\langle L(x - x^+), x - z\rangle}_{
                = - \langle \mathcal G_L (x), z - x\rangle
            }
            - \frac{\mu}{2}\Vert z - x\Vert^2
            - \frac{L}{2}\Vert x - x^+\Vert^2
        \end{align*}
        Moving everything except $h(z)$ from the RHS to the LHS yield the desired inequality. 
    \end{proof}
    \begin{remark}
        The inequality is analogous to \cite[(2.2.57)]{nesterov_lectures_2018}, however Nesterov stated it only for the case when $g$ is an indicator function of some convex set $Q$. 
        
    \end{remark}

\section{Different forms of Nesterov's accelerated gradient}
    \begin{assumption}
        Let $h=f + g$ satisfies 
        \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
        {Assumption \ref*{ass:smooth-nonsmooth-sum-lipschitz-grad}}, 
        Let $\mathcal G_L, \mathcal T_L$ be the gradient mapping and proximal gradient operator as given by 
        \hyperref[def:gradient_mapping]
        {Definition \ref*{def:gradient_mapping}}. 
        In addition, we also assume that $f$ is strongly convex with constant $L \ge \mu \ge 0$. 
    \end{assumption}

    \begin{definition}\label{def:S-CVX-PPM-APG}
        Let $h = f + g, \mathcal G_L, \mathcal T_L$ and parameter $L \ge \mu \ge 0$ be given by
        \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
        {Assumption \ref*{ass:smooth-nonsmooth-sum-lipschitz-grad}}
        Define the lower bouding function for any $x \in \RR^n$, we have for all $z \in \RR^n$: 
        $$
        \begin{aligned}
            l_h(z; x) = h(\mathcal T_L x) + \langle \mathcal G_L (x), z - x\rangle
            + 
            \frac{L}{2}\Vert x - \mathcal T_L (x)\Vert^2 + \frac{\mu}{2}\Vert z - x\Vert^2
        \end{aligned}
        $$
        For any positive sequence of stepsizes $(\tilde \eta_t)_{t \in \N}, (\eta_t)_{t \in \N}$, and any initial iterates $x_0 = y_0$, let parameter $L \ge \mu \ge 0$, we define an algorithm such that iterates $(x_t, y_t)_{t \in \N}$ is given by: 
        $$
        \begin{aligned}
            x_{t + 1} &= \argmin_{x} \left\lbrace
            l_h(x; y_t) + \frac{1}{2\tilde \eta_{t}} 
            \Vert x - x_t\Vert^2
            \right\rbrace
            \\
            &= (\mu\tilde \eta_{t} + 1)^{-1} 
            (\mu\tilde \eta_{t}y_t + x_t - \tilde \eta_{t}\mathcal G_L(y_t))
            \\
            y_{t + 1}&= 
            \argmin_{x}
            \left\lbrace
                h(\mathcal T_L y_t) + \langle \mathcal G_L(y_t), x - y_t\rangle + \frac{L}{2}\Vert x -y_t\Vert^2
                + \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
            \right\rbrace
            \\
            &= (1 + L \eta_{t +1})^{-1}(L\eta_{t + 1}(y_t - L^{-1}\mathcal G_L(y_t)) + x_{t + 1})
        \end{aligned}
        $$
    \end{definition}


    
% \printbibliography

\bibliographystyle{siam}
\bibliography{references/refs}

\appendix
\section{Postoned proof}
    \begin{proof}{Derivations for S-CVX-PPM APG}\label{proof:derivations-S-CVX-PPM-AGP}
        The functions inside of "argmin" is easy to solve because they are just quadratic functions. 
        We write it here for future verifications and a peace of the mind. 

        $$
        \begin{aligned}
            x_{t + 1} &= \argmin{x}\left\lbrace
                \langle \mathcal G_L(y_t), x - y_t\rangle 
                + 
                \frac{\mu}{2}\Vert x - y_t\Vert^2 +  
                \frac{1}{2\tilde \eta_{t}}\Vert x - x_t\Vert^2
            \right\rbrace
            \\
            \iff 
            \mathbf 0 & = 
            \mathcal G_L(y_t) + \mu(x - y_t) + \tilde \eta_{t}^{-1}(x - x_t)
            \\
            &= 
            \mathcal G_L(y_t) + (\mu + \tilde \eta_{t}^{-1}) x - \mu y_t - \tilde \eta_{t}^{-1} x_t
            \\
            \iff 
            (\mu + \tilde \eta_{t}^{-1})x 
            &= 
            \mu y_t + \tilde \eta_{t}^{-1} x_t - \mathcal G_L(y_t)
            \\
            \implies 
            x &= (\mu + \tilde \eta_{t}^{-1})^{-1 }
            (\mu y_t + \tilde \eta_{t}^{-1} x_t - \mathcal G_L(y_t)). 
        \end{aligned}
        $$

        We can make the assumption that $\mu + \eta_{t}^{-1} > 0$ because $\tilde\eta_t > 0$. 
        Similarly for $y_{t + 1}$, it's solving a simple quadratic minimization problem, yielding: 

        $$
        \begin{aligned}
            \mathbf 0 &= \mathcal G_L(y_t) + L(x - y_t) + \eta_{t + 1}^{-1}(x - x_{t + 1})
            \\
            &= (L + \eta_{t + 1}^{-1})x - L y_t - \eta_{t + 1}^{-1}x_{t + 1} + \mathcal G_L(y_t) 
            \\
            (L + \eta_{t + 1}^{-1})x &= 
            Ly_t + \eta_{t + 1}^{-1} x_{t + 1} - \mathcal G_L(y_t)
            \\
            \implies 
            x &= 
            (L\eta_{t + 1} + 1)^{-1}(L\eta_{t + 1}(y_t - L^{-1}\mathcal G_L(y_t)) + x_{t + 1}). 
        \end{aligned}
        $$

        We had verified the results for the peace of the mind. 
    \end{proof}

\end{document}
