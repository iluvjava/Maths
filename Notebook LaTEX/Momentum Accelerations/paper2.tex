\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont The Proximal Point interpretation of Nesterov accelerated proximal gradient}}

\author{
    Hongda Li
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~ and~Xianfu Wang
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    Nesterov accelreated gradient method has been in the spotlight for the past decades due its wide spread applications and theories of optimal convergence. 
    Decades later it still opens up new interpretations. 
    Our work suggests a proximal point interpretation of accelerated gradient method for the method of accelerated proximal gradient method as a major extension to the interpretation proposed by Ahn and Sra \cite{ahn_understanding_2022}. 
    The proofs had been streamlined, extended and new error terms are added to allow a larger set of stepsize sequence for the proximal point interpreation. 
    Additionally, we conduct numerical experiment for a line search method that dynamically adjust the strong convexity index $\mu$ and Lipschitz constant of the gradient in for algorithm  using the proximal point understanding of accelerated gradient. 
    
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

\section{Introduction}
    Recent works from Ahn and Sra \cite{ahn_understanding_2022} and Nesterov \cite{nesterov_lectures_2018} inspired content in this section.
    Ahn and Sra explored the interpretation of Nesterov acceleration as the proximal point method (PPM) applied to an upper surrogate function, and then a lower surrogate function. 
    Inspired by the interpretation, we generalize the framework to the case of $h = f + g$ with $f$ Lipschitz smooth and $g$ convex whose proximal mapping can be evaluated exactly for algorithm implementations. 
    In addition, we derive a relaxed step size conditions for the algorithm and their corresponding convergence rate. 
    As a major extension, our work describes the method of proximal gradient using the PPM interpretations. 
    We also compactify the proof using Lypunov analysis, which allows us to show the convergence still holds even if there is error in the evaluations of the proximal gradient operator. 
    We derive a major variant of the Nesterov acceleation algorithm
    \cite{chambolle_convergence_2015}, \cite{beck_fast_2009-1}, \cite[Chapter 12]{ryu_large-scale_2022}
    of the accelerated gradient (AG) and show that they all falls within the same framework. 

    \par
    Some of the earlier examples for the extension of the Nesterov acceleration method make use of the classical analysis introduced by Nestrov. 
    It uses the Nesterov acceleration sequence. 
    See \cite{guler_new_1992} for an extension of the Nesterov accelerated gradient method to the proximal point method for convex programming. 
    However the classical analysis found in \cite[chapter 2]{nesterov_lectures_2018} involves the assumption of a specific kind of Lypunov function and a specific format of the Nesterov's estimating sequence to accomodate the proof. 
    In Ahn's work however, the complexities are packaged into the proximal point interpretation of accelerated gradient. 
    It uses a lemma from Moreau envelope for the Lypunov analysis.
    The interpration allows choices of undetermined parameters to encompass several variants of the Nesterov accelerated gradient algorithm. 
    Ahn and Sra's approach is inspired by works from Defazio \cite{defazio_curved_2019}, Zeyuan and Lorenzo \cite{allen-zhu_linear_2016}. 
    Instead of a mirror descent step on the dual, they propose an alternative without the gradient of the dual. 
    
    \par
    Numerous notable variations of Nesterov accelerated gradient exists. \cite[(6.1.19)]{nesterov_lectures_2018} described a variant of accelerated gradient restricted to a convex domain $Q$ using Bregman Divergence. 
    Beck and Toubolle \cite{beck_fast_2009} introduced a variant the problem type of smooth plus non-smooth, known as FISTA. 
    For a variant of accelerated gradient where the iterates converge (weakly in Hilbert space), see Chambolle and Dossal \cite{chambolle_convergence_2015}. 
    Extension such as the Harpen acceleration for the resolvent operator of a maximally monotone opreator in general is outside of the scope since it not all maximal monotone operator corresponds to a subgradient operator. 
    \par
    A wide varieties of interpretation for the Nesterov accelerated gradient exist in the literatures. 
    Consult \cite{su_differential_2015} for a dynamical system interpretation of Nesterov acceleration. 
    The dynamical system interpretation of the algorithm leads to the valuable insight that restarting the accerlated gradient algorithm would lead to faster convergence rate for the class of strongly convex function. 
    In work by \cite{allen-zhu_linear_2016}, they interpreted the idea of Nesterov acceleration as a combinations of gradient descent and mirror descent. 

    % \par
    % The paper is organized as follow: 
    % \begin{enumerate}
    %     \item Section 
    % \end{enumerate}
    
\section{Preliminaries}\label{sec:preliminaries}
    Let the ambient space be $\RR^n$. 
    In this section, we define the proximal gradient mapping $\mathcal T_L$, and gradient mapping $\mathcal G_L$ for function satisfying 
    \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
    {Assumption \ref*{ass:smooth-nonsmooth-sum-lipschitz-grad}}. 
    A lower bound function is identified using the gradient mapping operator and proved in
    \hyperref[lemma:grad_map_linearization]{Lemma \ref*{lemma:grad_map_linearization}}, 
    this is a key component of the proximal point interpretation of the accelerated gradient method. 
    \begin{assumption}\label{ass:smooth-nonsmooth-sum-lipschitz-grad}
        Let $h = f + g$ where $f, g$ are convex, lower semi-continuous and $f$ is Lipschitz-Smooth with constant $L$. 
    \end{assumption}
    \begin{definition}[Strongly convex functions]
        A function $f: X \mapsto \RR$ is $\beta$-strongly convex
        with $\beta\geq 0$ if $f - \beta \frac{\Vert \cdot\Vert^2}{2}$ is convex.
    \end{definition}

    \begin{definition}[The gradient mapping]
        \label{def:gradient_mapping}
        Suppose $h = f + g$ satisfies 
        \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
        {Assumption \ref*{ass:smooth-nonsmooth-sum-lipschitz-grad}}. 
        Define the proximal gradient operator
        $$
            \mathcal T_L(x) := \hprox_{L^{-1}g}(x - L^{-1}\nabla f(x)),
        $$
        and the gradient mapping operator
        $$
            \mathcal G_L(x) = L(x - \mathcal T_L(x)). 
        $$
    \end{definition}
    \begin{remark}
        The name ``gradient mapping" comes from \cite[(2.2.54)]{nesterov_lectures_2018}, however, Nesterov was referring to only the case when $g$ is an indicator function of a convex set in his writing. 
        Of course, in Amir Beck \cite[10.3.2]{beck_first-order_nodate}, it has the exact same definition for gradient mapping as the above. 
    \end{remark}

    \begin{lemma}[Gradient mapping approximates subgradient]
    \label{lemma:grad-map-approx-subgrad}\; \\
        Suppose $h = f + g$ satisfies 
        \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
        {Assumption \ref{ass:smooth-nonsmooth-sum-lipschitz-grad}}, 
        let $\mathcal T_L, \mathcal G_L$ be given by 
        \hyperref[def:gradient_mapping]{Definition \ref*{def:gradient_mapping}}.
        Then for all $x$, the gradient mapping verifies
        \begin{align*}
            x^+ &= \mathcal T_L(x), 
            \\
            \mathcal G_L(x) := L(x - x^+) &\in  \nabla f(x) + \partial g(x^+). 
        \end{align*}
        Equivalently, $\exists v \in \partial g(x^+)$ such that $G_L(x) = \nabla f(x) + v = L(x - x^+)$. 
    \end{lemma}
    \begin{proof}
        Using the resolvent definition of the proximal gradient operator and the fact that the single-valuedness in the convex settings, $x^+$ has relations: 
        \begin{align*}
            x^+ &= [I + L^{-1}\partial g]^{-1}\circ [I - L^{-1}\nabla f](x)
            \\
            [I + L^{-1}\partial g](x^+) &\ni [I - L^{-1}\nabla f](x)
            \\
            x^+ + L^{-1}\partial g(x^+) &\ni x - L^{-1}\nabla f(x)
            \\
            x^+ - x + L^{-1}\partial g(x^+) &\ni L^{-1}\nabla f(x)
            \\
            L(x^+ - x) + \partial g(x^+) &\ni - \nabla f(x)
            \\
            L(x - x^+) &\in \nabla f(x) + \partial g(x^+)
            \\
            \mathcal G_L(x) &\in \nabla f(x) + \partial g(x^+). 
        \end{align*}
    \end{proof}

    \begin{lemma}[Linearized gradient mapping lower bound]
    \label{lemma:grad_map_linearization}\; \\
        Suppose that $h = f + g$ satisfies 
        \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
        {Assumption \ref*{ass:smooth-nonsmooth-sum-lipschitz-grad}}, 
        further assume that $f$ is strongly convex with index $\mu \ge 0$. 
        Let $x^+ = \mathcal T_L(x)$ as given in 
        \hyperref[def:gradient_mapping]{Definition \ref*{def:gradient_mapping}}. 
        Then for all $z \in \RR$, it satisfies
        \begin{align*}
            h(z) &\ge 
            h(x^+) + 
            \langle \mathcal G_L (x), z - x\rangle 
            + 
            \frac{L}{2}\Vert x - x^+\Vert^2 + \frac{\mu}{2}
            \Vert z - x\Vert^2. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Using the $L$-smoothness of $f$ and convexity of $g, f$, it has inequalities
        \begin{align*}
            &f(x^+) \le 
            f(x) + \langle \nabla f(x), x^+ - x\rangle
            + \frac{L}{2}\Vert x - x^+\Vert^2, 
            \\
            &
            \frac{\mu}{2}\Vert z - x\Vert^2+ 
            f(x) + \langle \nabla f(x), z - x\rangle 
            \le f(z), 
            \\
            &g(x^+) \le 
            g(z) + \langle v, x^+ - z\rangle\quad 
            \forall v \in \partial g(x^+)
        \end{align*}
        For all $v \in \partial g (x^+)$, apply the above by considering the following sequence of relations
        \begin{align*}
            h(x^+) &= f(x^+) + g(x^+)
            \\&
            \begin{aligned}
                &\le 
                \left(
                    f(x) + \langle \nabla f(x), x^+ - x\rangle
                    + \frac{L}{2}\Vert x - x^+\Vert^2
                \right)
                \\
                &\qquad  
                + (g(z) + \langle v, x^+ - z\rangle)
            \end{aligned}
            \\&
            \begin{aligned}
                &\le 
                \left(
                    f(z) - \langle \nabla f(x), z - x\rangle
                    - \frac{\mu}{2}\Vert z - x\Vert^2
                    + \langle \nabla f(x), x^+ - x\rangle
                    + 
                    \frac{L}{2}\Vert x - x^+\Vert^2
                \right)
                \\
                &\qquad 
                +
                (g(z) + \langle v, x^+ - z\rangle)
            \end{aligned}
            \\&
            \begin{aligned}
                &= 
                (f(z) + h(z)) 
                \\
                &\qquad 
                + \left(
                    \langle \nabla f(x), x - z\rangle + 
                    \langle \nabla f(x), x^+ - x\rangle + 
                    \langle v, x^+ - z\rangle
                \right) 
                \\ 
                &\qquad 
                - \frac{\mu}{2}\Vert z - x\Vert^2
                + \frac{L}{2}\Vert x - x^+\Vert^2
            \end{aligned}
            \\& 
            \begin{aligned}
                &= h(z) + 
                \left(
                    \langle \nabla f(x), x - x^+ + x^+ - z\rangle + 
                    \langle \nabla f(x), x^+ - x\rangle + 
                    \langle v, x^+ - z\rangle
                \right)
                \\
                &\qquad 
                - \frac{\mu}{2}\Vert z - x\Vert^2
                + \frac{L}{2}\Vert x - x^+\Vert^2
            \end{aligned}
            \\& 
            \begin{aligned}
                &= h(z) + 
                \langle \nabla f(x) + v, x^+ - z\rangle 
                - \frac{\mu}{2}\Vert z - x\Vert^2
                + \frac{L}{2}\Vert x - x^+\Vert^2
            \end{aligned}
        \end{align*}
        Showed in  
        \hyperref[lemma:grad-map-approx-subgrad]{Lemma \ref*{lemma:grad-map-approx-subgrad}}, 
        we have $\mathcal G_L(x) \in \nabla f(x) + \partial g(x^+)$, choose $v \in \partial g(x^+)$ such that $G_L(x) = \nabla f(x) + v = L(x - x^+)$, so it yields
        \begin{align*}
            h(x^+) & 
            \le  
            h(z) + \langle L(x - x^+), x^+ - x + x - z\rangle 
            - \frac{\mu}{2}\Vert z - x\Vert^2
            + \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &= h(z) + 
            \underbrace{\langle L(x - x^+), x - z\rangle}_{
                = - \langle \mathcal G_L (x), z - x\rangle
            }
            - \frac{\mu}{2}\Vert z - x\Vert^2
            - \frac{L}{2}\Vert x - x^+\Vert^2
        \end{align*}
        Moving everything except $h(z)$ from the RHS to the LHS yield the desired inequality. 
    \end{proof}
    \begin{remark}
        The inequality is analogous to \cite[(2.2.57)]{nesterov_lectures_2018}, however Nesterov stated it only for the case when $g$ is an indicator function of some convex set $Q$. 
        Additionally, the inequality has a bizzare resemblence to the proximal inequality. 
        Let $h = g + h$ be convex, $f \equiv 0$, consider $x^+ = \hprox_{\eta g}(x)$ for some $\eta > 0$, then by proximal operator being a resolvent it has $\eta^{-1}(x - x^+)\in \partial g(x^+)$, so by convexity it produces inequality for all $z\in \RR^n$: 
        \begin{align*}
            g(z) - g(x^+) 
            &\ge \langle \eta^{-1}(x - x^+), z - x^+\rangle
            \\
            &= \langle \eta^{-1}(x - x^+), z - x + x - x^+\rangle 
            \\
            &= \langle \eta^{-1}(x - x^+), z - x \rangle + 
            \eta\Vert \eta^{-1}(x - x^+)\Vert^2. 
        \end{align*}
    \end{remark}

\section{Different forms of Nesterov's accelerated gradient}
    \begin{assumption}\label{ass:smooth-non-smooth-lip-scvx}
        Let $h=f + g$ satisfies 
        \hyperref[ass:smooth-nonsmooth-sum-lipschitz-grad]
        {Assumption \ref*{ass:smooth-nonsmooth-sum-lipschitz-grad}}, 
        Let $\mathcal G_L, \mathcal T_L$ be the gradient mapping and proximal gradient operator as given by 
        \hyperref[def:gradient_mapping]
        {Definition \ref*{def:gradient_mapping}}. 
        In addition, we also assume that $f$ is strongly convex with constant $L \ge \mu \ge 0$. 
    \end{assumption}
    In this section, ``S-CVX'' means strongly convex, ``PPM'' means proximal point method and ``APG'' means accelerated proximal gradient. 
    \subsection{The definitions for generic forms}
        \begin{definition}[S-CVX PPM APG]\label{def:S-CVX-PPM-APG}
            Let $h = f + g, \mathcal G_L, \mathcal T_L$ and parameter $L \ge \mu \ge 0$ be given by
            \hyperref[ass:smooth-non-smooth-lip-scvx]
            {Assumption \ref*{ass:smooth-non-smooth-lip-scvx}}
            Define the lower bouding function for any $x \in \RR^n$, we have for all $z \in \RR^n$: 
            \begin{align*}
                l_h(z; x) = h(\mathcal T_L x) + \langle \mathcal G_L (x), z - x\rangle
                + 
                \frac{L}{2}\Vert x - \mathcal T_L (x)\Vert^2 + \frac{\mu}{2}\Vert z - x\Vert^2
            \end{align*}
            For any positive sequence of stepsizes $(\tilde \eta_t)_{t \in \N}, (\eta_t)_{t \in \N}$, and any initial iterates $x_0 = y_0$, let parameter $L \ge \mu \ge 0$, we define an algorithm such that iterates $(x_t, y_t)_{t \in \N}$ is given by: 
            \begin{align*}
                x_{t + 1} &= \argmin_{x} \left\lbrace
                l_h(x; y_t) + \frac{1}{2\tilde \eta_{t}} 
                \Vert x - x_t\Vert^2
                \right\rbrace
                \\
                &= (\mu\tilde \eta_{t} + 1)^{-1} 
                (\mu\tilde \eta_{t}y_t + x_t - \tilde \eta_{t}\mathcal G_L(y_t))
                \\
                y_{t + 1}&= 
                \argmin_{x}
                \left\lbrace
                    h(\mathcal T_L y_t) + \langle \mathcal G_L(y_t), x - y_t\rangle + \frac{L}{2}\Vert x -y_t\Vert^2
                    + \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
                \right\rbrace
                \\
                &= (1 + L \eta_{t +1})^{-1}(L\eta_{t + 1}(y_t - L^{-1}\mathcal G_L(y_t)) + x_{t + 1})
            \end{align*}
        \end{definition}
        \begin{remark}
            For a derivation of the iterates see 
            \hyperref[proof:derivations-S-CVX-PPM-AGP]
            {Proof \ref*{proof:derivations-S-CVX-PPM-AGP}}. 
            The update sequence can be equivalently written as: 
            \begin{align*}
                w_{t} &= (\mu\tilde \eta_{t} + 1)^{-1}(\mu \tilde \eta_{t} y_t + x_t) 
                \\
                x_{t + 1}&= w_t - \tilde \eta_{t}(\mu\tilde \eta_{t} + 1)^{-1} \mathcal G_L(y_t)
                \\
                z_{t + 1}&= y_t - L^{-1}\mathcal G_L(y_t)
                \\
                y_{t + 1} &= (1 + L\eta_{t + 1})^{-1}(L\eta_{t + 1}z_{t + 1} + x_{t + 1}). 
            \end{align*}
            If $h = f$, so $g \equiv 0$ then $\mathcal G_L = \nabla f$, then this is equivalent to \cite[(6.24)]{ahn_understanding_2022}. 
        \end{remark}

        \begin{definition}[Generic APG]\label{def:generic-apg}
            Let $h = f + g, \mathcal G_L, \mathcal T_L, L$ be given by Assumption Set 4. 
            This algorithm is a special case of S-CVX Generic AG
            (\hyperref[def:S-CVX-PPM-APG]
            {Definition \ref*{def:S-CVX-PPM-APG}})
            and it's obtained by setting $\mu = 0$: 
            \begin{align*}
                x_{t + 1} &= x_t - \tilde \eta_{t} \mathcal G_L (y_t)
                \\
                z_{t + 1} &= y_t - L^{-1}\mathcal G_L (y_t)
                \\
                y_{t + 1} &= (1 + L \eta_{t + 1})^{-1}(L\eta_{t + 1}z_{t + 1} + x_{t + 1})
            \end{align*}

        \end{definition}

        \begin{definition}[S-CVX Generic Similar Triangle]\label{def:S-CVS-generic-similar-triangle}
            Let $h = f + g, \mathcal G_L, \mathcal T_L, L$ be given by 
            \hyperref[ass:smooth-non-smooth-lip-scvx]
            {Assumption \ref*{ass:smooth-non-smooth-lip-scvx}}
            Let iterates $(x_t, y_t, z_t)$ be given by S-CVX Generic AG
            (\hyperref[def:S-CVX-PPM-APG]
            {Definition \ref*{def:S-CVX-PPM-APG}}). 
            If in addition, the stepsizes $\eta_t, \tilde \eta_t$ satisfies equality 
            $$
                \tilde\eta_{t} = \eta_t + L^{-1} + L^{-1} \mu \tilde\eta_{t},
            $$
            then 
            $$
            x_{t + 1} = z_{t + 1} + 
            \frac{L\eta_t}{1 + \mu \tilde \eta_{t}} 
            (z_{t + 1} - z_t). 
            $$ 
        \end{definition}

        \begin{definition}[Generic Nesterov's Momentum]\label{def:generic-nesterov-momentum-form}
            Let $h = f + g, \mathcal G_L, \mathcal T_L$ and parameters $L$ be given by 
            \hyperref[ass:smooth-non-smooth-lip-scvx]
            {Assumption \ref*{ass:smooth-non-smooth-lip-scvx}}. 
            Then the momentum form of the algorithm has iterates satisfying the recurrences: 
            $$
            \begin{aligned}
                z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t)
                \\
                y_{t + 1 } &= z_{t + 1} + \theta_{t + 1}(z_{t + 1} - z_t)
            \end{aligned}
            $$
            For some $\theta_{t + 1} \ge 0$. 
        \end{definition}
        \begin{remark}
            It's called Nesterov's momentum because there is also inertia method which is a different class of methods because the point where the gradient is evaluated is different. 
        \end{remark}

    \subsection{Nesterov type acceleration in the literatures}
        \begin{definition}[Chambolle, Dossal 2015]\label{def:chambolle-dossal-2015}
            Let $h = f + g, \mathcal G_L, \mathcal T_L, L$ be given by 
            \hyperref[ass:smooth-non-smooth-lip-scvx]
            {Assumption \ref*{ass:smooth-non-smooth-lip-scvx}}. 
            The algorithm generates iterates $(y_n, x_n, z_n)$ and the sequence $(t_n)_{n \in \N}$ satisfying the recurrences for all $n \in \N \cup \{0\}$: 
            \begin{align}
                z_{n + 1} &= y_n - L^{-1}\mathcal G_L(y_n)
                \\
                x_{n + 1} &= z_n + t_{n} (z_{n + 1} - z_n), 
                \\
                y_{n + 1} &= \left(
                    1 - \frac{1}{t_{n + 1}}
                \right)z_{n + 1} + \left(
                    \frac{1}{t_{n + 1}}
                \right)x_{n + 1}. 
            \end{align}
            Where the sequence $t_n$ satisfies $t_{n + 1}^2 - t_n^2 \le t_{n + 1}$ for all $n \in \N\cup \{0\}$. 
        \end{definition}
        \begin{remark}
            This is proposed in Chambolle and Dossal \cite{chambolle_convergence_2015}'s paper. 
        \end{remark}

        \begin{definition}[V-FISTA]\label{def:v-fista}
            With $h = f + g, \mathcal G_L, \mathcal T_L$ and parameters $L \ge \mu > 0$  be given by Assumption Set 2. 
            Then V-FISTA is presented as: 
            \begin{align*}
                z_{t + 1} 
                &= y_t - L^{-1}\mathcal G_L(y_t)
                \\
                y_{t + 1} &= z_{t + 1} + 
                \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}
                (z_{t +1} - z_t). 
            \end{align*}
            Where the condition number $\kappa = L/\mu$. 
        \end{definition}
        \begin{remark}
            This is presented in Beck's Book chapter 10 \cite[(10.7.7)]{beck_first-order_nodate}. 
        \end{remark}

    \subsection{The relations between these forms}
        One pivotal intermadiate form is 
        \hyperref[def:S-CVS-generic-similar-triangle]
        {Definition \ref*{def:S-CVS-generic-similar-triangle}}(S-CVX Similar Triangle Form), 
        it is a special case of
        \hyperref[def:S-CVX-PPM-APG]
        {Definition \ref*{def:S-CVX-PPM-APG}}
        where algorithms in the literatures can be derived from it. 
        \hyperref[prop:s-cvx-generic-sim-triangle-from-s-cvx-ppm-apg]
        {Proposition \ref*{prop:s-cvx-generic-sim-triangle-from-s-cvx-ppm-apg}}
        Shows that S-CVX Similar Triangle Form is a special case of S-CVX PPM APG where the step sizes sequence $\tilde \eta_t, \eta_t$ satisfies equality: 
        $$
            \tilde \eta_t =\eta_t + L^{-1} + L^{-1}\mu \tilde \eta_t. 
        $$
        \hyperref[prop:momentum-equiv-to-similar-triangle]
        {Proposition \ref*{prop:momentum-equiv-to-similar-triangle}}
        shows that S-CVX Similar Triangle Form is algebraically equivalent to 
        \hyperref[def:generic-nesterov-momentum-form]{Definition \ref*{def:generic-nesterov-momentum-form}} (Generic Nesterov's Momentum form).
        The momentum parameters is linked to the step size parameters by the equality: 
        \begin{align*}
            \theta_{t + 1} = \frac{L\eta_t}{(1 + \mu \tilde\eta_{t})(1 + L\eta_{t + 1})}.
        \end{align*} 
        \hyperref[prop:cham-dossal-2015-is-similar-triangle]
        {Proposition \ref{prop:cham-dossal-2015-is-similar-triangle}}
        shows that Chambolle Dossal 2015 (\hyperref[def:chambolle-dossal-2015]
            {Definition \ref*{def:chambolle-dossal-2015}})
        is a special case of S-CVX Similar Triangle form where $\mu = 0$. 
        The parameters $t_n$ is the literatures is linked to the step size parameters $\tilde \eta_n, \eta_n$ by the equality: 
        \begin{align*}
            t_n = L \tilde \eta_n. 
        \end{align*}

        
\section{Lypunov analysis of the algorithm}
    In this section we prove the convergence rate of Similar triangle form when $\mu = 0$. 

% \printbibliography

\bibliographystyle{siam}
\bibliography{references/refs}

\appendix
\section{Postoned proofs}
    \subsection{Proof for S-CVX-PPM-APG}\label{proof:derivations-S-CVX-PPM-AGP}
        \begin{proof}
            (Proof of 
            \hyperref[def:S-CVX-PPM-APG]
            {Definition \ref*{def:S-CVX-PPM-APG}})
            \\
            The functions inside of ``argmin" is easy to solve because they are just quadratic functions. 
            We write it here for future verifications and a peace of the mind. 
            \begin{align*}
                x_{t + 1} &= \argmin_{x}\left\lbrace
                    \langle \mathcal G_L(y_t), x - y_t\rangle 
                    + 
                    \frac{\mu}{2}\Vert x - y_t\Vert^2 +  
                    \frac{1}{2\tilde \eta_{t}}\Vert x - x_t\Vert^2
                \right\rbrace
                \\
                \iff 
                \mathbf 0 & = 
                \mathcal G_L(y_t) + \mu(x - y_t) + \tilde \eta_{t}^{-1}(x - x_t)
                \\
                &= 
                \mathcal G_L(y_t) + (\mu + \tilde \eta_{t}^{-1}) x - \mu y_t - \tilde \eta_{t}^{-1} x_t
                \\
                \iff 
                (\mu + \tilde \eta_{t}^{-1})x 
                &= 
                \mu y_t + \tilde \eta_{t}^{-1} x_t - \mathcal G_L(y_t)
                \\
                \implies 
                x &= (\mu + \tilde \eta_{t}^{-1})^{-1 }
                (\mu y_t + \tilde \eta_{t}^{-1} x_t - \mathcal G_L(y_t)). 
            \end{align*}
            We can make the assumption that $\mu + \eta_{t}^{-1} > 0$ because $\tilde\eta_t > 0$. 
            Similarly for $y_{t + 1}$, it's solving a simple quadratic minimization problem, yielding: 
            \begin{align*}
                \mathbf 0 &= \mathcal G_L(y_t) + L(x - y_t) + \eta_{t + 1}^{-1}(x - x_{t + 1})
                \\
                &= (L + \eta_{t + 1}^{-1})x - L y_t - \eta_{t + 1}^{-1}x_{t + 1} + \mathcal G_L(y_t) 
                \\
                (L + \eta_{t + 1}^{-1})x &= 
                Ly_t + \eta_{t + 1}^{-1} x_{t + 1} - \mathcal G_L(y_t)
                \\
                \implies 
                x &= 
                (L\eta_{t + 1} + 1)^{-1}(L\eta_{t + 1}(y_t - L^{-1}\mathcal G_L(y_t)) + x_{t + 1}). 
            \end{align*}
            We had verified the results for the peace of the mind. 
        \end{proof}

    \subsection{Analysis of the forms}
        \begin{proposition}[S-CVX Generic similar triangle]
            \label{prop:s-cvx-generic-sim-triangle-from-s-cvx-ppm-apg}
            \hyperref[def:S-CVS-generic-similar-triangle]
            {Definition \ref*{def:S-CVS-generic-similar-triangle}}
            is a special case of the 
            \hyperref[def:S-CVX-PPM-APG]{Definition \ref*{def:S-CVX-PPM-APG}}. 
            (S-CVX PPM APG). 
            Suppose that $(x_t, y_t, z_t), \eta_t, \tilde \eta_t$ be the iterates and the stepsize sequences be given by the 
            S-CVX PPM APG.
            If in addition, the sequence $\tilde \eta_t, \eta$ satisfies the conditions for all $t \in \N$
            \begin{align*}
                \tilde\eta_{t} &= \eta_t + L^{-1} + L^{-1} \mu \tilde\eta_{t}, 
            \end{align*}
            then $x_{t +1}$ in the the AG Generic Form has alternative representation: 
            \begin{align*}
                x_{t + 1} &= 
                z_{t + 1} + 
                \frac{L\eta_t}{1 + \mu \tilde \eta_{t}} 
                (z_{t + 1} - z_t). 
            \end{align*}
            This would at the end, produce the following relations for all $t \in \N$: 
            \begin{align*}
                z_{t + 1} &= 
                y_t - L^{-1}\mathcal G_L(y_t), 
                \\
                x_{t + 1}&= 
                z_{t + 1} + \frac{L\eta_t}{1 + \mu\tilde \eta_{t}}(z_{t + 1} - z_t), 
                \\
                y_{t + 1}&= 
                (1 + L\eta_{t + 1})^{-1} (L\eta_{t + 1}z_{t + 1} + x_{t + 1}). 
            \end{align*}
        \end{proposition}
        \begin{proof}
            We start by showing that there exists a constant $\alpha \in \RR$ such that $z_{t + 1} - z_t = \alpha (x_{t + 1} - z_{t + 1})$ by $\tilde \eta_{t} = \eta_t + L^{-1} + L^{-1} \mu \tilde \eta_{t}$. 
            Firstly, we have the equality which it's proved at the end. 
            \begin{align*}
                z_{t + 1} - z_t
                &= 
                - (L\eta_t)^{-1} y_t 
                - L^{-1}\mathcal G_L(y_t) + (L \eta_t)^{-1} x_t. 
                \tag{1}
            \end{align*}
            Next, we have this equality which we proved later at the end: 
            \begin{align*}
                x_{t + 1} - z_{t + 1}
                &= 
                (1 + \mu\tilde \eta_{t})^{-1}
                \left(
                    x_t - y_t +     
                    \left(
                        - \tilde \eta_{t} + L^{-1}
                        + \mu \tilde \eta_{t}L^{-1}
                    \right)
                    \mathcal G_L(y_t)
                \right).\tag{2} 
            \end{align*}
            Since 
            \begin{align*}
                \tilde\eta_{t} &= \eta_t + L^{-1} + L^{-1} \mu \tilde\eta_{t}
                \\
                (1 - L^{-1}\mu)\tilde \eta_{t}
                &= L^{-1} + \eta_t 
                \\
                - \tilde \eta_{t} + L^{-1}\mu \tilde \eta_{t}
                + L^{-1}
                &= - \eta_t, 
            \end{align*}
            so substituting 
            \begin{align*}
                x_{t + 1} - z_{t + 1}
                &= 
                (1 + \mu \tilde \eta_{t})^{-1}
                (x_t - y_t - \eta_t \mathcal G_L(y_t))
                \\
                &= (1 + \mu \tilde \eta_{t})^{-1}
                \eta_t(\eta_{t}^{-1}(x_t - y_t) - \mathcal G_L(y_t))
                \\
                &= (1 + \mu \tilde \eta_{t})^{-1}
                \eta_t L(z_{t + 1} - z_t)
                \\
                x_{t + 1} &= 
                z_{t + 1} + 
                \frac{L \eta_t}{1 + \mu \tilde \eta_t}(z_{t + 1} - z_t). 
            \end{align*}
            To show (1): 
            \begin{align*}
                y_{t} &= (1 + L\eta_{t})^{-1}(L\eta_{t}z_{t} + x_{t})
                \\
                (1 + L\eta_t)y_t - x_t &= L\eta_t z_t
                \\
                z_t & = (L\eta_t)^{-1}((1 + L\eta_t)y_t - x_t), 
                \\[1em]
                z_{t + 1} - z_t 
                &= \underbrace{ y_t - L^{-1}\mathcal G_L(y_t)}_{=z_{t + 1}}
                - \underbrace{(L\eta_t)^{-1}((1 + L\eta_t)y_t - x_t)}_{=z_t}
                \\
                &= 
                y_t - L^{-1} \mathcal G_L(y_t) - (L\eta_t)^{-1}y_t - y_t + (L\eta_t)^{-1} x_t
                \\
                &= 
                -L^{-1}\mathcal G_L(y_t) + (L\eta_t)^{-1}(x_t - y_t)
                \\
                &= 
                L^{-1}(\eta_t^{-1}(x_t - y_t) -\mathcal G_L(y_t)). 
            \end{align*}
            To show (2): 
            \begin{align*}
                x_{t + 1} - z_{t + 1}&= 
                \left(
                    (1 + \mu \tilde \eta_{t})^{-1} (\mu \tilde \eta_{t }y_t + x_t)
                    - \frac{\tilde \eta_{t }}{1 + \mu\tilde \eta_{t }}
                    \mathcal G_L(y_t)
                \right) - \left(
                    y_t - L^{-1}\mathcal G_L(y_t)
                \right)
                \\
                &= 
                (1 + \mu \tilde \eta_{t})^{-1}
                \left(
                    x_t + \mu \tilde \eta_{t} y_t
                    - \tilde \eta_{t} \mathcal G_L(y_t)
                    - (1 + \mu \tilde \eta_{t})
                    (y_t - L^{-1}\mathcal G_L(y_t))
                \right)
                \\
                &= 
                (1 + \mu\tilde \eta_{t})^{-1}
                \left(
                    x_t - y_t + 
                    (
                        -\tilde \eta_{t} + 
                        (1 + \mu\tilde \eta_{t})L^{-1}
                    )
                    \mathcal G_L(y_t)
                \right)
                \\
                &= 
                (1 + \mu\tilde \eta_{t})^{-1}
                \left(
                    x_t - y_t +     
                    (
                        - \tilde \eta_{t} + L^{-1}
                        + \mu \tilde \eta_{t}L^{-1}
                    )
                    \mathcal G_L(y_t)
                \right). 
            \end{align*}
        \end{proof}

        \begin{proposition}[Momentum form is equivalent to Similar Triangle form]
            \label{prop:momentum-equiv-to-similar-triangle}
            \;\\
            Let iterates $(x_t, y_t, z_t), \eta_t, \tilde \eta_t$ be given by the relations in 
            \hyperref[def:S-CVS-generic-similar-triangle]
            {Definition \ref*{def:S-CVS-generic-similar-triangle}}
            then it's algebraically equivalent to: 
            \begin{align*}
                y_{t + 1} &= z_{t + 1} + 
                \frac{L\eta_t}{(1 + \mu \tilde\eta_{t})(1 + L\eta_{t + 1})}(z_{t + 1} - z_t)
                \\
                z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t). 
            \end{align*}
            Which fits
            \hyperref[def:generic-nesterov-momentum-form]
            {Definition \ref*{def:generic-nesterov-momentum-form}}
            with 
            $$
                \theta_{t + 1} = \frac{L\eta_t}{(1 + \mu \tilde\eta_{t})(1 + L\eta_{t + 1})}.
            $$
        \end{proposition}
        \begin{proof}
            From the similar triangle form, directly substituting the value of $x_{t + 1}$ into the expression for $y_{t + 1}$ it yields: 
            \begin{align*}
                y_{t + 1} &= 
                (1 + L\eta_{t + 1})^{-1} (L\eta_{t + 1}z_{t + 1} + x_{t + 1})
                \\
                &= 
                (1 + L\eta_{t + 1})^{-1}
                \left(
                    L\eta_{t + 1}z_{t + 1} + z_{t + 1} + \frac{L\eta_t}{1 + \mu\tilde \eta_t}(z_{t + 1} - z_t)
                \right)
                \\
                &= 
                (1 + L\eta_{t + 1})^{-1}
                \left(
                    (1 + L\eta_{t + 1})z_{t +1} + 
                    \frac{L\eta_t}{1 + \mu\tilde \eta_t}(z_{t + 1} - z_t)
                \right)
                \\
                &= 
                z_{t + 1} + 
                \frac{L\eta_t}{(1 + L\eta_{t + 1})(1 + \mu\tilde \eta_t)}
                (z_{t + 1} - z_t)
            \end{align*}
        \end{proof}
        
        \begin{proposition}[Chambolle Dossal as an instance of Similar Triangle Form]
            \label{prop:cham-dossal-2015-is-similar-triangle}
            \;\\
            Let iterates $(x_t, y_t, z_t)$ and stepszie sequence $\tilde \eta_t, \eta_t$ be generated by S-CVX Generic Similar Triangle Form
            (\hyperref[def:S-CVS-generic-similar-triangle]
            {Definition \ref*{def:S-CVS-generic-similar-triangle}}). 
            Then the algorithm are equivalent to Chambolle and Dossal 2015 algorithms 
            (\hyperref[def:chambolle-dossal-2015]
            {Definition \ref*{def:chambolle-dossal-2015}})
            if and only if we have sequence $t_n = L \tilde \eta_{n}$ and $\mu = 0$. 
        \end{proposition}
        \begin{proof}
            First, we present $\eta_t$ using $\tilde \eta_t$. 
            The Generic Similar Triangle Form admits has $x_{t + 1}$ to be: 
            \begin{align*}
                x_{t + 1} 
                &= z_{t + 1} + (1 + \mu\tilde \eta_{t})^{-1}L\eta_t (z_{t + 1} - z_t)
                \\
                &=  
                (1 + L\eta_t(1 + \mu \tilde \eta_{t})^{-1})z_{t + 1}
                - L\eta_t(1 + \mu\tilde \eta_{t})^{-1} z_t
                \\
                &= 
                \left(
                    \frac{1 + \mu\tilde \eta_{t} + L \eta_t}{1 + \mu \tilde \eta_{t}}
                \right)z_{t + 1}
                + 
                z_t - 
                \left(
                    1 + \frac{ L\eta_t}{\mu \tilde \eta_t + 1}
                \right)z_t
                \\
                & 
                \textcolor{gray}{
                    \text{use: } \tilde \eta_t = \eta_t + L^{-1} + L^{-1}\mu\tilde \eta_t
                }
                \\
                &= 
                \left(
                    \frac{L\tilde \eta_{t}}{1 + \mu\tilde \eta_{t}}
                \right)z_{t + 1}
                + 
                z_t 
                - 
                \left(
                    \frac{L\tilde \eta_{t}}{1 + \mu \tilde \eta_{t}}
                \right)z_t 
                \\
                &= z_t + 
                \left(
                    \frac{L\tilde \eta_{t}}{1 + \mu \tilde \eta_{t}}
                \right)
                (z_{t + 1} - z_t). 
            \end{align*}
            Recall that Chmabolle Dossal 2015 has $x_{n + 1} = z_t + t_n(z_{t + 1} - z_t)$, 
            therefore above suggests that $t_n = L \tilde \eta_{n}(1 + \mu \tilde \eta_{n})^{-1}$. 
            From the second equality to the third equality, we use the relations $L\tilde \eta_{t} = \eta_t + 1 + \mu\tilde \eta_{t}$.
            At the same time, we check that: 
            \begin{align*}
                y_{t + 1} &= \left(
                    \frac{L\eta_{t + 1}}{1 + L \eta_{t + 1}}
                \right)z_{t + 1} + 
                \left(
                    \frac{1}{1 + L \eta_{t + 1}}
                \right) x_{t + 1}
                \\
                &= 
                \left(
                    1 - \frac{1}{1 + L\eta_{t + 1}}
                \right)z_{t + 1} + 
                \left(
                    \frac{1}{1 + L \eta_{t + 1}}
                \right)x_{t + 1}. 
            \end{align*}
            Recall that in Chambolle Dossal 2015, itertes $y_{n + 1} = (1 - t_{n + 1}^{-1})z_{n + 1} + t_{n + 1}^{-1} x_{n + 1}$. 
            Therefore, the above results suggests that $t_{n + 1} = 1 + L\eta_{n + 1}$. 
            However, by the relations between $\tilde \eta_n, \eta_n$ as asserted via the similar triangle form, we have 
            \begin{align*}
                \tilde \eta_t &= \eta_t + L^{-1} + L^{-1}\mu \tilde \eta_t
                \\
                L\tilde \eta_t - \mu \tilde \eta_t 
                &= 
                L \eta_t + 1
                \\
                (L - \mu)\tilde \eta_t &= L\eta_t + 1. 
            \end{align*}
            For $t_n$ to be the same, these two have to be equivalent, but for all $\mu > 0$
            \begin{align*}
                L\tilde \eta_n (1 + \mu \tilde \eta_n)^{-1} \not = 
                (L - \mu )\tilde \eta_n. 
            \end{align*}
            Therefore, they are only equivalent when $\mu = 0$, then the generic similar triangle algorithm would be equivalently represented as the algrithm in Chambolle Dossal 2015's paper. 
        \end{proof}
        \begin{proposition}[V-FISTA is Similar Triangle Method]

            
        \end{proposition}

        

    

\end{document}
