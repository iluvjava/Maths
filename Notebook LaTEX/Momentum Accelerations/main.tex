\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont Nesterov Type Momentum Methods}}

\author{
    The Alto Hivemind (My Pen name)
    \thanks{
        Subject type, Some Department of Some University, Location of the University,
        Country. E-mail: \texttt{author.name@university.edu}.
    }
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    These are notes for Nesterov Type Acceleration Methods in the convex case. 
    They can be made into papers, proposals, and a thesis in the future. 
    They are tayped in \LaTeX\; so it's easier to work with. 
\end{abstract}
\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

\pagebreak
\tableofcontents
\pagebreak
% ==============================================================================
\section{Preliminaries}
    This section lists foundational results important for proof in the coming sections. 
    For this section, let the ambient space be $\RR^n$ and $\Vert \cdot\Vert$ be the 2-norm until specified in the context. 
    For a general overview of smoothness and strong convexity in the Euclidean space, see \cite[theorem 2.1.5, theorem 2.1.10]{nesterov_lectures_2018} for a full exposition of the topic. 
    \par\noindent
    We assume the reader is well-versed in convex optimization and convex analysis. 
    \subsection{Lipschitz smoothness}
        \begin{definition}[Lipschitz Smooth]
            Let $f$ be differentiable. 
            It has Lipschitz smoothness with constant $L$ if for all $x, y$
            $$
                \Vert \nabla f(x) - \nabla f(y)\Vert 
                \le 
                L \Vert x - y\Vert. 
            $$    
        \end{definition}

        \begin{theorem}[Lipschitz Smoothness Equivalence]
            With $f$ convex and $L$-Lipschitz smooth, the following conditions are equivalent conditions for all $x, y$: 
            \begin{enumerate}
                \item $L^{-1}\Vert \nabla f(y) - \nabla f(x)\Vert^2 \le \langle \nabla f(y) - \nabla f(x), y - x\rangle \le L\Vert y - x\Vert^2$. 
                \item $x^+\in \argmin_x f(x) \implies \frac{1}{2L}\Vert \nabla f(x)\Vert^2 \le f(x) - f(x^+) \le (L/2) \Vert x - x^+\Vert^2$, co-coersiveness. 
                \item $ 1/(2L)\Vert \nabla f(x) - \nabla f(y)\Vert^2 \le  f(y) - f(x) - \langle \nabla f(x), y -x\rangle \le (L/2)\Vert x - y\Vert^2$
            \end{enumerate}    
        \end{theorem}
        \begin{remark}
            Lipschitz smoothness of the gradient of a convex function is an example of a firmly nonexpansive operator.  
        \end{remark}

        \begin{definition}[Strong Convexity]
            With $f: \RR^n \mapsto \overline \RR$, it is strongly convex with constant $\alpha$ if and only if $f - (\alpha/2)\Vert \cdot\Vert^2$ is a convex function. 
        \end{definition}
        
        \begin{theorem}[Stong convexity equivalences]\label{thm:str_cvx_equiv}
            With $f:\RR^n \mapsto \overline \RR$ $\alpha$-strongly convex, the following conditions are equivalent conditions for all $x, y$: 
            \begin{enumerate}
                \item $f(y) - f(x) - \langle \partial f(x),y - x \rangle\ge \frac{\alpha}{2}\Vert y - x\Vert^2$
                \item $\langle \partial f(y) - \partial f(x), y - x\rangle \ge \alpha\Vert y - x\Vert^2$. 
                \item $f(\lambda x + (1 - \lambda)y) \le \lambda f(x) + (1 - \lambda)f(y) -\alpha\frac{\lambda(1 - \lambda)}{2}\Vert y - x\Vert^2, \forall \lambda \in [0, 1]$. 
            \end{enumerate}
        \end{theorem}

        \begin{theorem}[Strong convexity implications]\label{thm:str_cvx_implied}
            With $f:\RR^n \mapsto \overline \RR$ $\alpha$-strongly convex, the following conditions are implied: 
            \begin{enumerate}
                \item $\frac{1}{2}\operatorname{dist}(\mathbf 0; \partial f(x))^2 \ge \alpha (f(x) - f^+)$ where $f^+$ is a minimum of the function, and this is called the Polyak-Lojasiewicz (PL) inequality.
                \item $\forall x, y\in \mathbb E, u\in \partial f(x), v\in \partial f(y): \Vert u - v\Vert\ge \alpha\Vert x - y\Vert$. 
                \item $f(y) \le f(x) + \langle \partial f(x), y - x\rangle + \frac{1}{2\alpha}\Vert u - v\Vert^2, \forall u\in  \partial f(x), v\in \partial f(y)$. 
                \item $\langle \partial f(x)-\partial f(y), x - y\rangle \le \frac{1}{\alpha}\Vert u - v\Vert^2, \forall u\in \partial f(x), v\in \partial f(y)$. 
                \item if $x^+\in \arg\min_{x}f(x)$ then $f(x) - f(x^+) \ge \frac{\alpha}{2}\Vert x - x^+\Vert^2$ and $x^+$ is a unique minimizer. 
            \end{enumerate}
        \end{theorem}
        \begin{remark}
            In operator theory, the subgradient of a strongly convex function is an example of a Strongly Monotone Operator. 
        \end{remark}
    
    \subsection{Proximal descent inequality}
        The proximal descent inequality below is a crucial piece of inequality for deriving the behaviours of algorithms. 
        \begin{theorem}[Proximal Descent Inequality]\label{thm:ppm_descent_ineq}
            With $f: \RR^n \mapsto \overline \RR^n$ $\beta$-convex where $\beta \ge 0$, fix any $x \in \RR^n$, let $p = \hprox_f(x)$, then for all $y$ we have inequality 
            $$
                \left(f(p) + \frac{1}{2}\Vert x - p\Vert^2\right)
                - 
                \left(
                    f(y) + \frac{1}{2}\Vert x - y\Vert^2 
                \right)
                \le 
                - \frac{(1 + \beta)}{2}\Vert y - p\Vert^2. 
            $$
            Recall: $\hprox_f(x) = \argmin_{u}\left\lbrace f(u) + \frac{1}{2}\Vert u - x\Vert^2 \right\rbrace$. 
        \end{theorem}
        \begin{remark}
            We use this theorem to prove the convergence of the proximal point method. 
            See the proof (\cite{bauschke_convex_2017}, theorem 12.26). 
            The additional strong convexity index is a consequence of \hyperref[thm:str_cvx_implied]{theorem \ref*{thm:str_cvx_implied}}, item (v). 
        \end{remark}
        
        \begin{theorem}[The Bregman proximal descent inequality]\label{thm:ppm_breg_descent_ineq}
            Let $\omega$ induce a Bregman Divergence $D_\omega$ in $\RR^n$ and assume that it satisfies Bregman Prox Admissibility conditions for the function $\varphi: \RR^n \mapsto \overline \RR$.
            Then we claim that for all $c \in \text{dom}(\omega), b \in \text{dom}(\partial \omega)$, 
            If 
            \begin{align*}
                a = \argmin_{x}\left\lbrace
                \varphi(x) + D_\omega(x, b)
                \right\rbrace, 
            \end{align*}
            we have the inequality, 
            \begin{align*}
                (\varphi(c) + D_\omega(c, b)) - 
                (\varphi(a) + D_\omega(a, b)) \ge 
                D_\omega(c, a). 
            \end{align*}
        \end{theorem}
        \begin{remark}
            For more information about what function $\omega$ can induce a Bregman divergence and the admissibility conditions for Bremgna proximal mapping, consult Heinz et.al \cite{bauschke_descent_2017}. 
        \end{remark}


\section{The proximal point method with convexity}
    This section reviews the convex case's Proximal point method (PPM) analysis and generalizes the theories to approximated PPM. 
    \subsection{Convex PPM literature reviews}
        Rockafellar \cite{rockafellar_monotone_1976} pioneered the analysis of the proximal point method in the convex case. 
        He developed the analysis in the context of maximal monotone operators in Hilbert spaces. 
        Applications in convex optimizations are covered. 
        Using his theorems appropriately requires some opportunities, realizations, and characterizations of assumptions (A), (B) in his paper in the context of the applications. 
        \par\noindent
        In this section, we will use the result from Rockafellar that, if a monotone operator $A$ is $\beta$ strongly convex, then the resolvent operator $\mathcal J_A = [I + A]^{-1}$ is a $(1 + \beta)^{-1}$ Lipschitz operator, making $I - \mathcal J_A$ is a $1 - (1 + \beta)^{-1}$ a strongly monotone operator. 
        

    \subsection{The proximal point method}
        With $f: \RR^n \mapsto \overline \RR$ lsc proper and convex, given any $x_0$ the PPM generates sequence $(x_n)_{n\in \NN}$ by $x_{k + 1} = \hprox_{\eta_{k + 1}}f(x_k)$ for all $k \in \NN$ where the sequence $(\eta_{k})_{k \in \NN}$ is a nonegative sequence of real numbers.
        

    \subsection{The Lyapunov function of convex PPM}
        We present some theorems that illustrate the use of \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}}. 
        The readers can find similar analyses and techniques in Guler's work \cite{guler_convergence_1991}. 
        \begin{theorem}[PPM Lyapunov Function]\label{ppm_lyapunov}
            With $f$ being $\beta\ge 0$ convex (it's strongly convex if $\beta > 0$, else it's just convex) and $x_{t + 1} = \hprox_{\eta_{t + 1}f}$ generated by PPM. 
            Define the Lyapunov function $\Phi_t$ for all $u \in \RR^n$: 
            \begin{align*}
                \Phi_t &:= 
                \left(
                    \sum_{i = 1}^{t} \eta_i
                \right)(f(x_t) - f(u)) + \frac{1}{2} \Vert u - x_t\Vert^2 \quad \forall t \ge 1, 
                \\
                \Phi_0 &:= (1/2)\Vert x_0 - u\Vert^2, 
            \end{align*}
            then it is a Lyapunov function for the PPM algorithm. 
            Meaning for all $(x_k)_{k \in \NN}$ generated by PPM, it satisfies that $\Phi_{t + 1} - \Phi_t \le 0$. 
            Additionally, by definition, we have 
            {\small
                \begin{align*}
                    \Phi_{t + 1} - \Phi_{t} 
                    &= 
                    \left(\sum_{i = 1}^{t}\eta_{i}\right)
                    (f(x_{t + 1}) - f(x_t)) 
                    + \frac{1}{2}\Vert x_{t + 1} - u\Vert^2 
                    - \frac{1}{2}\Vert x_{t} - u\Vert^2
                    +
                    \eta_{t + 1}(f(x_{t + 1}) - f(u))
                    \\
                    & \le 
                    -\left(\sum_{i = 1}^{t}\eta_{i}\right)
                    (1 + \beta\eta_{t + 1}/2)\Vert x_{t + 1} - x_t\Vert^2
                    + 
                    \left(
                        - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                        -\frac{\beta\eta_{t + 1}}{2}\Vert u - x_{t + 1}\Vert^2
                    \right)
                    \\
                    & \le 0,
                \end{align*}
            }
            And additionally, recovering the descent lemma: 
            \begin{align*}
                f(x_{t + 1}) - f(x_t) 
                &\le
                -\frac{1}{\eta_{t+1}} \Vert x_{t + 1} - x_t\Vert^2 
                - \frac{\beta}{2}\Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Let $\phi_{t + 1}: \RR^n \mapsto \overline \RR = \eta_{t + 1} f$ be convex,  consider proximal point method $x_{t + 1} = \hprox_{\phi}(x_t)$, apply \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}}, we have $\forall u \in \RR^n$
            \begin{align*}
                & \phi_{t + 1}(x_{t + 1}) 
                + 
                \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                - \phi_{t + 1}(u) - \frac{1}{2}\Vert u - x_t\Vert^2
                \le 
                - 
                \frac{1}{2}(1 + \beta\eta_{t + 1})\Vert 
                    u - x_{t + 1}
                \Vert^2
                \\
                & \text{let } u = x_*
                \\
                &\quad 
                \begin{aligned}
                    \implies &
                    \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) 
                    + 
                    \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2 
                    +  
                    \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                    -
                    \frac{1}{2}\Vert x_* - x_t\Vert^2 
                    \\
                    & \quad \le 
                    -\frac{\beta\eta_{t + 1}}{2}\Vert x_* - x_{t + 1}\Vert^2
                    \\
                    \iff & 
                    \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) 
                    + 
                    \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2 
                    -
                    \frac{1}{2}\Vert x_* - x_t\Vert^2 
                    \\
                    & \quad \le 
                    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                    -\frac{\beta\eta_{t + 1}}{2}\Vert x_* - x_{t + 1}\Vert^2
                    \le 0. 
                \end{aligned}
                \\
                & \text{let } u = x_{t}
                \\
                &\quad  
                \begin{aligned}
                    \implies& 
                    f(x_{t + 1}) - f(x_t)
                    \le 
                    -\frac{1}{\eta_{t+1}} \Vert x_{t + 1} - x_t\Vert^2 
                    - \frac{\beta}{2}\Vert x_t - x_{t + 1}\Vert^2
                    \le 0. 
                \end{aligned}
            \end{align*}
            Let's define the following quantities for all $u, \beta\ge 0$: 
            \begin{align*}
                \Upsilon_{1, t + 1}(u) &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(u)) + \frac{1}{2}(
                    \Vert x_{t + 1} - u\Vert^2 - 
                    \Vert x_t - u\Vert^2
                )
                \\
                & \le 
                - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                -\frac{\beta\eta_{t + 1}}{2}\Vert u - x_{t + 1}\Vert^2, 
                \\
                \Upsilon_{2, t + 1}
                &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t))
                \\
                &\le 
                - \Vert x_{t + 1} - x_t\Vert^2 - 
                \frac{\beta\eta_{t + 1}}{2}
                \Vert x_{t + 1} - x_t\Vert^2 
                \\
                &= 
                -(1 + \beta\eta_{t + 1}/2)\Vert x_{t + 1} - x_t\Vert^2 \le 0. 
            \end{align*}
            With $\Phi_t$ as defined in the theorem, observe the following demonstration for all $u$, $\beta \ge 0$: 
            {\small
            \begin{align*}
                \Phi_{t + 1} - \Phi_{t}
                &= 
                \left(
                    \sum_{i = 1}^{t + 1}\eta_i
                \right)(f(x_{t + 1}) - f(u)) + 
                \frac{1}{2}\Vert x_{t + 1} - u\Vert^2 
                - 
                \left(
                    \sum_{i = 1}^{t}\eta_i
                \right)(f(x_{t}) - f(u))
                - 
                \frac{1}{2}\Vert x_{t} - u\Vert^2
                \\
                &= 
                \left(\sum_{i = 1}^{t}\eta_{i}\right)
                (f(x_{t + 1}) - f(x_t)) 
                + \frac{1}{2}\Vert x_{t + 1} - u\Vert^2 
                - \frac{1}{2}\Vert x_{t} - u\Vert^2
                +
                \eta_{t + 1}(f(x_{t + 1}) - f(u))
                \\
                &= 
                \left(\sum_{i = 1}^{t}\eta_{i}\right)\Upsilon_{2, t + 1}
                + \Upsilon_{1, t + 1}(u)
                \\
                &\le 
                -\left(\sum_{i = 1}^{t}\eta_{i}\right)
                (1 + \beta\eta_{t + 1}/2)\Vert x_{t + 1} - x_t\Vert^2
                + 
                \left(
                    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                    -\frac{\beta\eta_{t + 1}}{2}\Vert u - x_{t + 1}\Vert^2
                \right)
                \le 0. 
            \end{align*}
            }
            Therefore, $\Phi_t$ is a legitimate Lyapunov function for all $u, \beta \ge 0$. 
        \end{proof}
        \begin{remark}
            The above Lyapunov is not unique, and it's not optimal for $\beta > 0$, strictly strongly convex functions. 

        \end{remark}

        \begin{theorem}[Convergence Rate of PPM]\label{thm:ppm_convergence_rate}
            The convergence rate of PPM applied to $f$, closed, convex proper, we have the convergence rate of the function value: 
            $$
            f(x_T) - f(x_*) \le O\left(\left(\sum_{i=1}^{T}\eta_t\right)^{-1}\right). 
            $$
            Where $x_*$ is the minimizer of $f$. 
        \end{theorem}
        \begin{proof}
            With $\Delta_t = f(x_t) - f(x_*), \Upsilon_t = \sum_{i = 1}^{t}\eta_i$ so $\Phi_t = \Upsilon_t\Delta_t + \frac{1}{2}\Vert x_t - x_*\Vert^2$ by consideration $u = x_*$, invoking previous theorem and do
            \begin{align*}
                \Upsilon_T\Delta_T \le \Phi_T 
                &\le 
                \Phi_0 = \frac{1}{2}\Vert x_0 - x_*\Vert^2 
                \\
                \implies \Delta_T 
                &\le 
                \frac{1}{2\Upsilon_T} \Vert x_0 - x_*\Vert^2. 
            \end{align*}
        \end{proof}
        \begin{remark}
            With the same choice of the sequence $(\eta_t)_{t \in \NN}$, convergence of the PPM method of a strongly convex function is faster. 
            The above proof is the same for $\beta = 0$, or $\beta > 0$, because it didn't use the property that $\eta_{t + 1}f$ is a $\eta_{t + 1}\beta$ strongly convex function. 
        \end{remark}

        \begin{theorem}[PPM Strongly Convex Lyapunov Function]\label{ppm_lyapunov_scvx}
            With $f$ being $\beta > 0$ strogly convex, with $x_{t + 1} = \hprox_{\eta_{t + 1}f}(x_t)$, then $\Phi_t = \Vert x_{t} - x_*\Vert$ is a Lyapunov function satisfying: 
            \begin{align*}
                \frac{\Vert x_{t + 1} - x_*\Vert}{
                    \Vert x_k - x_*\Vert
                } &\le (1 + \eta_{t + 1}\beta)^{-1}. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            This is a direct application that $\hprox_{\eta_{t + 1}f}$ is a contraction with constant $(1 + \beta\eta_{t +1})^{-1}$. 
        \end{proof}
        \begin{remark}
            It's still a mystery on how to show $f(x_t) - f(x_*)$ is a Lyapunov function. 
            The answer is not realized by us, nor it is in Rockafellar \cite{rockafellar_monotone_1976} or Guler's\cite{guler_convergence_1991} writings. 
            Do observe that, by the choice of $x_*$, the contraction property of the proximal operator is strictly stronger than necessary. 
            This inequality at the end is tighter than what we derived for gradient descent. 
        \end{remark}
        
\section{Applying the analysis of PPM}
    The PPM method and the Lyaounov function derived above serve as the template for other algorithms. 
    As an appetizer, we present an analysis of gradient descent using theorems related to the convergence of PPM
    \par\noindent
    In optimizations, people use a lower or an upper approximation of the objective function to approximate the PPM. 
    The methodology includes a diverse range of approaches.
    For example, it includes first-order optimization, such as gradient descents, and second-order algorithms, such as Newton's method. 
    Its scope broadens to primal-dual optimization algorithms with creativities in the Lyapunov functions or theories in monotone operators. 
    \par\noindent
    To demonstrate, assume that $f$ is a lsc convex function such that it can be approximated by a lower bounding function $l_f(x|\bar x)$ at $\bar x$ such that it satisfies for all $x$: 
    \begin{align*}
        l_f(x| \bar x) 
        \le f(x) \le l_f(x|\bar x) + \frac{L}{2}\Vert x - \bar x\Vert^2. 
    \end{align*}
    The above characterization is generic enough to include the case where $l_f(x|\bar x)$, the under-approximating function is nonsmooth. 
    We assume that $l_f(x|\bar x)$ is convex for all $x$, at all $\bar x$, so the previous theorems apply. 
    \par\noindent
    The approximated proximal point method applies PPM to the function $l_f(x|x_t)$ for each iteration, i.e.: $x_{t +1} = \hprox_{\eta_{t + 1}l_f(\cdot | x_t)}(x_t)$. 
    \subsection{Generic gradient descent}
        We will consider deriving gradient descent via the PPM approach as a warm-up. 
        Please pay attention to the remarks. They reveal parts of the proof that could inspire the idea of a non-monotone line search method in practical settings. 
        \begin{theorem}[Generic Approximated PPM]\label{thm:lower_approx_ppm_convergence}
            With $f$ convex having minimizer: $x_*$; $l_f(\cdot; x_t)$ convex, lsc and proper, define $\phi_t(x) = \eta_{t + 1}l_f(x; x_t)$. 
            Assume the following estimates hold: 
            $$
            \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) 
            + 
            \frac{L\eta_{t + 1}}{2}\Vert x - x_t\Vert^2 \quad \forall x \in \RR^n. 
            $$ 
            Fix any $x_0$, let the iterates $x_t$ defined for $t\in \NN$ satisfies
            \begin{align*}
                x_{t +1} = \argmin_{x} \left\lbrace
                    l_f(x; x_t) + \frac{1}{2\eta_{t + 1}}\Vert x - x_t\Vert^2
                \right\rbrace, 
            \end{align*}
            then it has 
            \begin{align*}
                \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) + \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2
                - \frac{1}{2}\Vert x_* - x_t\Vert^2
                & \le 
                \left(
                    \frac{L \eta_{n + 1}}{2} - \frac{1}{2}
                \right)\Vert x_{t + 1} - x_t\Vert^2.
            \end{align*}
            Additionally if $\exists \epsilon > 0: \eta_{t} \in (\epsilon, 2L^{-1} - \epsilon)$, for all $t \in \NN$, the algorithm has sublinear convergence rates of
            \begin{align*}
                f(x_T) - f(x_*)
                &\le 
                \frac{L - \epsilon^{-1}}{TL\epsilon} (f(x_0) - f(x_T)) 
                \\
                &\le 
                \frac{L - \epsilon^{-1}}{TL\epsilon} (f(x_0) - f(x_*)) 
            \end{align*}
        \end{theorem}
        \begin{proof}
            By $\phi_t$ convex, apply \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}} with $f = \phi_t$, $x = x_t$, $x_{t + 1} = p$, yielding $\forall y$
            {\footnotesize
                \begin{align*}
                    \phi_t(x_{t + 1}) + \frac{1}{2}\Vert x_t - x_{t + 1}\Vert^2 
                    - 
                    \phi_t(y) - \frac{1}{2}\Vert x_t - y\Vert^2
                    &\le 
                    - \frac{1}{2}\Vert y - x_{t + 1}\Vert^2
                    \\
                    \phi_t(x_{t + 1}) - \phi_t(y)
                    + \frac{1}{2}(
                        \Vert y - x_{t + 1}\Vert^2 - \Vert x_t - y\Vert^2 
                    ) &\le 
                    - \frac{1}{2} \Vert x_t - x_{t + 1}\Vert^2
                    \\
                    \left(
                        \phi_t(x_{t + 1}) + \frac{L\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert
                    \right)
                    - \phi_t(y)
                    + \frac{1}{2}(
                        \Vert y - x_{t + 1}\Vert^2 - \Vert x_t - y\Vert^2 
                    ) &\le 
                    \left(
                        \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                    \right) \Vert x_t - x_{t + 1}\Vert^2
                    \\
                    \implies 
                    \eta_{t + 1}f(x_{t + 1}) - \eta_{t + 1}f(y) 
                    + \frac{1}{2}(
                        \Vert y - x_{t + 1}\Vert^2 - \Vert x_t - y\Vert^2 
                    ) 
                    &\le 
                    \left(
                        \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                    \right) \Vert x_t - x_{t + 1}\Vert^2. 
                \end{align*}
            }
            Setting $y = x_t$ yields
            \begin{align*}
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t)) + 
                \frac{1}{2}
                    \Vert x_t - x_{t + 1}\Vert^2
                &\le 
                \left(
                        \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                \right) \Vert x_t - x_{t + 1}\Vert^2
                \\
                \iff 
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t))
                &\le 
                \left(
                        \frac{L\eta_{t + 1}}{2} - 1
                \right) 
                \Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
            In a similar manner to the derivation of the Lyapunov function for PPM, we make for all $y$: 
            \begin{align*}
                \Upsilon_{1, t + 1}(y) &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(y)) + \frac{1}{2}(
                    \Vert x_{t + 1} - y\Vert^2 - 
                    \Vert x_t - y\Vert^2
                )
                \\
                & \le 
                \left(
                    \frac{L\eta_{t + 1}}{2} - \frac{1}{2}
                \right) \Vert x_t - x_{t + 1}\Vert^2, 
                \\
                \Upsilon_{2, t + 1}
                &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t))
                \\
                &\le 
                \left(
                    \frac{L\eta_{t + 1}}{2} - 1
                \right) 
                \Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
            Now, consider defining $\Phi_t$ for all $y$: 
            $$
                \Phi_t = \left(
                    \sum_{i = 1}^{t} \eta_{i}
                \right)
                (f(x_t) - f(y)) + \frac{1}{2}\Vert y - x_t\Vert^2, 
            $$
            it is the proposed Lyapunov function for PPM; we define the base case $\Phi_0 = \frac{1}{2}\Vert y - x_0\Vert^2$. 
            Consider the difference $\forall y$: 
            \begin{align*}
                \Phi_{t + 1} - \Phi_t
                &= 
                \left(\sum_{i = 1}^{t}\eta_{i}\right)\Upsilon_{2, t + 1}
                + \Upsilon_{1, t + 1}(y)
                \\
                &\le 
                \left(\sum_{i = 1}^{t}\eta_{i}\right) 
                \left(
                    \frac{L\eta_{t + 1}}{2} - 1
                \right) 
                \Vert x_t - x_{t + 1}\Vert^2 + 
                \left(
                    \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                \right) \Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
            Observe that if $\eta_i \le L^{-1}$, then $\Phi_{t + 1} - \Phi_t \le 0$, hence the convergence rate of $\mathcal O\left((\sum_{i = 1}^{t}\eta_i)^{-1}\right)$ of PPM for $\Phi_t$ is applicable. 
            \par\noindent
            Surprisingly, if $\eta_i \in (0, 2L^{-1})$, $\Phi_{t}$ still converges under mild conditions. 
            For simplicity we set $\sigma_t := \sum_{i = 1}^{t}\eta_i$. 
            It starts with considerations that $(L\eta_{t + 1}/2 - 1) < 0$, so that 
            \begin{align*}
                f(x_{t + 1}) - f(x_t) &\le 
                \left(\frac{L\eta_{t + 1}}{2} - 1\right)\Vert x_{t + 1} - x_t\Vert^2
                \\
                f(x_T) - f(x_0)
                &\le 
                \underbrace{
                \left(
                    \frac{L\sigma_T}{2} - T
                \right)
                }_{< 0}
                \sum_{t = 0}^{T - 1}\Vert x_{t + 1} - x_t\Vert^2
                \\
                \implies 
                \sum_{t = 0}^{T -1}\Vert x_t - x_{t + 1}\Vert^2
                &\le 
                \left(
                    \frac{L}{2}\sigma_T  - T
                \right)^{-1} 
                (f(x_T) - f(x_0))
            \end{align*}
            Continue on the RHS of $\Phi_{t + 1} - \Phi_t$ so 
            \begin{align*}
                \sum_{t = 0}^{T - 1}\Phi_{t + 1} - \Phi_t 
                &\le 
                \left(
                    \frac{L}{2}\sigma_T - \frac{T}{2}
                \right)\sum_{t = 0}^{T - 1}\Vert x_{t + 1} - x_t\Vert^2
                \\
                \Phi_T - \Phi_0 &\le 
                \left(
                    \frac{\frac{L}{2}\sigma_T - \frac{T}{2}}{
                        \frac{L}{2}\sigma_T - T
                    }
                \right)
                (f(x_T) - f(x_0))
                \\
                &= 
                \left(
                    \frac{L\sigma_T - T}{L\sigma_T - 2T}
                \right)
                (f(x_T) - f(x_0)), 
            \end{align*}
            implies
            \begin{align*}
                \sigma_T (f(x_T) - f(y)) + \frac{1}{2}\Vert y - x_t\Vert^2
                - \frac{1}{2}\Vert y - x_0 \Vert^2 
                &\le 
                \left(
                    \frac{L\sigma_T - T}{L\sigma_T - 2T}
                \right)
                (f(x_T) - f(x_0))
                \\
                \iff
                f(x_T) - f(y) + 
                \frac{1}{2\sigma_T}(\Vert y - x_t\Vert^2 - \Vert y - x_0\Vert^2)
                &\le 
                \left(
                    \frac{L- T\sigma_T^{-1}}{2T - L\sigma_T}
                \right)
                (f(x_0) - f(x_T)), 
            \end{align*}
            therefore, we obtain the bound: 
            \begin{align*}
                f(x_T) - f(y) &\le 
                \left(
                    \frac{L- T\sigma_T^{-1}}{2T - L\sigma_T}
                \right)
                (f(x_0) - f(x_T))
                - 
                \frac{1}{2\sigma_T}(\Vert y - x_t\Vert^2 - \Vert y - x_0\Vert^2)
            \end{align*}
            In the case where $\sup_{i\in \NN} \eta_i \le 2L^{-1} - \epsilon$, and $\inf_{i\in \NN}\eta_i \ge \epsilon$ with $\epsilon > 0$. 
            Then we have 
            \begin{align*}
                \frac{L -T\sigma_T^{-1}}{2T - L\sigma_T}
                &\le 
                \frac{L - \epsilon^{-1}}{2T - LT(2L^{-1} - \epsilon)}
                \\
                &= 
                \frac{L - \epsilon^{-1}}{2T - T(2 - L\epsilon)}
                \\
                &= 
                \frac{L - \epsilon^{-1}}{TL\epsilon}. 
            \end{align*}
            With $y = x_*$, we get the claimed convergence rate because $f(x_t)$ is strictly monotone decreasing. 
        \end{proof}
        \begin{remark}
            Observe that inequality 
            $$
                \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) 
                + 
                \frac{L\eta_{t + 1}}{2}\Vert x - x_t\Vert^2 \quad \forall x \in \RR^n, 
            $$
            was invoked with $x = x_{t + 1}$ for the PPM descent inequality in the above proof, meaning that if  $\forall (x_t)_{t \in \NN}$ generated by the algorithm, $\exists (L_t)_{t \in \NN}$ such that
            $$
                \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) 
                + 
                \frac{L_t\eta_{t + 1}}{2}\Vert x - x_t\Vert^2,
            $$
            where the algorithm generates the sequence. 
            By smartly choosing the function $\phi_{t + 1}$ at each iteration, we can increase the stepsize while retaining a similar convergence proof. 
            In a practical setting, when $L_t = L$, and $\phi_{t}(x) = \eta_{t + 1}f$, this is called a line search.
            \par\noindent
            The convergence rate is loose, and when $f$ exhibits additional favourable properties, such as being strongly convex, the convergence rate can be faster. 
            Furthermore, if the choice of $y$ remains arbitrary, then the theorem is applicable for function without minimizers. 
        \end{remark}

    \subsection{Examples}
        \begin{example}[Convergence of the proximal gradient method]
            This section \\ illustrates algorithms that satisfy the above proof's lower and upper bound estimates. 
            Consider $f = g + h$ with $h$ nonsmooth convex, and $g$ being $L$-Lipschitz smooth convex and differentiable. 
            Define $D_g(x, y) = g(x) - g(y) - \langle \nabla f(x), y - x\rangle$, $l_g(x; y) = g(y) + \langle \nabla g(y), y - x\rangle$, which is the Bregman divergence of the function $g$. 
            Consider for all $x$: 
            \begin{align*}
                0 & \le 
                D_g (x, y) \le  \frac{L}{2} \Vert x - y\Vert^2 
                \\
                l_g (x; y) &\le 
                g(x) 
                \le l_g(x; y) + \frac{L}{2} \Vert x - y\Vert^2 
                \\
                h(x) + l_g(x; y) &\le f(x) = g(x) + h(x)
                \le 
                l_g(x; y) + h(x) + \frac{L}{2}\Vert x - y\Vert^2. 
            \end{align*}
            Define $\phi_{t + 1} (x) = \eta_{t + 1}(h(x) + l_g(x; x_t))$, then results from previous theorems apply.    
        \end{example}
        \begin{remark}
            The envelope interpretation restricts the use of the theorem since it requires that the proximal operator be a resolvent of a gradient. 
            Extending the usage of the PPM descent inequality to other contexts requires operator theories and creativities. 
        \end{remark}

        \begin{example}[The fundamental proximal gradient lemma]
            The fundamental proximal gradient lemma was used heavily in the literature to derive convergence results in the convex case. 
            The "fundamental proximal gradient lemma" originates from Beck's writings \cite[theorem 10.16]{beck_first-order_nodate}. 
            We demonstrate in this example that it's a consequence of \hyperref[thm:ppm_breg_descent_ineq]{theorem \ref*{thm:ppm_breg_descent_ineq}}. 
            \par
            With $f = g + h$, $h$ convex, lsc, $g$ be $L$-Lipschitz smooth, then for all $y\in \RR^n$, $x\in \RR^n$, $y^+ := \hprox_{L^{-1}h}(y - L^{-1}\nabla (y))$ satisfies: 
            \begin{align*}
                f(x) - f(y^+) \ge \frac{L}{2}\Vert x - y^+\Vert^2 - \frac{L}{2}\Vert x - y\Vert^2 + D_g(x, y).
            \end{align*}
            A similar analysis as \hyperref[thm:lower_approx_ppm_convergence]{ theorem \ref*{thm:lower_approx_ppm_convergence}} with \hyperref[thm:ppm_breg_descent_ineq]{theorem \ref*{thm:ppm_breg_descent_ineq}} obtains the same inquality. 
            With $\phi(y) = \eta(h(y) + g(x) + \langle \nabla g(x), y - x\rangle)$ as an lower bounding function of $f$. 
            Choose any $x$, let $x^+ = \hprox_\phi(x)$, then for all $u$:
            {\small
            \begin{align*}
                & \phi(u) + \frac{1}{2}\Vert u - x\Vert^2 - \phi(x^+) - \frac{1}{2}\Vert x^+ - x\Vert^2 
                \ge \frac{1}{2}\Vert x^+ - u\Vert^2
                \\
                \implies &
                \eta\underbrace{
                    \left(
                        h(u) + g(x) + \langle \nabla g(x), u - x\rangle 
                    \right)
                }_{= \phi(u)} 
                - \eta \underbrace{f(x^+)}_{\le \phi(x^+)} - \frac{1}{2}\Vert x - x^+\Vert^2 
                + \frac{1}{2} \Vert u - x\Vert^2 
                \ge 
                \frac{1}{2}\Vert x^+ - u\Vert^2 
                \\
                \iff & 
                f(u) + \left(
                    g(x) - g(u) + \langle \nabla g(x), u -x\rangle 
                \right)
                - f(x^+) - \frac{1}{2\eta}\Vert x - x^+\Vert^2
                + 
                \frac{1}{2}\Vert u - x\Vert^2 
                \ge 
                \frac{1}{2\eta}\Vert x^+ - u\Vert^2 
                \\
                \iff 
                & f(u) - f(x^+) - D_g(u, x)
                + \frac{1}{2\eta} \Vert u - x\Vert^2 - \frac{1}{2\eta}\Vert x^+ - x\Vert^2
                \ge 
                \frac{1}{2\eta} \Vert x^+ - u\Vert^2. 
            \end{align*}
            }
            \par\noindent
            Removing the negative term $-1/2\eta \Vert x - x^+\Vert^2$ makes LHS larger, establishing the fundamental proximal gradient lemma. 
        \end{example}
        \begin{remark}
            Linking the PPM descent inequality to the Bregman divergence of the smooth part of the function on parameters $u, x$ is a clever move. 
        \end{remark}

        
\section{Accelerated gradient descent and PPM}
    Recent works from Ahn and Sra \cite{ahn_understanding_2022} and Nesterov \cite{nesterov_lectures_2018} inspired content in this section.
    Ahn explored the interpretation of Nesterov acceleration via PPM. 
    They proposed the idea of ``similar triangle" for unifying all varieties of Nesterov accelerated gradient. 
    They used PPM to derive several variations of the Nesterov accelerated gradient algorithms. 
    Finally, they refurnished \hyperref[thm:lower_approx_ppm_convergence]{theorem \ref*{thm:lower_approx_ppm_convergence}} for the proof of convergence rate for the accelerated gradient.
    Their analysis results in relatively simple arguments that exhibits powerful extensions to several variants of the Nesterov accelerated gradient. 
    
    \par\noindent
    Interestingly, the Nesterov accelerated gradient applies to PPM too; Guler \cite{guler_new_1992} did it two decades ago. 
    He uses the idea of a Nesterov acceleration sequence faithfully. 
    One recent development of the accelerated PPM is an algorithmic framework named: ``Universal Catalyst acceleration", proposed by Lin et al \cite{lin_universal_2015}. 
    It is an application of Guler's work in the context of variance-reduction stochastic gradient algorithms for machine learning. 
    
    \par\noindent
    \textcolor{red}{We state our contributions}. 
    In \hyperref[sec:AG_varieties]{section \ref*{sec:AG_varieties}} 
    We stated different forms of accelerated gradient that appeared in the literatures and the equivalences between their forms. 
    They are abstract because the choice of stepsize parameters in the algorithms are unspecified. 
    In \hyperref[sec:AG_useful_information]{section \ref*{sec:AG_useful_information}}
    we prove and interpret some of the unproved claims made by Ahn, Sra \cite{ahn_understanding_2022} as good exercises and for the peace of the mind.
    \hyperref[lemma:smooth_agg_lyapunov_upper_bound]{Lemma \ref*{lemma:smooth_agg_lyapunov_upper_bound}} 
    rephrase content in the appendix in work by Ahn, Sra\cite{ahn_understanding_2022}. 
    It is our attempt at rephrasing the proofs to create a layer of abstraction so it can be reused for multiple proofs for several variants. 

    \subsection{Preliminaries}
        We introduce some additional lemma that are crucial to the derivations of non-smooth accelerated gradient method. 
        \begin{definition}[The Gradient Mapping]
            \label{def:gradient_mapping}
            Let $g = f + g$ where $f$ is $L$-Lipschitz smooth and convex, $g$ is convex. 
            Define the proximal gradient operator
            $$
                \mathcal T_L(x) := \hprox_{L^{-1}g}(x - L^{-1}\nabla f(x)),
            $$
            then the gradient mapping is defined as
            $$
                \mathcal G_L(x) = L(x - \mathcal T_L(x)). 
            $$
        \end{definition}
        \begin{remark}
            The name ``gradient mapping" comes from \cite[(2.2.54)]{nesterov_lectures_2018}, however, Nesterov was referring to only the case when $g$ is an indicator function of a convex set in his writing. 
            Of course, in Amir Beck \cite[10.3.2]{beck_first-order_nodate}, it has the exact same definition for gradient mapping as the above. 
        \end{remark}

        \begin{lemma}[Gradient Mapping Approximates Subgradient]
            \label{lemma:grad_map_lemma_first}
            Continue from 
            \hyperref[def:gradient_mapping]{definition \ref*{def:gradient_mapping}}, 
            the gradient mapping satisfies
            \begin{align*}
                x^+ &= \mathcal T_L(x), 
                \\
                L(x - x^+) &\in  \nabla f(x) + \partial g(x^+) \ni \mathcal G_L(x). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            \begin{align*}
                x^+ &= [I + L^{-1}\partial g]^{-1}\circ [I - L^{-1}\nabla f](x)
                \\
                [I + L^{-1}\partial g](x^+) &\ni [I - L^{-1}\nabla f](x)
                \\
                x^+ + L^{-1}\partial g(x^+) &\ni x - L^{-1}\nabla f(x)
                \\
                x^+ - x + L^{-1}\partial g(x^+) &\ni L^{-1}\nabla f(x)
                \\
                L(x^+ - x) + \partial g(x^+) &\ni - \nabla f(x)
                \\
                L(x - x^+) &\in \nabla f(x) + \partial g(x^+)
                \\
                \mathcal G_L(x) &\in \nabla f(x) + \partial g(x^+). 
            \end{align*}
        \end{proof}

        \begin{lemma}[Linearized Gradient Mapping Lower Bound]
        \label{lemma:grad_map_linearization}
            Continue from 
            \hyperref[lemma:grad_map_lemma_first]{definition \ref*{lemma:grad_map_lemma_first}}, 
            with $x^+ = \mathcal T_L(x)$, the gradient mapping satisfies the inequality for all $z$: 
            \begin{align*}
                h(z) &\ge
                h(z) - \frac{L}{2}\Vert x - x^+\Vert^2
                \ge h(x^+) + \langle \mathcal G_L(x), z - x\rangle. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Directly from the $L$-smoothness of $f$, convexity of $g, f$, we have the list of inequalities: 
            \begin{align*}
                &f(x^+) \le 
                f(x) + \langle \nabla f(x), x^+ - x\rangle
                + \frac{L}{2}\Vert x - x^+\Vert^2, 
                \\
                &f(x) + \langle \nabla f(x), z - x\rangle 
                \le f(z), 
                \\
                &g(x^+) \le 
                g(z) + \langle \partial g(x^+), x^+ - z\rangle. 
            \end{align*}
            Now, consider adding $g(x^+)$ to the first inequality from above we get 
            {\footnotesize 
            \begin{align*}
                f(x^+) + g(x^+) 
                &\le 
                f(x) + g(x^+) + \langle \nabla f(x), x^+ - x\rangle 
                + \frac{L}{2}\Vert x - x^+\Vert^2
                \\
                &\le 
                (f(z) - \langle \nabla f(x), z - x\rangle) + 
                \left(g(z) - \langle \partial g(x^+), x^+ - z\rangle\right)
                + 
                \langle \nabla f(x), x^+ - x\rangle + \frac{L}{2}\Vert x - x^+\Vert^2
                \\
                &= f(z) + g(z) + \langle \nabla f(x), x - z + x^+ - x\rangle
                + 
                \langle \partial g(x^+), x^+ - z\rangle + \frac{L}{2}\Vert x - x^+\Vert^2
                \\
                &= 
                h(z) + \langle \nabla f(x), x^+ - z\rangle + 
                \langle \partial g(x^+), x^+ - z\rangle
                + \frac{L}{2}\Vert x - x^+\Vert^2
                \\
                &= h(z) + \langle \nabla f(x) + \partial g(x^+), x^+ - z\rangle 
                + \frac{L}{2}\Vert x - x^+\Vert^2. 
            \end{align*}
            }
            By $\mathcal G_L(x) = L(x - x^+) \in \nabla f(x) + \partial g(x^+)$ from previous dicussion, we have 
            \begin{align*}
                h(x^+) &\le 
                h(z) + \langle \mathcal G_L(x), x^+ - z\rangle + \frac{L}{2}\Vert x - x^+\Vert^2
                \\
                &= h(z) - \langle L(x^+ - x), x^+ - x + x - z \rangle 
                + 
                \frac{L}{2}\Vert x - x^+\Vert^2
                \\
                &= 
                h(z) - L\Vert x^+ - x\Vert^2 
                + L \langle x^+ - x, x - z\rangle
                + \frac{L}{2}\Vert x - x^+\Vert^2
                \\
                &= h(z) + \langle \mathcal G_L(x), x - z\rangle - 
                \frac{L}{2}\Vert x - x^+\Vert^2. 
            \end{align*}
            Therefore, the inequality is justified. 
        \end{proof}
        \begin{remark}
            Observe that the linearization $h(x^+) + \langle \mathcal G_L(x), z - x\rangle$ is anchored at $x^+$, instead of $x$. 
            Geometrically, it's tilted and it "prefers" the sharp corners of a convex function, if, $x$ is close to a sharp corner. 
        \end{remark}

    \subsection{Varieties of Nesterov accelerated gradient}\label{sec:AG_varieties}
        Here, the acrynonym: ``AG" stands for accelerated gradient. 
        \subsubsection{AG Abstract Forms}
            In this section, we list different varieties of the Nesterov accelerated method. 
            \begin{definition}[Nestrov 2.2.7]\label{def:Nes2.2.7}
                Let $f$ be a $L$ Lipschitz smooth and $\mu\ge 0$ strongly convex function. 
                Choose $x_0$, $\gamma_0 > 0$, set $v_0 = x_0$, for iteration $k\ge 0$, it
                \begin{enumerate}
                    \item[1.] computes $\alpha_k \in (0, 1)$ by solving $L\alpha_k^2 = (1 - \alpha_k)\gamma_k + \alpha_k \mu$; 
                    \item[2.] sets $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$;
                    \item[3.] chooses $y_k = (\gamma_k + \alpha_k \mu)(\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k)$. Compute $f(y_k)$ and $\nabla f(y_k)$; 
                    \item[4.] finds $x_{k + 1}$ such that $f(x_{k + 1}) \le f(y_k) - (2L)^{-1} \Vert \nabla f(y_k)\Vert^2$; 
                    \item[5.] sets $v_{k + 1} = \gamma_{k+1}^{-1}((1 - \alpha_k)\gamma_kv_k + \alpha_k \mu y_k - \alpha_k \nabla f(y_k))$. 
                \end{enumerate}
            \end{definition}
            \begin{remark}
                This is in Nesterov's book \cite[(2.2.7)]{nesterov_lectures_2018}. 
                For more context, the sequence $\alpha_k \in (0, 1)$ such that $\sum_{k = 1}^{\infty}\alpha_k = \infty$, and recursively we have $\lambda_{k + 1} = (1 - \alpha_k)\lambda_k$. 
                the sequence $\lambda_k$ is called an Nesterov estimating sequence. 
                It is the most generic algorithm in his book about accelerated gradient method. 
                The genericity of the algorithm is provided by item 4., which is the a special case of the smooth descent lemma. 

            \end{remark}

            \begin{definition}[Ahn Sra 6.24]\label{def:agg_ppm}
                With $f$ $\mu \ge 0$ strongly convex and $L$-Lipschitz smooth, the generic PPM form is formulated for strictly positive stepsizes $\tilde \eta_i,\eta_i$: 
                \begin{align*}
                    x_{t + 1} &= \argmin_{x} \left\lbrace
                        l_f(x; y_t) 
                        + 
                        \frac{\mu}{2}\Vert x - y_t\Vert^2
                        + 
                        \frac{1}{2\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2
                    \right\rbrace, 
                    \\
                    y_{t + 1} &= \argmin_{x} 
                    \left\lbrace
                        l_f(x; y_t) + \frac{L}{2}\Vert x - y_t\Vert^2 + 
                        \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
                    \right\rbrace. 
                \end{align*}
            \end{definition}
            \begin{remark}
                This algorithm is the same as algorithm (6.24) described by Ahn and Sra \cite{ahn_understanding_2022}. 
                Observe that by setting $\mu = 0, \tilde \eta_{t + 1} = \eta_{t + 1}$ this recovers algorithm (4.8) described in Ahn and Sra \cite{ahn_understanding_2022}. 
            \end{remark}

            \begin{definition}[AG Proximal Gradient PPM Generic Form]
            \label{def:ag_prox_grad_ppm}
                Let $h=f + g$ be the sum of convex function $g$ and convex differentiable $f$ with $L$-Lipschitz gradient. 
                Define the proximal gradient and gradient mapping operator operator: 
                $$
                \begin{aligned}
                    \mathcal T_L^{g, f}(x) &:=  
                    \hprox_{L^{-1}g}\left(x - L^{-1}\nabla f(x)\right)
                    \\
                    &= 
                    \argmin_{u}
                    \left\lbrace
                        g(u) + f(x) + \langle \nabla f(x), u - x\rangle
                        + 
                        \frac{L}{2}\Vert u - x\Vert^2
                    \right\rbrace,
                    \\
                    \mathcal G_L^{g, f}(x) &= 
                    L\left(x - \mathcal T_L^{g, f}(x)\right). 
                \end{aligned}
                $$
                Omitting the superscript $f, g$ on $\mathcal P, \mathcal G$ for simplicity since it's clear in the context. 
                Define the linear lower bounding function for $f$ at $y$, for all $x$: 
                $$
                \begin{aligned}
                    l_h(x; y) &= h(\mathcal T_L y) + \langle \mathcal G_L(y), x - y \rangle \le f(x), 
                \end{aligned}
                $$
                With that we define the algorithm:
                $$
                \begin{aligned}
                    x_{t + 1} &= \argmin_{x} \left\lbrace
                        l_h(x; y_t) + \frac{1}{2\tilde \eta_{t + 1}} 
                        \Vert x - x_t\Vert^2
                    \right\rbrace,
                    \\
                    y_{t + 1}&= 
                    \argmin_{x}
                    \left\lbrace
                        g(x) + l_f(x; y_t) + \frac{L}{2}\Vert x - y_t\Vert^2 + 
                        \frac{1}{2\eta_{t + 1}} \Vert x - x_{t + 1}\Vert^2
                    \right\rbrace.
                \end{aligned}
                $$
            \end{definition}

            \begin{definition}[AG Proximal Gradient Tri-Points Generic Form]
            \label{def:ag_prox_grad_tri_pt}
                With $h = f + g$, where $g$ is convex, $f$ is convex and $L$-Lipschitz smooth. 
                Define proximal gradient and gradient mapping operator 
                $$
                \begin{aligned}
                    \mathcal T_L(x) 
                    &:= \hprox_{L^{-1}g}(x - L^{-1}\nabla f(x)), 
                    \\
                    x^+ &= \mathcal T_L(x), 
                    \\
                    \mathcal G_L(x) 
                    &:= L(x -  x^+). 
                \end{aligned}
                $$
                Then the algorithm updates $(y_t, x_{t + 1}, z_{t + 1})$ with expression: 
                $$
                \begin{aligned}
                    y_t^+ &= \mathcal T_L(y_t)
                    \\
                    y_t &= (1 + L\eta_t)^{-1}(x_t + L\eta_t z_t)
                    \\
                    x_{t + 1} &= x_t - \tilde \eta \mathcal G_L(y_t)
                    \\
                    z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t)
                \end{aligned}
                $$
                where the base case has $y_0 = x_0$. 
            \end{definition}
            \begin{remark}
                Observe that $z_{t + 1} = y_t^+$. 
            \end{remark}

            \begin{definition}[Tri-points Generic Form]\label{def:agg_tri}
                With $f$ be $L$-Lipschitz \\ 
                smooth and convex, choose any $y_0 = x_0=z_0$ and a non-negative sequence $\eta_t, \tilde\eta_t$, the algorithm admits form: 
                \begin{align*}
                    x_{t + 1} &= x_t - \tilde \eta_{t + 1} \nabla f(y_t) 
                    \\
                    z_{t + 1} &= y_t - L^{-1} \nabla f(y_t) 
                    \\
                    y_{t + 1} &= 
                    (1 + L\eta_{t + 1})^{-1}
                    (
                    x_{t + 1} + L\eta_{t + 1}z_{t + 1}
                    ). 
                \end{align*}
            \end{definition}
            \begin{remark}
                The parameter $\eta_t, \tilde\eta_{t + 1}$ are exactly the same as Ahn, Sra \cite[(6.24)]{ahn_understanding_2022}, but with the choice of $\mu = 0$. 
                This form is not explictly stated by Ahn and Sra but it's trivial to realize its presence regardless. 
                They are the same parameters for both algorithms and they are equivalent. 
                This will be proved in the coming parts. 
            \end{remark}
            \begin{definition}[Strongly Convex Tri-points Generic Form]
                \quad 
                With $f$ being $L$-Lipschitz smooth and $\mu > 0$ strongly convex then the Strongly convex Tri-Points generic form has updates of for iterates
                \begin{align*}
                    x_{t + 1} &= (1 + \mu\tilde \eta_{t + 1})^{-1}(x_t + \mu\tilde\eta_{t + 1}y_t^+), 
                    \\
                    z_{t + 1} &= y_t - L^{-1}\nabla f(y_t), 
                    \\
                    y_{t + 1} &= (1 + L\eta_{t + 1})^{-1}(x_{t + 1} + L\eta_{t + 1}z_{t +1}). 
                \end{align*}
            \end{definition}

        \subsubsection{Useful Observations about these Generic forms}
        \label{sec:AG_useful_information}
            In this section we list some incredibaly useful information about these forms of the Nestrov type accelerated gradient algorithm. 
            \hyperref[prop:tri_form_via_ppm]{Proposition \ref*{prop:tri_form_via_ppm}}, 
            \hyperref[prop:tri_scvx_from_ahn_sra_6.24]{proposition \ref*{prop:tri_scvx_from_ahn_sra_6.24}}, 
            \hyperref[prop:Nes2.2.7_via_ahn_sra_6.24]{proposition \ref*{prop:Nes2.2.7_via_ahn_sra_6.24}}
            are results assumed as true in Ahn, Sra. 
            They are implicitly left as necessary exercises for the readers. 
            Here, we fill out the blanks with our own interpretations. 
            \begin{proposition}[Tri-points generic form via Ahn Sra 6.24]
                \label{prop:tri_form_via_ppm}
                with $f$ being \\
                $L$-Lipschitz smooth, 
                let $l_f(x, \bar x) = f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle$, we can show that the AG generic triangular form is a consequence of th AG generic PPM form. 
            \end{proposition}
            \begin{proof}
                Solving the optimality on the first PPM yields: 
                \begin{align*}
                    \mathbf 0 &= \nabla f(y_t) + 
                    \frac{1}{\tilde \eta_{t + 1}} (x - x_t)
                    \\
                    x &= x_t - \tilde \eta_{t + 1} \nabla f(y_t).
                \end{align*}
                Therefore, $x_{t + 1} = x_t - \tilde \eta_{t + 1}\nabla f(y_y)$. 
                Similarly, for the updates of $y_{t + 1}$, we have optimality condition of 
                \begin{align*}
                    \mathbf 0 &= \nabla f (y_t) + L (x - y_t) + \eta_{t + 1}^{-1} (x - x_{t + 1})
                    \\
                    \mathbf 0 &= \eta_{t + 1}\nabla f (y_t) + \eta_{t + 1}L (x - y_t) + x - x_{t + 1}
                    \\
                    \mathbf 0 &= 
                    \eta_{t + 1}\nabla f(y_t) -\eta_{t + 1} Ly_t + (\eta_{t + 1}L + 1)x - x_{t + 1}
                    \\
                    (1 + \eta_{t + 1}L)x
                    &= 
                    x_{t + 1} - \eta_{t + 1}\nabla f(y_t) + \eta_{t + 1}L y_t
                    \\
                    \text{define: } y_{t + 1} &:= x. 
                \end{align*}
                In the above expression, it hides a step of gradient descent, continuing it we have 
                \begin{align*}
                    (1 + \eta_{t + 1}L)y_{t + 1} &= 
                    x_{t + 1}  + \eta_{t + 1}L (-L^{-1}\nabla f(y_t) + y_t)
                    \\
                    \text{let: } z_{t + 1} &= y_t - L^{-1}\nabla f(y_t), \text{ so, }
                    \\
                    (1 + \eta_{t + 1}L)y_{t + 1} &= 
                    x_{t + 1} + L\eta_{t + 1}z_{t + 1}. 
                \end{align*}
                Combining it yields the tree points update format 
                \begin{align*}
                    x_{t + 1} &= x_t - \tilde \eta_{t + 1} \nabla f(y_t) 
                    \\
                    z_{t + 1} &= y_t - L^{-1} \nabla f(y_t) 
                    \\
                    y_{t + 1} &= 
                    (1 + L\eta_{t + 1})^{-1}
                    (
                    x_{t + 1} + L\eta_{t + 1}z_{t + 1}
                    ), 
                \end{align*}
                the ordering of $x_{t +1}, z_{t + 1}$ can be permuted. 
                The base case is when $t = 0$, so then $x_0 = y_0$ for the initial guess.
            \end{proof}
            \begin{remark}
                It is quite obvious to us that the choice of $z_{t + 1} = y_t - L^{-1}\nabla f(y_t)$ is deliberate. 
                In fact, it's true that $z_{t + 1}$ is just a term such that it makes $y_{t +1}$ a convex combination between the vector $x_{t + 1}, z_{t + 1}$, and the choice here would be unique because of the consideration that 
                {\footnotesize
                \begin{align*}
                    & \quad 
                    l_f(x; y_t) + \frac{L}{2}\Vert x - y_t\Vert^2 
                    \\
                    &= 
                    f(y_t) + \langle \nabla f(y_t), x - y_t\rangle 
                    + 
                    \frac{L}{2}
                    \left\Vert 
                        x - (y_t - L^{-1}\nabla f(y_t)) - L^{-1}\nabla f(y_t)
                    \right\Vert^2
                    \\
                    &= 
                    f(y_t) + 
                    \langle \nabla f(y_t), x - (y_t - L^{-1}\nabla f(y_t)) - L^{-1}\nabla f(y_t)\rangle 
                    + 
                    \frac{L}{2}
                    \left\Vert 
                        x - (y_t - L^{-1}\nabla f(y_t)) - L^{-1}\nabla f(y_t)
                    \right\Vert^2
                    \\
                    &\quad  \text{Let }z_{t + 1} 
                    = y_t - L^{-1}\nabla f(y_t)
                    \\
                    &= f(y_t) + \langle \nabla f(y_t), x - z_{t + 1} - L^{-1}\nabla f(y_t)\rangle
                    + 
                    \frac{L}{2}\left\Vert
                        x - z_{t + 1} - L^{-1}\nabla f(y_t)
                    \right\Vert^2
                    \\
                    &= 
                    f(y_t) + \langle \nabla f(y_t), x - z_{t + 1}\rangle 
                    - L^{-1}\Vert \nabla f(y_t)\Vert^2
                    + 
                    \frac{L}{2}\Vert x - z_{t + 1}\Vert^2 + 
                    \frac{1}{2L}\Vert \nabla f(y_t)\Vert^2 - 
                    L\langle L^{-1}\nabla f(y_t), x - z_{t + 1}\rangle
                    \\
                    &= f(y_t) + (1/(2L)- L^{-1})\Vert \nabla f(y_t) \Vert^2 + 
                    \frac{L}{2}\Vert x - z_{t + 1}\Vert^2
                    \\
                    &= f(y_t) - \frac{1}{2L}\Vert \nabla f(y_t)\Vert^2 + \frac{L}{2}\Vert x - z_{t + 1}\Vert^2. 
                \end{align*}
                }
                Therefore 
                \begin{align*}
                    y_{t + 1} &= \argmin_{x}\left\lbrace
                        \frac{L}{2}\Vert x - z_{t + 1}\Vert^2 + 
                        \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
                    \right\rbrace
                    \\
                    &= \argmin_{x}\left\lbrace
                        \left\Vert x - 
                            \frac{Lz_{t + 1} + \eta_{t + 1}^{-1}x_{t + 1}}{
                                L + \eta_{t + 1}^{-1}
                            }
                        \right\Vert^2
                    \right\rbrace
                    \\
                    &= 
                    \frac{
                        L\eta_{t + 1}z_{t + 1} + 
                        x_{t + 1}
                    }{
                        1 + L \eta_{t + 1}
                    }. 
                \end{align*}
            \end{remark}

            \begin{proposition}[Tri-points Strongly Convex Form via Ahn Sra 6.24]\label{prop:tri_scvx_from_ahn_sra_6.24}
                Coninue \\ 
                from \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}} with but with $\mu > 0$, under the same set of assumptions it recovers a tri-points algorithm with the following updates: 
                \begin{align*}
                    y_t^+ &= y_t - \mu^{-1}\nabla f(y_t)
                    \\
                    x_{t + 1} &= \argmin_{x} 
                    \left\lbrace
                        l_f (x; y_t) + \frac{\mu}{2} \Vert x - y_t\Vert^2 + 
                        \frac{1}{2\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2 
                    \right\rbrace
                    \\
                    &= \argmin_{x}
                    \left\lbrace
                        f(y_t) - \frac{1}{2\mu} \Vert \nabla f(y_t)\Vert^2 
                        + 
                        \frac{\mu
                        }{2}\Vert x - y_t^+\Vert^2
                        + 
                        \frac{1}{2\tilde \eta_{t + 1}}\Vert x - x_t\Vert^2
                    \right\rbrace
                    \\
                    &= (1 + \mu\tilde \eta_{t + 1})^{-1}
                    \left(
                        x_t + \mu \tilde \eta_{t + 1}y_t^{+}
                    \right)
                    \\
                    z_{t + 1} &= y_t - L^{-1} \nabla f(y_t)
                    \\
                    y_{t + 1} &= (1 + L\eta_{t + 1})^{-1}(x_{t + 1} + L \eta_{t + 1} z_{t + 1})
                \end{align*}
            \end{proposition}
            \begin{proof}
                By a similar argument made in the remarks of \hyperref[prop:tri_form_via_ppm]{proposition \ref*{prop:tri_form_via_ppm}} we have 
                \begin{align*}
                    l_f(x; y_t) + \frac{\mu}{2}\Vert x - y_t\Vert^2
                    &= 
                    f(y_t) - \frac{1}{2\mu}\Vert \nabla f(y_t)\Vert^2 + \frac{\mu}{2}\Vert x - y_t^+\Vert^2
                    \\
                    \implies 
                    x_{t + 1} &= 
                    \argmin_{x} 
                    \left\lbrace
                        f(y_t) - \frac{1}{2\mu}\Vert \nabla f(y_t)\Vert^2 + \frac{\mu}{2}\Vert x - y_t^+\Vert^2
                        + 
                        \frac{1}{2\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2
                    \right\rbrace
                    \\
                    &= 
                    \argmin_{x}
                    \left\lbrace
                        \frac{\mu}{2}\Vert x - y_t^+\Vert^2 + 
                        \frac{2}{\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2
                    \right\rbrace
                    \\
                    &= (\mu + \tilde \eta_{t + 1})^{-1}
                    (\mu y_t^+ + \tilde \eta_{t + 1}x_t)
                    \\
                    &= (1 + \tilde \eta_{t + 1}\mu)^{-1}
                    (x_t + \mu\tilde \eta_{t + 1}y_t^+). 
                \end{align*}
                The updates for $y_{t + 1}, z_{t + 1}$ is the same as before because $\mu > 0$ only alters the update of the term $x_{t + 1}$ in \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}}. 
            \end{proof}
            
            \begin{proposition}[Nestrov 2.2.7 is Equivlant to Ahn Sra 6.24 in Form]
            \label{prop:Nes2.2.7_via_ahn_sra_6.24}
                The \hyperref[def:Nes2.2.7]{algorithm \ref*{def:Nes2.2.7}} is equivalent to \hyperref[def:agg_ppm]{\ref*{def:agg_ppm}} in form. 
                So the updates for $y_{k + 1}, x_{k + 1}, v_{k + 1}$: 
                \begin{align*}
                    \text{find } &
                    \alpha_k \in (0, 1) 
                    \text{ s.t: } L\alpha_k^2 
                    = (1 - \alpha_k)\gamma_k + \alpha_k \mu = \gamma_{k + 1} 
                    \\
                    y_k &= 
                    \left(
                        \gamma_k + \alpha_k \mu
                    \right)^{-1} \left(
                        \alpha_k \gamma_k v_k + \gamma_{k + 1}x_k
                    \right)
                    \\
                    \text{find } & x_{k + 1} \text{ s.t: }
                    f(x_{k + 1})
                    = f(y_k) - (2L)^{-1}\Vert \nabla f(y_k)\Vert^2
                    \\
                    v_{k+1} &= 
                    \gamma_{k + 1}^{-1} 
                    \left(
                        (1 - \alpha_k) \gamma_k v_k + 
                        \alpha_k \mu y_k 
                        - \alpha_k \nabla f(y_k)
                    \right). 
                \end{align*}
                Is the same as $y_{t + 1}, z_{t + 1}, x_{t + 1}$ from below: 
                \begin{align*}
                    x_{t + 1} 
                    & = 
                    (1 + \tilde \eta_{t + 1}\mu)^{-1}
                    (x_t + \mu\tilde \eta_{t + 1}y_t - \tilde \eta_{t + 1}\nabla f(y_t))
                    \\
                    z_{t + 1} &= y_t - L^{-1} \nabla f(y_t)
                    \\
                    y_{t + 1} &= 
                    (1 + L\eta_{t + 1})^{-1}
                    (x_{t + 1} + L \eta_{t + 1} z_{t + 1}).
                \end{align*}
                which is an equivalent representation of \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}} as showned in \hyperref[prop:tri_scvx_from_ahn_sra_6.24]{propostion \ref*{prop:tri_scvx_from_ahn_sra_6.24}}. 
            \end{proposition}
            \begin{proof}
                We simplify the Nesterov form into the Strongly convex Generic Triangular Form. 
                Consider update for $v_{k + 1}$ by substituting $\gamma_{k+1} = (1 - \alpha_k) \gamma_k + \alpha_k \mu$ as informed by the first step of the algorithm, we have 
                \begin{align*}
                    v_{k + 1} &= 
                    ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
                    \left(
                        (1 - \alpha_k)\gamma_k v_k + \alpha_k \mu y_k - \alpha_k \nabla f(y_k)
                    \right)
                    \\
                    &= ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
                    (
                        (1 - \alpha_k)\gamma_k v_k + \alpha_k \mu(y_k - \alpha_k \mu^{-1}\nabla f(y_k))
                    )
                    \\
                    &= 
                    \left(
                        1 + \frac{\alpha_k \mu}{(1 - \alpha_k)\gamma_k}
                    \right)^{-1}
                    \left(
                        v_k
                        + 
                        \left(
                            \frac{\alpha_k \mu}{(1 - \alpha_k)\gamma_k} 
                        \right)
                        \left(
                            y_k 
                            - \alpha_k \mu^{-1}\nabla f(y_k)
                        \right)
                    \right). 
                \end{align*}
                Notice that the right hand size has the same form as $x_{t + 1}$. 
                This is true by the observation that 
                \begin{align*}
                    x_{t + 1} &= 
                    (1 + \tilde\eta_{t + 1}\mu)^{-1}
                    \left( 
                        x_t + \mu\tilde \eta_{t + 1}
                        \left(y_t - \mu^{-1}\nabla f(y_t)\right)
                    \right). 
                \end{align*}
                Similarly, when $\mu = 0$, we have from the first step that 
                \begin{align*}
                    v_{k + 1} 
                    &= ((1 - \alpha_k)\gamma_k)^{-1}
                    (
                        (1 - \alpha_k)\gamma_k v_k
                        + \alpha_k \mu y_k - \alpha_k \nabla f(y_k)
                    )
                    \\
                    &= 
                    v_k - \alpha_k((1 - \alpha_k)\gamma_k)^{-1}\nabla f(y_k)
                \end{align*}
                which is the same as the AG generic PPM form where 
                $$
                    x_{t + 1} = x_t - \tilde \eta_{t + 1}\nabla f(y_t). 
                $$
                Next, we consider $y_{k}$ iterate of Nesterov 2.2.7. 
                We want to write it as a convex combination of the vector $v_k,x_k$. 
                To show that the updates $y_k$ can are of the same form, start by considering that 
                \begin{align*}
                    \gamma_{k + 1 } &= (1 - \alpha_k)\gamma_k + \alpha_k \mu
                    \\
                    &= (\gamma_k + \alpha_k \mu) - \alpha_k \gamma_k, 
                \end{align*}
                with the grouping we have
                \begin{align*}
                    y_k &= \left(
                        \gamma_k + \alpha_k \mu
                        \right)^{-1} \left(
                            \alpha_k \gamma_k v_k + \gamma_{k + 1}x_k
                        \right)
                    \\
                    &= 
                    \left(
                        \gamma_k + \alpha_k \mu
                    \right)^{-1}
                    \left(
                        \alpha_k \gamma_k v_k 
                        + 
                        ((\gamma_k + \alpha_k\mu) - \alpha_k\gamma_k)x_k
                    \right)
                    \\
                    &= 
                    \left(
                        \frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu}
                    \right)v_k
                    + 
                    \left(
                        1 - \frac{\alpha_k\gamma_k }{\gamma_k + \alpha_k \mu}
                    \right)x_k, 
                \end{align*}

                which is indeed, a convex combination of $v_k, x_k$, if, we assume that $(\alpha_k\gamma_k)(\gamma_k + \alpha_k \mu)^{-1}$ is in the interval $(0, 1)$. 
                We can assume it by considering $\gamma_k = L\alpha_{k-1}^2$, simplifying so that 
                \begin{align*}
                    \frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu}
                    &= 
                    \frac{L\alpha_k \alpha_{k - 1}^2}{L\alpha_{k - 1}^2 + \mu \alpha_k}
                    \\
                    &= \frac{\alpha_k \alpha_{k - 1}^2}{1 + q_f \alpha_k \alpha_{k - 1}^{-2}} \in (0, 1), 
                \end{align*}
                where $q_f = \mu / L \in (0, 1)$ and we recall the fact that the sequence $(\alpha_k)_{k \in \NN}$ has $\alpha_k \in (0, 1)$ and $\sum_{i = 1}^{\infty} \alpha_k = \infty$. 
                However, it would require more works to express $\eta_t, \tilde\eta$ from Ahn Sra 6.24 using $\gamma_k, \alpha_k$ from Nesterov 2.2.7. 
            \end{proof}


    \subsection{Generic Lyapunov analysis for Accelerated gradient via PPM}
        The first theorem states the generic convergence results when $f$ is differentiable with $L$-Lipschitz gradient, using \ref*{def:agg_ppm}. 
        \begin{lemma}[Smooth Generic AG Lyapunov Analysis]\label{lemma:smooth_agg_lyapunov_upper_bound}
            With $f$ having $L$-Lipschitz gradient and minimizer $x_*$, using 
            \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}} with $\mu = 0$
            and 
            \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}} ,
            then we can derive the upper bounds for the RHS of the PPM descent inequality: 
            \begin{align*}
                \Upsilon_{1, t + 1}^{\text{AG}}
                &= \tilde \eta_{t + 1} (f(z_{t + 1}) - f(x_*))
                + \frac{1}{2}(
                    \Vert x_{t + 1} - x_*\Vert^2 - \Vert x_t - x_* \Vert^2
                )  
                \\
                & \quad \le 
                - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 + 
                \frac{\tilde \eta_{t + 1} L}{2}\Vert z_{t + 1} - y_t\Vert^2
                - 
                \langle \tilde \eta_{t + 1} \nabla f(y_t), x_{t + 1} - z_{t + 1} \rangle, 
                \\
                \Upsilon_{2, t + 1}^{\text{AG}} 
                &= f(z_{t + 1}) - f(z_t) 
                \le 
                \langle \nabla f(y_t), z_{t + 1} - z_t\rangle + 
                \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Define $\phi_t(x) = \tilde \eta_{t +1}(f(y_t) + \langle \nabla f(y_t), x- y_t\rangle)$.  
            With $L$-smoothness of $f$ in mind, consider the following sequence of inequalities: 
            $$
            \begin{aligned}
                \phi_t(x_{t + 1}) 
                &= 
                \tilde\eta_{t + 1} (f(y_t) + \langle \nabla f(y_t), x_{t + 1} - y_t\rangle)
                \\
                \phi_t (x_{t + 1}) &= \tilde \eta_{t + 1}(
                    f(y_t) + \langle \nabla f(y_t), (x_{t +1} - z_{t + 1}) + (z_{t + 1} - y_t) \rangle
                )
                \\
                &\ge 
                \tilde \eta_{t + 1}
                \left(
                    f(z_{t + 1}) - \frac{L}{2} \Vert z_{t + 1} - y_t\Vert^2 + 
                    \langle \nabla f(y_t), x_{t +1} - z_{t + 1}\rangle
                \right), 
            \end{aligned}
            $$
            the AG generic performs PPM on $\phi_t$ to produce $x_{t + 1}$ so the PPM Lyapunov inequality from 
            \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}} applies,
            substituting yields equivalences for all $x_*$: 
            {\footnotesize
            \begin{align*}
                & \phi_t(x_{t + 1}) - \phi_t(x_*) + \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2 
                - \frac{1}{2}\Vert x_* - x_t\Vert^2 
                \\
                &\quad \le 
                - \frac{1}{2} \Vert x_{t + 1} - x_t\Vert^2 
                \\
                \implies & 
                \tilde \eta_{t + 1}\left(
                    f(z_{t + 1}) - \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2 
                    + 
                    \langle \nabla f(y_t), x_{t + 1} - z_{t + 1}\rangle
                \right) - \tilde \eta_{t + 1} f(x_*)
                + 
                \frac{1}{2}\left(
                    \Vert x_{t + 1} - x_*\Vert^2 - \Vert x_{t} - x_*\Vert^2
                \right)
                \\
                &\quad \le
                 - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 
                \\
                \iff 
                & 
                \tilde \eta_{t + 1} \left(
                    f(z_{t + 1}) - f(x_*)
                \right) + \frac{1}{2}\Vert x_{t + 1} - x_*\Vert^2 
                - \frac{1}{2}\Vert x_{t} - x_*\Vert^2  =: \Upsilon_{1, t + 1}^{\text{AG}}
                \\
                &\quad \le 
                -\frac{1}{2} \Vert x_{t + 1 } - x_t\Vert^2 + 
                \frac{\tilde \eta_{t + 1}}{2}\Vert z_{t + 1} - y_t\Vert^2 
                - \langle \tilde \eta_{t + 1}\nabla f(y_t), x_{t + 1} - z_{t + 1} \rangle. 
            \end{align*}
            }
            Observe that, the rhs and lhs of the Lyapunov inequality are anchored at $z_{t + 1}$. 
            Similarly for the descent inequality we wish to obtain: 
            \begin{align*}
                f(z_{t + 1}) - f(z_t) &= f(z_{t + 1}) - f(y_t) + f(y_t) - f(z_t) 
                \\
                &\le 
                \langle \nabla f(y_t), z_{t + 1} - y_t\rangle + \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2 
                + 
                \langle \nabla f(y_t), y_t - z_t\rangle
                \\
                &= 
                \langle \nabla f(y_t), z_{t + 1} - z_t\rangle + \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
                \\
                \text{let }\Upsilon_{2, t + 1}^{\text{AG}} 
                &:= f(z_{t + 1}) - f(z_t). 
            \end{align*}
            Which is the descent inequality anchored on $z_{t + 1}$.
            Merging the $(z_{t + 1} - y_t)$ with $y_t - z_t$ together yield the desired results. 
        \end{proof}

        \begin{lemma}[Nonsmooth Generic AG Lyapunov Analysis]
        \label{lemma:nsmooth_agg_lyapunov_upper_bound}
            
            
        \end{lemma}

        \begin{theorem}[Generic AG Convergence]\label{thm:generic_smooth_ag_convergence}
            If there is a choice of $\eta_i, \tilde \eta_i$ in \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}} such that we have 
            \begin{align*}
                \left(
                    \sum_{i = 1}^{t} \tilde \eta_i
                \right)
                \Upsilon_{2, t + 1}^{\text{AG}} + 
                \Upsilon_{1, t + 1}^{\text{AG}} \le 0, 
            \end{align*}
            then with 
            \begin{align*}
                \Phi_t &= \left(
                    \sum_{i = 1}^{t} \tilde\eta_{i}
                \right) (f(z_t) - f(x_*)) + \frac{1}{2}\Vert x_t - x_*\Vert^2 \quad \forall t \in \NN
                \\
                \Phi_0 &= \frac{1}{2}\Vert x_t - x_*\Vert^2, 
            \end{align*}
            we have $\Phi_{t + 1} - \Phi_t \le 0$, which allows for a convergence rate of $\mathcal O \left(\sum_{i = 1}^{T} \eta_i^{-1}\right)$ for $f(z_T) - f(x_*)$. 
        \end{theorem}
        \begin{proof}
            By definition we have
            {\footnotesize
            \begin{align*}
                \Phi_{t + 1} - \Phi_t 
                &= 
                \left(
                    \sum_{i = 1}^{t+1} \tilde\eta_{i}
                \right) (f(z_{t + 1}) - f(x_*)) 
                - 
                \left(
                    \sum_{i = 1}^{t} \tilde\eta_{i}
                \right) (f(z_{t}) - f(x_*)) 
                + \frac{1}{2}\Vert x_t - x_*\Vert^2
                - \frac{1}{2}\Vert x_{t + 1} - x_*\Vert^2
                \\
                &= 
                \tilde \eta_{t + 1} (f(z_{t + 1}) - f(z_*))
                +
                \left(
                    \sum_{i = 1}^{t} \tilde \eta_i
                \right)(f(z_{t + 1}) - f(z_t))
                + \frac{1}{2}\Vert x_t - x_*\Vert^2
                - \frac{1}{2}\Vert x_{t + 1} - x_*\Vert^2
                \\
                &= \left(
                    \sum_{i = 1}^{t} \tilde \eta_i
                \right)\Upsilon_{2, t + 1}^{\text{AG}} + \Upsilon_{1, t + 1}^{\text{AG}} \le 0. 
            \end{align*}
            }
            The derivation for the convergence raet is direct and similar to 
            \hyperref[thm:ppm_convergence_rate]{theorem \ref*{thm:ppm_convergence_rate}} 
            Therefore, if we can identify parameter for the generic algorithm that asserts the condition above, then we have convergence for the algorithm. 
        \end{proof}

    \subsection{Specific Varieties of Accelerated Gradient Method}
        In this section we state some specific variant of the Nesterov accelerated gradient method. 
        The "AG" stands for accelerated gradient. 

    \subsection{Scenario I}
        In this scenario, we aim to recover the following variant of the accelerated gradient algorithm from 12.1 in Ryu's writing \cite{ryu_large-scale_2022}, it is: 
        \begin{align*}
            z_{t + 1} &= y_k + L^{-1}\nabla f(y_t), 
            \\
            y_{t + 1} &= z_{t + 1} + \frac{t - 1}{t + 2}\left(
                z_{k + 1} - z_k
            \right). 
        \end{align*}
        An equivalent form of the above is also provided by Ryu: 
        \begin{align*}
            z_{t + 1} &= y_t + L^{-1}\nabla f(y_t), 
            \\
            x_{t + 1} &= x_t + (t + 1)(2L)^{-1}\nabla f(y_t), 
            \\
            y_{t + 1} &= \left(
                1 - \frac{2}{t + 2} 
            \right)z_{t + 1} + 
            \left(
                \frac{1}{t + 2}
            \right)x_{t + 1}. 
        \end{align*}
        The base case requires cares since it's not unique, and depending on how the base case is define, there even more representations of it. 
        \begin{observation}
            Observe that the second equivalent form is an example of \hyperref[def:agg_tri]{definition \ref*{def:agg_tri}}. 
            \par\noindent
            It is stated in Ryu's writing that the base case is $x_0 = y_0 = z_0$, 
            \textcolor{purple}{we think this is not the only options.} 
            We remind the reader that there is not an obvious choice of $t\in \mathbb Z$ such that $x_t = y_t = z_t$ where the above algorithm remains consistent for all $t \ge -1$. 
            The choice of base case presented in Ryu's writing is a choice and not a necessity. 
            We state that one of the base case scenario that only requires knowing $y_1$. 

            \par\noindent 
            We need to observe the case where $t = 0$ to understand the base case of the algorithm. 
            At each iteration $t$, Given $(x_t, y_t)$, the above formula maps to $(x_{t + 1}, y_{t + 1}, z_{t + 1})$. 
            When $t= 0$, it has $y_1 = x_1$. 
            Therefore, knowing only either $y_1$, or $x_1$ can initiate the algorithm. 
            This is an alternative scenario of the base case. 
            If, we wish to write $y_0 = x_0$ at $t = 0$ for the base case instead, then it creates the following algorithm 
            \begin{align*}
                y_{t} &= \left(
                1 - \frac{2}{t + 2} 
                \right)z_{t} + 
                \left(
                    \frac{1}{t + 2}
                \right)x_{t}, 
                \\
                z_{t + 1} &= y_t + L^{-1}\nabla f(y_t), 
                \\
                x_{t + 1} &= x_t + (t + 1)(2L)^{-1}\nabla f(y_t). 
            \end{align*}
            It has a different representations due to a specific choice of the base case. 
            \par\noindent
            Finally, by the observation that given $(z_t, y_t)$ for any $t\ge -1$, we can solve for $x_t$ \newline via $y_t = (1 - 2/(1 + t))z_t + 1/(t + 1)x_t$, obtaining $(x_t, y_t, z_t)$, hence the algorithm can be reduced to a representations with only $(z_t, y_t)$ and an updates using only $(z_t, y_t)$ to $(z_{t + 1}, y_{t + 1})$. 
            That is the momentum form presented above. 
        \end{observation}
        We advise the reader to read the Biliography notes by Ryu \cite[chapter 12]{ryu_large-scale_2022} of his book. 
        It goes over the full context and history for this particular variant of gradient acceleration method. 
        As the time of composing this notes, the writer is still reading on the topic of accelerated gradient. 

        \begin{proposition}
            With $\eta_t, \tilde \eta_t$ in \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}} where $\eta_{t + 1} = \tilde \eta_{t + 1} = 2/(2L)$, we derive the following algorithm: 
            \begin{align*}
                x_{t + 1} &= x_t - \frac{t + 1}{2L} \nabla f(y_t), 
                \\
                z_{t + 1} &= y_t - L^{-1} \nabla f(y_t), 
                \\
                y_{t + 1} &= 
                \left(
                    \frac{2}{t + 3}
                \right)x_{t +1} + 
                \left(
                    1 - \frac{2}{t + 3}
                \right)z_{t + 1}. 
            \end{align*}
        \end{proposition}
        \begin{observation}
            Observe that this is the same algorithm as stated in Ryu's book but with an alternative choice of indexing for $y_t$ and initial condition. 
        \end{observation}
        \begin{proof}
            Consider the upper bounds of $\Upsilon_{1, t + 1}^\text{AG}, \Upsilon_{2, t + 1}^\text{AG}$, together with the updates of the Generic Triangular Form of the algorithm:  
            \begin{align*}
                x_{t + 1} - x_t &= \tilde \eta_{t + 1}\nabla f(y_t)
                \\
                z_{t +1} - y_t &= L^{-1}\nabla f(y_t), 
            \end{align*}
            we consider the upper bound of $\Upsilon_{1, t + 1}^{\text{AG}}$, with that we can simplify it: 
            {\footnotesize
            \begin{align*}
                \Upsilon_{1, t + 1}^{\text{AG}}
                &\le 
                -\frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 
                + 
                \frac{\tilde \eta_{t + 1}L }{2} \Vert z_{t + 1} - y_t\Vert^2 
                - 
                \langle \tilde \eta_{t + 1} \nabla f(y_t), x_{t +1} - z_{t + 1}\rangle
                \\
                &\le 
                - \frac{1}{2}\Vert \tilde \eta_{t + 1} \nabla f(y_t)\Vert^2 
                + \frac{\tilde \eta_{t + 1} L}{2}\Vert L^{-1} \nabla f(y_t)\Vert^2 
                + 
                \langle 
                    - \tilde \eta_{t + 1} \nabla f(y_t), 
                    -L^{-1}\nabla f(y_t) + y_t - x_t + \tilde \eta_{t + 1}\nabla f(y_t)
                \rangle
                \\
                &= 
                \left(
                    \frac{\tilde \eta_{t + 1}^2}{2} + 
                    \frac{\tilde \eta_{t + 1}}{2L}
                \right)
                \Vert \nabla f(y_t)\Vert^2 
                + \tilde \eta_{t + 1} 
                \left\langle 
                    \nabla f(y_t), 
                    (\tilde \eta_{t + 1} - L^{-1})\nabla f(y_t) + y_t - x_t 
                \right\rangle
                \\
                &= 
                (1/2)(L^{-1}\tilde \eta_{t + 1} - \tilde \eta_{t + 1}^2)
                \Vert \nabla f(y_t)\Vert^2
                + 
                \tilde \eta_{t + 1}(\tilde \eta_{t + 1} - L^{-1})
                \Vert \nabla f(y_t)\Vert^2
                + 
                \tilde \eta_{t + 1}\langle \nabla f(y_t), y_t - x_t\rangle
                \\
                &= \frac{1}{2}\left(
                    -L^{-1}\tilde \eta_{t + 1} + \tilde \eta_{t + 1}^2
                \right)\Vert \nabla f(y_t)\Vert^2 
                + 
                \tilde\eta_{t + 1} \langle \nabla f(y_t), y_t - x_t\rangle. 
            \end{align*}
            }
            For $\Upsilon_{2, t + 1}^\text{AG}$, we simplify it with the same updates relation in mind: 
            \begin{align*}
                \Upsilon_{2, t + 1}^\text{AG}
                &\le 
                \langle \nabla f(y_t), z_{t +1} - y_t\rangle + 
                \frac{L}{2}\Vert z_{t +1} - y_{t} \Vert^2 
                + 
                \langle \nabla f(y_t), y_t - z_t\rangle
                \\
                &= 
                \langle 
                    \nabla f(y_t), -L^{-1} \nabla f(y_t)
                \rangle + 
                \frac{L}{2}\Vert L^{-1}\nabla f(y_t) \Vert^ 2
                + 
                \langle \nabla f(y_t), y_t - z_t\rangle
                \\
                &= -\frac{1}{2L}\Vert \nabla f(y_t)\Vert^2 
                + \langle \nabla f(y_t), y_t - z_t\rangle. 
            \end{align*}
            Observe that the cross product term for $\Upsilon_{1, t + 1}^\text{AG}, \Upsilon_{2, t + 1}^\text{AG}$ doesn't match. 
            Hence let's consider $y_t - x_t = L \eta_t (z_t - y_t)$ from the algorithm, we make the choice to do surgery on upper bound of $\Upsilon_{2, t + 1}^\text{AG}$, so $\langle \nabla f(y_t), y_t - x_t\rangle = \langle \nabla f(y_t), L \eta_t (z_t - y_t)\rangle$. 
            With this in mind, applying 
            \hyperref[thm:generic_smooth_ag_convergence]{theorem \ref*{thm:generic_smooth_ag_convergence}} 
            and upper bound from 
            \hyperref[lemma:smooth_agg_lyapunov_upper_bound]{lemma \ref*{lemma:smooth_agg_lyapunov_upper_bound}} 
            yields inequality: 
            {\footnotesize
            \begin{align*}
                & \Upsilon_{1, t + 1}^{\text{AG}} + \left(
                    \sum_{i=1}^{t}\tilde\eta_i 
                \right) \Upsilon_{2, t + 1}^{\text{AG}} 
                \\
                & \le 
                \frac{1}{2}\left(
                    -L^{-1}\tilde \eta_{t + 1} + \tilde \eta_{t + 1}^2
                \right)\Vert \nabla f(y_t)\Vert^2 
                + 
                \tilde\eta_{t + 1} \langle \nabla f(y_t), y_t - x_t\rangle
                + 
                \left(
                    \sum_{i = 1}^{t}\tilde \eta_i 
                \right)\left(
                    -\frac{1}{2L}\Vert \nabla f(y_t)\Vert^2 
                    + \langle \nabla f(y_t), y_t - z_t\rangle
                \right)
                \\
                & =
                \left(
                    \frac{1}{2}\tilde\eta_{t + 1}\left(
                        \tilde \eta_{t +1} - L^{-1}
                    \right)
                    - 
                    \frac{1}{2L}\sum_{i = 1}^{t}\tilde \eta_i
                \right)\Vert \nabla f(y_t)\Vert^2
                + 
                \tilde \eta_{t + 1}\langle \nabla f(y_t), L\eta_t (z_t - y_t)\rangle
                + 
                \left(
                    \sum_{i = 1}^{t}\tilde \eta_i
                \right)
                \langle \nabla f (y_t), y_t - z_t\rangle
                \\
                & = 
                \left(
                    \frac{1}{2}\tilde\eta_{t + 1}\left(
                        \tilde \eta_{t +1} - L^{-1}
                    \right)
                    - 
                    \frac{1}{2L}\sum_{i = 1}^{t}\tilde \eta_i
                \right)\Vert \nabla f(y_t)\Vert^2
                + 
                \left(
                    L\eta_t \tilde \eta_{t + 1} - \sum_{i = 1}^{t}\tilde \eta_i
                \right)
                \langle \nabla f(y_t), z_t - y_t\rangle. 
            \end{align*}
            }
            Next, we select a choice for stepsize parameter $\eta_t, \tilde \eta_{t +1}$ such that the coefficient for the $\Vert \nabla f(y_t)\Vert^2$ is negative, and is zero for the cross product term. 
            In this scenario, we makes the choice of $\tilde \eta_t = \eta_t$. 
            Continuting will simplify the upper bound so that it is: 
            \begin{align*}
                \frac{1}{2}\left(
                    \eta_{t + 1}^2 - L^{-1}\eta_{t +1} - L^{-1}\sum_{i = 1}^{t}\eta_i
                \right)\Vert \nabla f(y_t)\Vert^2
                + 
                \left(
                    L\eta_t \eta_{t + 1} - \sum_{i = 1}^{t} \eta_i
                \right)
                \langle \nabla f(y_t), z_t - y_t\rangle 
                &
                \le 0. 
            \end{align*}
            Assuming the above inequality holds then one of the sufficient conditions would happen when the coefficient of the cross product term is zero and the coefficient for the normed term is negative, yielding the condition for all $t \in \NN$: 
            \begin{align*}
                \begin{cases}
                    L\eta_{t + 1}^2 + \eta_{t + 1} - \sum_{i = 1}^{t}\eta_i 
                    \le 0, 
                    \\
                    L\eta_t \eta_{t + 1} - \sum_{i = 1}^{t} \eta_i 
                    = 0. 
                \end{cases}
            \end{align*}
            Substituting the sequence equality back to the first one yield: 
            \begin{align*}
                L\eta_{t + 1}^2 - (\eta_{t + 1} + L\eta_t\eta_{t + 1}) &\le 0 
                \\
                L\eta^2_{t + 1} - \eta_{t + 1}
                &\le 
                L\eta_t \eta_{t + 1} 
                \\
                \eta_{t + 1}(L\eta_{t + 1} - 1) 
                &\le L\eta_t\eta_{t + 1}
                \\
                \eta_t > 0 
                \implies 
                L\eta_{t + 1} - 1 &\le 
                L\eta_t 
                \\
                \eta_{t + 1} &\le \eta_t + L^{-1}. 
            \end{align*}
            To satisfy the equality, reader should verify that $\eta_{t} = t/(2L)$ is one of the options. 
            And there are not many other options for the choice of the stepszies for the equality to be satisfied. 
        \end{proof}
        \begin{remark}
            In the original writing by Ahn \cite[equation (4.9)]{ahn_understanding_2022}, they didn't identify the algorithm in the literature. 
            Here we identify it to be equivalent to the acceleration algorithm stated in Ryu \cite[algorithm 12.1]{ryu_large-scale_2022}. 
        \end{remark}

    \subsection{Scenario II}
        With the choice of $\tilde \eta_{t + 1} = \eta_t + L^{-1}$, we can recover the seminal Nesterov accelerated gradient algorithm from 1983. 
        The variants proposed by Chambolle Dossal \cite{chambolle_convergence_2015}, is distinct from the possibility proposed in this secenario. 
        \par\noindent
        To start, we make the claim that the new stepsizes results in a different algorithm. 
        \begin{lemma}[Similar Triangular Form]
            With the choice of stepszie $\tilde \eta_{t + 1} = \eta_t + L^{-1}$ as in \ref*{def:agg_tri}, we have the following specific similar triangular form of the accelerated gradient algorithm: 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1} \nabla f(y_t) 
                \\
                x_{t + 1} &= z_{t + 1} + L\eta_t (z_{t + 1} - z_t)
                \\
                y_{t + 1} &= 
                (1 + L\eta_{t + 1})^{-1}
                (
                x_{t + 1} + L\eta_{t + 1}z_{t + 1}
                ). 
            \end{align*}
            It also has an equivalent momentum form 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1}\nabla f(y_t)
                \\
                y_{t + 1} &= z_{t + 1} + (1 + L\eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            To do, we show that updates sequence $x_{t + 1} = z_{t + 1} + L\eta_t (z_{t + 1} - z_t)$ is equivalent to $x_{t + 1} = x_t + \tilde\eta_{t + 1}\nabla f(y_t)$. 
            Starting with the former, susbtitute definition of $z_{t + 1}$, $z_{t + 1} = z_t + L^{-1}\nabla f(y_t)$ expanding: 
            \begin{align*}
                x_{t + 1} &= y_t - L^{-1}\nabla f(y_t) 
                + L \eta_t y_t - \eta_t \nabla f(y_t) - L\eta_t z_t
                \\
                &= 
                (1 + L\eta_t)y_t - (\eta_t + L^{-1})\nabla f(y_t) - L\eta_t z_t
                \\
                &= \eta_t Lz_t + x_t -(\eta_t + L^{-1}) \nabla f(y_t)  - L\eta_t z_t
                \\
                &= x_t - (\eta_t + L^{-1})\nabla f(y_t). 
            \end{align*}
            By assumption $\tilde \eta_{t + 1} = \eta_t + L^{-1}$, we ends with $x_{t + 1} = x_t + \tilde \eta_{t + 1}\nabla f(y_t)$. 
            Reducing it to the classic momentum form starts by 
            \begin{align*}
                y_{t + 1} &= (1 + L\eta_{t + 1})^{-1} (x_{t + 1} + L\eta_{t + 1}z_{t + 1})
                \\
                &= (1 + L\eta_{t + 1})^{-1} (
                    z_{t + 1} + L\eta_t (z_{t + 1} - z_t) + L\eta_{t + 1} z_{t + 1}
                )
                \\
                &= 
                (1 + L\eta_{t + 1})^{-1} (
                    (1 + L\eta_{t + 1})z_{t + 1} + L\eta_t(z_{t + 1} - z_t)
                )
                \\
                &= z_{t + 1} + (1 + L\eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t), 
            \end{align*}
            it negates the $x_t$ variables, therefore we have 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1}\nabla f(y_t)
                \\
                y_{t + 1} &= z_{t + 1} + (1 + L\eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t).
            \end{align*}
        \end{proof}
        \begin{remark}
            In this remark we clarify the name ``similar triangule" as given in the literatures. 
            We think it is a fitting name, becaues it has a similar triangle in it. 
            We list the following observations
            \begin{enumerate}
                \item 
                The updates for $y_{t}$ from the algorithm has 
                $$
                    y_t = (1 + L\eta_t)^{-1} x_t + L\eta_t(1 + L\eta_t)^{-1} z_t, 
                $$
                therefore, $y_t$ is a convex combinations of $x_t, z_t$, so $z_t, y_t, x_t$ are three collinear points. 
                We have the ratio $\Vert y_t - x_t\Vert/\Vert z_t - y_t\Vert = L\eta_t$. 
                \item 
                The updates for $z_{t + 1}, x_{t + 1}$ are based on $y_t, x_t$ displaced by $L^{-1}\nabla f(y_t), \tilde\eta_{t +1}\nabla f(y_t)$, therefore vector $z_{t + 1} - y_t$ parallels to $x_{t + 1} - x_t$. 
                \item The updates for $x_{t + 1}$ has $x_{t + 1} - z_{t + 1} = L\eta_t \left(z_{t + 1} - z_t\right)$, therefore, the three points $z_t, x_{t + 1}, z_{t + 1}$ are collinear. 
                The ratio between line segment has $\Vert x_{t + 1} - z_{t + 1}\Vert/\Vert z_{t + 1} - z_t\Vert = L\eta_t$. 
            \end{enumerate}
            By these tree observations, the triangle $z_{t}, z_{t + 1}, y_t$ similar to triangle $z_t, x_{t + 1}, x_t$. 
        \end{remark}
        The next lemma will determine one choice of the stepsize sequence $\eta_t$ that recovers the classical form of Nesterov accelerated gradient method. 

        \begin{lemma}[Deriving the Nesterov Sequence]
            For all $t \in \mathbb N$
            \begin{align*}
                \left\lbrace
                \begin{aligned}
                    (1 + L\eta_{t + 1})^{-1}L \eta_t 
                    &= \frac{a_t - 1}{a_{t + 1}} , 
                    \\
                    a_{t + 1}^2 - a_t &= a_t^2. 
                \end{aligned} 
                \right. 
            \end{align*}
            allows \hyperref[def:agg_tri]{definition \ref*{def:agg_tri}} to converge at an optimal rate. 
            The base case is $\eta_0 = 0$. 
        \end{lemma}
        \begin{proof}
            Continue from \ref*{lemma:smooth_agg_lyapunov_upper_bound}, With $x_{t +1} - x_t = L\tilde\eta_{t+1}(z_{t + 1} - y_t)$ for $y_t$: 
            \begin{align*}
                \Upsilon_{1, t + 1}^\text{AG} &\le 
                - \frac{1}{2}\Vert x_{t+1 } - x_t\Vert^2 + 
                \frac{\tilde\eta_{t +1}L}{2}\Vert z_{t+1} - y_t\Vert^2
                \\
                &= 
                - \frac{1}{2}\Vert L \tilde \eta_{t+} (z_{t+1} - y_t)\Vert^2 + 
                \frac{\tilde\eta_{t +1}L}{2}\Vert z_{t+1} - y_t\Vert^2
                \\
                &= 
                \left(
                    \frac{L^2\tilde\eta_{t + 1}^2}{2}
                    + 
                    \frac{\tilde \eta_{t +1}}{2}
                \right)
                \Vert z_{t+1} - y_t\Vert^2
                - 
                \langle 
                    \tilde \eta_{t+ 1} \nabla f(y_t), x_{t+1} - z_{t+1}
                \rangle. 
            \end{align*}
            This completes the analysis for $\Upsilon_{1, t + 1}^\text{AG}$ with $\tilde\eta_{t+1}$. 
            Next we consider 
            \begin{align*}
                \Upsilon_{2, t + 1}^{\text{AG}} &= 
                \langle \nabla f(y_t), z_{t+1} - y_t\rangle
                + 
                \frac{L }{2} \Vert z_{t+1} - y_t\Vert^2 + 
                \langle \nabla f(y_t), y_t - z_t\rangle
                \\
                &= 
                \langle \nabla f(y_t), z_{t + 1} - z_t\rangle
                + 
                \frac{L }{2} \Vert z_{t+1} - y_t\Vert^2
                \\
                &= 
                \langle \nabla f(y_t), L^{-1}\eta_t^{-1}(x_{t+1} - z_{t + 1})\rangle
                + \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2. 
            \end{align*}
            Here we used $x_{t + 1} = z_{t + 1} + L\eta_t (z_{t + 1} - z_t)$, which was the consequence of $\tilde\eta_{t + 1} = \eta_t + L^{-1}$, as discussed from before. 
            Hence, the upper bound of the Lyapunov function for smooth AG PPM 
            (\hyperref[thm:generic_smooth_ag_convergence]{theorem \ref*{thm:generic_smooth_ag_convergence}}) 
            has 
            \begin{align*}
                & \Upsilon_{1, t + 1}^{\text{AG}} + 
                \left(
                    \sum_{i = 1}^{t}\tilde \eta_i
                \right)\Upsilon_{2, t + 1}^{\text{AG}}
                \\
                &\quad \le 
                \left(
                    - \frac{L^2\tilde \eta_{t + 1}^2}{2}
                    + \frac{\tilde\eta_{t+1}L}{2}
                \right)
                \Vert z_{t+1} - y_t\Vert^2
                - 
                \langle \tilde \eta_{t+1} \nabla f(y_t), x_{t+1} - z_{t + 1}\rangle
                \\
                & \qquad + 
                \left(
                    \sum_{i = 1}^{t}\tilde \eta_i
                \right)\left(
                    \langle \nabla f(y_t), L^{-1}\eta_t^{-1}(x_{t+1} - z_{t + 1})\rangle
                    + \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
                \right). 
            \end{align*}
            If the above quantity less than or equal to zero, it will allow for convergence of the algorithm. 
            One sufficient condition is to have the coefficients for all the cross product terms to sum up to zero, and the squared normed term to sum up to a non-positive number for all $t \in \mathbb N$. 
            This gives relations 
            \begin{align*}
                - \frac{1}{2}\left(
                    L^2 \tilde\eta_{t+1}^2 - \tilde\eta_{t+1}L 
                \right) + 
                \frac{L}{2}\left(
                    \sum_{i = 1}^{t} \tilde \eta_i
                \right) &\le 0
                \\
                - \left(
                    L\tilde \eta_{t + 1}^2 
                    - 
                    \tilde\eta_{t + 1}
                \right) + 
                \sum_{i = 1}^{t}\tilde \eta_i 
                &\le 0
                \\
                -L \tilde \eta_{t+1} + 
                \sum_{i = 1}^{t + 1} \tilde \eta_i 
                &\le 0
                \\
                \sum_{i = 1}^{t + 1}\tilde \eta_i 
                &\le L \tilde \eta_{t + 1}^2, 
            \end{align*}
            and equations 
            \begin{align*}
                -\tilde \eta_{t + 1} + 
                \left(
                    \sum_{i = 1}^{t}\tilde \eta_i
                \right)L^{-1}\eta_t^{-1} 
                &= 0
                \\
                \sum_{i = 1}^{t}\tilde \eta_i 
                &= 
                L \eta_t \tilde \eta_{t + 1}. 
            \end{align*}
            Susbtituting the equality into the inequalites we have 
            \begin{align*}
                \tilde \eta_{t + 1} + \sum_{i = 1}^{t} \tilde \eta_i 
                &\le L \tilde \eta_{t + 1}^2
                \\
                \tilde \eta_{t + 1} + L\eta_t \tilde \eta_{t + 1}
                &\le L \tilde \eta_{t + 1}^2
                \\
                1 + L\eta_t &\le L \tilde\eta_{t + 1}. 
            \end{align*}
            The choice $\tilde \eta_{t + 1} = \eta_t + L^{-1}$ makes the inequality is an equality. 
            Finally, to recover the momentum stepsizes for the classical Nesterov accelerated gradient, we consider relations 
            \begin{align*}
                L \sum_{i = 1}^{t + 1} \tilde \eta_i 
                &= L \tilde \eta_{t + 1} + L \sum_{i = 1}^{t} \tilde \eta_i 
                \\
                &= 
                L \tilde \eta_{t + 1} + L (L \eta_t \tilde \eta_{t + 1}) 
                \\
                &= L \tilde \eta_{t + 1} + L \eta_t (L \eta_t + 1)
                \\
                &= L (\eta_t + L^{-1}) + L\eta_t (L \eta_t + 1)
                \\
                &= (L\eta_t + 1)^2, 
            \end{align*}
            Simultanenously 
            \begin{align*}
                L \sum_{i = 1}^{t + 1} \tilde \eta_i 
                &= L^2 \eta_{t + 1} \tilde \eta_{t + 1}
                \\
                &= L \eta_{t + 1}(1+ L \eta_{t + 1}), 
            \end{align*}
            Combining yeilds that 
            \begin{align*}
                (L\eta_t + 1)^2 
                &= L\eta_{t + 1}(1 + L \eta_{t + 1})
                \\
                &= L\eta_{t + 1} + L^2 \eta_{t + 1}^2
                \\
                \iff 
                (L\eta_t + 1)^2 + 1/4 &= 
                1/4 + 2(1/2)L \eta_{t + 1} + (L \eta_{t + 1})^2
                \\
                \iff 
                (L\eta_t + 1)^2 + 1/4 &= 
                (L \eta_{t + 1} + 1/2)^2. 
            \end{align*}
            With $a_t = L\eta_t + 1 = L \tilde \eta_t$ this is 
            \begin{align*}
                a_t^2 + 1/4 &= (a_{t + 1} - 1/2)^2 
                \\
                a_t^2 + 1/4 &= a_{t + 1}^2 + 1/4 - a_{t + 1}
                \\
                a_t^2 + a_{t + 1} &= a_{t + 1}^2. 
                \\
                \implies 
                a_{t + 1} &= 
                \frac{1 + \sqrt{1 + 4a_t^2}}{2}
            \end{align*}
            And that is the updates for the Nesterov momentum, recall that the momentum term is linked to the PPM sequence via relation 
            \begin{align*}
                (1 + L \eta_{t+ 1})^{-1}L\eta_t = L\eta_t / a_{t + 1} = (a_t - 1)/a_{t + 1}. 
            \end{align*}
            We had recovered the classic Nesterov accelerated sequence method for the smooth gradient descent algorithm. 
        \end{proof}
        \begin{remark}
            This variant is distinct from the variants proposed by Chambolle \cite{chambolle_convergence_2015}, but both of which fits into the same frameworks. 
            The variantis presented as 
            \begin{align*}
                y_n &= (1 - t_n^{-1})z_n + t_n^{-1} x_n
                \\
                z_{n + 1} &= y_n - L^{-1}\nabla f(y_n)
                \\
                x_{n + 1} &= z_n + t_{n + 1}(z_{n + 1} - z_n)
            \end{align*}
            They listed the choice of $t_n = (n + a - 1)/a$ for all $a > 2$. 
            Under this choice, unfortunately, it's in no way obvious that this format reduces to \hyperref[def:agg_tri]{definition \ref*{def:agg_tri}}. 
            It fails the similar triangule description Without the explict representation $x_{n + 1} = z_n + \tilde\eta_{t + 1}\nabla f(y_t)$. 
            \par\noindent
            At the time of writing, we are still investigating the relations between these variants of algorithms.
        \end{remark}
        
    \subsection{Scenario III}
    


\section{Classical analysis of Nesterov accelerated gradient}
    In this section, we reproduce some of the analysis for Nesterov accelerated gradient method with excruciating details. 
    We will also go over some details about the history, development and motivations behind the accelerated gradient algorithm. 

\section{Modern techniques in analysis of accelerated gradient algorithms}
    

\printbibliography

\appendix
\section*{Postponed Proofs}
    

\end{document}
