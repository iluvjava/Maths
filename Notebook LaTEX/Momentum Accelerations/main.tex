\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont Nesterov Type Momentum Methods}}

\author{
    Alto
    \thanks{
        Subject type, Some Department of Some University, Location of the University,
        Country. E-mail: \texttt{author.name@university.edu}.
    }
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    These are ntoes for Nesterov Type Acceleration Methods, in the convex case. 
    They may get made into papers, proposal, and thesis in the future. 
\end{abstract}
\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

% ==============================================================================
\section{Preliminaries}
    In this section we list fundational results that are important for proofs in coming sections. 
    For this section, let the ambient space be $\RR^n$ and $\Vert \cdot\Vert$ be the 2-norm until it's specified in the context. 
    For a generate over views of smoothness and strong convexity in the Euclidean space, see \cite[theorem 2.1.5, theorem 2.1.10]{nesterov_lectures_2018} for full exposition of the topic. 
    \subsection{Lipschitz smoothness}
        \begin{definition}[Lipschitz Smooth]
            Let $f$ be differentiable. 
            It has Lipschitz smoothness with constant $L$ if for all $x, y$
            $$
                \Vert \nabla f(x) - \nabla f(y)\Vert 
                \le 
                L \Vert x - y\Vert. 
            $$    
        \end{definition}

        \begin{theorem}[Lipschitz Smoothness Equivalence]
            With $f$ convex and $L$-Lipschitz smooth, the following conditions are equivalent conditions for all $x, y$: 
            \begin{enumerate}
                \item $L^{-1}\Vert \nabla f(y) - \nabla f(x)\Vert^2 \le \langle \nabla f(y) - \nabla f(x), y - x\rangle \le L\Vert y - x\Vert^2$. 
                \item $x^+\in \argmin_x f(x) \implies \frac{1}{2L}\Vert \nabla f(x)\Vert^2 \le f(x) - f(x^+) \le (L/2) \Vert x - x^+\Vert^2$, co-coersiveness. 
                \item $ 1/(2L)\Vert \nabla f(x) - \nabla f(y)\Vert^2 \le  f(y) - f(x) - \langle \nabla f(x), y -x\rangle \le (L/2)\Vert x - y\Vert^2$
            \end{enumerate}    
        \end{theorem}
        \begin{remark}
            Lipschitz smoothness of the gradient of a convex function is an example of a firmly nonexpansive operator.  
        \end{remark}

        \begin{definition}[Strong Convexity]
            With $f: \RR^n \mapsto \overline \RR$, it is strongly convex with constant $\alpha$ if and only if $f - (\alpha/2)\Vert \cdot\Vert^2$ is a convex function. 
        \end{definition}
        
        \begin{theorem}[Stong convexity equivalences]\label{thm:str_cvx_equiv}
            With $f:\RR^n \mapsto \overline \RR$ $\alpha$-strongly convex, the following conditions are equivalent conditions for all $x, y$: 
            \begin{enumerate}
                \item $f(y) - f(x) - \langle \partial f(x),y - x \rangle\ge \frac{\alpha}{2}\Vert y - x\Vert^2$
                \item $\langle \partial f(y) - \partial f(x), y - x\rangle \ge \alpha\Vert y - x\Vert^2$. 
                \item $f(\lambda x + (1 - \lambda)y) \le \lambda f(x) + (1 - \lambda)f(y) -\alpha\frac{\lambda(1 - \lambda)}{2}\Vert y - x\Vert^2, \forall \lambda \in [0, 1]$. 
            \end{enumerate}
        \end{theorem}

        \begin{theorem}[Strong convexity implications]\label{thm:str_cvx_implied}
            With $f:\RR^n \mapsto \overline \RR$ $\alpha$-strongly convex, the following conditions are implied: 
            \begin{enumerate}
                \item $\frac{1}{2}\operatorname{dist}(\mathbf 0; \partial f(x))^2 \ge \alpha (f(x) - f^+)$ where $f^+$ is a minimum of the function, and this is called the Polyak-Lojasiewicz (PL) inequality.
                \item $\forall x, y\in \mathbb E, u\in \partial f(x), v\in \partial f(y): \Vert u - v\Vert\ge \alpha\Vert x - y\Vert$. 
                \item $f(y) \le f(x) + \langle \partial f(x), y - x\rangle + \frac{1}{2\alpha}\Vert u - v\Vert^2, \forall u\in  \partial f(x), v\in \partial f(y)$. 
                \item $\langle \partial f(x)-\partial f(y), x - y\rangle \le \frac{1}{\alpha}\Vert u - v\Vert^2, \forall u\in \partial f(x), v\in \partial f(y)$. 
                \item if $x^+\in \arg\min_{x}f(x)$ then $f(x) - f(x^+) \ge \frac{\alpha}{2}\Vert x - x^+\Vert^2$ and $x^+$ is a unique minimizer. 
            \end{enumerate}
        \end{theorem}
        \begin{remark}
            In the context of operator theory, the subgradient of a strongly convex function is an example of a Strongly Monotone Operator. 
        \end{remark}
    
    \subsection{Proximal descent inequality}
        The proximal descent inequality below is a crucial piece of inequality for deriving the behaviors of algorithms. 
        \begin{theorem}[Proximal Descent Inequality]\label{thm:ppm_descent_ineq}
            With $f: \RR^n \mapsto \overline \RR^n$ $\beta$-convex where $\beta \ge 0$, fix any $x \in \RR^n$, let $p = \hprox_f(x)$, then for all $y$ we have inequality 
            $$
                \left(f(p) + \frac{1}{2}\Vert x - p\Vert^2\right)
                - 
                \left(
                    f(y) + \frac{1}{2}\Vert x - y\Vert^2 
                \right)
                \le 
                - \frac{(1 + \beta)}{2}\Vert y - p\Vert^2. 
            $$
            Recall: $\hprox_f(x) = \argmin_{u}\left\lbrace f(u) + \frac{1}{2}\Vert u - x\Vert^2 \right\rbrace$. 
        \end{theorem}
        \begin{remark}
            We make use of this theorem in the proof of convergence of proximal point method. 
            See the proof (\cite{bauschke_convex_2017}, theorem 12.26). 
            The additional strong convexity index is a consequence of \hyperref[thm:str_cvx_implied]{theorem \ref*{thm:str_cvx_implied}}, item (v). 
        \end{remark}
        
        \begin{theorem}[The Bregman proximal descent inequality]\label{thm:ppm_breg_descent_ineq}
            Let $\omega$ induce a Bregman Divergence $D_\omega$ in $\RR^n$ and assume that it satisfies Bregman Prox Admissibility conditions for function $\varphi: \RR^n \mapsto \overline \RR$.
            Then we claim that for all $c \in \text{dom}(\omega), b \in \text{dom}(\partial \omega)$, 
            If 
            \begin{align*}
                a = \argmin_{x}\left\lbrace
                \varphi(x) + D_\omega(x, b)
                \right\rbrace, 
            \end{align*}
            we have the inequality, 
            \begin{align*}
                (\varphi(c) + D_\omega(c, b)) - 
                (\varphi(a) + D_\omega(a, b)) \ge 
                D_\omega(c, a). 
            \end{align*}
        \end{theorem}
        \begin{remark}
            For more information about what function $\omega$ can induce a bregman divergence and the admissibility conditions for Bremgna proximal mapping, consult (CITATION NEEDED for Heinz's NopLips Paper). 
        \end{remark}


\section{The Proximal Point Method in the Convex Case}
    In this section we go over the analysis of Proximal point method (PPM) in the convex case and see how the theories can be generalized into the cases where PPM is approximated. 
    \subsection{Literature reviews}
        Rockafellar \cite{rockafellar_monotone_1976} pioneered the analysis of the method of proximal point method in the convex case. 
        The analysis is developed in the context of maximal monotone operators in Hilbert spaces. 
        Applications in convex optimizations are covered. 
        Using his theorems appropriately requires some opportunities, realizations, and characterizations of assumptions (A), (B) in his paper in the context of the applications. 

    \subsection{The proximal point method}
        With $f: \RR^n \mapsto \overline \RR$ lsc proper and convex, given any $x_0$ the PPM generates sequence $(x_n)_{n\in \NN}$ by $x_{k + 1} = \hprox_{\eta_{k + 1}}f(x_k)$ for all $k \in \NN$ where the sequence $(\eta_{k})_{k \in \NN}$ is a nonegative sequence of real numbers.

    \subsection{The Lyapunov function of Convex PPM}
        We present some theorems that illusrate the use of \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}}. 
        The following analysis, techniques can be found in Guler's work \cite{guler_convergence_1991}. 
        \begin{theorem}
            With $f$ being $\beta\ge 0$ strongly convex and $x_{t + 1} = \hprox_{\eta_{t + 1}f}$ generated by PPM. 
            Define the Lyapunov function $\Phi_t$ for all $u \in \RR^n$: 
            \begin{align*}
                \Phi_t &:= 
                \left(
                    \sum_{i = 1}^{t} \eta_i
                \right)(f(x_t) - f(u)) + \frac{1}{2} \Vert u - x_t\Vert^2 \quad \forall t \ge 1, 
                \\
                \Phi_0 &:= (1/2)\Vert x_0 - u\Vert^2, 
            \end{align*}
            then it is a Lyapunov function for the PPM algorithm. 
            Meaning for all $(x_k)_{k \in \NN}$ generated by PPM, it satisfies that $\Phi_{t + 1} - \Phi_t \le 0$. 
            Additionally, by the definition we have 
            {\small
                \begin{align*}
                    \Phi_{t + 1} - \Phi_{t} 
                    &= 
                    \left(\sum_{i = 1}^{t}\eta_{i}\right)
                    (f(x_{t + 1}) - f(x_t)) 
                    + \frac{1}{2}\Vert x_{t + 1} - u\Vert^2 
                    - \frac{1}{2}\Vert x_{t} - u\Vert^2
                    +
                    \eta_{t + 1}(f(x_{t + 1}) - f(u))
                    \\
                    & \le 
                    -\left(\sum_{i = 1}^{t}\eta_{i}\right)
                    (1 + \beta\eta_{t + 1}/2)\Vert x_{t + 1} - x_t\Vert^2
                    + 
                    \left(
                        - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                        -\frac{\beta\eta_{t + 1}}{2}\Vert u - x_{t + 1}\Vert^2
                    \right)
                    \\
                    & \le 0,
                \end{align*}
            }
            and additionarlly, recovering the descent lemma: 
            \begin{align*}
                f(x_{t + 1}) - f(x_t) 
                &\le
                -\frac{1}{\eta_{t+1}} \Vert x_{t + 1} - x_t\Vert^2 
                - \frac{\beta}{2}\Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Let $\phi_{t + 1}: \RR^n \mapsto \overline \RR = \eta_{t + 1} f$ be convex,  consider proximal point method $x_{t + 1} = \hprox_{\phi}(x_t)$, apply \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}}, we have $\forall u \in \RR^n$
            \begin{align*}
                & \phi_{t + 1}(x_{t + 1}) 
                + 
                \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                - \phi_{t + 1}(u) - \frac{1}{2}\Vert u - x_t\Vert^2
                \le 
                - 
                \frac{1}{2}(1 + \beta\eta_{t + 1})\Vert 
                    u - x_{t + 1}
                \Vert^2
                \\
                & \text{let } u = x_*
                \\
                &\quad 
                \begin{aligned}
                    \implies &
                    \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) 
                    + 
                    \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2 
                    +  
                    \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                    -
                    \frac{1}{2}\Vert x_* - x_t\Vert^2 
                    \\
                    & \quad \le 
                    -\frac{\beta\eta_{t + 1}}{2}\Vert x_* - x_{t + 1}\Vert^2
                    \\
                    \iff & 
                    \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) 
                    + 
                    \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2 
                    -
                    \frac{1}{2}\Vert x_* - x_t\Vert^2 
                    \\
                    & \quad \le 
                    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                    -\frac{\beta\eta_{t + 1}}{2}\Vert x_* - x_{t + 1}\Vert^2
                    \le 0. 
                \end{aligned}
                \\
                & \text{let } u = x_{t}
                \\
                &\quad  
                \begin{aligned}
                    \implies& 
                    f(x_{t + 1}) - f(x_t)
                    \le 
                    -\frac{1}{\eta_{t+1}} \Vert x_{t + 1} - x_t\Vert^2 
                    - \frac{\beta}{2}\Vert x_t - x_{t + 1}\Vert^2
                    \le 0. 
                \end{aligned}
            \end{align*}
            Let's define the following quantities for all $u, \beta\ge 0$: 
            \begin{align*}
                \Upsilon_{1, t + 1}(u) &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(u)) + \frac{1}{2}(
                    \Vert x_{t + 1} - u\Vert^2 - 
                    \Vert x_t - u\Vert^2
                )
                \\
                & \le 
                - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                -\frac{\beta\eta_{t + 1}}{2}\Vert u - x_{t + 1}\Vert^2, 
                \\
                \Upsilon_{2, t + 1}
                &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t))
                \\
                &\le 
                - \Vert x_{t + 1} - x_t\Vert^2 - 
                \frac{\beta\eta_{t + 1}}{2}
                \Vert x_{t + 1} - x_t\Vert^2 
                \\
                &= 
                -(1 + \beta\eta_{t + 1}/2)\Vert x_{t + 1} - x_t\Vert^2 \le 0. 
            \end{align*}
            With $\Phi_t$ as defined in the theorem, observe the following demonstration for all $u$, $\beta \ge 0$: 
            {\small
            \begin{align*}
                \Phi_{t + 1} - \Phi_{t}
                &= 
                \left(
                    \sum_{i = 1}^{t + 1}\eta_i
                \right)(f(x_{t + 1}) - f(u)) + 
                \frac{1}{2}\Vert x_{t + 1} - u\Vert^2 
                - 
                \left(
                    \sum_{i = 1}^{t}\eta_i
                \right)(f(x_{t}) - f(u))
                - 
                \frac{1}{2}\Vert x_{t} - u\Vert^2
                \\
                &= 
                \left(\sum_{i = 1}^{t}\eta_{i}\right)
                (f(x_{t + 1}) - f(x_t)) 
                + \frac{1}{2}\Vert x_{t + 1} - u\Vert^2 
                - \frac{1}{2}\Vert x_{t} - u\Vert^2
                +
                \eta_{t + 1}(f(x_{t + 1}) - f(u))
                \\
                &= 
                \left(\sum_{i = 1}^{t}\eta_{i}\right)\Upsilon_{2, t + 1}
                + \Upsilon_{1, t + 1}(u)
                \\
                &\le 
                -\left(\sum_{i = 1}^{t}\eta_{i}\right)
                (1 + \beta\eta_{t + 1}/2)\Vert x_{t + 1} - x_t\Vert^2
                + 
                \left(
                    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                    -\frac{\beta\eta_{t + 1}}{2}\Vert u - x_{t + 1}\Vert^2
                \right)
                \le 0. 
            \end{align*}
            }
            Therefore, $\Phi_t$ is a legitimate Lyapunov funtion for all $u, \beta \ge 0$. 
        \end{proof}
        \begin{remark}
            The above Lyapunov is not unique and it's not optimal for $\beta > 0$, striclty strongly convex functions. 

        \end{remark}

        \begin{theorem}[Convergence Rate of PPM]\label{thm:ppm_convergence_rate}
            The convergence rate of PPM applied to $f$, closed, convex proper, we have convergence rate of the function value: 
            $$
            f(x_T) - f(x_*) \le O\left(\left(\sum_{i=1}^{T}\eta_t\right)^{-1}\right). 
            $$
            Where $x_*$ is the minimizer of $f$. 
        \end{theorem}
        \begin{proof}
            With $\Delta_t = f(x_t) - f(x_*), \Upsilon_t = \sum_{i = 1}^{t}\eta_i$ so $\Phi_t = \Upsilon_t\Delta_t + \frac{1}{2}\Vert x_t - x_*\Vert^2$ by consideration $u = x_*$, invoking previous theorem and do
            \begin{align*}
                \Upsilon_T\Delta_T \le \Phi_T 
                &\le 
                \Phi_0 = \frac{1}{2}\Vert x_0 - x_*\Vert^2 
                \\
                \implies \Delta_T 
                &\le 
                \frac{1}{2\Upsilon_T} \Vert x_0 - x_*\Vert^2. 
            \end{align*}
        \end{proof}
        \begin{remark}
            With the same choice of the sequence $(\eta_t)_{t \in \NN}$, convergence of PPM method of a strongly convex function is faster.     
        \end{remark}
        
        
\section{Applying the analysis of PPM}
    The PPM method and the Lyaounov function derived above serves as the tamplate for other algorithms. 
    In optimizations, people use a lower, or an upper approximation of the objective function to approximate the PPM. 
    The approaches are a diverse, including second order algorithms such as Newton's method. 
    To demonstrate, assume that $f$ is a lsc convex function such that it can be approximated by an lower bounding function $l_f(x|\bar x)$ at $\bar x$ such that it satisfies for all $x$: 
    \begin{align}
        l_f(x| \bar x) 
        \le f(x) \le l_f(x|\bar x) + \frac{L}{2}\Vert x - \bar x\Vert^2. 
    \end{align}
    The above characterization is generic enough to include the case where $l_f(x|\bar x)$, the under approximating function is nonsmooth. 
    We assume that $l_f(x|\bar x)$ is convex for all $x$, at all $\bar x$ so that the previous theorems are applicable. 
    \par 
    The approximated proximal point method is applying PPM to the function $l_f(x|x_t)$ for each iteration, i.e: $x_{t +1} = \hprox_{\eta_{t + 1}l_f(\cdot | x_t)}(x_t)$. 
    \subsection{Generic gradient descent}
        As a warm up, we consider deriving gradient descent via the PPM approach. 
        Please pay attention to the remarks, it reveals parts of the proof that could inspirate the idea of non-monotone line search method in a practical settings. 
        \begin{theorem}[Generic Approximated PPM]\label{thm:lower_approx_ppm_convergence}
            With $f$ convex having minimizer: $x_*$; $l_f(\cdot; x_t)$ convex, lsc and proper, define $\phi_t(x) = \eta_{t + 1}l_f(x; x_t)$. 
            Assume the following estimates hold: 
            $$
            \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) 
            + 
            \frac{L\eta_{t + 1}}{2}\Vert x - x_t\Vert^2 \quad \forall x \in \RR^n. 
            $$ 
            Fix any $x_0$, let the iterates $x_t$ defined for $t\in \NN$ satisfies
            \begin{align*}
                x_{t +1} = \argmin_{x} \left\lbrace
                    l_f(x; x_t) + \frac{1}{2\eta_{t + 1}}\Vert x - x_t\Vert^2
                \right\rbrace, 
            \end{align*}
            then it has 
            \begin{align*}
                \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) + \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2
                - \frac{1}{2}\Vert x_* - x_t\Vert^2
                & \le 
                \left(
                    \frac{L \eta_{n + 1}}{2} - \frac{1}{2}
                \right)\Vert x_{t + 1} - x_t\Vert^2.
            \end{align*}
            Additionally if $\exists \epsilon > 0: \eta_{t} \in (\epsilon, 2L^{-1} - \epsilon)$, for all $t \in \NN$, the algorithm has sublinear convergence rates of
            \begin{align*}
                f(x_T) - f(x_*)
                &\le 
                \frac{L - \epsilon^{-1}}{TL\epsilon} (f(x_0) - f(x_T)) 
                \\
                &\le 
                \frac{L - \epsilon^{-1}}{TL\epsilon} (f(x_0) - f(x_*)) 
            \end{align*}
        \end{theorem}
        \begin{proof}
            By $\phi_t$ convex, apply \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}} with $f = \phi_t$, $x = x_t$, $x_{t + 1} = p$, yielding $\forall y$
            {\footnotesize
                \begin{align*}
                    \phi_t(x_{t + 1}) + \frac{1}{2}\Vert x_t - x_{t + 1}\Vert^2 
                    - 
                    \phi_t(y) - \frac{1}{2}\Vert x_t - y\Vert^2
                    &\le 
                    - \frac{1}{2}\Vert y - x_{t + 1}\Vert^2
                    \\
                    \phi_t(x_{t + 1}) - \phi_t(y)
                    + \frac{1}{2}(
                        \Vert y - x_{t + 1}\Vert^2 - \Vert x_t - y\Vert^2 
                    ) &\le 
                    - \frac{1}{2} \Vert x_t - x_{t + 1}\Vert^2
                    \\
                    \left(
                        \phi_t(x_{t + 1}) + \frac{L\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert
                    \right)
                    - \phi_t(y)
                    + \frac{1}{2}(
                        \Vert y - x_{t + 1}\Vert^2 - \Vert x_t - y\Vert^2 
                    ) &\le 
                    \left(
                        \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                    \right) \Vert x_t - x_{t + 1}\Vert^2
                    \\
                    \implies 
                    \eta_{t + 1}f(x_{t + 1}) - \eta_{t + 1}f(y) 
                    + \frac{1}{2}(
                        \Vert y - x_{t + 1}\Vert^2 - \Vert x_t - y\Vert^2 
                    ) 
                    &\le 
                    \left(
                        \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                    \right) \Vert x_t - x_{t + 1}\Vert^2. 
                \end{align*}
            }
            Setting $y = x_t$ yields
            \begin{align*}
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t)) + 
                \frac{1}{2}
                    \Vert x_t - x_{t + 1}\Vert^2
                &\le 
                \left(
                        \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                \right) \Vert x_t - x_{t + 1}\Vert^2
                \\
                \iff 
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t))
                &\le 
                \left(
                        \frac{L\eta_{t + 1}}{2} - 1
                \right) 
                \Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
            In a similar manner to the derivation of Lyapunov function for PPM, we make for all $y$: 
            \begin{align*}
                \Upsilon_{1, t + 1}(y) &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(y)) + \frac{1}{2}(
                    \Vert x_{t + 1} - y\Vert^2 - 
                    \Vert x_t - y\Vert^2
                )
                \\
                & \le 
                \left(
                    \frac{L\eta_{t + 1}}{2} - \frac{1}{2}
                \right) \Vert x_t - x_{t + 1}\Vert^2, 
                \\
                \Upsilon_{2, t + 1}
                &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t))
                \\
                &\le 
                \left(
                    \frac{L\eta_{t + 1}}{2} - 1
                \right) 
                \Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
            Now, consider defining $\Phi_t$ for all $y$: 
            $$
                \Phi_t = \left(
                    \sum_{i = 1}^{t} \eta_{i}
                \right)
                (f(x_t) - f(y)) + \frac{1}{2}\Vert y - x_t\Vert^2, 
            $$
            which is previously proposed Lyapunov function for PPM, we define the basecase $\Phi_0 = \frac{1}{2}\Vert y - x_0\Vert^2$. 
            Consider the difference $\forall y$: 
            \begin{align*}
                \Phi_{t + 1} - \Phi_t
                &= 
                \left(\sum_{i = 1}^{t}\eta_{i}\right)\Upsilon_{2, t + 1}
                + \Upsilon_{1, t + 1}(y)
                \\
                &\le 
                \left(\sum_{i = 1}^{t}\eta_{i}\right) 
                \left(
                    \frac{L\eta_{t + 1}}{2} - 1
                \right) 
                \Vert x_t - x_{t + 1}\Vert^2 + 
                \left(
                    \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                \right) \Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
            Observe that if $\eta_i \le L^{-1}$, then $\Phi_{t + 1} - \Phi_t \le 0$, hence the convergence rate of $\mathcal O\left((\sum_{i = 1}^{t}\eta_i)^{-1}\right)$ of PPM for $\Phi_t$ is applicable. 
            \par\noindent
            Surprisingly, if $\eta_i \in (0, 2L^{-1})$, $\Phi_{t}$ still convergeces under mild conditions. 
            For simplicity we set $\sigma_t := \sum_{i = 1}^{t}\eta_i$. 
            It starts with considerations that $(L\eta_{t + 1}/2 - 1) < 0$, so that 
            \begin{align*}
                f(x_{t + 1}) - f(x_t) &\le 
                \left(\frac{L\eta_{t + 1}}{2} - 1\right)\Vert x_{t + 1} - x_t\Vert^2
                \\
                f(x_T) - f(x_0)
                &\le 
                \underbrace{
                \left(
                    \frac{L\sigma_T}{2} - T
                \right)
                }_{< 0}
                \sum_{t = 0}^{T - 1}\Vert x_{t + 1} - x_t\Vert^2
                \\
                \implies 
                \sum_{t = 0}^{T -1}\Vert x_t - x_{t + 1}\Vert^2
                &\le 
                \left(
                    \frac{L}{2}\sigma_T  - T
                \right)^{-1} 
                (f(x_T) - f(x_0))
            \end{align*}
            Continue on the RHS of $\Phi_{t + 1} - \Phi_t$ so 
            \begin{align*}
                \sum_{t = 0}^{T - 1}\Phi_{t + 1} - \Phi_t 
                &\le 
                \left(
                    \frac{L}{2}\sigma_T - \frac{T}{2}
                \right)\sum_{t = 0}^{T - 1}\Vert x_{t + 1} - x_t\Vert^2
                \\
                \Phi_T - \Phi_0 &\le 
                \left(
                    \frac{\frac{L}{2}\sigma_T - \frac{T}{2}}{
                        \frac{L}{2}\sigma_T - T
                    }
                \right)
                (f(x_T) - f(x_0))
                \\
                &= 
                \left(
                    \frac{L\sigma_T - T}{L\sigma_T - 2T}
                \right)
                (f(x_T) - f(x_0)), 
            \end{align*}
            implies
            \begin{align*}
                \sigma_T (f(x_T) - f(y)) + \frac{1}{2}\Vert y - x_t\Vert^2
                - \frac{1}{2}\Vert y - x_0 \Vert^2 
                &\le 
                \left(
                    \frac{L\sigma_T - T}{L\sigma_T - 2T}
                \right)
                (f(x_T) - f(x_0))
                \\
                \iff
                f(x_T) - f(y) + 
                \frac{1}{2\sigma_T}(\Vert y - x_t\Vert^2 - \Vert y - x_0\Vert^2)
                &\le 
                \left(
                    \frac{L- T\sigma_T^{-1}}{2T - L\sigma_T}
                \right)
                (f(x_0) - f(x_T)), 
            \end{align*}
            therefore we obtain the bound: 
            \begin{align*}
                f(x_T) - f(y) &\le 
                \left(
                    \frac{L- T\sigma_T^{-1}}{2T - L\sigma_T}
                \right)
                (f(x_0) - f(x_T))
                - 
                \frac{1}{2\sigma_T}(\Vert y - x_t\Vert^2 - \Vert y - x_0\Vert^2)
            \end{align*}
            In the case where $\sup_{i\in \NN} \eta_i \le 2L^{-1} - \epsilon$, and $\inf_{i\in \NN}\eta_i \ge \epsilon$ with $\epsilon > 0$. 
            Then we have 
            \begin{align*}
                \frac{L -T\sigma_T^{-1}}{2T - L\sigma_T}
                &\le 
                \frac{L - \epsilon^{-1}}{2T - LT(2L^{-1} - \epsilon)}
                \\
                &= 
                \frac{L - \epsilon^{-1}}{2T - T(2 - L\epsilon)}
                \\
                &= 
                \frac{L - \epsilon^{-1}}{TL\epsilon}. 
            \end{align*}
            With $y = x_*$, we get the claimed convergence rate because $f(x_t)$ is strictly monotone decreasing. 
        \end{proof}
        \begin{remark}
            Observe that inequality 
            $$
                \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) 
                + 
                \frac{L\eta_{t + 1}}{2}\Vert x - x_t\Vert^2 \quad \forall x \in \RR^n, 
            $$
            was invoked with $x = x_{t + 1}$ for the PPM descent inequality in the above proof, meaning that if  $\forall (x_t)_{t \in \NN}$ generated by the algorithm, $\exists (L_t)_{t \in \NN}$ such that
            $$
                \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) 
                + 
                \frac{L_t\eta_{t + 1}}{2}\Vert x - x_t\Vert^2,
            $$
            where the sequence is generated by the algorithm. 
            This can be achieved by choosing the function $\phi_{t + 1}$ at each iteration smartly, then it's possible to still have the same convergenace rate. 
            In a practical setting, when $L_t = L$, and $\phi_{t}(x) = \eta_{t + 1}f$, this is called a line search.
            \par
            The convergence rate is loose and when function $f$ exhibits additional favorable properties, such as being strongly convex, the convergence rate can be faster. 
        \end{remark}
    \subsection{Examples}
        \begin{example}[Convergence of the proximal gradient method]
            In this section, we illustrate algorithms that satisfies the lower and uppwer bound estimate used in the above proof. 
            Consider $f = g + h$ with $h$ nonsmooth convex, and $g$ being $L$-Lipschitz smooth convex and differentiable. 
            Define $D_g(x, y) = g(x) - g(y) - \langle \nabla f(x), y - x\rangle$, $l_g(x; y) = g(y) + \langle \nabla g(y), y - x\rangle$, which is the Bregman divergence of the function $g$. 
            Consider for all $x$: 
            \begin{align*}
                0 & \le 
                D_g (x, y) \le  \frac{L}{2} \Vert x - y\Vert^2 
                \\
                l_g (x; y) &\le 
                g(x) 
                \le l_g(x; y) + \frac{L}{2} \Vert x - y\Vert^2 
                \\
                h(x) + l_g(x; y) &\le f(x) = g(x) + h(x)
                \le 
                l_g(x; y) + h(x) + \frac{L}{2}\Vert x - y\Vert^2. 
            \end{align*}
            Define $\phi_{t + 1} (x) = \eta_{t + 1}(h(x) + l_g(x; x_t))$, then results from previous theorems apply.    
        \end{example}
        \begin{remark}
            The envelope interpretation restricts the use of the theorem, since it requires that the proixmal operator is applied to the gradient of a function. 
            Extending the usage of the PPM descent inequality to other context requires operator theories and creativities. 
        \end{remark}

        \begin{example}[The fundamental proximal gradient lemma]
            The fundamental proximal gradient lemma was used heavily in the literatures to derive convergence results in the convex case. 
            The "fundamental proximal gradient lemma" originate from Beck's writings\cite[theorem 10.16]{beck_first-order_nodate}. 
            We demonstrate in this example that it's a consequence of \hyperref[thm:ppm_breg_descent_ineq]{theorem \ref*{thm:ppm_breg_descent_ineq}}. 
            \par
            With $f = g + h$, $h$ convex, lsc, $g$ be $L$-Lipschitz smooth, then for all $y\in \RR^n$, $x\in \RR^n$, $y^+ := \hprox_{L^{-1}h}(y - L^{-1}\nabla (y))$ satisfies: 
            \begin{align*}
                f(x) - f(y^+) \ge \frac{L}{2}\Vert x - y^+\Vert^2 - \frac{L}{2}\Vert x - y\Vert^2 + D_g(x, y).
            \end{align*}
            A similar analysis as \hyperref[thm:lower_approx_ppm_convergence]{ theorem \ref*{thm:lower_approx_ppm_convergence}} with \hyperref[thm:ppm_breg_descent_ineq]{theorem \ref*{thm:ppm_breg_descent_ineq}} obtains the same inquality. 
            With $\phi(y) = \eta(h(y) + g(x) + \langle \nabla g(x), y - x\rangle)$ as an lower bounding function of $f$. 
            Let $x^+ = \hprox_\phi(x)$, then for all $u$:
            \begin{align*}
                & \phi(u) + \frac{1}{2}\Vert u - x\Vert^2 - \phi(x^+) - \frac{1}{2}\Vert x^+ - x\Vert^2 
                \ge \frac{1}{2}\Vert x^+ - u\Vert^2, 
                \\
                \implies &
                \eta\underbrace{(h(u) + g(x) + \langle \nabla g(x), u - x\rangle + \frac{L}{2}\Vert u - x\Vert^2)}_{\ge \phi(u)} 
                - \eta \underbrace{f(x^+)}_{\le \phi(x^+)} - \frac{1}{2}\Vert x - x^+\Vert^2 
                \\
                &\quad  
                + \left(
                    \frac{1}{2} - \frac{\eta L}{2}
                \right)\Vert u - x\Vert^2 
                \ge 
                \frac{1}{2}\Vert x^+ - u\Vert^2 
                \\
                \iff & 
                f(u) + g(x) - g(u) + 
                \langle \nabla g(x), u -x\rangle 
                - f(x^+) + \frac{L}{2}\Vert u - x\Vert^2 - \frac{1}{2\eta}\Vert x - x^+\Vert^2
                \\
                & \quad 
                + 
                \left(
                    \frac{1}{2\eta} - \frac{L}{2}
                \right)\Vert u - x\Vert^2 
                \ge 
                \frac{1}{2\eta}\Vert x^+ - u\Vert^2 
                \\
                \iff 
                & f(u) - f(x^+) - D_g(u, x)
                + \frac{1}{2\eta} \Vert u - x\Vert^2 - \frac{1}{2\eta}\Vert x^+ - x\Vert^2
                \ge 
                \frac{1}{2\eta} \Vert x^+ - u\Vert^2. 
            \end{align*}
            Remove the negative term $-1/2\eta \Vert x - x^+\Vert^2$ makes LHS larger, which establish the fundamental proximal gradient lemma. 
        \end{example}
        
\section{Accelerated gradient descent and PPM}
    This section is heavily inspired by recent works from Ahn \cite{ahn_understanding_2022}, Nesterov \cite{nesterov_lectures_2018}.
    Ahn in his works explored the interpretation of Nesterov acceleration via PPM. 
    They proposed the idea of ``similar triangle" for unifying all varieties of Nesterov accelerated gradient. 
    The PPM is used as a tool to derive different variations of the Nesterov accelerated gradient algorithms. 
    Finally, they refurnished \hyperref[thm:lower_approx_ppm_convergence]{Theorem \ref*{thm:lower_approx_ppm_convergence}} for the proof of convergece rate of accelerated gradient.
    Their analysis results in relatively simple and exhibits powerful extension to several variants of Nesterov accelerated gradient. 
    \par\noindent
    Interestingly, Nesterov accelerated gradient is applicable for PPM.
    It's been done by \cite{guler_new_1992} two decades ago. 
    He uses the idea of a Nesterov acceleration sequence faithfully. 
    One recent development of the accelerated PPM is an algorithmic framework named: ``Universial Catalyst accelreation", proposed by \cite{lin_universal_2015}.
    It is an application of Guler work in the context of veriance-reduction stochatic gradient algorithms for machine learning. 
    
    
    \subsection{Varieties of Nesterov accelerated gradient}
        In this section, we list different varities of Nesterov accelerated method. 
        We present these varieties generically because the form of these algorithms are of interests.  


    \subsection{Nesterov accelerated gradient via PPM}
    \subsection{Convergence rate of Nesterov accelerated gradient via PPM}

\section{Classical analysis of Nesterov accelerated gradient}
    


\printbibliography

\appendix
\section*{Postponed Proofs}


\end{document}
