\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont Nesterov Type Momentum Methods}}

\author{
    Alto Legato
    \thanks{
        Subject type, Some Department of Some University, Location of the University,
        Country. E-mail: \texttt{author.name@university.edu}.
    }
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    These are ntoes for Nesterov Type Acceleration Methods, in the convex case. 
    They may get made into papers, proposal, and thesis in the future. 
\end{abstract}
\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

% ==============================================================================
\section{Preliminaries}
    In this section we list fundational results that are important for proofs in coming sections. 
    For this section, let the ambient space be $\RR^n$ and $\Vert \cdot\Vert$ be the Euclidean 2 norm until it's specified in the context. 
    \subsection{Lipschitz Smoothness}
        \begin{definition}[Lipschitz Smooth]
            Let $f$ be differentiable. 
            It has Lipschitz smoothness with constant $L$ if for all $x, y$
            $$
                \Vert \nabla f(x) - \nabla f(y)\Vert 
                \le 
                L \Vert x - y\Vert. 
            $$    
        \end{definition}

        \begin{theorem}[Lipschitz Smoothness Equivalence]
            With $f$ convex, the following conditions are equivalent conditions for all $x, y$: 
            \begin{enumerate}
                \item $|f(y) - f(x) - \langle \nabla f(x), y - x\rangle| \le \frac{L}{2}\Vert y - x\Vert^2$. 
                \item $|\langle \nabla f(y) - \nabla f(x), y - x\rangle| \le L\Vert y - x\Vert^2$. 
                \item $x^+\in \arg\min_x{f(x)} \implies \frac{1}{2L}\Vert \nabla f(x)\Vert^2 \le f(x) - f(x^+) \le (L/2) \Vert x - x^+\Vert^2$
                \item $ 1/(2L)\Vert x - y\Vert^2 \le  f(y) - f(x) - \langle \nabla f(x), y -x\rangle \le (L/2)\Vert x - y\Vert^2$
            \end{enumerate}    
        \end{theorem}
        \begin{remark}
            In the convex of operator theorem, Lipschitz smoothness of the gradient of a convex function is an example of a Firmly Nonexpansive operator. 
        \end{remark}

        \begin{definition}[Strong Convexity]
            With $f: \RR^n \mapsto \overline \RR$, it is strongly convex with constant $\alpha$ if and only if $f - (\alpha/2)\Vert \cdot\Vert^2$ is a convex function. 
        \end{definition}
        
        \begin{theorem}[Strongly Convex Equivalent Results]
            With $f:\RR^n \mapsto \overline \RR$ $\alpha$-strongly convex, the following conditions are equivalent conditions for all $x, y$: 
            \begin{enumerate}
                \item $f(y) - f(x) - \langle \partial f(x),y - x \rangle\ge \frac{\alpha}{2}\Vert y - x\Vert^2$
                \item $\langle \partial f(y) - \partial f(x), y - x\rangle \ge \alpha\Vert y - x\Vert^2$. 
                \item $f(\lambda x + (1 - \lambda)y) \le \lambda f(x) + (1 - \lambda)f(y) -\alpha\frac{\lambda(1 - \lambda)}{2}\Vert y - x\Vert^2, \forall \lambda \in [0, 1]$. 
            \end{enumerate}
        \end{theorem}

        \begin{theorem}[Strong Convexity Implications]
            With $f:\RR^n \mapsto \overline \RR$ $\alpha$-strongly convex, the following conditions are implied: 
            
        \end{theorem}
        \begin{remark}
            In the context of operator theory, the subgradient of a strongly convex function is an example of a Strongly Monotone Operator. 
        \end{remark}
    \subsection{Proximal Descent Inequality}
        \begin{theorem}[Proximal Descent Inequality]\label{thm:prox_descent_ineq}
            With $f: \RR^n \mapsto \overline \RR^n$ convex, fix any $x \in \RR^n$, let $p = \hprox_\alpha f(x)$, then for all $y$ we have inequality 
            $$
                \left(f(p) + \frac{1}{2\alpha}\Vert x - p\Vert^2\right)
                - 
                \left(
                    f(y) + \frac{1}{2\alpha}\Vert x - y\Vert^2 
                \right)
                \le 
                - \frac{1}{2\alpha}\Vert y - p\Vert^2. 
            $$
            Recall $\hprox_\alpha f(x) = \argmin_{u}\left\lbrace f(u) + \frac{1}{2}\Vert u - x\Vert^2 \right\rbrace$. 
        \end{theorem}
        We make use of this theorem in the proof of convergence of proximal point method.     
        \begin{remark}
            This descent inequality can be generalized to bregman proximal mapping as well. 
        \end{remark}


\section{The Proximal Point Method in the Convex Case}
    In this section we quickly go over the analysis of Proximal point method (PPM) in the convex case and see how the theories can be generalized into the cases where PPM is approximated. 
    \par
    With $f: \RR^n \mapsto \overline \RR$ lsc proper and convex, 
    given any $x_0$ the PPM generates sequence $(x_n)_{n\in \NN}$ by $x_{k + 1} = \hprox_{\eta_{k + 1}}f(x_k)$ for all $k \in \NN$ where the sequence $(\eta_{k})_{k \in \NN}$ is a nonegative sequence of real numbers. 
    \subsection{The Lyapunov function of PPM}
        \begin{theorem}
            Define the quantity for all $u \in \RR^n$
            \begin{align*}
                \Phi_t &= 
                \left(
                    \sum_{i = 1}^{t} \eta_i
                \right)(f(x_t) - f(u)) + \frac{1}{2} \Vert u - x_t\Vert^2 \; \forall t \ge 1, 
                \\
                \Phi_0 &= (1/2)\Vert x_0 - u\Vert^2, 
            \end{align*}
            then it is a Lyapunov function for the PPM algorithm. 
            Meaning for all $(x_k)_{k \in \NN}$ generated by PPM, it satisfies that $\Phi_{t + 1} - \Phi_t \le 0$. 
            Additionally, by the definition we have 
            \begin{align*}
                \Phi_{t + 1} - \Phi_{t} = 
                \eta_{t + 1}(f(x_{t + 1}) - f(u)) 
                + 
                \frac{1}{2}\Vert u - x_{t + 1}\Vert^2 
                -
                \frac{1}{2}\Vert u - x_t\Vert^2 
                \le 
                - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2, 
            \end{align*}
            as a consequence of choosing $u = x_{t + 1}$
            \begin{align*}
                f(x_{t + 1}) - f(x_t) \le \frac{1}{\eta_{t + 1}}\Vert x_{t + 1} - x_t\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            With $p, \alpha, y$ in theorem \ref*{thm:prox_descent_ineq} as $x_{t + 1}, \eta_{t + 1}, u$ we have for all $u$
            \begin{align*}
                \eta_{t + 1} (f(x_{t + 1}) - f(u)) + 
                \frac{1}{2}(
                    \Vert x_t - x_{t + 1} \Vert^2 
                    - 
                    \Vert x_t - u\Vert^2
                ) &\le 
                - \frac{1}{2}\Vert u - x_{t + 1}\Vert^2
                \\
                \eta_{t + 1}(f(x_{t + 1}) - f(u)) 
                + 
                \frac{1}{2}\Vert u - x_{t + 1}\Vert^2 
                -
                \frac{1}{2}\Vert u - x_t\Vert^2 
                & \le 
                - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2, 
            \end{align*}
            next it's not hard to check that $\Phi_{t + 1} - \Phi_t$ equals to the above quantity. 
        \end{proof}
        \begin{remark}
            This theorem act as a descent inequality and it can be generalized under a diverse context, such as all algorithms that are approximation to the PPM method. 
        \end{remark}

        \begin{theorem}[Convergence Rate of PPM]
            The convergence rate of PPM applied to $f$, closed, convex proper, we have convergence rate of the function value: 
            $$
            f(x_T) - f(x_*) \le O\left(\left(\sum_{i=1}^{T}\eta_t\right)^{-1}\right). 
            $$
            Where $x_*$ is the minimizer of $f$. 
        \end{theorem}
        \begin{proof}
            With $\Delta_t = f(x_t) - f(x_*), \Upsilon_t = \sum_{i = 1}^{t}\eta_i$ so $\Phi_t = \Upsilon_t\Delta_t + \frac{1}{2}\Vert x_t - x_*\Vert^2$ by consider $u = x_*$, invoking previous theorem and do
            \begin{align*}
                \Upsilon_T\Delta_T \le \Phi_T 
                &\le 
                \Phi_0 = \frac{1}{2}\Vert x_0 - x_*\Vert^2 
                \\
                \implies \Delta_T 
                &\le 
                \frac{1}{2\Upsilon_T} \Vert x_0 - x_*\Vert^2. 
            \end{align*}
            Hence, the convergence rate of PPM for all convex function is the above. 
        \end{proof}
        \begin{remark}
            The analysis of the above is taken from (REFERENCE NEEDED). 
        \end{remark}
\section{Application of the Proximal Point Method}
    The PPM method and the Lyaounov function derived above serves as tamplate for other algorithms. 
    In optimizations, people uses lower and upper approximation of the objective function to approximate the PPM. 
    Such an approach involves a diverse range of algorithms, including second order algorithms such as Newton's method. 
    To demonstrate, assume that $f$ is a lsc convex function such that it can be approximated by an lower bounding function $l_f(x|\bar x)$ at $\bar x$ such that it satisfies for all $x$: 
    \begin{align}
        l_f(x| \bar x) 
        \le f(x) \le l_f(x|\bar x) + \frac{L}{2}\Vert x - \bar x\Vert^2. 
    \end{align}
    The above characterization is generic enough to include the case where $l_f(x|\bar x)$ under approximates function that is non-smooth. 
    To make use of the theorems discussed previously, we assume that $l_f(x|\bar x)$ is convex for all $x$, at all $\bar x$. 
    \par 
    The approximated proximal point method is applying PPM to the function $l_f(x|x_t)$ for each iteration, i.e: $x_{t +1} = \hprox_{\eta_{t + 1}}[u\mapsto l_f(u | x_t)](x_t)$. 
    


\bibliographystyle{plain}


\end{document}
