\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont Nesterov Type Momentum Methods}}

\author{
    Alto
    \thanks{
        Subject type, Some Department of Some University, Location of the University,
        Country. E-mail: \texttt{author.name@university.edu}.
    }
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    These are notes for Nesterov Type Acceleration Methods in the convex case. 
    They can be made into papers, proposals, and a thesis in the future. 
\end{abstract}
\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

% ==============================================================================
\section{Preliminaries}
    This section lists foundational results important for proof in the coming sections. 
    For this section, let the ambient space be $\RR^n$ and $\Vert \cdot\Vert$ be the 2-norm until specified in the context. 
    For a general overview of smoothness and strong convexity in the Euclidean space, see \cite[theorem 2.1.5, theorem 2.1.10]{nesterov_lectures_2018} for a full exposition of the topic. 
    \subsection{Lipschitz smoothness}
        \begin{definition}[Lipschitz Smooth]
            Let $f$ be differentiable. 
            It has Lipschitz smoothness with constant $L$ if for all $x, y$
            $$
                \Vert \nabla f(x) - \nabla f(y)\Vert 
                \le 
                L \Vert x - y\Vert. 
            $$    
        \end{definition}

        \begin{theorem}[Lipschitz Smoothness Equivalence]
            With $f$ convex and $L$-Lipschitz smooth, the following conditions are equivalent conditions for all $x, y$: 
            \begin{enumerate}
                \item $L^{-1}\Vert \nabla f(y) - \nabla f(x)\Vert^2 \le \langle \nabla f(y) - \nabla f(x), y - x\rangle \le L\Vert y - x\Vert^2$. 
                \item $x^+\in \argmin_x f(x) \implies \frac{1}{2L}\Vert \nabla f(x)\Vert^2 \le f(x) - f(x^+) \le (L/2) \Vert x - x^+\Vert^2$, co-coersiveness. 
                \item $ 1/(2L)\Vert \nabla f(x) - \nabla f(y)\Vert^2 \le  f(y) - f(x) - \langle \nabla f(x), y -x\rangle \le (L/2)\Vert x - y\Vert^2$
            \end{enumerate}    
        \end{theorem}
        \begin{remark}
            Lipschitz smoothness of the gradient of a convex function is an example of a firmly nonexpansive operator.  
        \end{remark}

        \begin{definition}[Strong Convexity]
            With $f: \RR^n \mapsto \overline \RR$, it is strongly convex with constant $\alpha$ if and only if $f - (\alpha/2)\Vert \cdot\Vert^2$ is a convex function. 
        \end{definition}
        
        \begin{theorem}[Stong convexity equivalences]\label{thm:str_cvx_equiv}
            With $f:\RR^n \mapsto \overline \RR$ $\alpha$-strongly convex, the following conditions are equivalent conditions for all $x, y$: 
            \begin{enumerate}
                \item $f(y) - f(x) - \langle \partial f(x),y - x \rangle\ge \frac{\alpha}{2}\Vert y - x\Vert^2$
                \item $\langle \partial f(y) - \partial f(x), y - x\rangle \ge \alpha\Vert y - x\Vert^2$. 
                \item $f(\lambda x + (1 - \lambda)y) \le \lambda f(x) + (1 - \lambda)f(y) -\alpha\frac{\lambda(1 - \lambda)}{2}\Vert y - x\Vert^2, \forall \lambda \in [0, 1]$. 
            \end{enumerate}
        \end{theorem}

        \begin{theorem}[Strong convexity implications]\label{thm:str_cvx_implied}
            With $f:\RR^n \mapsto \overline \RR$ $\alpha$-strongly convex, the following conditions are implied: 
            \begin{enumerate}
                \item $\frac{1}{2}\operatorname{dist}(\mathbf 0; \partial f(x))^2 \ge \alpha (f(x) - f^+)$ where $f^+$ is a minimum of the function, and this is called the Polyak-Lojasiewicz (PL) inequality.
                \item $\forall x, y\in \mathbb E, u\in \partial f(x), v\in \partial f(y): \Vert u - v\Vert\ge \alpha\Vert x - y\Vert$. 
                \item $f(y) \le f(x) + \langle \partial f(x), y - x\rangle + \frac{1}{2\alpha}\Vert u - v\Vert^2, \forall u\in  \partial f(x), v\in \partial f(y)$. 
                \item $\langle \partial f(x)-\partial f(y), x - y\rangle \le \frac{1}{\alpha}\Vert u - v\Vert^2, \forall u\in \partial f(x), v\in \partial f(y)$. 
                \item if $x^+\in \arg\min_{x}f(x)$ then $f(x) - f(x^+) \ge \frac{\alpha}{2}\Vert x - x^+\Vert^2$ and $x^+$ is a unique minimizer. 
            \end{enumerate}
        \end{theorem}
        \begin{remark}
            In operator theory, the subgradient of a strongly convex function is an example of a Strongly Monotone Operator. 
        \end{remark}
    
    \subsection{Proximal descent inequality}
        The proximal descent inequality below is a crucial piece of inequality for deriving the behaviours of algorithms. 
        \begin{theorem}[Proximal Descent Inequality]\label{thm:ppm_descent_ineq}
            With $f: \RR^n \mapsto \overline \RR^n$ $\beta$-convex where $\beta \ge 0$, fix any $x \in \RR^n$, let $p = \hprox_f(x)$, then for all $y$ we have inequality 
            $$
                \left(f(p) + \frac{1}{2}\Vert x - p\Vert^2\right)
                - 
                \left(
                    f(y) + \frac{1}{2}\Vert x - y\Vert^2 
                \right)
                \le 
                - \frac{(1 + \beta)}{2}\Vert y - p\Vert^2. 
            $$
            Recall: $\hprox_f(x) = \argmin_{u}\left\lbrace f(u) + \frac{1}{2}\Vert u - x\Vert^2 \right\rbrace$. 
        \end{theorem}
        \begin{remark}
            We use this theorem to prove the convergence of the proximal point method. 
            See the proof (\cite{bauschke_convex_2017}, theorem 12.26). 
            The additional strong convexity index is a consequence of \hyperref[thm:str_cvx_implied]{theorem \ref*{thm:str_cvx_implied}}, item (v). 
        \end{remark}
        
        \begin{theorem}[The Bregman proximal descent inequality]\label{thm:ppm_breg_descent_ineq}
            Let $\omega$ induce a Bregman Divergence $D_\omega$ in $\RR^n$ and assume that it satisfies Bregman Prox Admissibility conditions for the function $\varphi: \RR^n \mapsto \overline \RR$.
            Then we claim that for all $c \in \text{dom}(\omega), b \in \text{dom}(\partial \omega)$, 
            If 
            \begin{align*}
                a = \argmin_{x}\left\lbrace
                \varphi(x) + D_\omega(x, b)
                \right\rbrace, 
            \end{align*}
            we have the inequality, 
            \begin{align*}
                (\varphi(c) + D_\omega(c, b)) - 
                (\varphi(a) + D_\omega(a, b)) \ge 
                D_\omega(c, a). 
            \end{align*}
        \end{theorem}
        \begin{remark}
            For more information about what function $\omega$ can induce a Bregman divergence and the admissibility conditions for Bremgna proximal mapping, consult Heinz et.al \cite{bauschke_descent_2017}. 
        \end{remark}


\section{The proximal point method with convexity}
    This section reviews the convex case's Proximal point method (PPM) analysis and generalizes the theories to approximated PPM. 
    \subsection{Convex PPM literature reviews}
        Rockafellar \cite{rockafellar_monotone_1976} pioneered the analysis of the proximal point method in the convex case. 
        He developed the analysis in the context of maximal monotone operators in Hilbert spaces. 
        Applications in convex optimizations are covered. 
        Using his theorems appropriately requires some opportunities, realizations, and characterizations of assumptions (A), (B) in his paper in the context of the applications. 
        \par\noindent
        In this section, we will use the result from Rockafellar that, if a monotone operator $A$ is $\beta$ strongly convex, then the resolvent operator $\mathcal J_A = [I + A]^{-1}$ is a $(1 + \beta)^{-1}$ Lipschitz operator, making $I - \mathcal J_A$ is a $1 - (1 + \beta)^{-1}$ a strongly monotone operator. 
        

    \subsection{The proximal point method}
        With $f: \RR^n \mapsto \overline \RR$ lsc proper and convex, given any $x_0$ the PPM generates sequence $(x_n)_{n\in \NN}$ by $x_{k + 1} = \hprox_{\eta_{k + 1}}f(x_k)$ for all $k \in \NN$ where the sequence $(\eta_{k})_{k \in \NN}$ is a nonegative sequence of real numbers.
        

    \subsection{The Lyapunov function of convex PPM}
        We present some theorems that illustrate the use of \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}}. 
        The readers can find similar analyses and techniques in Guler's work \cite{guler_convergence_1991}. 
        \begin{theorem}[PPM Lyapunov Function]\label{ppm_lyapunov}
            With $f$ being $\beta\ge 0$ convex (it's strongly convex if $\beta > 0$, else it's just convex) and $x_{t + 1} = \hprox_{\eta_{t + 1}f}$ generated by PPM. 
            Define the Lyapunov function $\Phi_t$ for all $u \in \RR^n$: 
            \begin{align*}
                \Phi_t &:= 
                \left(
                    \sum_{i = 1}^{t} \eta_i
                \right)(f(x_t) - f(u)) + \frac{1}{2} \Vert u - x_t\Vert^2 \quad \forall t \ge 1, 
                \\
                \Phi_0 &:= (1/2)\Vert x_0 - u\Vert^2, 
            \end{align*}
            then it is a Lyapunov function for the PPM algorithm. 
            Meaning for all $(x_k)_{k \in \NN}$ generated by PPM, it satisfies that $\Phi_{t + 1} - \Phi_t \le 0$. 
            Additionally, by definition, we have 
            {\small
                \begin{align*}
                    \Phi_{t + 1} - \Phi_{t} 
                    &= 
                    \left(\sum_{i = 1}^{t}\eta_{i}\right)
                    (f(x_{t + 1}) - f(x_t)) 
                    + \frac{1}{2}\Vert x_{t + 1} - u\Vert^2 
                    - \frac{1}{2}\Vert x_{t} - u\Vert^2
                    +
                    \eta_{t + 1}(f(x_{t + 1}) - f(u))
                    \\
                    & \le 
                    -\left(\sum_{i = 1}^{t}\eta_{i}\right)
                    (1 + \beta\eta_{t + 1}/2)\Vert x_{t + 1} - x_t\Vert^2
                    + 
                    \left(
                        - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                        -\frac{\beta\eta_{t + 1}}{2}\Vert u - x_{t + 1}\Vert^2
                    \right)
                    \\
                    & \le 0,
                \end{align*}
            }
            And additionally, recovering the descent lemma: 
            \begin{align*}
                f(x_{t + 1}) - f(x_t) 
                &\le
                -\frac{1}{\eta_{t+1}} \Vert x_{t + 1} - x_t\Vert^2 
                - \frac{\beta}{2}\Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Let $\phi_{t + 1}: \RR^n \mapsto \overline \RR = \eta_{t + 1} f$ be convex,  consider proximal point method $x_{t + 1} = \hprox_{\phi}(x_t)$, apply \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}}, we have $\forall u \in \RR^n$
            \begin{align*}
                & \phi_{t + 1}(x_{t + 1}) 
                + 
                \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                - \phi_{t + 1}(u) - \frac{1}{2}\Vert u - x_t\Vert^2
                \le 
                - 
                \frac{1}{2}(1 + \beta\eta_{t + 1})\Vert 
                    u - x_{t + 1}
                \Vert^2
                \\
                & \text{let } u = x_*
                \\
                &\quad 
                \begin{aligned}
                    \implies &
                    \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) 
                    + 
                    \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2 
                    +  
                    \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                    -
                    \frac{1}{2}\Vert x_* - x_t\Vert^2 
                    \\
                    & \quad \le 
                    -\frac{\beta\eta_{t + 1}}{2}\Vert x_* - x_{t + 1}\Vert^2
                    \\
                    \iff & 
                    \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) 
                    + 
                    \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2 
                    -
                    \frac{1}{2}\Vert x_* - x_t\Vert^2 
                    \\
                    & \quad \le 
                    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                    -\frac{\beta\eta_{t + 1}}{2}\Vert x_* - x_{t + 1}\Vert^2
                    \le 0. 
                \end{aligned}
                \\
                & \text{let } u = x_{t}
                \\
                &\quad  
                \begin{aligned}
                    \implies& 
                    f(x_{t + 1}) - f(x_t)
                    \le 
                    -\frac{1}{\eta_{t+1}} \Vert x_{t + 1} - x_t\Vert^2 
                    - \frac{\beta}{2}\Vert x_t - x_{t + 1}\Vert^2
                    \le 0. 
                \end{aligned}
            \end{align*}
            Let's define the following quantities for all $u, \beta\ge 0$: 
            \begin{align*}
                \Upsilon_{1, t + 1}(u) &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(u)) + \frac{1}{2}(
                    \Vert x_{t + 1} - u\Vert^2 - 
                    \Vert x_t - u\Vert^2
                )
                \\
                & \le 
                - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                -\frac{\beta\eta_{t + 1}}{2}\Vert u - x_{t + 1}\Vert^2, 
                \\
                \Upsilon_{2, t + 1}
                &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t))
                \\
                &\le 
                - \Vert x_{t + 1} - x_t\Vert^2 - 
                \frac{\beta\eta_{t + 1}}{2}
                \Vert x_{t + 1} - x_t\Vert^2 
                \\
                &= 
                -(1 + \beta\eta_{t + 1}/2)\Vert x_{t + 1} - x_t\Vert^2 \le 0. 
            \end{align*}
            With $\Phi_t$ as defined in the theorem, observe the following demonstration for all $u$, $\beta \ge 0$: 
            {\small
            \begin{align*}
                \Phi_{t + 1} - \Phi_{t}
                &= 
                \left(
                    \sum_{i = 1}^{t + 1}\eta_i
                \right)(f(x_{t + 1}) - f(u)) + 
                \frac{1}{2}\Vert x_{t + 1} - u\Vert^2 
                - 
                \left(
                    \sum_{i = 1}^{t}\eta_i
                \right)(f(x_{t}) - f(u))
                - 
                \frac{1}{2}\Vert x_{t} - u\Vert^2
                \\
                &= 
                \left(\sum_{i = 1}^{t}\eta_{i}\right)
                (f(x_{t + 1}) - f(x_t)) 
                + \frac{1}{2}\Vert x_{t + 1} - u\Vert^2 
                - \frac{1}{2}\Vert x_{t} - u\Vert^2
                +
                \eta_{t + 1}(f(x_{t + 1}) - f(u))
                \\
                &= 
                \left(\sum_{i = 1}^{t}\eta_{i}\right)\Upsilon_{2, t + 1}
                + \Upsilon_{1, t + 1}(u)
                \\
                &\le 
                -\left(\sum_{i = 1}^{t}\eta_{i}\right)
                (1 + \beta\eta_{t + 1}/2)\Vert x_{t + 1} - x_t\Vert^2
                + 
                \left(
                    - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2
                    -\frac{\beta\eta_{t + 1}}{2}\Vert u - x_{t + 1}\Vert^2
                \right)
                \le 0. 
            \end{align*}
            }
            Therefore, $\Phi_t$ is a legitimate Lyapunov function for all $u, \beta \ge 0$. 
        \end{proof}
        \begin{remark}
            The above Lyapunov is not unique, and it's not optimal for $\beta > 0$, strictly strongly convex functions. 

        \end{remark}

        \begin{theorem}[Convergence Rate of PPM]\label{thm:ppm_convergence_rate}
            The convergence rate of PPM applied to $f$, closed, convex proper, we have the convergence rate of the function value: 
            $$
            f(x_T) - f(x_*) \le O\left(\left(\sum_{i=1}^{T}\eta_t\right)^{-1}\right). 
            $$
            Where $x_*$ is the minimizer of $f$. 
        \end{theorem}
        \begin{proof}
            With $\Delta_t = f(x_t) - f(x_*), \Upsilon_t = \sum_{i = 1}^{t}\eta_i$ so $\Phi_t = \Upsilon_t\Delta_t + \frac{1}{2}\Vert x_t - x_*\Vert^2$ by consideration $u = x_*$, invoking previous theorem and do
            \begin{align*}
                \Upsilon_T\Delta_T \le \Phi_T 
                &\le 
                \Phi_0 = \frac{1}{2}\Vert x_0 - x_*\Vert^2 
                \\
                \implies \Delta_T 
                &\le 
                \frac{1}{2\Upsilon_T} \Vert x_0 - x_*\Vert^2. 
            \end{align*}
        \end{proof}
        \begin{remark}
            With the same choice of the sequence $(\eta_t)_{t \in \NN}$, convergence of the PPM method of a strongly convex function is faster. 
            The above proof is the same for $\beta = 0$, or $\beta > 0$, because it didn't use the property that $\eta_{t + 1}f$ is a $\eta_{t + 1}\beta$ strongly convex function. 
        \end{remark}

        \begin{theorem}[PPM Strongly Convex Lyapunov Function]\label{ppm_lyapunov_scvx}
            With $f$ being $\beta > 0$ strogly convex, with $x_{t + 1} = \hprox_{\eta_{t + 1}f}(x_t)$, then $\Phi_t = \Vert x_{t} - x_*\Vert$ is a Lyapunov function satisfying: 
            \begin{align*}
                \frac{\Vert x_{t + 1} - x_*\Vert}{
                    \Vert x_k - x_*\Vert
                } &\le (1 + \eta_{t + 1}\beta)^{-1}. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            This is a direct application that $\hprox_{\eta_{t + 1}f}$ is a contraction with constant $(1 + \beta\eta_{t +1})^{-1}$. 
        \end{proof}
        \begin{remark}
            It's still a mystery on how to show $f(x_t) - f(x_*)$ is a Lyapunov function. 
            The answer is not realized by us, nor it is in Rockafellar \cite{rockafellar_monotone_1976} or Guler's\cite{guler_convergence_1991} writings. 
            Do observe that, by the choice of $x_*$, the contraction property of the proximal operator is strictly stronger than necessary. 
            This inequality at the end is tighter than what we derived for gradient descent. 
        \end{remark}
        
\section{Applying the analysis of PPM}
    The PPM method and the Lyaounov function derived above serve as the template for other algorithms. 
    As an appetizer, we present an analysis of gradient descent using theorems related to the convergence of PPM
    \par\noindent
    In optimizations, people use a lower or an upper approximation of the objective function to approximate the PPM. 
    The methodology includes a diverse range of approaches.
    For example, it includes first-order optimization, such as gradient descents, and second-order algorithms, such as Newton's method. 
    Its scope broadens to primal-dual optimization algorithms with creativities in the Lyapunov functions or theories in monotone operators. 
    \par\noindent
    To demonstrate, assume that $f$ is a lsc convex function such that it can be approximated by a lower bounding function $l_f(x|\bar x)$ at $\bar x$ such that it satisfies for all $x$: 
    \begin{align*}
        l_f(x| \bar x) 
        \le f(x) \le l_f(x|\bar x) + \frac{L}{2}\Vert x - \bar x\Vert^2. 
    \end{align*}
    The above characterization is generic enough to include the case where $l_f(x|\bar x)$, the under-approximating function is nonsmooth. 
    We assume that $l_f(x|\bar x)$ is convex for all $x$, at all $\bar x$, so the previous theorems apply. 
    \par\noindent
    The approximated proximal point method applies PPM to the function $l_f(x|x_t)$ for each iteration, i.e.: $x_{t +1} = \hprox_{\eta_{t + 1}l_f(\cdot | x_t)}(x_t)$. 
    \subsection{Generic gradient descent}
        We will consider deriving gradient descent via the PPM approach as a warm-up. 
        Please pay attention to the remarks. They reveal parts of the proof that could inspire the idea of a non-monotone line search method in practical settings. 
        \begin{theorem}[Generic Approximated PPM]\label{thm:lower_approx_ppm_convergence}
            With $f$ convex having minimizer: $x_*$; $l_f(\cdot; x_t)$ convex, lsc and proper, define $\phi_t(x) = \eta_{t + 1}l_f(x; x_t)$. 
            Assume the following estimates hold: 
            $$
            \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) 
            + 
            \frac{L\eta_{t + 1}}{2}\Vert x - x_t\Vert^2 \quad \forall x \in \RR^n. 
            $$ 
            Fix any $x_0$, let the iterates $x_t$ defined for $t\in \NN$ satisfies
            \begin{align*}
                x_{t +1} = \argmin_{x} \left\lbrace
                    l_f(x; x_t) + \frac{1}{2\eta_{t + 1}}\Vert x - x_t\Vert^2
                \right\rbrace, 
            \end{align*}
            then it has 
            \begin{align*}
                \eta_{t + 1}(f(x_{t + 1}) - f(x_*)) + \frac{1}{2}\Vert x_* - x_{t + 1}\Vert^2
                - \frac{1}{2}\Vert x_* - x_t\Vert^2
                & \le 
                \left(
                    \frac{L \eta_{n + 1}}{2} - \frac{1}{2}
                \right)\Vert x_{t + 1} - x_t\Vert^2.
            \end{align*}
            Additionally if $\exists \epsilon > 0: \eta_{t} \in (\epsilon, 2L^{-1} - \epsilon)$, for all $t \in \NN$, the algorithm has sublinear convergence rates of
            \begin{align*}
                f(x_T) - f(x_*)
                &\le 
                \frac{L - \epsilon^{-1}}{TL\epsilon} (f(x_0) - f(x_T)) 
                \\
                &\le 
                \frac{L - \epsilon^{-1}}{TL\epsilon} (f(x_0) - f(x_*)) 
            \end{align*}
        \end{theorem}
        \begin{proof}
            By $\phi_t$ convex, apply \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}} with $f = \phi_t$, $x = x_t$, $x_{t + 1} = p$, yielding $\forall y$
            {\footnotesize
                \begin{align*}
                    \phi_t(x_{t + 1}) + \frac{1}{2}\Vert x_t - x_{t + 1}\Vert^2 
                    - 
                    \phi_t(y) - \frac{1}{2}\Vert x_t - y\Vert^2
                    &\le 
                    - \frac{1}{2}\Vert y - x_{t + 1}\Vert^2
                    \\
                    \phi_t(x_{t + 1}) - \phi_t(y)
                    + \frac{1}{2}(
                        \Vert y - x_{t + 1}\Vert^2 - \Vert x_t - y\Vert^2 
                    ) &\le 
                    - \frac{1}{2} \Vert x_t - x_{t + 1}\Vert^2
                    \\
                    \left(
                        \phi_t(x_{t + 1}) + \frac{L\eta_{t + 1}}{2}\Vert x_{t + 1} - x_t\Vert
                    \right)
                    - \phi_t(y)
                    + \frac{1}{2}(
                        \Vert y - x_{t + 1}\Vert^2 - \Vert x_t - y\Vert^2 
                    ) &\le 
                    \left(
                        \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                    \right) \Vert x_t - x_{t + 1}\Vert^2
                    \\
                    \implies 
                    \eta_{t + 1}f(x_{t + 1}) - \eta_{t + 1}f(y) 
                    + \frac{1}{2}(
                        \Vert y - x_{t + 1}\Vert^2 - \Vert x_t - y\Vert^2 
                    ) 
                    &\le 
                    \left(
                        \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                    \right) \Vert x_t - x_{t + 1}\Vert^2. 
                \end{align*}
            }
            Setting $y = x_t$ yields
            \begin{align*}
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t)) + 
                \frac{1}{2}
                    \Vert x_t - x_{t + 1}\Vert^2
                &\le 
                \left(
                        \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                \right) \Vert x_t - x_{t + 1}\Vert^2
                \\
                \iff 
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t))
                &\le 
                \left(
                        \frac{L\eta_{t + 1}}{2} - 1
                \right) 
                \Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
            In a similar manner to the derivation of the Lyapunov function for PPM, we make for all $y$: 
            \begin{align*}
                \Upsilon_{1, t + 1}(y) &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(y)) + \frac{1}{2}(
                    \Vert x_{t + 1} - y\Vert^2 - 
                    \Vert x_t - y\Vert^2
                )
                \\
                & \le 
                \left(
                    \frac{L\eta_{t + 1}}{2} - \frac{1}{2}
                \right) \Vert x_t - x_{t + 1}\Vert^2, 
                \\
                \Upsilon_{2, t + 1}
                &= 
                \eta_{t + 1}(f(x_{t + 1}) - f(x_t))
                \\
                &\le 
                \left(
                    \frac{L\eta_{t + 1}}{2} - 1
                \right) 
                \Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
            Now, consider defining $\Phi_t$ for all $y$: 
            $$
                \Phi_t = \left(
                    \sum_{i = 1}^{t} \eta_{i}
                \right)
                (f(x_t) - f(y)) + \frac{1}{2}\Vert y - x_t\Vert^2, 
            $$
            it is the proposed Lyapunov function for PPM; we define the base case $\Phi_0 = \frac{1}{2}\Vert y - x_0\Vert^2$. 
            Consider the difference $\forall y$: 
            \begin{align*}
                \Phi_{t + 1} - \Phi_t
                &= 
                \left(\sum_{i = 1}^{t}\eta_{i}\right)\Upsilon_{2, t + 1}
                + \Upsilon_{1, t + 1}(y)
                \\
                &\le 
                \left(\sum_{i = 1}^{t}\eta_{i}\right) 
                \left(
                    \frac{L\eta_{t + 1}}{2} - 1
                \right) 
                \Vert x_t - x_{t + 1}\Vert^2 + 
                \left(
                    \frac{L\eta_{t + 1}}{2}- \frac{1}{2}
                \right) \Vert x_t - x_{t + 1}\Vert^2. 
            \end{align*}
            Observe that if $\eta_i \le L^{-1}$, then $\Phi_{t + 1} - \Phi_t \le 0$, hence the convergence rate of $\mathcal O\left((\sum_{i = 1}^{t}\eta_i)^{-1}\right)$ of PPM for $\Phi_t$ is applicable. 
            \par\noindent
            Surprisingly, if $\eta_i \in (0, 2L^{-1})$, $\Phi_{t}$ still converges under mild conditions. 
            For simplicity we set $\sigma_t := \sum_{i = 1}^{t}\eta_i$. 
            It starts with considerations that $(L\eta_{t + 1}/2 - 1) < 0$, so that 
            \begin{align*}
                f(x_{t + 1}) - f(x_t) &\le 
                \left(\frac{L\eta_{t + 1}}{2} - 1\right)\Vert x_{t + 1} - x_t\Vert^2
                \\
                f(x_T) - f(x_0)
                &\le 
                \underbrace{
                \left(
                    \frac{L\sigma_T}{2} - T
                \right)
                }_{< 0}
                \sum_{t = 0}^{T - 1}\Vert x_{t + 1} - x_t\Vert^2
                \\
                \implies 
                \sum_{t = 0}^{T -1}\Vert x_t - x_{t + 1}\Vert^2
                &\le 
                \left(
                    \frac{L}{2}\sigma_T  - T
                \right)^{-1} 
                (f(x_T) - f(x_0))
            \end{align*}
            Continue on the RHS of $\Phi_{t + 1} - \Phi_t$ so 
            \begin{align*}
                \sum_{t = 0}^{T - 1}\Phi_{t + 1} - \Phi_t 
                &\le 
                \left(
                    \frac{L}{2}\sigma_T - \frac{T}{2}
                \right)\sum_{t = 0}^{T - 1}\Vert x_{t + 1} - x_t\Vert^2
                \\
                \Phi_T - \Phi_0 &\le 
                \left(
                    \frac{\frac{L}{2}\sigma_T - \frac{T}{2}}{
                        \frac{L}{2}\sigma_T - T
                    }
                \right)
                (f(x_T) - f(x_0))
                \\
                &= 
                \left(
                    \frac{L\sigma_T - T}{L\sigma_T - 2T}
                \right)
                (f(x_T) - f(x_0)), 
            \end{align*}
            implies
            \begin{align*}
                \sigma_T (f(x_T) - f(y)) + \frac{1}{2}\Vert y - x_t\Vert^2
                - \frac{1}{2}\Vert y - x_0 \Vert^2 
                &\le 
                \left(
                    \frac{L\sigma_T - T}{L\sigma_T - 2T}
                \right)
                (f(x_T) - f(x_0))
                \\
                \iff
                f(x_T) - f(y) + 
                \frac{1}{2\sigma_T}(\Vert y - x_t\Vert^2 - \Vert y - x_0\Vert^2)
                &\le 
                \left(
                    \frac{L- T\sigma_T^{-1}}{2T - L\sigma_T}
                \right)
                (f(x_0) - f(x_T)), 
            \end{align*}
            therefore, we obtain the bound: 
            \begin{align*}
                f(x_T) - f(y) &\le 
                \left(
                    \frac{L- T\sigma_T^{-1}}{2T - L\sigma_T}
                \right)
                (f(x_0) - f(x_T))
                - 
                \frac{1}{2\sigma_T}(\Vert y - x_t\Vert^2 - \Vert y - x_0\Vert^2)
            \end{align*}
            In the case where $\sup_{i\in \NN} \eta_i \le 2L^{-1} - \epsilon$, and $\inf_{i\in \NN}\eta_i \ge \epsilon$ with $\epsilon > 0$. 
            Then we have 
            \begin{align*}
                \frac{L -T\sigma_T^{-1}}{2T - L\sigma_T}
                &\le 
                \frac{L - \epsilon^{-1}}{2T - LT(2L^{-1} - \epsilon)}
                \\
                &= 
                \frac{L - \epsilon^{-1}}{2T - T(2 - L\epsilon)}
                \\
                &= 
                \frac{L - \epsilon^{-1}}{TL\epsilon}. 
            \end{align*}
            With $y = x_*$, we get the claimed convergence rate because $f(x_t)$ is strictly monotone decreasing. 
        \end{proof}
        \begin{remark}
            Observe that inequality 
            $$
                \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) 
                + 
                \frac{L\eta_{t + 1}}{2}\Vert x - x_t\Vert^2 \quad \forall x \in \RR^n, 
            $$
            was invoked with $x = x_{t + 1}$ for the PPM descent inequality in the above proof, meaning that if  $\forall (x_t)_{t \in \NN}$ generated by the algorithm, $\exists (L_t)_{t \in \NN}$ such that
            $$
                \phi_t(x) \le \eta_{t + 1}f(x) \le \phi_t(x) 
                + 
                \frac{L_t\eta_{t + 1}}{2}\Vert x - x_t\Vert^2,
            $$
            where the algorithm generates the sequence. 
            By smartly choosing the function $\phi_{t + 1}$ at each iteration, we can increase the stepsize while retaining a similar convergence proof. 
            In a practical setting, when $L_t = L$, and $\phi_{t}(x) = \eta_{t + 1}f$, this is called a line search.
            \par\noindent
            The convergence rate is loose, and when $f$ exhibits additional favourable properties, such as being strongly convex, the convergence rate can be faster. 
            Furthermore, if the choice of $y$ remains arbitrary, then the theorem is applicable for function without minimizers. 
        \end{remark}

    \subsection{Examples}
        \begin{example}[Convergence of the proximal gradient method]
            This section illustrates algorithms that satisfy the above proof's lower and upper bound estimates. 
            Consider $f = g + h$ with $h$ nonsmooth convex, and $g$ being $L$-Lipschitz smooth convex and differentiable. 
            Define $D_g(x, y) = g(x) - g(y) - \langle \nabla f(x), y - x\rangle$, $l_g(x; y) = g(y) + \langle \nabla g(y), y - x\rangle$, which is the Bregman divergence of the function $g$. 
            Consider for all $x$: 
            \begin{align*}
                0 & \le 
                D_g (x, y) \le  \frac{L}{2} \Vert x - y\Vert^2 
                \\
                l_g (x; y) &\le 
                g(x) 
                \le l_g(x; y) + \frac{L}{2} \Vert x - y\Vert^2 
                \\
                h(x) + l_g(x; y) &\le f(x) = g(x) + h(x)
                \le 
                l_g(x; y) + h(x) + \frac{L}{2}\Vert x - y\Vert^2. 
            \end{align*}
            Define $\phi_{t + 1} (x) = \eta_{t + 1}(h(x) + l_g(x; x_t))$, then results from previous theorems apply.    
        \end{example}
        \begin{remark}
            The envelope interpretation restricts the use of the theorem since it requires that the proximal operator be a resolvent of a gradient. 
            Extending the usage of the PPM descent inequality to other contexts requires operator theories and creativities. 
        \end{remark}

        \begin{example}[The fundamental proximal gradient lemma]
            The fundamental proximal gradient lemma was used heavily in the literature to derive convergence results in the convex case. 
            The "fundamental proximal gradient lemma" originates from Beck's writings \cite[theorem 10.16]{beck_first-order_nodate}. 
            We demonstrate in this example that it's a consequence of \hyperref[thm:ppm_breg_descent_ineq]{theorem \ref*{thm:ppm_breg_descent_ineq}}. 
            \par
            With $f = g + h$, $h$ convex, lsc, $g$ be $L$-Lipschitz smooth, then for all $y\in \RR^n$, $x\in \RR^n$, $y^+ := \hprox_{L^{-1}h}(y - L^{-1}\nabla (y))$ satisfies: 
            \begin{align*}
                f(x) - f(y^+) \ge \frac{L}{2}\Vert x - y^+\Vert^2 - \frac{L}{2}\Vert x - y\Vert^2 + D_g(x, y).
            \end{align*}
            A similar analysis as \hyperref[thm:lower_approx_ppm_convergence]{ theorem \ref*{thm:lower_approx_ppm_convergence}} with \hyperref[thm:ppm_breg_descent_ineq]{theorem \ref*{thm:ppm_breg_descent_ineq}} obtains the same inquality. 
            With $\phi(y) = \eta(h(y) + g(x) + \langle \nabla g(x), y - x\rangle)$ as an lower bounding function of $f$. 
            Choose any $x$, let $x^+ = \hprox_\phi(x)$, then for all $u$:
            {\small
            \begin{align*}
                & \phi(u) + \frac{1}{2}\Vert u - x\Vert^2 - \phi(x^+) - \frac{1}{2}\Vert x^+ - x\Vert^2 
                \ge \frac{1}{2}\Vert x^+ - u\Vert^2
                \\
                \implies &
                \eta\underbrace{
                    \left(
                        h(u) + g(x) + \langle \nabla g(x), u - x\rangle 
                    \right)
                }_{= \phi(u)} 
                - \eta \underbrace{f(x^+)}_{\le \phi(x^+)} - \frac{1}{2}\Vert x - x^+\Vert^2 
                + \frac{1}{2} \Vert u - x\Vert^2 
                \ge 
                \frac{1}{2}\Vert x^+ - u\Vert^2 
                \\
                \iff & 
                f(u) + \left(
                    g(x) - g(u) + \langle \nabla g(x), u -x\rangle 
                \right)
                - f(x^+) - \frac{1}{2\eta}\Vert x - x^+\Vert^2
                + 
                \frac{1}{2}\Vert u - x\Vert^2 
                \ge 
                \frac{1}{2\eta}\Vert x^+ - u\Vert^2 
                \\
                \iff 
                & f(u) - f(x^+) - D_g(u, x)
                + \frac{1}{2\eta} \Vert u - x\Vert^2 - \frac{1}{2\eta}\Vert x^+ - x\Vert^2
                \ge 
                \frac{1}{2\eta} \Vert x^+ - u\Vert^2. 
            \end{align*}
            }
            \par\noindent
            Removing the negative term $-1/2\eta \Vert x - x^+\Vert^2$ makes LHS larger, establishing the fundamental proximal gradient lemma. 
        \end{example}
        \begin{remark}
            Linking the PPM descent inequality to the Bregman divergence of the smooth part of the function on parameters $u, x$ is a clever move. 
        \end{remark}

        
\section{Accelerated gradient descent and PPM}
    Recent works from Ahn \cite{ahn_understanding_2022} and Nesterov \cite{nesterov_lectures_2018} inspired content in this section.
    In his works, Ahn explored the interpretation of Nesterov acceleration via PPM. 
    They proposed the idea of ``similar triangle" for unifying all varieties of Nesterov accelerated gradient. 
    They used PPM to derive several variations of the Nesterov accelerated gradient algorithms. 
    Finally, they refurnished \hyperref[thm:lower_approx_ppm_convergence]{theorem \ref*{thm:lower_approx_ppm_convergence}} for the proof of convergence rate of the accelerated gradient.
    Their analysis results in relatively simple arguments that exhibits powerful extensions to several variants of the Nesterov accelerated gradient. 
    \par\noindent
    Interestingly, the Nesterov accelerated gradient applies to PPM; Guler \cite{guler_new_1992} did it two decades ago. 
    He uses the idea of a Nesterov acceleration sequence faithfully. 
    One recent development of the accelerated PPM is an algorithmic framework named: ``Universal Catalyst acceleration", proposed by Lin et al \cite{lin_universal_2015}. 
    It is an application of Guler's work in the context of variance-reduction stochastic gradient algorithms for machine learning. 
    \par\noindent
    In this section, we
    \begin{enumerate}
        \item State Nesterov accelerated gradient and their varieties, and point to the literature discussing them. 
        \item Derive the Nesterov accelerated gradient. 
        \item Derive the popular step size choices along with the convergence rate. 
    \end{enumerate}
    Some content will differ from Ahn's works because we hope to generalize these ideas for our own use. 
    
    
    \subsection{Varieties of Nesterov accelerated gradient}
        In this section, we list different varieties of the Nesterov accelerated method. 
        We present these varieties generically because these algorithms' forms are of interest.  
        \begin{definition}[AG Generic Original Form]\label{def:agg_original}
            Let $f$ be a $L$ Lipschitz smooth and $\mu\ge 0$ strongly convex function. 
            Choose $x_0$, $\gamma_0 > 0$, set $v_0 = x_0$, for iteration $k\ge 0$, it
            \begin{enumerate}
                \item[1.] computes $\alpha_k \in (0, 1)$ by solving $L\alpha_k^2 = (1 - \alpha_k)\gamma_k + \alpha_k \mu$; 
                \item[2.] sets $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$;
                \item[3.] chooses $y_k = (\gamma_k + \alpha_k \mu)(\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k)$. Compute $f(y_k)$ and $\nabla f(y_k)$; 
                \item[4.] finds $x_{k + 1}$ such that $f(x_{k + 1}) \le f(y_k) - (2L)^{-1} \Vert \nabla f(y_k)\Vert^2$; 
                \item[5.] sets $v_{k + 1} = \gamma_{k+1}^{-1}((1 - \alpha_k)\gamma_kv_k + \alpha_k \mu y_k - \alpha_k \nabla f(y_k))$. 
            \end{enumerate}
        \end{definition}
        \begin{remark}
            This is in Nesterov's book \cite[(2.2.7)]{nesterov_lectures_2018}. 
            It is the most generic algorithm in his book about accelerated gradient method. 
            The genericity of the algorithm is provided by item 4., which is the a special case of the smooth descent lemma. 
        \end{remark}

        \begin{definition}[AG Generic Triangular Form]\label{def:agg_tri}
            With $f$ be $L$-Lipschitz smooth and $\mu \ge 0$  strongly convex, choose any $y_0 = x_0$, the algorithm admits form: 
            \begin{align*}
                x_{t + 1} &= x_t - \tilde \eta_{t + 1} \nabla f(y_t) 
                \\
                z_{t + 1} &= y_t - L^{-1} \nabla f(y_t) 
                \\
                y_{t + 1} &= 
                (1 + L\eta_{t + 1})^{-1}
                (
                x_{t + 1} + L\eta_{t + 1}z_{t + 1}
                ). 
            \end{align*}
        \end{definition}

        \begin{definition}[AG Generic PPM Form]\label{agg_ppm}
            
        \end{definition}


    \subsection{Nesterov accelerated gradient via PPM}
    \subsection{Convergence rate of Nesterov accelerated gradient via PPM}

\section{Classical analysis of Nesterov accelerated gradient}
    In this section, we reproduce some of the analysis for Nesterov accelerated gradient method with excruciating details. 


\printbibliography

\appendix
\section*{Postponed Proofs}


\end{document}
