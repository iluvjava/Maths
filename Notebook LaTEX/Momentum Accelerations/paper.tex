\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont The Proximal Point interpretation of Nesterov accelerated proximal gradient}}

\author{
    Hongda Li
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~ and~Xianfu Wang
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    Nesterov accelreated gradient method has been in the spotlight for the past decades due its wide spread applications and theories of optimal convergence. 
    Decades later it still opens up new interpretations. 
    Our work suggests a proximal point interpretation of accelerated gradient method for the method of accelerated proximal gradient method as a major extension to the interpretation proposed by Ahn and Sra \cite{ahn_understanding_2022}. 
    The proofs had been streamlined, extended and new error terms are added to allow a larger set of stepsize sequence for the proximal point interpreation. 
    Additionally, we conduct numerical experiment for a line search method that dynamically adjust the strong convexity index $\mu$ and Lipschitz constant of the gradient in for algorithm  using the proximal point understanding of accelerated gradient. 
    
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

\section{Introduction}
    Recent works from Ahn and Sra \cite{ahn_understanding_2022} and Nesterov \cite{nesterov_lectures_2018} inspired content in this section.
    Ahn and Sra explored the interpretation of Nesterov acceleration as the proximal point method (PPM) applied to an upper surrogate function, and then a lower surrogate function. 
    Inspired by the interpretation, we generalize the framework to the case of $h = f + g$ with $f$ Lipschitz smooth and $g$ convex whose proximal mapping can be evaluated exactly for algorithm implementations. 
    As a major extension, our work describes the method of proximal gradient using the PPM interpretations. We also compactify the Lypunov analysis with the introduction of an error terms for the convergence proof. We derive major variants of the Nesterov acceleation algorithm
    \cite{chambolle_convergence_2015}, \cite{beck_fast_2009-1}, \cite[Chapter 12]{ryu_large-scale_2022}
    of the accelerated gradient (AG) and find the match of the stepsize parameters in the PPM form. 


    \par
    Some of the earlier examples for the extension of the Nesterov acceleration method make use of the classical analysis introduced by Nestrov. 
    It uses the Nesterov acceleration sequence. 
    See \cite{guler_new_1992} for an extension of the Nesterov accelerated gradient method to the proximal point method for convex programming. 
    However the classical analysis found in \cite[chapter 2]{nesterov_lectures_2018} involves the assumption of a specific kind of Lypunov function and a specific format of the Nesterov's estimating sequence to accomodate the proof. 
    In Ahn's work however, the complexities are packaged into the proximal point interpretation of accelerated gradient. 
    It uses a lemma from Moreau envelope for the Lypunov analysis.
    The interpration allows choices of undetermined parameters to encompass several variants of the Nesterov accelerated gradient algorithm. 
    Ahn and Sra's approach is inspired by works from Defazio \cite{defazio_curved_2019}, Zeyuan and Lorenzo \cite{allen-zhu_linear_2016}. 
    Instead of a mirror descent step on the dual, they propose an alternative without the gradient of the dual. 
    \par
    Numerous notable variations of Nesterov accelerated gradient exists. \cite[(6.1.19)]{nesterov_lectures_2018} described a variant of accelerated gradient restricted to a convex domain $Q$ using Bregman Divergence. 
    Beck and Toubolle \cite{beck_fast_2009} introduced a variant the problem type of smooth plus non-smooth, known as FISTA. 
    For a variant of accelerated gradient where the iterates converge (weakly in Hilbert space), see Chambolle and Dossal \cite{chambolle_convergence_2015}. 
    Extension such as the Harpen acceleration for the resolvent operator of a maximally monotone opreator in general is outside of the scope since it not all maximal monotone operator corresponds to a subgradient operator. 
    \par
    A wide varieties of interpretation for the Nesterov accelerated gradient exist in the literatures. 
    Consult \cite{su_differential_2015} for a dynamical system interpretation of Nesterov acceleration. 
    The dynamical system interpretation of the algorithm leads to the valuable insight that restarting the accerlated gradient algorithm would lead to faster convergence rate for the class of strongly convex function. 
    In work by \cite{allen-zhu_linear_2016}, they interpreted the idea of Nesterov acceleration as a combinations of gradient descent and mirror descent. 

    % \par
    % The paper is organized as follow: 
    % \begin{enumerate}
    %     \item Section 
    % \end{enumerate}
    
\section{Preliminaries}\label{sec:preliminaries}
    In this section we introduce the a descent lemma for Proximal Point Method (PPM) in the convex case. 
    We define the proximal gradient mapping $\mathcal T_L$, and gradient mapping $\mathcal G_L$ for function satisfying 
    \hyperref[assumption:smooth-nonsmooth-sum]
    {assumption \ref*{assumption:smooth-nonsmooth-sum}}. 
    A lower bound function is identified using the gradient mapping operator and proved in
    \hyperref[lemma:grad_map_linearization]{Lemma \ref*{lemma:grad_map_linearization}}, 
    this is a key component of the proximal point interpretation of the accelerated gradient method. 
    \begin{assumption}\label{assumption:smooth-nonsmooth-sum}
        Let $h = f + g$ where $f, g$ are convex and $f$ is Lipschitz-Smooth with constant $L$. 
    \end{assumption}
    \begin{definition}[Strongly convex functions]
        A function $f: X \mapsto \RR$ is $\beta$-strongly convex
        with $\beta\geq 0$ if $f - \beta \frac{\Vert \cdot\Vert^2}{2}$ is convex.
    \end{definition}

    \begin{theorem}[Proximal descent inequality]\label{thm:ppm_descent_ineq}
        Let $f: \RR^n \mapsto \overline \RR$ be $\beta$ strongly convex with $\beta \ge 0$, fix any $x \in \RR^n$, define $p = \hprox_f(x)$.
        For all $y \in \RR$ it verifies
        $$
            \left(f(p) + \frac{1}{2}\Vert x - p\Vert^2\right)
            - 
            \left(
                f(y) + \frac{1}{2}\Vert x - y\Vert^2 
            \right)
            \le 
            - \frac{(1 + \beta)}{2}\Vert y - p\Vert^2. 
        $$
        Recall: $\hprox_f(x) = \argmin_{u}\left\lbrace f(u) + \frac{1}{2}\Vert u - x\Vert^2 \right\rbrace$. 
    \end{theorem}
    \begin{proof}
        See 
        \hyperref[appendix:proof_ppm_descent_lemma]
            {Appendix \ref*{appendix:proof_ppm_descent_lemma}}. 
    \end{proof}
    \begin{remark}
        We use this theorem to prove the convergence of the proximal point method. 
        Observe that when $\beta = 0$, this reduces to \cite[theorem 12.26]{bauschke_convex_2017}. 
    \end{remark}
 
    \begin{definition}[The gradient mapping]
        \label{def:gradient_mapping}
        Suppose $h = f + g$ satisfies 
        \hyperref[assumption:smooth-nonsmooth-sum]{Assumption \ref*{assumption:smooth-nonsmooth-sum}}. 
        Define the proximal gradient operator
        $$
            \mathcal T_L(x) := \hprox_{L^{-1}g}(x - L^{-1}\nabla f(x)),
        $$
        and the gradient mapping operator
        $$
            \mathcal G_L(x) = L(x - \mathcal T_L(x)). 
        $$
    \end{definition}
    \begin{remark}
        The name ``gradient mapping" comes from \cite[(2.2.54)]{nesterov_lectures_2018}, however, Nesterov was referring to only the case when $g$ is an indicator function of a convex set in his writing. 
        Of course, in Amir Beck \cite[10.3.2]{beck_first-order_nodate}, it has the exact same definition for gradient mapping as the above. 
    \end{remark}

    \begin{lemma}[Gradient mapping approximates subgradient]
    \label{lemma:grad-map-approx-subgrad}\; \\
        Suppose $h = f + g$ satisfies 
        \hyperref[assumption:smooth-nonsmooth-sum]{Assumption \ref{assumption:smooth-nonsmooth-sum}}, 
        let $\mathcal T_L, \mathcal G_L$ be given by 
        \hyperref[def:gradient_mapping]{Definition \ref*{def:gradient_mapping}}.
        Then for all $x$, the gradient mapping verifies
        \begin{align*}
            x^+ &= \mathcal T_L(x), 
            \\
            \mathcal G_L(x) := L(x - x^+) &\in  \nabla f(x) + \partial g(x^+). 
        \end{align*}
        Equivalently, $\exists v \in \partial g(x^+)$ such that $G_L(x) = \nabla f(x) + v = L(x - x^+)$. 
    \end{lemma}
    \begin{proof}
        Using the resolvent definition of the proximal gradient operator and the fact that the single-valuedness in the convex settings, $x^+$ has relations: 
        \begin{align*}
            x^+ &= [I + L^{-1}\partial g]^{-1}\circ [I - L^{-1}\nabla f](x)
            \\
            [I + L^{-1}\partial g](x^+) &\ni [I - L^{-1}\nabla f](x)
            \\
            x^+ + L^{-1}\partial g(x^+) &\ni x - L^{-1}\nabla f(x)
            \\
            x^+ - x + L^{-1}\partial g(x^+) &\ni L^{-1}\nabla f(x)
            \\
            L(x^+ - x) + \partial g(x^+) &\ni - \nabla f(x)
            \\
            L(x - x^+) &\in \nabla f(x) + \partial g(x^+)
            \\
            \mathcal G_L(x) &\in \nabla f(x) + \partial g(x^+). 
        \end{align*}
    \end{proof}

    \begin{lemma}[Linearized gradient mapping lower bound]
    \label{lemma:grad_map_linearization}\; \\
        Suppose that $h = f + g$ satisfies 
        \hyperref[assumption:smooth-nonsmooth-sum]{Assumption \ref*{assumption:smooth-nonsmooth-sum}}, 
        further assume that $f$ is strongly convex with index $\mu \ge 0$. 
        Let $x^+ = \mathcal T_L(x)$ as given in 
        \hyperref[def:gradient_mapping]{Definition \ref*{def:gradient_mapping}}. 
        Then for all $z \in \RR$, it satisfies
        \begin{align*}
            h(z) &\ge 
            h(x^+) + 
            \langle \mathcal G_L (x), z - x\rangle 
            + 
            \frac{L}{2}\Vert x - x^+\Vert^2 + \frac{\mu}{2}
            \Vert z - x\Vert^2. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Using the $L$-smoothness of $f$ and convexity of $g, f$, it has inequalities
        \begin{align*}
            &f(x^+) \le 
            f(x) + \langle \nabla f(x), x^+ - x\rangle
            + \frac{L}{2}\Vert x - x^+\Vert^2, 
            \\
            &
            \frac{\mu}{2}\Vert z - x\Vert^2+ 
            f(x) + \langle \nabla f(x), z - x\rangle 
            \le f(z), 
            \\
            &g(x^+) \le 
            g(z) + \langle v, x^+ - z\rangle\quad 
            \forall v \in \partial g(x^+)
        \end{align*}
        For all $v \in \partial g (x^+)$, apply the above by considering the following sequence of relations
        \begin{align*}
            h(x^+) &= f(x^+) + g(x^+)
            \\&
            \begin{aligned}
                &\le 
                \left(
                    f(x) + \langle \nabla f(x), x^+ - x\rangle
                    + \frac{L}{2}\Vert x - x^+\Vert^2
                \right)
                \\
                &\qquad  
                + (g(z) + \langle v, x^+ - z\rangle)
            \end{aligned}
            \\&
            \begin{aligned}
                &\le 
                \left(
                    f(z) - \langle \nabla f(x), z - x\rangle
                    - \frac{\mu}{2}\Vert z - x\Vert^2
                    + \langle \nabla f(x), x^+ - x\rangle
                    + 
                    \frac{L}{2}\Vert x - x^+\Vert^2
                \right)
                \\
                &\qquad 
                +
                (g(z) + \langle v, x^+ - z\rangle)
            \end{aligned}
            \\&
            \begin{aligned}
                &= 
                (f(z) + h(z)) 
                \\
                &\qquad 
                + \left(
                    \langle \nabla f(x), x - z\rangle + 
                    \langle \nabla f(x), x^+ - x\rangle + 
                    \langle v, x^+ - z\rangle
                \right) 
                \\ 
                &\qquad 
                - \frac{\mu}{2}\Vert z - x\Vert^2
                + \frac{L}{2}\Vert x - x^+\Vert^2
            \end{aligned}
            \\& 
            \begin{aligned}
                &= h(z) + 
                \left(
                    \langle \nabla f(x), x - x^+ + x^+ - z\rangle + 
                    \langle \nabla f(x), x^+ - x\rangle + 
                    \langle v, x^+ - z\rangle
                \right)
                \\
                &\qquad 
                - \frac{\mu}{2}\Vert z - x\Vert^2
                + \frac{L}{2}\Vert x - x^+\Vert^2
            \end{aligned}
            \\& 
            \begin{aligned}
                &= h(z) + 
                \langle \nabla f(x) + v, x^+ - z\rangle 
                - \frac{\mu}{2}\Vert z - x\Vert^2
                + \frac{L}{2}\Vert x - x^+\Vert^2
            \end{aligned}
        \end{align*}
        Showed in  
        \hyperref[lemma:grad-map-approx-subgrad]{Lemma \ref*{lemma:grad-map-approx-subgrad}}, 
        we have $\mathcal G_L(x) \in \nabla f(x) + \partial g(x^+)$, choose $v \in \partial g(x^+)$ such that $G_L(x) = \nabla f(x) + v = L(x - x^+)$, so it yields
        \begin{align*}
            h(x^+) & 
            \le  
            h(z) + \langle L(x - x^+), x^+ - x + x - z\rangle 
            - \frac{\mu}{2}\Vert z - x\Vert^2
            + \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &= h(z) + 
            \underbrace{\langle L(x - x^+), x - z\rangle}_{
                = - \langle \mathcal G_L (x), z - x\rangle
            }
            - \frac{\mu}{2}\Vert z - x\Vert^2
            - \frac{L}{2}\Vert x - x^+\Vert^2
        \end{align*}
        Moving everything except $h(z)$ from the RHS to the LHS yield the desired inequality. 
    \end{proof}
    \begin{remark}
        The inequality is analogous to \cite[(2.2.57)]{nesterov_lectures_2018}, however Nesterov stated it only for the case when $g$ is an indicator function of some convex set $Q$. 
    \end{remark}

\section{The PPM interpreation of accelerated gradient}\label{sec:ppm_interp_of_ag}
    In this section, we present the generic PPM formulation of the accelerated proximal gradient method. 
    This is general form presents us with a set of undetermine stepsize parameters $\eta_i, \tilde \eta_i$ that can be changed later to accomodate proofs that take Lypunov functions and phrase the convergence rate using the stepsizes sequence $\tilde \eta_i$. 
    These interpretations are extension in the non-smooth context of Ahn and Sra's work \cite{ahn_understanding_2022} using the proximal gradient mapping operator. 
    The next definition will start the discussion. 
    \begin{definition}[Linear lower bounding function]\label{def:gradmap-linear-lowerbnd-fxn}
        Let $h = f+ g$ satisfies
        \hyperref[assumption:smooth-nonsmooth-sum]{Definition \ref*{assumption:smooth-nonsmooth-sum}}, 
        $\mathcal G_L$ given by 
        \hyperref[def:gradient_mapping]{Definition \ref*{def:gradient_mapping}}. 
        Define for all $y$ the function
        \begin{align*}
            l_h(x; y) = h(\mathcal T_L y) + \langle \mathcal G_L(y), x - y \rangle 
            + \frac{L}{2}\Vert y - \mathcal T_L y\Vert^2. 
        \end{align*}
    \end{definition} 
    \begin{remark}
        The function satisfies $l_h(x; y) \le h(x)$ for all $x \in \RR, y \in \RR$ by 
        \hyperref[lemma:grad_map_linearization]
        {Lemma \ref*{lemma:grad_map_linearization}}. 
    \end{remark}
    \par
    With the above definition, it's possible to formulate a generic form of accelerated gradient method using $l_h(x,y)$ as two proximal point methods anchored at some iterates $x_t, x_{t + 1}$.
    The formulation is generic because of undetermined stepsize parameter $\eta_t, \tilde \eta_t$ from the two proximal point. 
    \par
    We state the PPM interpretation of accelerated gradient in 
    \hyperref[def:ag_prox_grad_ppm]{Definition \ref*{def:ag_prox_grad_ppm}}, 
    which has the equivalent form as presented in
    \hyperref[def:ag_prox_grad_generic]
    {Definition \ref*{def:ag_prox_grad_generic}}. 
    \hyperref[prop:derive_ag_prox_grad_tript]
    {Proposition \ref*{prop:derive_ag_prox_grad_tript}} 
    shows their equivalence. 
    \par
    Through out this section, we assume 
    \begin{enumerate}
        \item $h=f + g$ satisfies 
            \hyperref[assumption:smooth-nonsmooth-sum]
            {Assumption \ref*{assumption:smooth-nonsmooth-sum}}, 
        \item Using $h$ as given from above, let $\mathcal T_L, \mathcal G_L$ be given by 
            \hyperref[def:gradient_mapping]
            {Definition \ref*{def:gradient_mapping}}. 
        \item Using all above, let $l_h$ be given by 
            \hyperref[def:gradmap-linear-lowerbnd-fxn]
            {Definition \ref*{def:gradmap-linear-lowerbnd-fxn}}. 
    \end{enumerate}

    \begin{definition}[AG proximal gradient PPM generic form]
    \label{def:ag_prox_grad_ppm}
        Define $\eta_t, \tilde \eta_t$ to be strictly larger than zero for all $t \in \N$. 
        With initial iterate $x_0, y_0$, 
        The generic form has iterates $x_t, y_t$ for all $t \in \N$ that satisfy: 
        $$
        \begin{aligned}
            x_{t + 1} &= \argmin_{x} \left\lbrace
                l_h(x; y_t) + \frac{1}{2\tilde \eta_{t + 1}} 
                \Vert x - x_t\Vert^2
            \right\rbrace,
            \\
            y_{t + 1}&= 
            \argmin_{x}
            \left\lbrace
                l_h(x; y_t) + \frac{L}{2}\Vert x - y_t\Vert^2 + 
                \frac{1}{2\eta_{t + 1}} \Vert x - x_{t + 1}\Vert^2
            \right\rbrace.
        \end{aligned}
        $$
    \end{definition}

    \begin{definition}[AG proximal gradient generic form]
    \label{def:ag_prox_grad_generic}
        Define $\eta_t, \tilde \eta_t$ to be $> 0$ for all $t \in \N$. 
        With initial iterate $x_0, y_0$.
        The generic form has iterates $(y_t, x_{t + 1}, z_{t + 1})$ such that 
        $$
        \begin{aligned}
            y_t &= (1 + L\eta_t)^{-1}(x_t + L\eta_t z_t)
            \\
            x_{t + 1} &= x_t - \tilde \eta_{t + 1} \mathcal G_L(y_t)
            \\
            z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t)
        \end{aligned}
        $$
        for all $t\in \mathbb N$. 
    \end{definition}
    \begin{remark}
        Observe that $z_{t + 1} = y_t^+$. 
    \end{remark}
    
    \begin{proposition}
    \label{prop:derive_ag_prox_grad_tript}
       We have equalities
        \begin{align*}
            x_{t + 1} &= \argmin_{x}
            \left\lbrace
                l_h(x, y_t) + \frac{1}{2\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2
            \right\rbrace
            \\
            &= x_t - \tilde\eta_{t + 1} \mathcal G_L(y_t), 
            \\
            y_{t + 1} &= \argmin_{x}
            \left\lbrace
                    h(y_t^+) + \langle \mathcal G_L(y_t), x - y_t\rangle + \frac{L}{2}\Vert x -y_t\Vert^2 + \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
            \right\rbrace
            \\
            &= (1 + L\eta_{t + 1})^{-1}
            (x_{t + 1} + L\eta_{t + 1}(y_t - L^{-1}\mathcal  G_L(y_t))). 
        \end{align*}
        This thows
        \hyperref[def:ag_prox_grad_ppm]{Definition \ref*{def:ag_prox_grad_ppm}}
        and 
        \hyperref[def:ag_prox_grad_generic]{Definition \ref*{def:ag_prox_grad_generic}} 
        are equivalent. 
    \end{proposition}
    \begin{proof}
        Let $y_t^+ = \mathcal T_L(y_t)$, recall that $l_h(x; y_t) = h(y_t^+) + \langle \mathcal G_L(y_t), x -y_t\rangle \le f(x)$ by 
        \hyperref[lemma:grad_map_linearization]
        {Lemma \ref*{lemma:grad_map_linearization}}
        Since $l_h(x; y_t)$ is a simple linear function wrt $x$, minimizing the quandratic where to get $x_{t + 1} = x_t - \tilde\eta_{t + 1} \mathcal G_L(y_t)$; for $y_{t + 1}$, complete the square on the second and the third terms: 
        \begin{align*}
            & \frac{L}{2}\left(
                2\langle L^{-1}\mathcal G_L(y_t), x - y_t\rangle + 
                \Vert x - y_t\Vert^2
            \right)
            \\
            &= 
            \frac{L}{2}
            \left(
                - \Vert L^{-1} \mathcal G_L(y_t)\Vert^2  
                + \Vert L^{-1} \mathcal G_L(y_t)\Vert^2 
                + 
                2\langle L^{-1} \mathcal G_L(y_t), x - y_t\rangle + 
                \Vert x - y_t\Vert^2
            \right)
            \\
            &= \frac{L}{2}\left(
                - \Vert L^{-1}\mathcal G_L(y_t)\Vert^2  
                + \Vert x - (y_t - L^{-1}\mathcal G_L(y_t))
                \Vert^2
            \right), 
        \end{align*}
        therefore it transforms into 
        \begin{align*}
            y_{t + 1} &=\argmin_{x} \left\lbrace
                \frac{L}{2}\left\Vert 
                    x - (y_t - L^{-1}\mathcal G_L(y_t))
                \right\Vert^2
                + \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
            \right\rbrace
            \\
            &=
            \frac{
                \left(y_t - L^{-1}\mathcal G_L(y_t)\right) + x_{t + 1}
            }{L + \eta_{t + 1}^{-1}}.
        \end{align*}
        Define $z_{t + 1} = y_t - \mathcal G_L(y_t) = \mathcal T_L(y_t)$, then the above expression simplifies to 
        $$
            y_{t + 1} = (1 + L\eta_{t +1})^{-1}(x_{t + 1}+ L\eta_{t + 1}z_{t + 1}). 
        $$
    \end{proof}
    \begin{remark}
        $y_{t + 1}$ is the minimizer of a simple quadratic. 
        Given that the original function $h$ is potentially non-smooth, therefore it's not always an upper bound of $h(x)$. 
        The upper bound interpretation of the smooth case as proposed by Ahn \cite{ahn_understanding_2022}, Sra for the update of $y_{t + 1}$ fails when $h$ is non-smooth! 
    \end{remark}

\section{Generic Lyapunov analysis for accelerated gradient via PPM}
\label{sec:generic_ag_ppm_lyapunov_analysis}
    In this section we derive the generic convergence rate formuated by $\eta_i, \tilde \eta_i$. 
    \par
    The Lypunov function in 
    \hyperref[thm:generic_ag_convergence]{Theorem \ref*{thm:generic_ag_convergence}}
    is identical to Ahn and Sra 
    \cite[section 4.2]{ahn_understanding_2022}, 
    except for the involvement of the gradient mapping. 
    We adapated it into the context of proximal gradient by 
    \hyperref[lemma:nsmooth_agg_lyapunov_upper_bound]
    {Lemma \ref*{lemma:nsmooth_agg_lyapunov_upper_bound}}
    \par
    We present the convergence rate by compatifying results into 
    \hyperref[lemma:nsmooth_agg_lyapunov_upper_bound]
    {Lemma \ref*{lemma:nsmooth_agg_lyapunov_upper_bound}} 
    to assist the proof of  
    \hyperref[thm:generic_ag_convergence]
    {Theorem \ref*{thm:generic_ag_convergence}} 
    which states new results that are extension to Ahn and Sra's works by introducing an error term to the Lypunov analysis, allowing for a weaker constraints for the stepsize paramters $\tilde \eta_i, \eta_i$ from 
    \hyperref[def:ag_prox_grad_generic]{Definition \ref*{def:ag_prox_grad_generic}}. 
    \hyperref[thm:ag_generic_stepsize_constraints]{Theorem \ref*{thm:ag_generic_stepsize_constraints}}
    states the weakened constraints on the stepsize parameters $\tilde \eta_i, \eta_i$. 
    \par
    Throughout the section, we continue the assumption for $h = f + g$, gradient mapping $\mathcal G_L$ and proximal gradient mapping $\mathcal T_L$ from the previous section. 

    \begin{lemma}[Lyapunov inequality]
    \label{lemma:nsmooth_agg_lyapunov_upper_bound}\;\\
        Fix any $\bar x, x, z',\tilde\eta > 0$, define
        \begin{align*}
            \phi(u) &:= \tilde\eta
            \left(
                h( \mathcal T_L\bar x) + \langle \mathcal G_L\bar x, u - \bar x\rangle
                + \frac{L}{2}\Vert \bar x - \mathcal T_L \bar x\Vert^2 
            \right), 
            \\
            x^+ &:= 
            \hprox_\phi(x) = x - \tilde \eta\mathcal G_L \bar x,
            \\
            \Upsilon_{1}  &:= 
            \tilde \eta(h(\mathcal T_L \bar x) - h(x_*))
            + 
            \frac{1}{2}(\Vert x^+ - x_*\Vert^2 - \Vert x - x_*\Vert^2), 
            \\
            \Upsilon_2 &:= 
            h(\mathcal T_L \bar x) - h(z'). 
        \end{align*}
        with $x_*$ being the minimizer of $h$, it has 
        \begin{align*}
            \Upsilon_1 &\le 
            - \tilde \eta \langle \mathcal G_L\bar x, x^+ - \mathcal T_L \bar x\rangle
            + 
            \frac{\tilde \eta L}{2} \Vert \bar x - \mathcal T_L \bar x\Vert^2
            - 
            \frac{1}{2}\Vert x^+ - x\Vert^2, 
            \\
            \Upsilon_2 &\le 
            \langle \mathcal G_L\bar x, \mathcal T_L \bar x - z'\rangle + 
            \frac{L}{2}\Vert \mathcal T_L \bar x - \bar x\Vert^2. 
        \end{align*}
    \end{lemma}
    \begin{observation}
        Function $\phi$ is a linear function that qualifies as a lower bound of $\tilde \eta h$, at $\bar x$ by 
        \hyperref[lemma:grad_map_linearization]{lemma \ref*{lemma:grad_map_linearization}}.
    \end{observation}
    \begin{proof}
        Observe that we have for all $u$: 
        \begin{align*}
            h(u) &\ge 
            h(\mathcal T_L\bar x) + 
            \langle \mathcal G_L \bar x, u - \bar x\rangle + 
            \frac{L}{2}\Vert \bar x - \mathcal T_L \bar x\Vert^2
            \\
            &= 
            h(\mathcal T_L \bar x) + 
            \langle \mathcal G_L \bar x, u - \mathcal T_L \bar x\rangle
            + 
            \langle \mathcal G_L \bar x, \mathcal T_L \bar x - \bar x\rangle
            + 
            \frac{L}{2}\Vert \bar x - \mathcal T_L \bar x \Vert^2
            \\
            &= 
            h(\mathcal T_L \bar x) + 
            \langle \mathcal G_L \bar x, u - \mathcal T_L \bar x\rangle
            + 
            \langle 
                L (\bar x - \mathcal T_L \bar x)
                , 
                \mathcal T_L \bar x - \bar x
            \rangle
            + 
            \frac{L}{2}\Vert \bar x - \mathcal T_L \bar x\Vert^2
            \\
            &= 
            h(\mathcal T_L \bar x) + 
            \langle \mathcal G_L \bar x, u - \mathcal T_L \bar x\rangle 
            - \frac{L}{2} \Vert \bar x - \mathcal T_L \bar x\Vert^2. 
        \end{align*}
        By PPM descent inequality
        \hyperref[thm:ppm_descent_ineq]{theorem \ref*{thm:ppm_descent_ineq}}
        , let $x_*$ be a minimizer, it claims that 
        {\small
        \begin{align*}
            & 
            \phi(x^+) - \phi (x^*) + 
            \frac{1}{2}\left(
                \Vert x^+ - x_*\Vert^2 - 
                \Vert x - x_*\Vert^2
            \right)
            \le 
            - \frac{1}{2}\Vert x^+ - x\Vert^2
            \\
            \implies &
            \tilde 
            \eta 
            \left(   
                h(\mathcal T_L \bar x) + 
                \langle \mathcal G_L \bar x, u - \mathcal T_L \bar x\rangle 
                - \frac{L}{2} \Vert \bar x - \mathcal T_L \bar x\Vert^2
            \right)
            - \tilde \eta h (x_*)
            +
            \frac{1}{2}\left(
                \Vert x^+ - x_*\Vert^2 - 
                \Vert x - x_*\Vert^2
            \right)
            \\
            &\le 
            - \frac{1}{2}\Vert x^+ - x\Vert^2
            \\
            \iff &
            \tilde \eta (h(\mathcal T_L\bar x) - h(x_*))  
            + 
            \frac{1}{2}\left(
                \Vert x^+ - x_*\Vert^2 - 
                \Vert x - x_*\Vert^2
            \right)
            \\
            & \le 
            - \tilde \eta \langle \mathcal G_L\bar x, x^+ - \mathcal T_L \bar x\rangle
            + 
            \frac{\tilde \eta L}{2} \Vert \bar x - \mathcal T_L \bar x\Vert^2
            - 
            \frac{1}{2}\Vert x^+ - x\Vert^2. 
        \end{align*}
        }
        Next, for all $z, z'$, it would have by smoothness of $f$: 
        \begin{align*}
            f(z) - f(z') &= 
            f(z) - f(\bar x) + f(\bar x) - f(z')
            \\
            &\le 
            \langle 
                \nabla f(\bar x), z - \bar x
            \rangle + 
            \frac{L}{2}\Vert z - \bar x\Vert^2 
            + 
            \langle 
                \nabla f(\bar x), 
                \bar x - z'
            \rangle
            \\
            &= 
            \langle \nabla f(\bar x), z - z'\rangle
            + 
            \frac{L}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
        The convexity of $g$ yields for all $v \in \partial g(z)$: 
        \begin{align*}
            g(z) + 
            \langle 
                v, z' - z
            \rangle 
            &\le g(z')
            \\
            g(z) - g(z') 
            &\le 
            \langle v, z - z'\rangle. 
        \end{align*}
        Adding them yield 
        \begin{align*}
            h(z) - h(z') &\le 
            \langle \nabla f(\bar x) +v, z - z'\rangle + 
            \frac{L}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
        Setting $z = \mathcal T_L \bar x$, use 
        \hyperref[lemma:grad-map-approx-subgrad]
        {Lemma \ref*{lemma:grad-map-approx-subgrad}}, 
        there exists $v \in \partial g (\mathcal T_L(\bar x))$ such that $\mathcal G_L(\bar x) = \nabla f(z) + v$, hence the theorem is proved. 
    \end{proof}
    \begin{remark}\label{remark:upsilon-upperbound-for-iterates}
        Let $y_t, x_t$ be given by
        \hyperref[def:ag_prox_grad_generic]
        {Definition \ref*{def:ag_prox_grad_generic}}.
        Set $\bar x = y_t, x = x_t$, then 
        \begin{align*}
            x^+ &= x_t - \mathcal G_L y_t = x_{t + 1},
            \\
            z_{t + 1} &= \mathcal T_L y_t = \mathcal T_L \bar x.
        \end{align*}
        Set $z' = z_t, \tilde \eta = \tilde \eta_{t + 1}$, we define the quantities: 
        \begin{align*}
            \Upsilon_{1, t + 1}^{\text{AG}}
            &:= 
            \tilde \eta_{t + 1}(h(z_{t + 1}) - h(x_*))
            + 
            \frac{1}{2}(\Vert x_{t + 1} - x_*\Vert^2 + \Vert x_t - x_*\Vert^2),
            \\
            \Upsilon_{2, t + 1}^{\text{AG}}
            &:= 
            h(z_{t + 1}) - h(z_t). 
        \end{align*}
        Then it as inequalities: 
        \begin{align*}
            \Upsilon_{1, t + 1}^{\text{AG}}
            &\le 
            - \tilde \eta_{t + 1}\langle \mathcal G_L y_t, x_{t + 1} - z_{t + 1}\rangle
            + 
            \frac{\tilde \eta_{t + 1}L}{2}
            \Vert  
                y_t - z_{t + 1}
            \Vert^2
            - 
            \frac{1}{2}
            \Vert x_{t + 1} - x_t\Vert^2,  
            \\ 
            \Upsilon_{2, t + 1}^{\text{AG}}
            &\le 
            \langle \mathcal G_L \bar x, z_{t + 1} - z_t \rangle + 
            \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2.
        \end{align*}
        This is useful for the convergence proof of the algorithm. 
    \end{remark}

    \begin{theorem}[Generic AG convergence]
    \label{thm:generic_ag_convergence}
        Let $\Upsilon_{1, t + 1}^\text{AG}, \Upsilon_{2, t + 1}^\text{AG}$ be given by 
        \hyperref[remark:upsilon-upperbound-for-iterates]
        {Remark \ref*{remark:upsilon-upperbound-for-iterates}}. 
        Let $z_t$ be given by 
        \hyperref[def:ag_prox_grad_generic]
        {Definition \ref*{def:ag_prox_grad_generic}}. 
        Define the Lyapunov function $\Phi_t\; \forall t \in \{0\}\cup \N$, $S_t$ and $\sigma_t$: 
        \begin{align*}
            S_t &:= \sum_{i = 1}^{t} \delta_i \quad \forall t \in \N,
            \\
            \sigma_t &:= \sum_{i = 1}^{t}\tilde \eta_i \quad \forall t \in \N, 
            \\
            \Phi_0 &:= \frac{1}{2}\Vert x_0 - x_*\Vert^2, 
            \\
            \Phi_t &:= 
                \sigma_t(h(z_t) - h(x_*)) + \frac{1}{2}\Vert x_t - x_*\Vert^2 
            \quad \forall t \in \N.
        \end{align*}
        If there exists a sequence of $(\eta_i)_{i \in \N}$ as defined in
        \hyperref[def:ag_prox_grad_ppm]
        {Definition \ref*{def:ag_prox_grad_ppm}} 
        such that
        \begin{align*}
            & \Phi_{t + 1} - \Phi_{t} =
            \sigma_t\Upsilon_{2, t + 1}^{\text{AG}} 
            + 
            \Upsilon_{1, t + 1}^{\text{AG}} 
            \le \delta_{t + 1} \quad 
            \forall t \in \mathbb N, 
            \\
            & \Upsilon_{1, 1}^{\text{AG}} \le \delta_1. 
        \end{align*}
        for some $\delta_i, i \in \N$. 
        Then
        \begin{align*}
            h(z_T) - h(x_*) &\le 
            \sigma_T^{-1}\left(
                S_{T} + \frac{1}{2}\Vert x_0 - x_*\Vert^2
            \right). 
        \end{align*}
        Where $x_*$ is an minimizer of $h$. 
        So $h(z_T) - h(x_*)$ has convergence rate $\mathcal O(\sigma_T^{-1})$ when $S_T \le 0$, and $\mathcal O(\sigma_T^{-1}S_T)$ when $S_T > 0$. 
    \end{theorem}
    \begin{proof}
        By definition we have
        {\footnotesize
        \begin{align*}
            \Phi_{t + 1} - \Phi_t 
            &= 
            \left(
                \sum_{i = 1}^{t+1} \tilde\eta_{i}
            \right) (h(z_{t + 1}) - h(x_*)) 
            - 
            \left(
                \sum_{i = 1}^{t} \tilde\eta_{i}
            \right) (h(z_{t}) - h(x_*)) 
            + \frac{1}{2}\Vert x_t - x_*\Vert^2
            - \frac{1}{2}\Vert x_{t + 1} - x_*\Vert^2
            \\
            &= 
            \tilde \eta_{t + 1} (h(z_{t + 1}) - h(z_*))
            +
            \left(
                \sum_{i = 1}^{t} \tilde \eta_i
            \right)(h(z_{t + 1}) - h(z_t))
            + \frac{1}{2}\Vert x_t - x_*\Vert^2
            - \frac{1}{2}\Vert x_{t + 1} - x_*\Vert^2
            \\
            &= \left(
                \sum_{i = 1}^{t} \tilde \eta_i
            \right)\Upsilon_{2, t + 1}^{\text{AG}} + \Upsilon_{1, t + 1}^{\text{AG}} \le \delta_{t + 1}. 
        \end{align*}
        }
        Telescoping for $t = 0, \cdots, T- 1$
        \begin{align*}
            \Phi_T - \Phi_0 = 
            \sum_{i = 0}^{T - 1}\Phi_{i + 1} - \Phi_i 
            &\le 
            \sum_{i = 0}^{T - 1}\delta_{i + 1}
            = S_{T}. 
        \end{align*}
        So then $\Phi_T - \Phi_0$ yields: 
        \begin{align}
            \sigma_T (h(z_T) - h(x_*)) 
            + \frac{1}{2}\Vert x_t - x_*\Vert^2 
            - \frac{1}{2}\Vert x_0 - x_*\Vert^2 
            &\le S_{T}
            \\
            \implies 
            \sigma_T(h(z_T) - h(x_*))
            &\le 
            S_T + \frac{1}{2}\Vert x_0 - x_*\Vert^2
            \\
            h(z_T) - h(x_*) &\le 
            \sigma_T^{-1}\left(
                S_{T} + \frac{1}{2}\Vert x_0 - x_*\Vert^2
            \right),
        \end{align}
        which yields a convergence rate $\mathcal O(\sigma_T^{-1}S_{T})$. 
        When $S_T = 0$, the convergence rate is $O(\sigma_T^{-1})$ instead. 
    \end{proof}

    \begin{theorem}[Constraints of PPM stepsize sequence]
    \label{thm:ag_generic_stepsize_constraints}\;\\
        Let iterates $z_t, x_t, y_t$ be given by
        \hyperref[def:ag_prox_grad_generic]
        {Definition \ref*{def:ag_prox_grad_generic}}. 
        If the stepsize $\eta_i, \tilde \eta_i$ satisfies relations 
        \begin{align*}
            \begin{cases}
                \tilde \eta_{t + 1} (\tilde \eta_{t + 1} - L^{-1})
                - L^{-1} \sum_{i= 1}^{t}\tilde \eta_i 
                = 
                \epsilon_{t + 1} \tilde \eta_{t + 1}
                & \forall t \in \mathbb N, 
                \\
                L \eta_t \tilde \eta_{t + 1} = \sum_{i=1}^{t}\tilde \eta_i 
                & \forall t \in \mathbb N. 
            \end{cases}
        \end{align*}
        Then for all $t \in \N$
        $$
            \epsilon_{t + 1} = \tilde \eta_{t + 1} - \eta_t - L^{-1}, 
        $$
        and 
        \begin{align*}
            \Phi_{t + 1} - \Phi_t =
            \Upsilon_{1, t + 1}^\text{AG} + 
            \sigma_t\Upsilon_{1, t + 1}^{\text{AG}}
            &\le
            \epsilon_{t + 1}\tilde\eta_{t + 1} \Vert \mathcal G_L(y_t)\Vert^2.
        \end{align*}
        Define $\delta_{t + 1} = \epsilon_{t + 1}\tilde\eta_{t + 1} \Vert \mathcal G_L(y_t)\Vert^2$. 
        Then the convergence rate stated in 
        \hyperref[thm:generic_ag_convergence]
        {Theorem \ref*{thm:generic_ag_convergence}}
        applies. 
    \end{theorem}
    \begin{proof}
        With $t \in \mathbb N \cup \{0\}$ fixed, 
        recall that for the proximal gradient PPM generic form 
        (as formulated in 
        \hyperref[def:ag_prox_grad_generic]{Definition \ref*{def:ag_prox_grad_generic}}) 
        for $t\in \mathbb N$ it has: 
        \begin{align*}
            y_t &= (1 + L\eta_t)^{-1}(x_t + L\eta_t z_t)
            \\
            x_{t + 1} &= x_t - \tilde \eta_{t + 1} \mathcal G_L(y_t)
            \\
            z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t). 
        \end{align*}
        Recall the upper bounds from 
        \hyperref[remark:upsilon-upperbound-for-iterates]
        {Remark \ref*{remark:upsilon-upperbound-for-iterates}}
        it has 
        \begin{align*}
            \Upsilon_{1, t + 1}^\text{AG}
            &= 
            \tilde\eta_{t + 1} (h(z_{t + 1}) - h(x_*)) + 
            \frac{1}{2} (
                \Vert x_{t + 1} - x_*\Vert^2
                - 
                \Vert x_t - x_*\Vert^2
            )
            \\
            &\le 
            - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 
            + \frac{\tilde\eta_{t + 1}L}{2}\Vert z_{t + 1} - y_t\Vert^2
            - \langle 
                \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                x_{t + 1} - z_{t + 1}
            \rangle, 
            \\
            \Upsilon_{2, t + 1}^\text{AG}
            &= 
            h(z_{t + 1}) - h(z_t) 
            \le 
            \langle \mathcal G_L(y_t), z_{t + 1} - z_t\rangle + 
            \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2. 
        \end{align*}
        Because the iterates are produced by 
        \hyperref[def:ag_prox_grad_generic]
        {Definition \ref*{def:ag_prox_grad_generic}}
        vector $x_{t + 1} - x_t$ and $z_{t + 1} - y_t$ are parallel by observations: 
        \begin{align*}
            x_{t + 1} - x_t &= -\tilde\eta_{t + 1}\mathcal G_L(y_t), 
            \\
            z_{t + 1} - y_t &= -L^{-1}\mathcal G_L(y_t). 
        \end{align*}
        This allows for 
        \begin{align*}
            \Upsilon_{1, t + 1}^{\text{AG}} 
            &\le 
            - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 + 
            \frac{\tilde\eta_{t + 1}L}{2}\Vert z_{t + 1} - y_t\Vert^2 
            - 
            \langle \tilde\eta_{t + 1}\mathcal G_L (y_t), x_{t + 1} - z_{t + 1} \rangle
            \\
            &= 
            - \frac{1}{2}\Vert \tilde\eta_{t + 1} \mathcal G_L(y_t)\Vert^2 + 
            \frac{\tilde\eta_{t + 1}L}{2}\Vert L^{-1} \mathcal G_L(y_t)\Vert^2
            - 
            \langle \tilde\eta_{t + 1} \mathcal G_L(y_t), x_{t + 1} - z_{t + 1} \rangle
            \\
            &= 
            \frac{1}{2}\left(
                - \tilde\eta_{t + 1}^2 + 
                L^{-1}\tilde\eta_{t + 1}
            \right)\Vert \mathcal G_L(y_t)\Vert^2
            - 
            \langle 
                \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                (x_{t + 1} - x_{t}) + x_t
                + (y_t - z_{t + 1}) - y_t
            \rangle
            \\
            &= 
            \frac{1}{2}\left(
                L^{-1}\tilde\eta_{t + 1}
                - \tilde\eta_{t + 1}^2
            \right)\Vert \mathcal G_L(y_t)\Vert^2
            - 
            \langle 
                \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                -\tilde\eta_{t + 1}\mathcal G_L(y_t) + x_t 
                + L^{-1}\mathcal G_L(y_t) - y_t
            \rangle
            \\
            &= 
            \frac{1}{2}\left(
                L^{-1}\tilde\eta_{t + 1}
                - \tilde\eta_{t + 1}^2
            \right)\Vert \mathcal G_L(y_t)\Vert^2
            - \langle 
                \tilde\eta_{t +1}\mathcal G_L(y_t), 
                (L^{-1} - \tilde\eta_{t + 1})\mathcal G_L(y_t) + x_t - y_t
            \rangle
            \\
            &= \frac{1}{2}\left(
                L^{-1}\tilde\eta_{t + 1} - \tilde\eta_{t + 1}^2 
                + 2 \tilde\eta_{t + 1}^2 - 2\tilde\eta_{t + 1}L^{-1}
            \right)\Vert \mathcal G_L(y_t)\Vert^2
            - 
            \langle 
                \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                x_t - y_t
            \rangle
            \\
            &= 
            \frac{1}{2}\left(
                \tilde\eta_{t + 1}^2 - \tilde\eta_{t + 1}L^{-1}
            \right)\Vert \mathcal G_L(y_t)\Vert^2 
            + \langle \tilde\eta_{t + 1} \mathcal G_L(y_t), y_t - x_t\rangle.
        \end{align*}
        Similarly 
        \begin{align*}
            \Upsilon_{2, t + 1}^{\text{AG}} 
            &\le 
            \langle \mathcal G_L(y_t), z_{t + 1} - z_t\rangle + 
            \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
            \\
            &= 
            \langle \mathcal G_L(y_t), z_{t + 1} - y_t + y_t - z_t\rangle
            + \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
            \\
            &= 
            \langle \mathcal G_L(y_t), - L^{-1} \mathcal G_L(y_t) + y_t - z_t\rangle
            + 
            \frac{L}{2}\Vert L^{-1}\mathcal G_L(y_t)\Vert^2
            \\
            &= 
            -L^{-1}\Vert \mathcal G_L(y_t)\Vert^2 
            + 
            (1/2)L^{-1}\Vert \mathcal G_L(y_t)\Vert^2 
            + 
            \langle \mathcal G_L(y_t), y_t - z_t\rangle
            \\
            &= 
            -(1/2)L^{-1}\Vert \mathcal G_L(y_t)\Vert^2
            + 
            \langle \mathcal G_L(y_t), y_t - z_t\rangle. 
        \end{align*}
        Observe that the cross product term for $\Upsilon_{1, t + 1}^\text{AG}, \Upsilon_{2, t + 1}^\text{AG}$ doesn't match. 
        Hence let's consider the update for $y_t$, which can be written as $y_t - x_t = L \eta_t (z_t - y_t)$. We make the choice to do surgery on upper bound of $\Upsilon_{2, t + 1}^\text{AG}$, so $\langle \mathcal G_L(y_t), y_t - x_t\rangle = \langle \mathcal G_L(y_t), L \eta_t (z_t - y_t)\rangle$. 
        With this in mind, RHS of $\Phi_{t + 1} - \Phi_t$ yields: 
        {\footnotesize
        \begin{align*}
            &\Upsilon_{1, t + 1}^\text{AG} + 
            \left(
                \sum_{i = 1}^{t}\tilde\eta_i 
            \right)\Upsilon_{2, t + 1}^{\text{AG}}
            \\
            &\le 
            \frac{1}{2}\left(
                \tilde\eta_{t + 1}^2 - \tilde\eta_{t + 1}L^{-1}
            \right)\Vert \mathcal G_L(y_t)\Vert^2 
            + 
            \langle \tilde\eta_{t + 1} \mathcal G_L(y_t), L\eta_t(z_t - y_t)\rangle
            \\ 
            &\quad 
            + 
            \left(
                \sum_{i = 1}^{t}\tilde\eta_i 
            \right)\left(
                -(1/2)L^{-1}\Vert \mathcal G_L(y_t)\Vert^2
                + 
                \langle \mathcal G_L(y_t), y_t - z_t\rangle
            \right)
            \\
            &= 
            \left(
                \frac{1}{2}\tilde\eta_{t + 1}\left(
                    \tilde \eta_{t +1} - L^{-1}
                \right)
                - 
                \frac{1}{2L}\sum_{i = 1}^{t}\tilde \eta_i
            \right)\Vert \mathcal G_L(y_t)\Vert^2 + 
            \left(
                L\eta_t \tilde \eta_{t + 1} - \sum_{i = 1}^{t}\tilde \eta_i
            \right)\langle \mathcal G_L(y_t), z_t - y_t\rangle. 
        \end{align*}
        }
        The non-negativity of $\Vert \mathcal G_L(y_t) \Vert^2$ characterize the culmulative error $\delta_{t + 1}$ in the Lypunov analysis through sequence $\epsilon_i$. 
        Setting the coefficient of $\Vert \mathcal G_L(y_t) \Vert^2$ to be $\epsilon_{t + 1}\tilde \eta_{t + 1}$, it yields a system of inequality: 
        \begin{align*}
            \begin{cases}
                \tilde \eta_{t + 1} (\tilde \eta_{t + 1} - L^{-1})
                - L^{-1} \sum_{i= 1}^{t}\tilde \eta_i 
                = 
                \epsilon_{t + 1} \tilde \eta_{t + 1}
                & \forall t \in \N, 
                \\
                L \eta_t \tilde \eta_{t + 1} = \sum_{i=1}^{t}\tilde \eta_i 
                & \forall t \in \N. 
            \end{cases}
        \end{align*}
        It requires base case $L\eta_0\tilde\eta_{1} = 0$, assume $\sigma_0 = 0$. 
        We use $\sum_{i = 1}^t \tilde \eta_i = \sigma_t$, simplifying the first equation we have 
        \begin{align*}
            \tilde \eta_{t + 1} (\tilde \eta_{t + 1} - L^{-1})
            - L^{-1} \sigma_t
            &= 
            \epsilon_{t + 1} \tilde \eta_{t + 1}
            \\
            \iff 
            \tilde \eta_{t + 1} ^2 - L^{-1} \tilde \eta_{t + 1} 
            &= 
            \epsilon_{t + 1} \tilde \eta_{t + 1} + L^{-1} \sigma_t
            \\
            &= 
            \epsilon_{t + 1} \tilde \eta_{t + 1} 
            + L^{-1}(L \eta_t \tilde \eta_{t + 1})
            \\
            \text{divide by } \tilde \eta_{t + 1} > 0
            \implies 
            \tilde \eta_{t + 1} &= \epsilon_{t + 1} + \eta_t + L^{-1}. 
        \end{align*}
        The parameter gives relation $\epsilon_{t+1} = \tilde \eta_{t + 1} - \eta_t - L^{-1}$. 
        Hence, it gives us the following system of equality to work with 
        \begin{align*}
            \forall t \in \N: 
            \begin{cases}
                \tilde \eta_{t + 1} = \epsilon_{t + 1} + \eta_t + L^{-1}, 
                \\
                L \eta_t \tilde \eta_{t + 1} = \sigma_t.     
            \end{cases}
        \end{align*}
        With that, we can solve a relation between $\eta_{t + 1}$ in terms of the sequence $\epsilon_t$ and $\eta_t$.
        Consider the equality 
        \begin{align*}
            L \sigma_{t + 1} &= L \tilde \eta_{t + 1} + L \sigma_t
            \\
            &=
            L \tilde \eta_{t + 1}  + L (L \eta_t \tilde \eta_{t + 1})
            \\
            &= L \tilde \eta_{t+ 1} + L \eta_t (L \tilde \eta_{t + 1})
            \\
            &=  L \tilde \eta_{t+ 1} + L \eta_t (L \epsilon_{t + 1} + L \eta_t + 1)
            \\
            &=  L \tilde \eta_{t+ 1} + L \eta_t (L \eta_t + 1) + L^2 \eta_t \epsilon_{t + 1}
            \\
            &= L (\epsilon_{t +1} + \eta_t + L^{-1}) + L\eta_t(L \eta_t + 1) + L^2\eta_t \epsilon_{t + 1}
            \\
            &= L \epsilon_{t + 1} + (L\eta_t + 1)^2 + L^2\eta_t \epsilon_{t + 1}
            \\
            &= 
            L \epsilon_{t + 1}(1 + L \eta_t) + (1 + L \eta_t)^2. 
        \end{align*}
        At the same time we have 
        \begin{align*}
            L \sigma_{t + 1} &= L^2 \eta_{t + 1}\tilde \eta_{t + 1} 
            \\
            &= L\eta_{t + 1}(1 + L \eta_{t + 1} + \epsilon_{t + 2})
            \\
            &= L\eta_{t + 1}(1 + L \eta_{t + 1}) + \epsilon_{t + 2}L\eta_{t + 1}. 
        \end{align*}
        Therefore, it generates the following equation: 
        \begin{align*}
            L\eta_{t + 1} (1 + L \eta_{t + 1}) 
            + 
            \epsilon_{t + 2} L \eta_{t + 1} 
            &= 
            L\epsilon_{t + 1}(1 + L \eta_t)  + (1 + L\eta_t)^2
            \\
            (L\eta_{t + 1} + L^2\eta_{t + 1}^2)
            + 
            \epsilon_{t + 2} L \eta_{t + 1} 
            + 
            \frac{1}{4}
            &= 
            L\epsilon_{t + 1}(1 + L \eta_t)  + (1 + L\eta_t)^2
            + 
            \frac{1}{4}
            \\
            (L\eta_{t + 1} + L^2\eta_{t + 1}^2 + 1/4)
            + 
            \epsilon_{t + 2} L \eta_{t + 1} 
            + \epsilon_{t + 2}
            &= 
            L\epsilon_{t + 1}(1 + L \eta_t)  + (1 + L\eta_t)^2
            + \frac{1}{4}
            + \epsilon_{t + 2}
            \\
            (L\eta_{t + 1} + 1/2)^2 + \epsilon_{t + 2}(L \eta_{t + 1} + 1)
            &= 
            L \epsilon_{t + 1}(1 + L \eta_t) + (1 + L\eta_t)^2
            + \frac{1}{4} + \epsilon_{t + 2}
            \\
            \text{ with: } & a_t = 1 + L \eta_t = \tilde \eta_{t + 1} - \epsilon_{t + 1}
            \\
            (a_{t + 1} - 1/2)^2 + \epsilon_{t + 2}a_{t + 1}
            &= 
            L \epsilon_{t + 1}a_t + a_t^2 + 1/4 + \epsilon_{t + 1}
            \\
            a_{t + 1}^2 + 1/4 - a_{t + 1} + \epsilon_{t + 2}a_{t + 1}
            &= 
            L \epsilon_{t + 1}a_t + a_t^2 + 1/4 + \epsilon_{t + 1}
            \\
            a_{t + 1}^2 + a_{t + 1}(\epsilon_{t + 2} - 1)
            &= 
            \underbrace{
                a_t(L \epsilon_{t + 1} + a_t) + \epsilon_{t + 1}
            }_{c_{t + 1}}. 
        \end{align*}
        Solving reveals the relations: 
        \begin{align*}
            \begin{cases}
                a_{t + 1} = (1/2)\left(
                1 - \epsilon_{t + 2} + \sqrt{(1 - \epsilon_{t + 2}) + 4 c_{t + 1}}
                \right), 
                \\
                c_{t + 1} = a_t (L \epsilon_{t + 1} + a_t) + \epsilon_{t + 1}. 
            \end{cases}
        \end{align*}
        With $\delta_i$ given by
        \hyperref[thm:generic_ag_convergence]
        {Theorem \ref*{thm:generic_ag_convergence}}. 
        Define $\delta_i = \epsilon_{i} \tilde \eta_i$, then 
        \begin{align*}
            S_{T} = 
            \sum_{i = 0}^{T- 1} \delta_i 
            &= 
            \sum_{i = 0}^{T - 1} \epsilon_{i + 1}\tilde\eta_{i + 1}\Vert \mathcal G_L(y_i)\Vert^2
            \\
            &\le \sum_{i = 0}^{T - 1}\max(\epsilon_{i + 1} \tilde\eta_{i + 1}\Vert \mathcal G_L(y_i)\Vert^2, 0). 
        \end{align*}
        Under an ideal case where we wish to attain the best convergence, we want $\lim_{T \rightarrow \infty} S_T < \infty$. 
        One way to accomplish is choose the error sequence $\epsilon_i, i \in \N$ to be bounded by for all $i \in \N$, $\epsilon_i$ should satisfy: 
        \begin{align*}
            \epsilon_{i + 1}\tilde \eta_{i + 1}
            \Vert \mathcal G_L(y_i)\Vert^2 
            &\le \delta_{i + 1}
            \\
            \iff 
            \epsilon_{i + 1}
            &\le 
            \frac{\delta_{i + 1} }{\tilde\eta_{t + 1}\Vert \mathcal G_L(y_i)\Vert^2}. 
        \end{align*}
        for any $\sum_{i = 0}^{T - 1}\delta_{i + 1}$ converges to a limit as $T \rightarrow \infty$. 

    \end{proof}

\section{Recovering existing variants of Nesterov accelerated gradient}\label{sec:recovery}
    In this section, we recovers several variants of the accelerated gradient in the literatures as a special case of 
    \hyperref[def:ag_prox_grad_generic]{Definition \ref*{def:ag_prox_grad_generic}}, 
    we sate the choice of stepsize parameter $\eta_i, \tilde \eta_i$, verify that they satisfies 
    \hyperref[thm:ag_generic_stepsize_constraints]
    {Theorem \ref*{thm:ag_generic_stepsize_constraints}}
    and derive their convergence rate respectively. 
    
    \subsection{Recovering FISTA} 
        Let $\epsilon_i$ be given by
        \hyperref[thm:ag_generic_stepsize_constraints]
        {Theorem \ref*{thm:ag_generic_stepsize_constraints}}, 
        setting $\epsilon_i = 0$ for all $i$ yields $a_t = 1 + L\eta_t = \tilde \eta_{t + 1}$, for all $t \in \N$ where $a_t$ produces the relations 
        \begin{align*}
            a_{t + 1} &= (1/2)\left(
                1 + \sqrt{1 + 4 c_{t + 1}}
            \right), 
            \\
            c_{t + 1} &= a_t^2. 
        \end{align*}
        The recurrenct relations of $a_t$ indicates that it's the famous Nesterov momentum sequence. 
        In this case, there exists a momentum form of the generic form and it reduces to FISTA from Beck and Toubolle \cite{beck_fast_2009-1}. 
        The following lemma uses $a_t$ to establish equivalence between the generic form of the algorithm and FISTA, a specific variant of accelerated gradient method. 
        \begin{lemma}[Similar triangle form]\label{lemma:sim_triangle_form}
            \quad \\
            With the choice of stepszie $\tilde \eta_{t + 1} = \eta_t + L^{-1}$,
            \hyperref[def:ag_prox_grad_generic]
            {Definition \ref*{def:ag_prox_grad_generic}}
            simplifies into 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1} \mathcal G_L(y_t)
                \\
                x_{t + 1} &= z_{t + 1} + L\eta_t (z_{t + 1} - z_t)
                \\
                y_{t + 1} &= 
                (1 + L\eta_{t + 1})^{-1}
                (
                x_{t + 1} + L\eta_{t + 1}z_{t + 1}
                ). 
            \end{align*}
            It is also equivalent to 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t)
                \\
                y_{t + 1} &= z_{t + 1} + (1 + L\eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t). 
            \end{align*}
            And this is the FISTA algorithm. 
        \end{lemma}

        \begin{proof}
            Firstly, we show that updates sequence $x_{t + 1} = z_{t + 1} + L\eta_t (z_{t + 1} - z_t)$ is equivalent to $x_{t + 1} = x_t + \tilde\eta_{t + 1}\nabla f(y_t)$. 
            Starting with the former, susbtitute $z_{t + 1} = y_t + L^{-1}\nabla f(y_t)$ as given by 
            \hyperref[def:ag_prox_grad_generic]{Definition \ref*{def:ag_prox_grad_generic}},
            expanding: 
            \begin{align*}
                x_{t + 1} &= 
                (y_t - L^{-1}\mathcal G_L(y_t)) 
                + L\eta_t ((y_t - \mathcal G_L(y_t)) - z_t)
                \\
                &= y_t - L^{-1}\mathcal G_L(y_t) 
                + L \eta_t y_t - \eta_t \mathcal G_L(y_t) - L\eta_t z_t
                \\
                &= 
                (1 + L\eta_t)y_t - (\eta_t + L^{-1})\mathcal G_L(y_t) - L\eta_t z_t
                \\
                &= \eta_t Lz_t + x_t -(\eta_t + L^{-1}) \mathcal G_L(y_t)  - L\eta_t z_t
                \\
                &= x_t - (\eta_t + L^{-1})\mathcal G_L(y_t). 
            \end{align*}
            So $x_{t + 1} = x_t + \tilde \eta_{t + 1}\mathcal G_L(y_t)$, by assumption $\tilde \eta_{t + 1} = \eta_t + L^{-1}$. 
            Next, consider the updates $y_{t + 1}$ as given by 
            \hyperref[def:ag_prox_grad_generic]{Definition \ref*{def:ag_prox_grad_generic}}
            \begin{align*}
                y_{t + 1} &= (1 + L\eta_{t + 1})^{-1} (x_{t + 1} + L\eta_{t + 1}z_{t + 1})
                \\
                &= (1 + L\eta_{t + 1})^{-1} (
                    z_{t + 1} + L\eta_t (z_{t + 1} - z_t) + L\eta_{t + 1} z_{t + 1}
                )
                \\
                &= 
                (1 + L\eta_{t + 1})^{-1} (
                    (1 + L\eta_{t + 1})z_{t + 1} + L\eta_t(z_{t + 1} - z_t)
                )
                \\
                &= z_{t + 1} + (1 + L\eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t), 
            \end{align*}
            it negates the $x_t$ variables, therefore we have 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1} \mathcal G_L (y_t)
                \\
                y_{t + 1} &= z_{t + 1} + (1 + L\eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t).
            \end{align*}
            This is FISTA by the relation $a_t = 1 + L\eta_t$ by setting $\epsilon_i = 0$ in
            \hyperref[thm:ag_generic_stepsize_constraints]
            {Theorem \ref*{thm:ag_generic_stepsize_constraints}}, 
            making $a_t$ the Nesterov momentum sequence. 
            Observe that $(1 + L\eta_{t + 1})^{-1}L\eta_t$ is $(a_t - 1)/a_{t + 1}$.
            Express using $a_t$, the above algorithm is equivalent to the FISTA algorithm. 
            \par
            By 
            \hyperref[thm:ag_generic_stepsize_constraints]{Theorem \ref*{thm:ag_generic_stepsize_constraints}}
            and 
            \hyperref[thm:generic_ag_convergence]{Theorem \ref*{thm:generic_ag_convergence}},
            a convergence of $\mathcal O(\sigma_T^{-1})$ is possible. 
            Since $a_t = L\tilde \eta_t$, and the Nesterov sequence exhibits $a_{t} \ge (t + 2)/2$, the convergence becomes $\mathcal O(T^{-2})$. 
            It is the optimal convergence rate for all convex functions. 
        \end{proof}
        \begin{remark}
            In this remark we clarify the name ``similar triangle" as given in the literatures. 
            We think it is a fitting name, becaues it has a similar triangle in it. 
            We list the following observations:
            \begin{enumerate}
                \item 
                The updates for $y_{t}$ from the algorithm has 
                $$
                    y_t = (1 + L\eta_t)^{-1} x_t + L\eta_t(1 + L\eta_t)^{-1} z_t, 
                $$
                Rearrainging yields: 
                $$
                    y_t - x_t = L\eta_t (z_t - y_t). 
                $$
                So vector $y_t - x_t$, $z_t - y_t$ colinear and points in the same direction by $L\eta_t > 0$. 
                \item 
                The updates for $z_{t + 1}, x_{t + 1}$ are based on $y_t, x_t$ displaced by $L^{-1} \mathcal G_L(y_t), \tilde\eta_{t +1} \mathcal G_L(y_t)$, therefore vector $z_{t + 1} - y_t$ parallels to $x_{t + 1} - x_t$. 
                \item 
                The updates for $x_{t + 1}$ has $x_{t + 1} - z_{t + 1} = L\eta_t \left(z_{t + 1} - z_t\right)$, therefore, the three points $z_t, x_{t + 1}, z_{t + 1}$ are collinear. 
            \end{enumerate}
            By these three observations, the triangle $z_{t}, z_{t + 1}, y_t$ similar to triangle $z_t, x_{t + 1}, x_t$. 
            See \hyperref[fig:1a]{Figure \ref*{fig:1a}}
            for an illustration. 
            \begin{figure}[h]
                \centering
                \subfloat[a][similar triangle]{
                    \includegraphics*[width=7cm]{assets/drawing_Layer 1.png}
                    \label{fig:1a}
                }
                \subfloat[b][similar triangle with errors]{
                    \includegraphics*[width=6cm]{assets/drawing_Layer 1 copy.png}
                    \label{fig:1b}
                }
                \caption{Similar triangle illustration}
            \end{figure}
            Finally, similar remarks about the similar triangle form can be found in Ahn, Sra's paper \cite{ahn_understanding_2022} as well. 
            Finally, recall from 
            \hyperref[thm:ag_generic_stepsize_constraints]
            {Theorem \ref*{thm:ag_generic_stepsize_constraints}}
            that $\epsilon_{t + 1} = \tilde \eta_{t + 1} - \eta_t - L^{-1}$, setting $\epsilon_{t + 1} =0$ for all $t \in \N \cup \{0\}$. 
            If we choose to incorperate $\epsilon_{t + 1}$, it yields: 
            \begin{align*}
                x_{t + 1} &= z_{t + 1} + L\eta_t(z_{t + 1} - z_t) - \epsilon_{t + 1}\mathcal G_L(y_t) = x_t - (\eta_t + L^{-1} + \epsilon_{t + 1})\mathcal G_L(y_t). 
            \end{align*}
            Where $\epsilon_{t + 1}\mathcal G_L(y_t)$ can be viewed as a perturbations from the similar triangle form of the algorithm. 
            See 
            \hyperref[fig:1b]{Figure \ref*{fig:1b}}
            for an illustration for some $\epsilon_{t + 1} > 0$. 
        \end{remark} 
    
    % \subsection{Recovering the accelerated gradient variant in Ryu's book}
    %     In this section we should that the following algorithm which appeared in Ryu \cite[chapter 12]{ryu_large-scale_2022}, the next definition present the algorithm in our notations. 
    %     \begin{definition}\label{def:ryu-ch12}
    %         Assume that $f$ is a L-Smooth convex function, with initial condition $x_0 = y_0$ an algorithm that generates iterates satisfying: 
    %         \begin{align*}
    %             z_{t + 1} &= y_t - L^{-1}\nabla f(y_t) 
    %             \\
    %             x_{t + 1} &= x_t - \frac{t + 1}{2L} \nabla f(y_t) 
    %             \\
    %             y_{t + 1} &=
    %             \left(
    %                 1 - \frac{2}{t + 1}
    %             \right)z_{t+ 1}
    %             + 
    %             \left(
    %                 \frac{2}{t + 2}
    %             \right)x_{t + 1}, 
    %         \end{align*}
    %         achieves an optimal convergence rate. 
    %     \end{definition}
        
    %     \begin{lemma}
    %         Suppose that the generic form as given by 
    %         \hyperref[def:ag_prox_grad_ppm]{Definition \ref*{def:ag_prox_grad_ppm}}
    %         admits stepsize $\tilde \eta_t, \eta_t$ as
    %         \begin{align*}
    %             \tilde \eta_{t} &= \frac{t}{2L},
    %             \\
    %             \eta_t &= (t - 1)/(2L). 
    %         \end{align*}
    %         Then it's algebraically equivalent to 
    %         \hyperref[def:ryu-ch12]{Definition \ref*{def:ryu-ch12}}. 
    %     \end{lemma}
    %     \begin{proof}
    %         Observe $\tilde \eta_t = t/(2L)$ admits a linear growth rate. 
    %         If $\tilde \eta_i, \eta_i$ is a sequence that satisfies
    %         \hyperref[thm:ag_generic_stepsize_constraints]
    %         {Theorem \ref*{thm:ag_generic_stepsize_constraints}}
    %         then 
    %         \hyperref[thm:generic_ag_convergence]
    %         {Theorem \ref*{thm:generic_ag_convergence}}
    %         asserts a convergence rate of $\mathcal O(1/T^2)$ for $h(z_T) - h(x_*)$, where $x_*$ is a minimizer assumed to exist. 
    %         To show that the sequence satisfies 
    %         \hyperref[thm:ag_generic_stepsize_constraints]
    %         {Theorem \ref*{thm:ag_generic_stepsize_constraints}}
    %         first observe that $\epsilon_t = 0$ because 
    %         \begin{align*}
    %             L^{-1} + \eta_t 
    %             &= 
    %             L^{-1} + \frac{t - 1}{2L}
    %             \\
    %             &= \frac{t + 1}{2L}
    %             \\
    %             &= \tilde \eta_{t + 1}. 
    %         \end{align*}
    %         Therefore this is a similar triangle method. 
    %         Next observe that conditions showed in 
    %         \hyperref[thm:ag_generic_stepsize_constraints]
    %         {Theorem \ref*{thm:ag_generic_stepsize_constraints}}
    %         can be simplified in this context because the two equalities reduces to just one when $\epsilon_t =0$ for all $t \in \N$ by 
    %         \begin{align*}
    %             \tilde \eta_{t + 1}(\tilde \eta_{t + 1} - L^{-1}) 
    %             - L^{-1}\sigma_t &= 0
    %             \\
    %             \iff 
    %             \tilde \eta_{t + 1}\eta_t &= L^{-1}\sigma_t. 
    %         \end{align*}
    %         Hence it only requires to verify the equality $\tilde \eta_{t + 1}\eta_t = L^{-1} \sigma_t$. 
    %         With that we have 
    %         \begin{align*}
    %             \sigma_t &= \sum_{i = 1}^{t} \frac{i}{2L}
    %             \\
    %             &= \frac{1}{2L}\left(
    %                 \frac{t(t + 1)}{2}
    %             \right)
    %             \\
    %             &= \frac{t(1 + t)}{4L}
    %             \\
    %             \iff 
    %             L^{-1}\sigma_t &= 
    %             \frac{t(1 + t)}{4L^2}
    %         \end{align*}
    %         By definition of the stepsizes the LHS of the equality evaluates to
    %         \begin{align*}
    %             \tilde \eta_{t + 1} \eta_{t + 1}
    %             &= \frac{t + 1}{2L} \frac{t}{2L} 
    %             \\
    %             &= L^{-1}\sigma_t. 
    %         \end{align*}
    %     \end{proof}
        
    % \subsection{Recovering Chambolle, Dossal 2015}
    %     In this section we show that the algorithm presented in the literature by Chambolle Dossal \cite{chambolle_convergence_2015} is an example of 
    %     Definition \ref*{def:ag_prox_grad_generic}. 
    %     The next definiton present the algorithm in our notations
    %     \begin{definition}\label{def:chamb_dossal}
    %         Assume that $h = f + g$ where $f$ is $L$-smooth, $f,g$ are convex. 
    %         Let $t_n = (n + a - 1)/a$ for all $t \in \N$, $a > 2$. 
    %         The algorithm by Chambolle Dossal has iterates $(x_n, y_n, z_n)$ satisfying for all $n \in \N$: 
    %         \begin{align*}
    %             z_{n + 1} &= y_n - L^{-1} \mathcal G_L(y_n), 
    %             \\
    %             x_{n + 1} &= z_n + t_{n +1} (z_{n + 1} - z_n), 
    %             \\
    %             y_{n + 1} &= \left(
    %                 1 - \frac{1}{t_{n + 1}}
    %             \right)z_{n + 1} + \left(
    %                 \frac{1}{t_{n + 1}}
    %             \right)x_{n + 1}. 
    %         \end{align*}
    %     \end{definition}
    %     \begin{lemma}
    %         Suppose that the generic form as given by 
    %         \hyperref[def:ag_prox_grad_ppm]{Definition \ref*{def:ag_prox_grad_ppm}}
    %         admits stepsize $\tilde \eta_t, \eta_t$ as
    %         \begin{align*}
    %             L\eta_n = t_n - 1,
    %             \\
    %             \tilde \eta_{n + 1} = L^{-1} + \eta_n. 
    %         \end{align*}
    %         Then it's algebraically equivalent to 
    %         \hyperref[def:chamb_dossal]{Definition \ref*{def:chamb_dossal}}. 
    %     \end{lemma}
    %     \begin{proof}
    %         By second equality: $\tilde \eta_{n + 1} = \eta_n + L^{-1}$, we can use 
    %         \hyperref[lemma:sim_triangle_form]{Lemma \ref*{lemma:sim_triangle_form}}. 
    %         Therefore, substituting $L\eta_n = t_n - 1$, $x_{n + 1}, y_{n + 1}$ in the generic form has 
    %         \begin{align*}
    %             x_{n + 1} &= 
    %             (1 + L\eta_n) z_{n + 1} - L\eta_n z_n
    %             \\
    %             & = 
    %             t_n z_{n + 1} + (1 - t_n) z_n 
    %             \\
    %             &= z_n + t_n(z_{n +1} - z_n)
    %             \\
    %             y_{n + 1} &= (1 + L\eta_{n + 1})^{-1} (L\eta_{n + 1} z_{n + 1} + x_{n + 1})
    %             \\
    %             &= t^{-1}_{n + 1} ((t_{n + 1} - 1)z_n + x_{n + 1})
    %             \\
    %             &= (1 - t_{n + 1}^{-1})z_{n + 1} + t^{-1}x_{n + 1}. 
    %         \end{align*}
    %         This is the same as
    %         \hyperref[def:chamb_dossal]{Definition \ref*{def:chamb_dossal}}. 
    %     \end{proof}
    %     \begin{remark}
            
    %     \end{remark}


\section{Algorithm implementations using line search}\label{sec:algorithm_improved}
    In this section, we state a new variant of the accelerated gradient algorithm using the PPM interpretation to algorithmically conduct a line search routine for $L, \mu \ge 0$ the Lipschitz constant and strong convexity index of the smooth part of the objective function. 
    This variant will be parameter free while still retaining the optimal convergence rate for all convex functions. 
    

\section{Numerical experiments}\label{sec:numerical_experiments}

\appendix
\section{Scratch paper stuff}
    Throughout this subsection, we make the following list of assumptions: 
    \begin{enumerate}
        \item $h = f + g$, 
        \item $f, g$ are convex functions, 
        \item $f$ is differentiable with $L$-Lipschitz smooth gradient and it's $\mu \ge 0$ strongly convex,
        \item Let $\mathcal T_L, \mathcal G_L$ be the proximal gradient and gradient mapping operator be given by \\
        \hyperref[def:gradient_mapping]{Definition \ref*{def:gradient_mapping}}. 
    \end{enumerate}

    \subsection{Nesterov momentum with strong convexity}
        \begin{definition}[generic S-CVX PPM form]\label{def:generic_s-cvx_form}
            Define the lower bouding function at $x$: 
            \begin{align*}
                l_h(z; x) = h(\mathcal T_L x) + \langle \mathcal G_L (x), z - x\rangle
                + 
                \frac{L}{2}\Vert x - \mathcal T_L (x)\Vert^2 + \frac{\mu}{2}\Vert z - x\Vert^2
            \end{align*}
            We define the following algorithm. 
            \begin{align*}
                x_{t + 1} &= \argmin_{x} \left\lbrace
                    l_h(x; y_t) + \frac{1}{2\tilde \eta_{t + 1}} 
                    \Vert x - x_t\Vert^2 
                \right\rbrace
                \\
                &= (\mu\tilde \eta_{t + 1} + 1)^{-1} 
                (\mu\tilde \eta_{t + 1}y_t + x_t - \tilde \eta_{t + 1}\mathcal G_L(y_t))
                \\
                y_{t + 1}&= 
                \argmin_{x}
                \left\lbrace
                    h(\mathcal T_L y_t) + \langle \mathcal G_L(y_t), x - y_t\rangle + \frac{L}{2}\Vert x -y_t\Vert^2
                    + \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
                \right\rbrace
                \\
                &= (1 + L \eta_{t +1})^{-1}(L\eta_{t + 1}(y_t - L^{-1}\mathcal G_L(y_t)) + x_{t + 1})
            \end{align*}
        \end{definition}
        \begin{proof}
            The functions inside of ``argmin" is easy to solve because they are just quadratic functions. 
            We write it here for future verifications and a peace of the mind. 
            \begin{align*}
                x_{t + 1} &= \argmin_{x}\left\lbrace
                    \langle \mathcal G_L(y_t), x - y_t\rangle 
                    + 
                    \frac{\mu}{2}\Vert x - y_t\Vert^2 +  
                    \frac{1}{2\tilde \eta_{t + 1}}\Vert x - x_t\Vert^2
                \right\rbrace
                \\
                \iff 
                \mathbf 0 & = 
                \mathcal G_L(y_t) + \mu(x - y_t) + \tilde \eta_{t + 1}^{-1}(x - x_t)
                \\
                &= 
                \mathcal G_L(y_t) + (\mu + \tilde \eta_{t + 1}^{-1}) x - \mu y_t - \tilde \eta_{t + 1}^{-1} x_t
                \\
                \iff 
                (\mu + \tilde \eta_{t + 1}^{-1})x 
                &= 
                \mu y_t + \tilde \eta_{t + 1}^{-1} x_t - \mathcal G_L(y_t)
                \\
                \implies 
                x &= (\mu + \tilde \eta_{t + 1}^{-1})^{-1 }
                (\mu y_t + \tilde \eta_{t + 1}^{-1} x_t - \mathcal G_L(y_t)). 
            \end{align*}
            We can make the assumption that $\mu + \eta_{t + 1}^{-1} \neq 0$ because $\tilde\eta_t > 0$. 
            Similarly for $y_{t + 1}$, it's solving a simple quadratic minimization problem, yielding: 
            \begin{align*}
                \mathbf 0 &= \mathcal G_L(y_t) + L(x - y_t) + \eta_{t + 1}^{-1}(x - x_{t + 1})
                \\
                &= (L + \eta_{t + 1}^{-1})x - L y_t - \eta_{t + 1}^{-1}x_{t + 1} + \mathcal G_L(y_t) 
                \\
                (L + \eta_{t + 1}^{-1})x &= 
                Ly_t + \eta_{t + 1}^{-1} x_{t + 1} - \mathcal G_L(y_t)
                \\
                \implies 
                x &= 
                (L\eta_{t + 1} + 1)^{-1}(L\eta_{t + 1}(y_t - L^{-1}\mathcal G_L(y_t)) + x_{t + 1}). 
            \end{align*}
            And hence the results are verified for the peace of the mind. 
        \end{proof}
        \begin{remark}\label{remark:generic_s-cvx_ppm_alternative_representation}
            In the literatures, people accenturate the term $z_{t + 1} = y_t - L^{-1} \mathcal G_L(y_t)$, as a step of gradient descent, and the term $w_t = (\mu\tilde \eta_{t + 1} + 1)^{-1}(\mu\tilde \eta_{t + 1}y_t + x_t)$ so the algorithm can be alternatively presented by the relation 
            \begin{align*}
                w_{t} &= (\mu\tilde \eta_{t + 1} + 1)^{-1}(\mu \tilde \eta_{t + 1} y_t + x_t) 
                \\
                x_{t + 1}&= w_t - \tilde \eta_{t + 1}(\mu\tilde \eta_{t + 1} + 1)^{-1} \mathcal G_L(y_t)
                \\
                z_{t + 1}&= y_t - L^{-1}\mathcal G_L(y_t)
                \\
                y_{t + 1} &= (1 + L\eta_{t + 1})^{-1}(L\eta_{t + 1}z_{t + 1} + x_{t + 1}). 
            \end{align*}
            When $\mu = 0$, it has $w_t = x_t$, producing a simpler relations 
            \begin{align*}
                x_{t + 1} &= 
                x_t - \tilde \eta_{t + 1} \mathcal G_L(y_t)
                \\
                z_{t + 1}&= y_t - L^{-1}\mathcal G_L(y_t)
                \\
                y_{t + 1} &= (1 + L\eta_{t + 1})^{-1}
                (L\eta_{t + 1} z_{t + 1} + x_{t + 1}). 
            \end{align*}
            The above representation is the same as 
            \hyperref[def:ag_prox_grad_generic]{Definition \ref*{def:ag_prox_grad_generic}}. 

        \end{remark}
        
        \begin{definition}[Similar triangle form]\label{def:s-cvx_similar_triangle_form}
            Let $\mathcal G_L, \mathcal T_L$ be the gradient mapping and proixmal gradient operator of $h = f + g$. 
            An algorithm is referred to as similar triangle form if its iterates $(x_t, y_t, z_t)$ satisfies the conditions: 
            \begin{align*}
                z_{t + 1} &= 
                y_t - L^{-1}\mathcal G_L(y_t)
                \\
                x_{t + 1}&= 
                z_{t + 1} + \frac{L\eta_t}{1 + \mu\tilde \eta_{t + 1}}(z_{t + 1} - z_t)
                \\
                y_{t + 1}&= 
                (1 + L\eta_{t + 1})^{-1} (L\eta_{t + 1}z_{t + 1} + x_{t + 1}). 
            \end{align*}
        \end{definition}

        \begin{definition}[Generic momentum form]\label{def:generic_momentum}
            Let $\mathcal G_L, \mathcal T_L$ be the gradient mapping and the proximal gradient operator for $h$. 
            Then the generic momentum form is an algorithm with iterates $(z_t, y_t)$ satisfying the relations 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t)
                \\
                y_{t + 1 } &= z_{t + 1} + \theta_{t + 1}(z_{t + 1} - z_t)
            \end{align*}
            For some $\theta_t \ge 0$. 
        \end{definition}
        \begin{remark}
            Sometimes in the literates the update is expressed differently and it's 
            \begin{align*}
                y_{t + 1} &= z_t + (1 + \theta_{t + 1})(z_{t + 1} - z_t). 
            \end{align*}
        \end{remark}

        \begin{theorem}[Momentum is algebraically equivalent to similar triangle]
        \label{thm:momentum_is_similar_triangle}
            \;\\
            Let $\tilde \eta_t, \eta_t$
            satisfies 
            \begin{align*}
                \tilde\eta_{t + 1} &= \eta_t + L^{-1} + L^{-1} \mu \tilde\eta_{t + 1},
            \end{align*}
            then
            \hyperref[def:generic_s-cvx_form]{Definition \ref*{def:generic_s-cvx_form}}
            reduces into similar triangle form which is given by 
            \hyperref[def:s-cvx_similar_triangle_form]
            {Definition \ref*{def:s-cvx_similar_triangle_form}}, 
            which is also algebraically equivalent 
            \hyperref[def:generic_momentum]{Definition \ref*{def:generic_momentum}}
            and it's presented as:
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t), 
                \\
                y_{t + 1} &= z_{t + 1} + 
                \frac{L\eta_t}{(1 + \mu \tilde\eta_{t + 1})(1 + L\eta_{t + 1})}(z_{t + 1} - z_t)
            \end{align*}
        \end{theorem}
        \begin{proof}
            We work with the alternative representation listed in 
            \hyperref[remark:generic_s-cvx_ppm_alternative_representation]
            {Remark \ref*{remark:generic_s-cvx_ppm_alternative_representation}} which has iterates $(w_t, x_{t + 1}, z_{t + 1}, y_{t + 1})$. 
            We start by showing that there exists a constant $\alpha \in \RR$ such that $z_{t + 1} - z_t = \alpha (x_{t + 1} - z_{t + 1})$ by $\tilde \eta_{t + 1} = \eta_t + L^{-1} + L^{-1} \mu \tilde \eta_{t + 1}$. 
            Firstly, we have the equality. 
            \begin{align*}
                z_{t + 1} - z_t
                &= 
                - (L\eta_t)^{-1} y_t 
                - L^{-1}\mathcal G_L(y_t) + (L \eta_t)^{-1} x_t. 
            \end{align*}
            Because 
            \begin{align*}
                y_{t} &= (1 + L\eta_{t})^{-1}(L\eta_{t}z_{t} + x_{t})
                \\
                (1 + L\eta_t)y_t - x_t &= L\eta_t z_t
                \\
                z_t & = (L\eta_t)^{-1}((1 + L\eta_t)y_t - x_t), 
                \\[1em]
                z_{t + 1} - z_t 
                &= \underbrace{ y_t - L^{-1}\mathcal G_L(y_t)}_{=z_{t + 1}}
                - \underbrace{(L\eta_t)^{-1}((1 + L\eta_t)y_t - x_t)}_{=z_t}
                \\
                &= 
                y_t - L^{-1} \mathcal G_L(y_t) - (L\eta_t)^{-1}y_t - y_t + (L\eta_t)^{-1} x_t
                \\
                &= 
                -L^{-1}\mathcal G_L(y_t) + (L\eta_t)^{-1}(x_t - y_t)
                \\
                &= 
                L^{-1}(\eta_t^{-1}(x_t - y_t) -\mathcal G_L(y_t)). 
            \end{align*}
            Next, we have 
            \begin{align*}
                x_{t + 1} - z_{t + 1}&= 
                \left(
                    (1 + \mu \tilde \eta_{t+ 1})^{-1} (\mu \tilde \eta_{t + 1}y_t + x_t)
                    - \frac{\tilde \eta_{t + 1}}{1 + \mu\tilde \eta_{t + 1}}
                    \mathcal G_L(y_t)
                \right) - \left(
                    y_t - L^{-1}\mathcal G_L(y_t)
                \right)
                \\
                &= 
                (1 + \mu \tilde \eta_{t + 1})^{-1}
                \left(
                    x_t + \mu \tilde \eta_{t + 1} y_t
                    - \tilde \eta_{t + 1} \mathcal G_L(y_t)
                    - (1 + \mu \tilde \eta_{t + 1})
                    (y_t - L^{-1}\mathcal G_L(y_t))
                \right)
                \\
                &= 
                (1 + \mu\tilde \eta_{t + 1})^{-1}
                \left(
                    x_t - y_t + 
                    (
                        -\tilde \eta_{t + 1} + 
                        (
                            1 + \mu\tilde \eta_{t + 1})L^{-1}
                        )
                        \mathcal G_L(y_t)
                    )
                \right)
                \\
                &= 
                (1 + \mu\tilde \eta_{t + 1})^{-1}
                \left(
                    x_t - y_t +     
                    (
                        - \tilde \eta_{t + 1} + L^{-1}
                        + \mu \tilde \eta_{t + 1}L^{-1}
                    )
                    \mathcal G_L(y_t)
                \right). 
            \end{align*}
            Since 
            \begin{align*}
                (1 - L^{-1}\mu)\tilde \eta_{t +1}
                &= L^{-1} + \eta_t 
                \\
                - \tilde \eta_{t + 1} + L^{-1}\mu \tilde \eta_{t + 1}
                + L^{-1}
                &= - \eta_t, 
            \end{align*}
            so substituting 
            \begin{align*}
                x_{t + 1} - z_{t + 1}
                &= 
                (1 + \mu \tilde \eta_{t + 1})^{-1}
                (x_t - y_t - \eta_t \mathcal G_L(y_t))
                \\
                &= (1 + \mu \tilde \eta_{t + 1})^{-1}
                \eta_t(\eta_{t}^{-1}(x_t - y_t) - \mathcal G_L(y_t))
                \\
                &= (1 + \mu \tilde \eta_{t + 1})^{-1}
                \eta_t L(z_{t + 1} - z_t), 
            \end{align*}
            therefore 
            \begin{align*}
                z_{t + 1} - z_t 
                &= 
                \eta^{-1}_tL^{-1}(1 + \mu \tilde \eta_{t + 1})(x_{t + 1} - z_{t + 1})
                \\
                \iff 
                x_{t + 1} - z_{t + 1} &= 
                \frac{L\eta_t}{1 + \mu \tilde \eta_{t + 1}} 
                (z_{t + 1} - z_t). 
            \end{align*}
            This produces the new updates for $x_{t +1}$, making it the same as the stated similar triangle form as in 
            \hyperref[def:s-cvx_similar_triangle_form]
                {Definition \ref*{def:s-cvx_similar_triangle_form}}. 
            To attain momentum form, we consider the equation for $y_{t + 1}$, and susbtitute $x_{t + 1}$ producing 
            \begin{align*}
                y_{t + 1} &= (1 + L\eta_{t + 1})^{-1}
                (
                    L\eta_{t + 1} z_{t + 1} + x_{t + 1}
                )
                \\
                &= 
                (1 + L\eta_{t + 1})^{-1}
                \left(
                    L\eta_{t + 1} z_{t + 1} + z_{t + 1}
                    + 
                    \frac{L\eta_t}{1 + \mu \tilde \eta_{t + 1}}
                    (z_{t + 1} - z_t)
                \right)
                \\
                &= z_{t + 1} 
                + 
                \frac{L\eta_t}{(1 + L\eta_{t + 1})(1 + \mu \tilde \eta_{t + 1})}
                (z_{t + 1} - z_t). 
            \end{align*}
            The above exposes the 
            $\theta_{t + 1} = L\eta_t(1 + L\eta_{t + 1})^{-1}(1 + \mu\tilde \eta_{t + 1})^{-1}$. 
            Alternatively we also have: 
            \begin{align*}
                x_{t + 1} &= z_{t + 1} + (1 + \mu\tilde \eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t)
                \\
                &=  (1 + L\eta_t(1 + \mu \tilde \eta_{t + 1})^{-1})z_{t + 1}
                - L\eta_t(1 + \mu\tilde \eta_{t + 1})z_t
                \\
                &= 
                \left(
                    \frac{1 + \mu\tilde \eta_{t + 1} + L \eta_t}{1 + \mu \tilde \eta_{t + 1}}
                \right)z_{t + 1}
                + 
                z_t - 
                \left(
                    1 + \frac{ L\eta_t}{1 + \mu \tilde \eta_t + 1}
                \right)z_t 
                \\
                &= 
                \left(
                    \frac{L\tilde \eta_{t+ 1}}{1 + \mu\tilde \eta_{t + 1}}
                \right)z_{t + 1}
                + 
                z_t 
                - 
                \left(
                    \frac{L\tilde \eta_{t + 1}}{1 + \mu \tilde \eta_{t + 1}}
                \right)z_t 
                \\
                &= z_t + 
                \left(
                    \frac{L\tilde \eta_{t + 1}}{1 + \mu \tilde \eta_{t + 1}}
                \right)
                (z_{t + 1} - z_t). 
            \end{align*}
        \end{proof}



    \subsection{The line search algorithm idea}
        \par
        The Nesterov acceleration algorithm can not achieve linear convergence rate for function that has a strongly convexity constant $\mu > 0$. 
        This claim is showed in Aujol et.al \cite{aujol_optimal_2019}. 
        Variants of the Nesterov accelerated gradient method that can achieve linear rate of convergence exists but it requires knowledge of $\mu, L$ a priori. 
        \par
        Literatures attempts at improving the convergence of FISTA under strong convexity, and other characterizations are vast. 
        But it can be approximately categorized as two types: 
        \begin{enumerate}
            \item Heuristic regarding restarting the algorithm. 
            \item Line search and estaimating the parameters: $L, \mu$. 
        \end{enumerate}
        Beck and Toubolle \cite{beck_fast_2009} proposed the idea of line search for the Lipschitz constant and restarting the algorithm based on function value for assure monotone descending property. 
        Work by Su et.al \cite{su_differential_2015} proposed an ODE understanding and suggested provable linear convergence rate of Nesterov accelerated gradient algorithm under the presence of strong convexity. 
        The ODE understandings provoked interest in the restart techniques of AG/FISTA and the approaches are diverse and fruitful. 
        For example Aujol et.al \cite{aujol_parameter-free_2023,aujol_fista_2022}, Luca et.al \cite{calatroni_backtracking_2019} proposed methods of restarting the FISTA algorithm with an estimate to the strong convexity constant. 
        They proved the convergence rate in $\bar L$ and $\rho$ which are the estimates of Lipschitz constant $L$ through out the algorithm and $\rho \in (0, 1)$ another parameter that assist with the search for strong convexity constant $\mu$. 


        \par
        Let $h = f + g$ where $f, g$ are convex and $f$ is $L$-Lipschitz and $\mu\ge 0$ strongly convex. 
        List in Beck \cite[(10.7.7)]{beck_first-order_nodate}, the V-FISTA algorithm is: 
        \begin{align*}
            z_{n + 1} &= \mathcal T_L(y_n)
            \\
            y_{n + 1} &= z_{n + 1} + \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}(z_{n + 1} - z_n). 
        \end{align*}
        $\kappa = L/\mu$ is the condition number. 
        The parameters $L, \mu$ can be estimated during the run time of the algorithm. 
        To establish the estimation we first match the parameters $\tilde \eta_i, \eta_i$ to the momentum parameter presented in V-FISTA. 
        Then we perform the estimation of $\tilde \eta_i, \eta_i$ using the PPM interpretation for the parameter $L, \mu$. 
        The following theorem will reduce the generic S-CVX form of the PPM interpretations into the V-FISTA algorithm. 
        \begin{theorem}[V-FISTA algorithm as similar triangle]
            \;\\
            If
            \begin{align*}
                \tilde \eta_t 
                &= \frac{1}{\mu(\sqrt{\kappa} - 1)}
                \quad \forall t \in \N, 
                \\
                \eta_t
                &= 
                \frac{1}{\mu\sqrt{\kappa}}
                \quad \forall t \in \N. 
            \end{align*}
            Then the AG generic form simplifies to 
            \begin{align*}
                y_{t + 1} &= z_{t + 1} + 
                \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}
                (z_{t +1} - z_t)
                \\
                z_{t + 1} 
                &= y_t - L^{-1}\mathcal G_L(y_t). 
            \end{align*}
            And it is a valid sequence that meaning that it satisfies 
            \begin{align*}
            \tilde \eta_{t + 1} = \eta_t + L^{-1} + L^{-1} \mu \tilde \eta_{t + 1}. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Observe that we have 
            \begin{align*}
                L\eta &= \frac{L}{\mu \sqrt{\kappa}} = \frac{\kappa}{\sqrt{\kappa}} = \sqrt{\kappa}, 
                \\
                \mu \tilde \eta &= 
                \frac{1}{\sqrt{\kappa} - 1}, 
                \\
                L\tilde \eta_t &= 
                \frac{\kappa}{\sqrt{\kappa} - 1}. 
            \end{align*}
            since it's a constant wrt to $t$, we use $\eta, \tilde \eta$ to ease the notations. 
            With that it establishes relations
            \begin{align*}
                \frac{L\eta }{(1 + \mu \tilde \eta)(1 + L\eta)}
                &= 
                \frac{\sqrt{\kappa}}{
                    \left(
                        1 + \frac{1}{\sqrt{\kappa} - 1}
                    \right)
                    \left(
                        1 + \sqrt{\kappa}
                    \right)
                }
                \\
                &= \frac{\sqrt{\kappa}}{
                    \left(
                        \frac{\sqrt{\kappa}}{\sqrt{\kappa} - 1}
                    \right)(1 + \sqrt{\kappa})
                }
                \\
                &=
                \frac{\sqrt{\kappa}}{1 + \sqrt{\kappa}}\left(
                    \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa}}
                \right)
                \\
                &= 
                \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}. 
            \end{align*}
            The sequence is valid because 
            \begin{align*}
                \tilde \eta_{t + 1} 
                &= \eta_t + L^{-1} + L^{-1} \mu \tilde \eta_{t + 1}
                \\
                (L - \mu)\tilde \eta_{t + 1}
                &= 
                1 + L \eta_t 
                \\
                L \tilde \eta_{t + 1} - 
                \mu \tilde \eta_{t + 1}
                &= 1 + L \eta_t. 
            \end{align*}
            Starting from the LHS it yields: 
            \begin{align*}
                L\tilde \eta - \mu \tilde \eta 
                &= \frac{\kappa}{\sqrt{\kappa} - 1} - 
                \frac{1}{\sqrt{\kappa} - 1}
                \\
                &= 
                \frac{\kappa - 1}{\sqrt{\kappa} - 1}
                \\
                &= 
                \frac{(\sqrt{\kappa} + 1)(\sqrt{\kappa} - 1)}{\sqrt{\kappa} - 1}
                \\
                &= 1 + \sqrt{\kappa} = 1 + L \eta. 
            \end{align*}
        \end{proof}
        \par
        Therefore, when we have strong convexity constant $\mu > 0$ and Lipschitz parameters $L$, we have a fixed stepsizes $\tilde \eta, \eta$ for the generic form of the algorithm. 
        When strong convexity doesn't exist locally during the computations of the algorithm, we have to resolve it back to the case when $\mu = 0$. 
        To do that we have to match the parameters $\tilde \eta_i, \eta_i$ to the Nesterov's momentum used in FISTA. 
        Proposed by Chambolle, Dossal \cite{chambolle_convergence_2015}, we have the following representation of the Nesterov accelerated gradient: 
        \begin{definition}[FISTA]\label{def:fista}
            The FISTA algorithm has iterates such that they satisfy the following conditions recursively: 
            \begin{align*}
                z_{n + 1} &= y_n - L^{-1}\mathcal G_L(y_n)
                \\
                x_{n + 1} &= z_n + t_{n +1} (z_{n + 1} - z_n), 
                \\
                y_{n + 1} &= \left(
                    1 - \frac{1}{t_{n + 1}}
                \right)z_{n + 1} + \left(
                    \frac{1}{t_{n + 1}}
                \right)x_{n + 1}
            \end{align*}
            where $(t_n)_{n\in \N}$ satisfies: $t_{n + 1}^2 - t_n^2 \le t_n$. 
        \end{definition}
        \begin{observation}
            Observe the fact that the updates of the iterates resemble the similar triangle form in
            \hyperref[def:ag_prox_grad_generic]{Definition \ref*{def:ag_prox_grad_generic}}
        \end{observation}
        \begin{theorem}[FISTA as a special case of similar triangle form]
        \label{thm:fista_spacial_case_of_similar_triangle}
            \;\\
            if the stepsize $L\eta_n = t_n - 1$, and $\tilde \eta_{n + 1} = L^{-1} + \eta_n$, then
            \hyperref[def:fista]{Definition \ref*{def:fista}}
            is an example of 
            \hyperref[def:ag_prox_grad_generic]{Definition \ref*{def:ag_prox_grad_generic}}, 
        \end{theorem}
        \begin{proof}
            By statement hypothesis we have $\tilde \eta_{n + 1} = L^{-1} + \eta_n$, using 
            \hyperref[thm:momentum_is_similar_triangle]
            {Theorem \ref*{thm:momentum_is_similar_triangle}}
            by setting $\mu = 0$, then by similar triangle form the update for $x_{n + 1}$ is 
            \begin{align*}
                x_{n + 1} &= z_{n + 1} + L\eta_t(z_{n +1} - z_n)
                \\
                &= z_{n + 1} + (L\tilde \eta_{n + 1} - 1)(z_{n + 1} - z_t)
                \\
                &= z_n + L\tilde \eta_{n + 1}(z_{n + 1} - z_n)
                \\
                &= z_n + t_n(z_{n + 1} - z_n).
            \end{align*}
            At the last line it's simply because $L\tilde \eta_{n + 1} = 1 + L\eta_n = t_n$. 
            Similarly for the updates for $y_{n + 1}$ we would have: 
            \begin{align*}
                y_{n + 1} &= \left(
                    \frac{L\eta_{n + 1}}{1 + L \eta_{n + 1}}
                \right)z_{n + 1} + 
                \left(
                    \frac{1}{1 + L \eta_{n + 1}}
                \right) x_{n + 1}
                \\
                &= 
                \left(
                    1 - \frac{1}{1 + L\eta_{n + 1}}
                \right)z_{n + 1} + 
                \left(
                    \frac{1}{1 + L \eta_{n + 1}}
                \right)x_{n + 1}
                \\
                &= (1 - t_{n + 1}^{-1})z_{n + 1} + t_{n + 1}^{-1}x_{n + 1}. 
            \end{align*}

        \end{proof}
        \par
        Define the $l_h$ to be the lower bounding function for $h$ parameterized by any fixed $x\in \RR^n$ as a function of $y$: 
        \begin{align*}
            l_h(z; x, \mu)&=  h(\mathcal T_Lx) + \langle \mathcal G_L, z- x\rangle + \frac{L}{2}\Vert x - \mathcal T_Lx\Vert^2 + 
            \frac{\mu}{2}\Vert z - x\Vert^2
        \end{align*}
        Let the sequence $\theta_n$ be a momentum sequence such that it satisfies for all $n\in \N$: 
        \begin{align*}
            \theta_{n + 1}^2 - t_n^2 
            &\le \theta_{n + 1}
            \\
            \theta_n &\ge \frac{n + 1}{2}.
        \end{align*}
        Define $\mathcal T_L^{h}, G_L^h$ to be the proximal gradient and gradient mapping operator for function $h = f + g$. 
        Then consider the following algorithmic routine that estimates the strong convexity constant during its executions: 
        
        \begin{algorithm}
            \caption{Routine 1}\label{alg:routine1}
            \begin{algorithmic}[1]
                \Procedure{Routine1}{$L, \tilde \mu,h=f+g, (x_i, y_i), \epsilon > 0, N\in N, i \in \N$}
                    \State \textbf{Initialize: } $\eta_i = 0, \tilde \eta_{i + 1} = \eta_i + L^{-1} + \tilde \mu L^{-1} \eta_t$
                    \State $\tilde x \gets (\tilde \mu\tilde \eta_{i + 1} + 1)^{-1}(\tilde \mu \tilde \eta_{i + 1}y_i + x_i - \tilde \eta_{i + 1}\mathcal G_l(y_t))$
                    \If{$\exists \tilde \mu \in [\epsilon, L]: l_h(\tilde x, x_t, \tilde \mu) \le h(\tilde x)$ }
                        \State \textbf{find } $\tilde \mu \in [\epsilon, L]$ s.t: $l_h(\tilde x, x_t, \tilde \mu) \le h(\tilde x)$  
                        \State \textbf{updates} $\tilde \mu$ accordingly. 
                        \Comment{Use binary search}
                        \For{$j = i, \cdots, N$} 
                            \State $\tilde \eta_{t + j} \gets (\sqrt{L\tilde \mu} - \tilde \mu)^{-1}$
                        \EndFor
                    \Else \Comment{$\tilde \mu = 0$}
                        \State $\tilde \mu \gets 0$
                        \For{$j = i, \cdots, N$}
                            \State $\tilde \eta_{t + i} \gets \theta_{j}L^{-1}$
                        \EndFor
                    \EndIf
                    \For{$t = i, \cdots, N$}
                        \State $\eta_t \gets \tilde \eta_{t + 1} - L^{-1} - L^{-1}\mu\tilde\eta_{t + 1}$
                        \State $z_{t + 1} \gets y_t - L^{-1}\mathcal G_L(y_t)$
                        \State $x_{t + 1} \gets (\tilde \mu\tilde \eta_{t + 1} + 1)^{-1}(\tilde \mu \tilde \eta_{t + 1}y_t + x_t - \tilde \eta_{t + 1}\mathcal G_l(y_t))$
                        \State $y_{t + 1} \gets (1 + L\eta_{t + 1})^{-1}(L\eta_{t + 1}z_{t + 1} + x_{t + 1})$
                    \EndFor
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
        In above 
        \hyperref[alg:routine1]
            {Routine \ref*{alg:routine1}}
        we have 
        \begin{enumerate}
            \item $\epsilon$ is the tolerance for line search to terminate if it can't find any such value. 
            \item $(x_i, y_i)$ be any two iterates. 
            \item $\tilde \mu$ is any estimate of the strong convexity constant. It should be less than $L$ which is the true Lipschitz smoothness. 
            \item $i\in \N$ is an index that indicates $(x_i, y_i)$ could already be generated by Routine 1 in prior calls to the Routine 1. 
        \end{enumerate}
        Routine 1 estimates $\mu$ then performs accelerated gradient, the strong convexity index locally it then determine the stepsize for $\tilde \eta_t, \eta_t$ to fit either the traditional FISTA algorithm, or their strong convexity variants. 
        It executes for a fixed number of steps $N$ and it repeats periodically to check and update the strong convexity parameter. 
        However, there are many alternative ways of determining the strong convexity index $\mu$ while performing AG. 
        When evaluating function value is costly, alternatves that doesn't involve the use of function will accelerates the algorithm. 
        We propose the following method of estimating the strong convexity constant without using the function value. 
        Consider any fixed $x, y \in \RR^n$, then 
        \begin{align*}
            \tilde \mu = 
            \frac{
                \langle \mathcal G_L(x) - \mathcal G_L(y), x - y\rangle
            }{\Vert x - y\Vert^2}
        \end{align*}
        provides an estimate. 
        
        
        



\section{Postoned proof}
    \begin{fact}[Equivalent characterization of strong convexity]
    \label{appendix:fact:equiv_char_s-cvx}
        $f$ is $\beta$-strongly convex if and only if
        $$
            (\forall y\in X)(\forall x^* \in \partial f(x))
            \quad f(y) \ge f(x) + \langle x^*, y - x\rangle +
            \frac{\beta}{2}\Vert y - x\Vert^2
            .
        $$
    \end{fact}

    \begin{theorem}[Quadratic growth]\label{appendix:thm:q_growth}
        If $f$ is $\beta$-strongly convex, let $x^*$ be a minimizer of $f$ then 
        \begin{align*}
            (\forall y \in X) \quad 
            f(y) &\ge 
            f(x) + \frac{\beta}{2}\Vert y - x\Vert^2. 
        \end{align*}
    \end{theorem}
    \begin{proof}
        Fact \ref*{appendix:fact:equiv_char_s-cvx} 
        implies a quadratic growth condition. 
        By convexity $x^*$ is a minimizer of $f$ if and only if $x^*= \mathbf 0 \in \partial f(x^*)$, therefore it sets the inner product on the RHS to zero which yields the quadratic growth condition. 
    \end{proof}

    \begin{theorem}[strongly convex PPM descent lemma]
    \label{appendix:proof_ppm_descent_lemma}
        Let $f: \RR^n \mapsto \overline \RR^n$ $\beta$ be strongly convex with $\beta \ge 0$, fix any $x \in \RR^n$, define $p = \hprox_f(x)$.
        For all $y \in \RR$ it verifies
        $$
            \left(f(p) + \frac{1}{2}\Vert x - p\Vert^2\right)
            - 
            \left(
                f(y) + \frac{1}{2}\Vert x - y\Vert^2 
            \right)
            \le 
            - \frac{(1 + \beta)}{2}\Vert y - p\Vert^2. 
        $$
        Recall: $\hprox_f(x) = \argmin_{u}\left\lbrace f(u) + \frac{1}{2}\Vert u - x\Vert^2 \right\rbrace$. 
    \end{theorem}
    \begin{proof}
        With $x$ being fixed,
        consider the function
        $$
            y\mapsto h(y)=f(y)+\frac{\|y-x\|^2}{2}=f(y)+\frac{\|y\|^2}{2}-\scal{y}{x}+\frac{\|x\|^2}{2}.
        $$
        Because $f$ is $\beta$-strongly convex, the function
        $y\mapsto f(y) + \frac{1}{2}\Vert y\Vert^2$ is $(\beta + 1)$-strongly convex.
        Then $h$ is $(\beta + 1)$-strongly convex.
        Since $(\forall y)\; h(y) \ge h(p)$, because $p = \hprox_f(x)$ 
        \hyperref[appendix:thm:q_growth]{Theorem \ref*{appendix:thm:q_growth}}
        (quadratic growth)
        yields
        \begin{align*}
            h(y) &\ge
            h(p) + \frac{\beta + 1}{2}\Vert y - p\Vert^2.
        \end{align*}
        That is,
        \begin{align*}
            f(y) + \frac{1}{2}\Vert y - x\Vert^2
            \ge
            f(p) + \frac{1}{2} \Vert p - x\Vert^2
            + \frac{\beta + 1}{2}\Vert y - p\Vert^2.
        \end{align*}
        Rearraging yield the desired results. 
    \end{proof}


% \printbibliography

\bibliographystyle{siam}
\bibliography{references/refs}

\end{document}
