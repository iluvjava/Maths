\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont 
        A Parameter Free Accelerated Proximal Gradient Method Without Restarting
    }}

\author{
    Hongda Li
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~ and~Xianfu Wang
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    
    \cite{nesterov_lectures_2018}
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 65K10, 90c25, 90C30; Secondary 65Y20. 
\noindent{\bfseries Keywords: } Nesterov acceleration, Proximal point method. 

\section{Preliminaries}
    Throughout, assume problem of types $F = f + g$ where $f: \RR \rightarrow \RR$ is $L$ Lipschitz smooth and $\mu \ge 0$ strongly convex and $g: Q \rightarrow \overline \RR$ is convex. 
    We consider optimization problem of the form
    \begin{align*}
        \min_x \left\lbrace
            F(x) \defeq f(x) + g(x)
        \right\rbrace. 
    \end{align*}
    To start we define the following quantities
    \begin{enumerate}
        \item The proximal gradient model function: 
        $$
            \widetilde{\mathcal M}^{L^{-1}}
            (x; y) \defeq
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle 
            + \frac{L}{2}\Vert x - y\Vert^2. 
        $$
        \item The proximal point model function: 
        $$
            \mathcal M^{L^{-1}}(x; y) := F(x) + \frac{L}{2}\Vert x - y\Vert^2
        $$
        \item The proximal gradient operator $T_L = [I + L^{-1}\partial g]\circ [I - L^{-1}\nabla f]$. Since $L$ is fixed throughout, the subscript may be omitted and become $T$ instead to make the notations simpler. 
    \end{enumerate}
    To state the following lemma, we define the Bregman divergence of $f$ to be 
    \begin{align*}
        D_f(x, y): \RR^n \times \RR^n \rightarrow \RR 
        \defeq f(x) - f(y) - \langle \nabla f(x), x - y\rangle. 
    \end{align*}
    
    \begin{lemma}[Proximal gradient envelope] 
        Fix any $y$, we will have for all $x \in \RR^n$ that: 
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y)
            &= 
            \mathcal M^{L^{-1}}(x; y)- D_f(x, y) \ge \mathcal M^{L^{-1}}(x; y). 
        \end{align*}
    \end{lemma}
    \begin{proof}
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) 
            &= 
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            g(x) + f(x) - f(x) + f(y) 
            + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            F(x) - D_f(x, y) + \frac{L}{2}\Vert x - y\Vert^2 
            \\
            &= \mathcal M^{L^{-1}}(x; y) - D_f(x, y). 
        \end{align*}
    \end{proof}
    
    \subsection{Proximal inequality}
    \begin{theorem}[Proximal inequality]\label{thm:prox-grad-ineq}
        Fix any $y$, we have for all $x$: 
        \begin{align*}
            F(x) - F(Ty) - \langle L(y - Ty), x - Ty\rangle
            &\ge  D_f(x, y). 
        \end{align*}
    \end{theorem}
    \begin{proof}
        $\widetilde {\mathcal M}(\cdot; x)$ is $L$ strongly convex with minimizer $Ty$
        {\small
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) - 
            \widetilde{\mathcal M}^{L^{-1}}(Ty; y)
            - 
            \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 
            0
            \\
            \iff
            \left(
                \mathcal M^{L^{-1}}(x; y) - D_f(x, y)
            \right) - 
            \mathcal M^{L^{-1}}(Ty; y) 
            - 
            \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                \mathcal M^{L^{-1}}(x; y)
                - 
                \mathcal M^{L^{-1}}(Ty; y)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}\Vert x - y\Vert^2 - 
                \frac{L}{2}\Vert Ty - y\Vert^2
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - Ty + Ty - y\Vert^2
                    - 
                    \Vert y - Ty\Vert^2
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - Ty\Vert^2 + 
                    2\langle x - Ty, Ty - y\rangle
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff
            \left(
                F(x) - F(Ty) + \frac{L}{2}\Vert x - Ty\Vert^2 
                - L\langle  x - Ty, y - Ty\rangle
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            F(x) - F(Ty)
            - \langle L(y - Ty), x - Ty\rangle
            - D_f(x, y) 
            &\ge 0. 
        \end{align*}
        }
    \end{proof}
    \begin{remark}
        In our proof, we may use the following alternative representation of the above inequality which is 
        \begin{align*}
            F(x) - F(Ty) - \langle L(y - Ty), x - Ty\rangle - D_f(x, y) &\ge 0
            \\
            \iff
            F(x) - F(Ty)
            - \langle L(y - Ty), x - y + y - Ty\rangle - D_f(x, y) 
            &\ge 0
            \\
            \iff
            F(x) - F(Ty)
            - \langle L(y - Ty),x - y \rangle
            - L\Vert y - Ty\Vert^2
            - D_f(x, y) 
            &\ge 0
            \\
            \implies 
            F(x) - F(Ty)
            - \langle L(y - Ty),x - y \rangle
            - \frac{L}{2}\Vert y - Ty\Vert^2
            - D_f(x, y) 
            &\ge 0
            \\
            \implies
            F(x) - F(Ty)
            - \langle L(y - Ty),x - y \rangle
            - \frac{L}{2}\Vert y - Ty\Vert^2
            - \frac{\mu}{2}\Vert x - y\Vert^2
            &\ge 0. 
        \end{align*}
        
    \end{remark}

\section{Stepwise formulation of weak accelerated proximal gradient}
    In this section, we introduce the following new quantities and their notations. 
    It's there to make the notations simpler and easier to follow for the proofs that will come later. 
    The algorithm is parameterized by the sequence $y_k, x_k$. 
    We define for all $k \ge 0$ the following quantities: 
    \begin{assumption}
        Choose any integer $k\ge 0$. 
        Given $x_k, y_k, v_k$, we define the following quantities
        \begin{align}
            g_k &\defeq L(y_k - T_L y_k), 
            \label{eqn:grad-map}
            \\
            l_F(x; y_k) &\defeq F(T_Ly_k) + \langle g_k, x - y_k\rangle + \frac{1}{2L}\Vert g_k\Vert^2, 
            \label{eqn:lower-linearization}
            \\
            \epsilon_{k} &\defeq F(x_k) - l_F(x_k; y_k), 
            \label{eqn:regret}
        \end{align}
        Observe that by convexity of $F$, $\epsilon_k \ge 0$ always. 
    \end{assumption}

    
    \begin{theorem}[Stepwise weak accelerated proximal gradient]\label{def:stepwise-wapg}\;\\
        Let $0 \le \mu \le L$ be the strong convexity and Lipschitz smoothness parameter of $f$. 
        Given iterates $(v_k, x_k)$, or equivalently $(y_k, x_k)$, $(x_k, y_k)$, any $\alpha_k \in (0, 1), \gamma > 0$. 
        Define scalar $\hat \gamma$, and vectors $y_k, v_{k + 1}, x_{k + 1}$ by: 
        \begin{align*}
            \hat \gamma &:= (1 - \alpha_k)\gamma + \mu \alpha_k, 
            \\
            y_k &= 
            (\gamma + \alpha_k \mu)^{-1}(\alpha_k \gamma v_k + \hat\gamma x_k), 
            \\
            g_k &= \mathcal G_L y_k, 
            \\
            v_{k + 1} &= \hat\gamma^{-1}
            (\gamma(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
            \\
            x_{k + 1} &= T_L y_k. 
        \end{align*}
    \end{theorem}
    \begin{observation}\label{obs:stepwise-wapg}
        Let's observe Definition \ref{def:stepwise-wapg} closely. 
        We make two crucial observations here. 
        With the equalities, we define the following quantities associate with it. 
        Fix any integer $k \ge 0$. 
        To verify, we use the fundamental proximal gradient inequality. 
        Next, observe that we have $x_k, y_k, v_k$ lie on the same line because they have 
        \begin{align*}
            (Q1): 
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k),
            \\
            (Q2): 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
    \end{observation}
        To see (Q1), observe 
        \begin{align*}
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k)
            \\
            \iff 
            -(\alpha_k \gamma\hat \gamma^{-1} + 1)y_k
            &= 
            - \alpha_k \gamma \hat \gamma^{-1}v_k - x_k
            \\
            \iff 
            y_k &= 
            \frac{
                \alpha_k \gamma \hat \gamma^{-1}v_k + x_k
            }{1 + \alpha_k \gamma \hat \gamma^{-1}}
            \\
            &=  
            \frac{\alpha_k \gamma v_k + \hat \gamma x_k}{\gamma + \alpha_k \mu}.
        \end{align*}
        On the first equality (Q1), we multiplied both side of the equation by $\gamma_{k + 1}/\alpha_k \gamma_k$ and then group $y_k$ on the LHS. 
        The last equality comes by multiplying both the numerator and denominator by $\hat \gamma$, leaving the numerator $\hat \gamma + \alpha_k \gamma = \gamma + \alpha_k \mu$. 
        We used the definition of $\hat \gamma$ here. 
        To see the second equality (Q2), consider: 
        \begin{align*}
            y_k &= (\gamma + \alpha_k \mu)^{-1}(\alpha_k \gamma v_k + \hat\gamma x_k)
            \\
            \iff
            y_k - x_k &= 
            (\gamma + \alpha_k \mu)^{-1}
            (\alpha_k \gamma_k v_k - (\gamma + \alpha_k \mu)x_k + \hat \gamma x_k)
            \\
            \iff 
            (\gamma + \alpha_k \mu)(y_k - x_k)
            &= 
            \alpha_k\gamma v_k + 
            (\hat \gamma - \gamma - \alpha_k \mu) x_k
            \\
            &= \alpha_k \gamma v_k - \alpha_k \gamma x_k 
            \\
            &= \alpha_k \gamma(v_k - x_k)
            \\
            \iff 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
        On the second equality that follows the second $\iff$ we just substituted $\hat\gamma = (1 - \alpha_k)\gamma + \alpha_k \mu$. 
        Therefore, we discover that $x_k, v_k, y_k$ lies on the same line. 
    
    \begin{proposition}[Stepwise lyapunov]\label{prop:stepwise-lyapunov}
        For all $k \ge 0$, given $v_k, x_k, \alpha_k, \gamma_k$, let $\mu \ge 0$ iterates: $y_k, v_{k + 1}, x_{k + 1}, \alpha_{k + 1},\gamma \ge 0$ be given by Definition \ref{def:stepwise-wapg}. 
        Let $F^* = F(x^*)$ where is an arbitrary $x^* \in \RR^n$. 
        If we define $R_{k + 1}$ by some arbitrary $R_k \in \RR^n$: 
        \begin{align*}
            R_{k + 1}
            &\defeq 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat\gamma}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat\gamma}
                \Vert v_k - y_k\Vert^2
            \right). 
        \end{align*}
        Then we have the inequality: 
        \begin{align*}
            F(x_{k + 1}) - F^* + R_{k + 1} + \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}
        Start by considering the first and the third term of the LHS of the inequality that we want to prove. 
        \begin{align*}
            F(x_{k + 1}) &= 
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2, 
            \\
            R_{k + 1}
            &= 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right), 
            \\
            \implies 
            F(x_{k + 1}) + R_{k + 1}
            &= 
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            \\
            &\quad 
                + 
                (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma_k}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right). 
        \tag{1*}
        \end{align*}
        Coefficient of $\epsilon_k$ and $\Vert g_k\Vert^2$ are grouped. 
        Next, we have: 
        \begin{align*}
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^* \Vert^2
            &= 
            \frac{\hat \gamma}{2}\Vert 
                \hat \gamma^{-1}
                (
                    \gamma_k(1 - \alpha_k)v_k - 
                    \alpha_k g_k + \mu \alpha_k y_k
                )
                - x^* 
            \Vert^2
            \\
            &=  
            \frac{\hat \gamma}{2}
            \Vert 
                \hat \gamma^{-1}
                (
                \hat \gamma v_k + \mu \alpha_k(y_k - v_k)
                    - \alpha_k g_k
                )
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert 
                v_k + \hat \gamma^{-1} \mu \alpha_k (y_k - v_k)
                - \hat \gamma^{-1}\alpha_k g_k
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert v_k - x^*\Vert^2 
            + 
            \frac{\alpha_k^2}{2\hat \gamma}\Vert \mu(y_k - v_k) - g_k\Vert^2 
            \\ &\quad 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            \\ &\quad
                + 
                \frac{\alpha_k^2}{2\hat \gamma}
                \Vert \mu(y_k - v_k) - g_k\Vert^2 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle. 
        \tag{2*}
        \end{align*}
        On the above derivation, the first equality is by definition of $v_{k + 1}$; the second equality is by $\hat \gamma = (1 - \alpha_k)\gamma + \mu \alpha_k$ the second equality comes by considering: 
        \begin{align*}
            \gamma(1 - \alpha_k) v_k &= 
            (\hat \gamma  - \mu \alpha_k)v_k
            = \hat \gamma v_k - \mu\alpha_k v_k
            \\
            \iff 
            \gamma(1 - \alpha_k) v_k + \mu \alpha_k y_k
            &= 
            \hat \gamma v_k + \mu \alpha_k(y_k - v_k). 
        \end{align*}
        Focusing on the last two terms by the end of expression (2*), we have  
        \begin{align*}
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            & = 
            \frac{\alpha_k^2\mu}{\hat \gamma}
            \left(
                \frac{\mu}{2}\Vert y_k - v_k\Vert^2 
                - \langle y_k - v_k, g_k\rangle
            \right)
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2, 
            \\
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            &= 
            \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle 
            - \alpha_k \langle v_k - x^*, g_k\rangle. 
        \tag{2.1*}
        \end{align*}
        Adding the LHS of both equations above together gives: 
        {\small
        \begin{align*}
            & \quad 
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            + 
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
            + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \end{align*}
        }
        With the above we can conclude that (2*) simplifies to 
        {\small
        \begin{align*} 
            & 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            + 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \tag{2.2*}
        \end{align*}
        }
        Recall from 
        \hyperref[obs:stepwise-wapg]{Observations \ref*{obs:stepwise-wapg}} that: 
        \begin{align*}
            (Q1): 
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k),
            \\
            (Q2): 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
        Ok that is a lot, we list the following equations to assist things: 
        \begin{align*}
            &  
            - \alpha_k(v_k - x^*) - \frac{\alpha_k^2 \mu}{\hat \gamma}(y_k - v_k) - (x_k - y_k)
            \\
            \text{use Q1}: & =
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k^2}{\hat \gamma}\frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k)
            - (x_k - y_k) 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k \mu}{\gamma}(x_k - y_k)
            - (x_k - y_k) 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \left(
                1 + \frac{\alpha_k \mu}{\gamma}
            \right)(x_k - y_k)
            \\
            \text{use Q2}: 
            &= 
            -\alpha_k(v_k - x^*) - 
            \frac{\alpha_k \mu + \gamma}{\gamma}
            \frac{\alpha_k \gamma}{\gamma_k + \alpha_k \mu}(x_k - v_k)
            \\
            &= 
            -\alpha_k(v_k - x^*)
            - \alpha_k(x_k - v_k)
            \\
            &= \alpha_k(x^* - x_k). 
        \tag{Q3}
        \end{align*}
        Adding (2.2*) to (1*) gives: 
        \begin{align*}
            &
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\quad 
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
                + 
                \left\langle g_k, 
                    - \alpha_k(v_k - x^*) 
                    - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \epsilon_k 
            + \left\langle 
                g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                - (x_k - y_k)
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            \text{Use Q3}&= 
            F(x_k) - \epsilon_k 
            + \alpha_k\left\langle 
                g_k, 
                x^* - x_k
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k\epsilon_k + \alpha_k\langle g_k, x^* - x_k\rangle
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat\gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \tag{3*}
        \end{align*}
        From the left to the right on the first equality, coefficients of $\Vert g_k\Vert^2$ cancels out to zero and the inner product containing $g_k$ are grouped. 
        Going from the left to the right of the third equality, we applied (Q3) derived earlier to simplify the inner product term. 
        The last equalities re-arranged terms and grouped the coefficients of $\epsilon_k$ together. 
        Taking a page break, continuing on (3*) we have
        \begin{align*}
            &
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \left(
                    \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}
                    + 
                    \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
                \right)\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            & =
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \frac{\mu \alpha_k}{2}\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\ &=
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                +
                \frac{\mu\alpha_k}{2} \Vert y_k - x^*\Vert^2
            \\&= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
            \tag{3.1*}
        \end{align*}
        On the first equality, coefficients of $\Vert y_k - v_k\Vert^2$ are grouped together. 
        On the second to the third equality, the terms had been simplified by 
        \begin{align*}
            \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma} + 
            \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
            &= 
            \frac{\mu\alpha_k}{2}\left(
                \frac{(1 - \alpha_k)\gamma_k + \alpha_k \mu}{\hat \gamma}
            \right)
            \\
            &= \frac{\mu\alpha_k}{2}\left(
                \frac{\hat \gamma}{\hat \gamma}
            \right) = \frac{\mu\alpha_k}{2}. 
        \end{align*}
        (3.1*) was adding (1*) and (2.2*) together, which is the same as adding (1*) and (2*) together. 
        So that was all equal to (1*) + (2*) and it says: 
        \begin{align*}
            & F(x_{k + 1}) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            & \iff 
            \\
            & F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_k) - F(x^*) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &= (1 - \alpha_k)(F(x_k) - F(x^*))
            + \alpha_k\left(
                F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                (1 - \alpha_k)\left(
                    R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
                \right). 
        \tag{3.2*}
        \end{align*}
        Focusing on the second term, we simplify the multiplier inside: 
        {\small
        \begin{align*}
            & F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= 
            F(x_k) - F(x^*) - \left(
                F(x_k) - F(T_L y_k) - \langle g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2
            \right)- \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= F(T_L y_k) - F(x^*) + \langle g_k, x^* - y_k\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            + \frac{1}{2L}\Vert g_k\Vert^2 \le 0. 
        \tag{4*}    
        \end{align*}
        }
        On the last line, we expand the definition of $g_k$ and then used 
        \hyperref[thm:prox-grad-ineq]{Theorem \ref*{thm:prox-grad-ineq}}. 
        Therefore, we conclude that: 
        {\small
        \begin{align*}
            F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)\left(
                F(x_k) - F(x^*) + R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
        }
    \end{proof}
    \begin{remark}
        \textcolor{red}
        {
        Use $\mu = D_f(x^*, y_k)/\Vert y_k - x^*\Vert^2$ also works, $\mu$ is a pessimistic choice for the inequality above. 
        }
        But in general the choice of $\mu$ remains the strong convexity modulus or equivalently, any value that is smallar than the true strong convexity constant for claiming the convergence rate for all initial guesses. 
    \end{remark}

\section{Formulation of R-WAPG and its convergence rates}

    \begin{definition}[Relaxed weak accelerated proximal gradient (R-WAPG)]\label{def:wapg}
        \;\\
        Initialize any $\gamma_0 \ge 0$, $(x_0, v_0)$ or equivalently $(x_0, y_0), (y_0, v_0)$. 
        Given any sequence $(\rho_k)_{k\ge0}, (\alpha_k)_{k \ge0}$ such that for all integer $k\ge 0$, $\textcolor{red}{\rho_k \ge 0}$, $\alpha_k \in (0, 1)$. 
        The algorithm generates a sequence of $(x_k, y_k, v_k)$ such that they satisfy for all $k\ge 0$ recursively: 
        \begin{align*}
            \gamma_k &:= \left.\begin{cases}
                \rho_{k -1}L\alpha_{k - 1}^2 & k \ge 1,
                \\
                \gamma_0 & k = 0. 
            \end{cases}\right\rbrace,
            \\
            \hat \gamma_{k + 1} &:= L\alpha_k^2 = (1 - \alpha_k)\gamma_k + \mu \alpha_k, 
            \\
            y_k &= 
            (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k), 
            \\
            g_k &= \mathcal G_L y_k, 
            \\
            v_{k + 1} &= 
            \hat\gamma^{-1}_{k + 1}
            (\gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
            \\
            x_{k + 1} &= T_L y_k. 
        \end{align*}
    \end{definition}
    \begin{observation}
        Observe that if $\rho_k = 1$ for all $k\ge 0$, then the above algorithm is similar to (2.2.7) in Nesterov's book \cite{nesterov_lectures_2018}. 
    \end{observation}
    
    \begin{proposition}[R-WAPG convergence claim]\label{prop:wagp-convergence}
        Let vectors $(x_k, y_k, v_k)_{k \ge0}$ and scalars $\alpha_k$ be generated by R-WAPG. 
        Define recursively for all $k\ge1$ by 
        \begin{align*}
        R_{k + 1}
        := 
        \frac{1}{2}\left(
            L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
        \right)\Vert g_k\Vert^2
        + 
        (1 - \alpha_k)
        \left(
            \epsilon_k + R_k + 
            \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
            \Vert v_k - y_k\Vert^2
        \right). 
        \end{align*}
        Choose $R_0= 0$ to be the base case. 
        Choose any $x^* \in \RR^n$ then
        \begin{align*}
            & F(x_{k + 1}) - F(x^*) + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k} \max(1, \rho_{k - 1})
            \right)
            \left(
                \prod_{i = 0}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_0) - F(x^*) + \frac{\gamma_0}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}  
        Recall the stepwise convergence theorem, since the choice of $\gamma, \hat \gamma$ is arbitrary choices for the theorem, we choose $\gamma = \rho_{k - 1} L \alpha_{k - 1}^2, \hat \gamma = L \alpha_k^2$, giving us: 
        {\small
        \begin{align*}
            &F(x_{k + 1}) - F^* + R_{k + 1} + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{\rho_{k - 1}L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \max(1, \rho_{k - 1})\frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \max(1, \rho_{k - 1})(1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k} \max(1, \rho_{k - 1})
            \right)
            \left(
                \prod_{i = 0}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_0) - F^* + R_0 + \frac{\gamma_0}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
        }
        Additionally, the algorithm converges by considering $R_k$: 
        \begin{align*}
            R_{k + 1}
            &= 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &= (1 - \alpha_k)
            \left(
                \epsilon_k + R_k 
                + \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\ge 
            (1 - \alpha_k) R_k
            \\
            &\ge R_0 \prod_{i = 0}^{k} \left(1 - \alpha_i\right) = 0. 
        \end{align*}
        Going from the left to the right on the first equality, we used the fact that $\hat \gamma_{k + 1} = L \alpha_{k}^2$.
        This makes coefficient of $\Vert g_k\Vert^2$ zero. 
        The fist inequality is by $\epsilon_k \ge 0$ and the non-negativity of the remaining terms. 
        The last equality is by the assumption that $R_0 = 0$. 
        Therefore: 
        {\small
        \begin{align*}
            & F(x_{k + 1}) - F^* +
            \frac{L\alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k} \max(1, \rho_{k - 1})
            \right)
            \left(
                \prod_{i = 0}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_0) - F^* + R_0 + \frac{\gamma_0}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
        }
    \end{proof}
    \begin{remark}        
        \textcolor{red}
        {
            The choice of $\rho_k$ allows for divergence to traditional choices of step sizes in the literatures. 
            As we can see from the above convergence claim, there exists choices of $\rho_k\ge 1$ where convergence remains possible. 
        }
    \end{remark}


\section{R-WAPG covers variants of APG in the literatures}
    Many variants of the accelerated proximal gradient algorithm fits into the following form which we called ``abstract similar triangle". 
    \begin{proposition}[Abstract similar triangle form]\label{prop:abs-st-form}\;\\
        For all $k\ge 0$, iterates $(w_t, y_t, x_{t + 1}, z_{t + 1})$ satisfies recursively that: 
        \begin{align*}
            y_k &= (1 + \tau_k)^{-1}(v_k + \tau_k x_k),
            \\
            v_{k + 1} &= (1 + \xi_k)^{-1}(v_k + \xi_k y_k) - (1 + \xi_k)^{-1}\delta_k g_k,
            \\
            x_{k + 1} &= y_k - L^{-1} g_k. 
        \end{align*}
        If $1 + \xi_t + \tau_t = L\delta_t$. Then 
        $$
            v_{k + 1} - x_{k + 1} = (1 + \xi_t)^{-1}\tau_t(x_{k + 1} - x_k). 
        $$
        Which makes the algorithm a similar triangle form. 
    \end{proposition}
    \begin{proof}
        We are interested in identifying the conditions required for the sequence of $\xi_t, \tau_t, \delta_t$ such that there exists $\theta_t$ satisfying: 
        \begin{align*}
            x_{t + 1} - z_{t + 1} 
            &= \theta_t(z_{t + 1} - z_t)
        \end{align*}
        To verify, do 
        \begin{align*}
            x_{t + 1} &= 
            (1 + \xi_t)^{-1}(x_t + \xi_t y_t - \delta_t \mathcal G_L(y_t))
            \\
            & \textcolor{gray}{
                x_t = (1 + \tau_t)y_t - \tau_t z_t
            }
            \\
            &= 
            (1 + \xi_t)^{-1}((1 + \tau_t)y_t - \tau_t z_t + \xi_t y_t - \delta_t \mathcal G_L(y_t))
            \\
            &= 
            (1 + \xi_t)^{-1}((1 + \tau_t + \xi_t)y_t - \tau_t z_t - \delta_t \mathcal G_L(y_t))
            \\
            x_{t + 1} - z_{t + 1}
            &= 
            (1 + \xi_t)^{-1}((1 + \tau_t + \xi_t)y_t - \tau_t z_t - \delta_t \mathcal G_L(y_t))
            - (y_t - L^{-1}\mathcal G_Ly_t)
            \\
            &= 
            (1 + \xi_t)^{-1}(\tau_ty_t - \tau_t z_t - \delta_t \mathcal G_L(y_t))
            + L^{-1}\mathcal G_Ly_t
            \\
            &= 
            (1 + \xi_t)^{-1}
            \left(
                \tau_ty_t - \tau_t z_t + (L^{-1}(1 + \xi_t) - \delta_t) \mathcal G_L(y_t)
            \right)
            \\
            &= 
            (1 + \xi_t)^{-1}\tau_t
            \left(
                y_t - z_t + 
                \tau_t^{-1}(L^{-1}(1 + \xi_t) - \delta_t) \mathcal G_L(y_t)
            \right)
        \end{align*}
        The RHS is can be verified through 
        \begin{align*}
            z_{t + 1} - z_t &= 
            y_t - L^{-1}\mathcal G_L(y_t) - z_t
            \\
            &= (y_t - z_t) - L^{-1}\mathcal G_L(y_t). 
        \end{align*}
        It necessitates the condition: 
        \begin{align*}
            \tau_t^{-1}(L^{-1}(1 + \xi_t) - \delta_t) 
            &= - L^{-1}
            \\
            (1 + \xi_t) - L\delta_t
            &= 
            - \tau_t
            \\
            1 + \xi_t + \tau_t
            &=
            L\delta_t. 
        \end{align*}
        Which allows for: 
        \begin{align*}
            x_{t + 1} - z_{t + 1} &= 
            (1 + \xi_t)^{-1}\tau_t
            \left(y_t - z_t - L^{-1}\mathcal G_L(y_t)\right) 
            = 
            (1 + \xi_t)^{-1}\tau_t(z_{t + 1} - z_t). 
        \end{align*}
    \end{proof}

    \subsection{Narrowing it down}
        \begin{proposition}[R-WAPG intermediate Form I]\label{prop:wapg-intermediate-form}
            Assume $\mu < L$. 
            Suppose that $(y_i, x_i, v_i)$ are generated by the WAPG algorithm for all $0 \le i \le k$ using sequence $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge0}$ such that for any integer $k\ge 0$, $\alpha_k \in (0, 1)$ and $\rho_k \ge 0$. 
            Then the iterates can be expressed without the sequence $(\gamma_k)_{k \ge0}, (\rho_k)_{k\ge0}$, and they satisfy: 
            \begin{align*}
                y_k &= 
                \left(
                    1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
                \right), 
                \\
                x_{k + 1} &= 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                v_{k + 1} &= 
                \left(
                    1 + \frac{\mu}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k, 
                \\
                L &\alpha_k^2 = (1 - \alpha_k)L \rho_{k - 1}\alpha_{k - 1}^2 + \mu \alpha_k. 
            \end{align*}
        \end{proposition}
        \begin{proof}
            \begin{align*}
                y_{k} &= 
                (\gamma_k + \alpha_k \mu)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                (\hat \gamma_{k + 1} + \alpha_k \gamma_k)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                \left(
                    \frac{\hat \gamma_{k + 1}}{\alpha_k\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    1 + \frac{L\alpha_k^2}{\alpha_k \rho_{k - 1}L\alpha_{k - 1}^2} 
                \right)^{-1}
                \left(
                    v_k + \frac{L\alpha_k^2}{\alpha_k \rho_{k - 1}L\alpha_{k - 1}^2} x_k
                \right)
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k}{\rho_{k - 1}\alpha_{k - 1}^2}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k}{\rho_{k - 1}\alpha_{k - 1}^2} x_k
                \right)
                \\
                &= 
                \left(
                    1 + \frac{L - L \alpha_k}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{L - L \alpha_k}{L \alpha_k - \mu} x_k
                \right). 
            \end{align*}
            From the left to right of the second inequality, we used the fact that $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k\mu$. 
            Going from the left to the right of the second last inequality, we did the following: 
            \begin{align*}
                L\alpha_k^2 &= 
                (1 - \alpha_k)L\rho_{k- 1}\alpha_{k - 1}^2 + \alpha_k \mu 
                \\
                \iff 
                L \alpha_k^2 - \alpha_k\mu &= 
                (1 - \alpha_k)L \rho_{k - 1}\alpha_{k - 1}^2
                \\
                \iff 
                \rho_{k - 1}\alpha_{k - 1}^2
                &= 
                \frac{L \alpha_k^2 - \alpha_k\mu}{L (1 - \alpha_k)}
                \\
                \iff 
                \frac{1}{\rho_{k - 1}\alpha_{k - 1}^2}
                &= 
                \frac{L (1 - \alpha_k)}{L \alpha_k^2 - \alpha_k\mu}
                \\
                \iff 
                \frac{\alpha_k}{\rho_{k - 1}\alpha_{k - 1}^2}
                &= 
                \frac{L - L\alpha_k}{L\alpha_k - \mu}. 
            \end{align*}
            On the third $\iff$, we can assume $\alpha_k \neq \mu/L$ because $\alpha_k = \mu/L$ is impossible. 
            For contradiction, assume $a_k = \mu/L$. 
            Then
            \begin{align*}
                L \alpha_k^2 - \alpha_k\mu &= 
                (1 - \alpha_k)L \rho_{k - 1}\alpha_{k - 1}^2 = 0 \implies \alpha_{k - 1} = 0. 
            \end{align*}
            Which contradict $\alpha_{k - 1} \in (0, 1)$. 
            For $v_{k + 1}$ considers 
            \begin{align*}
                v_{k + 1} &= 
                \hat \gamma_{k + 1}^{-1}
                ((1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
                \left(
                    (1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k} y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k\mu}{(1 -\alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k\mu}{(1 - \alpha_k)\rho_{k - 1}L\alpha_{k - 1}^2} y_k
                \right)
                - \frac{1}{L\alpha_{k}}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu} y_k
                \right)
                - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
            Going from the left to the right of the second equality, we substitute $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu\alpha_k$. 
            Going from the left to right of the second last equality, we made the substitution $\hat \gamma_{k + 1} = L \alpha_k^2$, and $\gamma_k = \rho_{k - 1}L \alpha_{k - 1}^2$. 
            To arrive at the last equality, we considered substituting the denominator by the definition for $\hat\gamma_{k + 1}$. 
            \begin{align*}
                (1 - \alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2
                &= 
                \hat \gamma_{k + 1} - \mu \alpha_k
                = 
                L\alpha_{k}^2 - \alpha_k\mu. 
            \end{align*}
            The proof of the intermediate form is no complete. 
            This form doesn't have $\rho_k, \gamma_k, \hat \gamma_k$ in it. 
            Finally, to see the inequality, consider the fact that $\rho_k \in [0, 1]$ so it has: 
            \begin{align*}
                \hat \gamma_{k + 1} = L\alpha_k^2 = (1 - \alpha_k)\rho_{k - 1}L \alpha_{k - 1}^2 + \alpha_k \mu 
                \le (1 - \alpha_k)L \alpha_{k - 1}^2 + \alpha_k \mu. 
            \end{align*}
            The only auxiliary sequence it needs is $\alpha_k$, which directly links to the convergence rate of the algorithm. 
        \end{proof}
        \begin{remark}
            Observe that this is fits the abstract similar triangle form described in Proposition \ref{prop:abs-st-form} perfectly by exchanging $x_k$ with $z_k$, $v_k$ with $x_k$. 
        \end{remark}

        \begin{proposition}[WAPG fits similar triangle form]\label{prop:wagp-st-form}
            Given any sequence $(\alpha_i)_{i \ge k}$ such that $\alpha_i \in (0, 1)$, $\rho_k \ge 0$ for all $0 \le i \le k$ and $0 < L\alpha_k^2 = (1 - \alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2 + \alpha_k \mu$.  
            Let $(x_k, y_k, v_k)$ to be generated as specified by Algorithm 2.2 by such $(\alpha_k)_{k \ge0}$. Then iterate $(v_k)$ can be expressed without $y_k, v_k$ by: 
            \begin{align*}
                y_k &= 
                \left(
                    1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
                \right), 
                \\
                x_{k + 1} &= 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                v_{k + 1} &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proposition}
        \begin{proof}
            From Proposition \ref{prop:wapg-intermediate-form}, set 
            \begin{align*}
                \tau_k &= \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}, 
                \\
                \xi_k &= \frac{\mu}{L \alpha_k - \mu},
                \\
                (1 + \xi_k)^{-1}\delta_k &= \frac{1}{L\alpha_k}
                \iff L \delta_k = \frac{1 + \xi_k}{\alpha_k}. 
            \end{align*}
            We claim that these parameters satisfy $L\delta_k = 1 + \tau_k + \xi_k$. 
            Hence $v_{k + 1} - x_{k + 1} = (1 + \xi_k)^{-1}(x_{k + 1} - x_k)$. 
            To see, we have 
            \begin{align*}
                1 + \tau_k + \xi_k &= 
                1 + \frac{L(1 - \alpha_k)}{L \alpha_k - \mu} 
                + \frac{\mu}{L \alpha_k - \mu}
                \\
                &= 
                1 + \frac{L - L \alpha_k + \mu}{L\alpha_k - \mu}
                \\
                &= 
                \frac{L - L \alpha_k + \mu + L \alpha_k - \mu}{L\alpha_k - \mu}
                \\
                &= \frac{L}{L\alpha_k - \mu}. 
            \end{align*}
            Next
            \begin{align*}
                1 + \tau_k + \xi_k &= 
                1 + \frac{L(1 - \alpha_k)}{L \alpha_k - \mu} 
                + \frac{\mu}{L \alpha_k - \mu}
                \\
                &= 
                1 + \frac{L - L \alpha_k + \mu}{L\alpha_k - \mu}
                \\
                &= 
                \frac{L - L \alpha_k + \mu + L \alpha_k - \mu}{L\alpha_k - \mu}
                \\
                &= \frac{L}{L\alpha_k - \mu}. 
            \end{align*}
            Therefore, substituting it gives: 
            \begin{align*}
                v_{k + 1} &= 
                x_{k + 1} + \left(
                    1 + \frac{\mu}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k - \mu}{L\alpha_k}
                \right)\left(
                    \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proof}
    
    \begin{proposition}[R-WAPG fits into Momentum Form]\label{prop:wapg-momentum-form}
        \;\\
        Suppose any sequence $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge0}$ satisfies recursively the conditions
        $L\alpha_k^2 = (1 - \alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2 + \alpha_k \mu$ where $\rho_{k} \ge 0\; \forall k \ge0$. 
        Assume that $0 \le \mu < L$. 
        If $\alpha_k$ is used in the similar triangle form to generate the iterates $(x_k, y_k, v_k)$. 
        Then the iterates are algebraically equivalent to: 
        \begin{align*}
            x_{k + 1} &= y_k - L^{-1}\mathcal G_Ly_k, 
            \\
            y_{k + 1} &= 
            x_{k + 1} + 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        In the special case when $\mu = 0$, the momentum term admits simpler representation 
        \begin{align*}
        \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
        & = \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proposition}
    \begin{proof}
                
        Start by considering the update rule for $v_k$ from the similar triangle form of WAPG which is: 
        \begin{align*}
            v_{k + 1} &= 
            x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \\
            \iff 
            (L \alpha_{k + 1} - \mu)v_{k + 1} 
            &= 
            (L \alpha_{k + 1} - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
        \end{align*}
        Next, we simplify the update for the iterates $y_{k}$ by the similar triangle form of R-WAPG. 
        \begin{align*}
            y_k &= 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \left(
            \frac{L - \mu}{L\alpha_k - \mu} 
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \frac{L\alpha_k - \mu}{L - \mu} v_k
            + 
            \frac{L - L \alpha_k}{L - \mu} x_k
            \\
            &= (L - \mu)^{-1}((L \alpha_k - \mu)v_k + (L - L \alpha_k)x_k). 
        \end{align*}
        Now considering substituting $v_{k + 1}$ into $y_{k + 1}$ so it has: 
        {\small
        \begin{align*}
            y_{k + 1} &= 
            (L - \mu)^{-1}((L\alpha_{k + 1} - \mu)v_{k + 1} + (L - L \alpha_{k + 1})x_{k + 1})
            \\
            &= (L - \mu)^{-1}
            \left(
                (L\alpha_{k + 1} - \mu)x_{k + 1} + 
                (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
                + (L - L \alpha_{k + 1})x_{k + 1}
            \right)
            \\
            &= 
            (L - \mu)^{-1}
            \left(
                (L - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \right)
            \\
            &= x_{k + 1} + \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}(x_{k + 1} - x_k). 
        \end{align*}
        }
        The coefficient for $(x_{k + 1} - x_k)$ needs some works to formulate it without the parameter $\mu, L$. 
        To do that we have: 
        \begin{align*}
            \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}
            &= \frac{(L\alpha_{k + 1} - \mu)\alpha_k(1 - \alpha_k)}{\alpha_k^2(L - \mu)}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \left(
                \frac{\alpha_k^2(L - \mu)}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \alpha_k(1 - \alpha_k)
            \left(
                \frac{L\alpha_k^2 - \mu\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \rho_k\left(
                \frac{L\rho_k\alpha_k^2 - \mu\rho_k\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \rho_k\alpha_k(1 - \alpha_k)
            \left(
                \frac{(L\alpha_{k + 1} - \mu)(\rho_k\alpha_k^2 + \alpha_{k + 1})}
                {L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}.
        \end{align*}
        Going from the left to right on the fourth equality, we used $L\alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_kL\alpha_k^2 + \mu \alpha_{k + 1}$ with 
        \begin{align*}
            L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2 
            &= 
            (1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            ((1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \mu \alpha_{k + 1}) - \mu\alpha_{k + 1} + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= L \alpha_{k + 1}^2 - \mu\alpha_{k + 1} + \alpha_{k + 1}L\rho_k\alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            L\alpha_{k + 1}(\alpha_{k + 1} + \rho_k \alpha_k^2) - \alpha_{k + 1}\mu - \mu \rho_k \alpha_k^2
            \\
            &= (L \alpha_{k + 1} - \mu)(\alpha_{k + 1} + \rho_k \alpha_k^2). 
        \end{align*}
        When $\mu = 0$, things simplify. 
        Consider that $\alpha_{k +1}^2 = (1 - \alpha_{k + 1})\alpha_k^2$. 
        \begin{align*}
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \alpha_{k + 1}^2}
            \\
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \rho_k(1 - \alpha_{k + 1})\alpha_k^2}
            \\
            &= \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2}
            \\
            &= \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proof}
    \begin{remark}
        \textcolor{red}
        {
        There exists a choice of $\rho_k \in [0, 1), \mu = 0$ such that the above algorithm is equivalent to Chambolle Dossal 2015 \cite{chambolle_convergence_2015}. 
        When $\rho_k = 1$, this becomes just FISTA \cite{beck_fast_2009}. 
        When $L > \mu > 0$, then choosing $\rho_k = 1$ yields equation 
        \begin{align*}
            L \alpha_{k + 1}^2 = (1- \alpha_{k + 1})L \alpha_k^2 + \alpha_{k + 1} \mu. 
        \end{align*}
        Here, if we assume sequence $\alpha_k$ is constant, then $\alpha_k = \sqrt{\mu/L}$ is a solution to the above iterative equation. 
        Substituting it back to the momentum form of R-WAPG, we recovers the V-FISTA algorithm, which appears as (10.7.7) in Beck's Book \cite{beck_first-order_2017}. 
        }
    \end{remark}
    
\bibliographystyle{siam}
\bibliography{references/Spectral_Momentum.bib}

\end{document}
