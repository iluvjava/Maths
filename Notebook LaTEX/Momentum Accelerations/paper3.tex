\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont 
        A Parameter Free Accelerated Proximal Gradient Method Without Restarting
    }}

\author{
    Hongda Li
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~ and~Xianfu Wang
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    
    \cite{nesterov_lectures_2018}
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 65K10, 90c25, 90C30; Secondary 65Y20. 
\noindent{\bfseries Keywords: } Nesterov acceleration, Proximal point method. 
\section{Introduction}
    \subsection{Literature reviews}
    \subsection{Contributions}
        Our contributions are two folds. 
        
        We present a theoretical unification through the formulation of ``Relaxed weak accelerated proximal gradient algorithm (R-WAPG)'' which we invented. 
        It unifies convergence claim for accelerated method for the strongly convex objective and Lipschitz smooth but non-strongly convex case and it relaxes the conditions on the momentum sequence. 
        
        We connect the theories and practice by pointing out one practical exploits inspired the convergence proof of the algorithm which we called ``Spectral Momentum''. 
        Numerical experiments are conducted to illustrate the theoretical convergence of R-WAPG and the performance of Spectral Momentum in with applications. 

\section{Preliminaries}
    Throughout Section 1, 2, 3, we assume the problem of types $F = f + g$ where $f: \RR \rightarrow \RR$ is $L$ Lipschitz smooth and $\mu \ge 0$ strongly convex and $g: Q \rightarrow \overline \RR$ is convex. 
    We consider optimization problem of the form
    \begin{align*}
        \min_x \left\lbrace
            F(x) \defeq f(x) + g(x)
        \right\rbrace. 
    \end{align*}
    To start we define the following quantities
    \begin{enumerate}
        \item The proximal gradient model function: 
        $$
            \widetilde{\mathcal M}^{L^{-1}}
            (x; y) \defeq
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle 
            + \frac{L}{2}\Vert x - y\Vert^2. 
        $$
        \item The proximal point model function: 
        $$
            \mathcal M^{L^{-1}}(x; y) := F(x) + \frac{L}{2}\Vert x - y\Vert^2
        $$
        \item The proximal gradient operator $T_L = [I + L^{-1}\partial g]\circ [I - L^{-1}\nabla f]$. Since $L$ is fixed throughout, the subscript may be omitted and become $T$ instead to make the notations simpler. 
    \end{enumerate}
    To state the following lemma, we define the Bregman divergence of $f$ to be 
    \begin{align*}
        D_f(x, y): \RR^n \times \RR^n \rightarrow \RR 
        \defeq f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
    \end{align*}
    
    \begin{lemma}[Proximal gradient envelope] 
        Fix any $y$, we will have for all $x \in \RR^n$ that: 
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y)
            &= 
            \mathcal M^{L^{-1}}(x; y)- D_f(x, y) \ge \mathcal M^{L^{-1}}(x; y). 
        \end{align*}
    \end{lemma}
    \begin{proof}
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) 
            &= 
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            g(x) + f(x) - f(x) + f(y) 
            + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            F(x) - D_f(x, y) + \frac{L}{2}\Vert x - y\Vert^2 
            \\
            &= \mathcal M^{L^{-1}}(x; y) - D_f(x, y). 
        \end{align*}
    \end{proof}
    
    \subsection{Proximal inequality}
    \begin{theorem}[Proximal inequality]\label{thm:prox-grad-ineq}
        Fix any $y$, we have for all $x$: 
        \begin{align*}
            F(x) - F(Ty) - \langle L(y - Ty), x - Ty\rangle
            &\ge  D_f(x, y). 
        \end{align*}
    \end{theorem}
    \begin{proof}
        $\widetilde {\mathcal M}(\cdot; x)$ is $L$ strongly convex with minimizer $Ty$
        {\small
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) - 
            \widetilde{\mathcal M}^{L^{-1}}(Ty; y)
            - 
            \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 
            0
            \\
            \iff
            \left(
                \mathcal M^{L^{-1}}(x; y) - D_f(x, y)
            \right) - 
            \mathcal M^{L^{-1}}(Ty; y) 
            - 
            \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                \mathcal M^{L^{-1}}(x; y)
                - 
                \mathcal M^{L^{-1}}(Ty; y)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}\Vert x - y\Vert^2 - 
                \frac{L}{2}\Vert Ty - y\Vert^2
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - Ty + Ty - y\Vert^2
                    - 
                    \Vert y - Ty\Vert^2
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - Ty\Vert^2 + 
                    2\langle x - Ty, Ty - y\rangle
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff
            \left(
                F(x) - F(Ty) + \frac{L}{2}\Vert x - Ty\Vert^2 
                - L\langle  x - Ty, y - Ty\rangle
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            F(x) - F(Ty)
            - \langle L(y - Ty), x - Ty\rangle
            - D_f(x, y) 
            &\ge 0. 
        \end{align*}
        }
    \end{proof}
    \begin{remark}
        In our proof, we may use the following alternative representation of the above inequality which is 
        \begin{align*}
            F(x) - F(Ty) - \langle L(y - Ty), x - Ty\rangle - D_f(x, y) &\ge 0
            \\
            \iff
            F(x) - F(Ty)
            - \langle L(y - Ty), x - y + y - Ty\rangle - D_f(x, y) 
            &\ge 0
            \\
            \iff
            F(x) - F(Ty)
            - \langle L(y - Ty),x - y \rangle
            - L\Vert y - Ty\Vert^2
            - D_f(x, y) 
            &\ge 0
            \\
            \implies 
            F(x) - F(Ty)
            - \langle L(y - Ty),x - y \rangle
            - \frac{L}{2}\Vert y - Ty\Vert^2
            - D_f(x, y) 
            &\ge 0
            \\
            \implies
            F(x) - F(Ty)
            - \langle L(y - Ty),x - y \rangle
            - \frac{L}{2}\Vert y - Ty\Vert^2
            - \frac{\mu}{2}\Vert x - y\Vert^2
            &\ge 0. 
        \end{align*}
        
    \end{remark}

\section{Stepwise formulation of weak accelerated proximal gradient}
    In this section, we introduce the following new quantities and their notations. 
    It's there to make the notations simpler and easier to follow for the proofs that will come later. 
    The algorithm is parameterized by the sequence $y_k, x_k$. 
    We define for all $k \ge 0$ the following quantities: 
    \begin{assumption}
        Choose any integer $k\ge 0$. 
        Given $x_k, y_k, v_k$, we define the following quantities
        \begin{align}
            g_k &\defeq L(y_k - T_L y_k), 
            \label{eqn:grad-map}
            \\
            l_F(x; y_k) &\defeq F(T_Ly_k) + \langle g_k, x - y_k\rangle + \frac{1}{2L}\Vert g_k\Vert^2, 
            \label{eqn:lower-linearization}
            \\
            \epsilon_{k} &\defeq F(x_k) - l_F(x_k; y_k), 
            \label{eqn:regret}
        \end{align}
        Observe that by convexity of $F$, $\epsilon_k \ge 0$ always. 
        To see, consider the last inequality in the remark of Lemma \ref{thm:prox-grad-ineq} with $y = y_k, x = x_k$: 
        \begin{align*}
            F(x_k) - F(T_Ly_k)
            - \langle L(y_k - T_Ly_k),x_k - y_k \rangle
            - \frac{L}{2}\Vert y_k - T_Ly_k\Vert^2
            - \frac{\mu}{2}\Vert x_k - y_k\Vert^2
            &\ge 0
            \\
            \implies 
            F(x_k) - F(T_Ly_k)
            - \langle g_k,x_k - y_k \rangle
            - \frac{1}{2L}\Vert g_k\Vert^2
            &\ge 0. 
        \end{align*}
    \end{assumption}
    
    \begin{definition}[Stepwise weak accelerated proximal gradient]\label{def:stepwise-wapg}\;\\
        Let $0 \le \mu \le L$ be the strong convexity and Lipschitz smoothness parameter of $f$. 
        Given iterates $(v_k, x_k)$, or equivalently $(y_k, x_k)$, $(x_k, y_k)$, any $\alpha_k > 0, \gamma > 0$, the algorithm generates scalar $\hat \gamma$, and vectors $y_k, v_{k + 1}, x_{k + 1}$ by equalities: 
        \begin{align*}
            \hat \gamma &:= (1 - \alpha_k)\gamma + \mu \alpha_k, 
            \\
            y_k &= 
            (\gamma + \alpha_k \mu)^{-1}(\alpha_k \gamma v_k + \hat\gamma x_k), 
            \\
            g_k &= \mathcal G_L y_k, 
            \\
            v_{k + 1} &= \hat\gamma^{-1}
            (\gamma(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
            \\
            x_{k + 1} &= T_L y_k. 
        \end{align*}
    \end{definition}
    \begin{observation}\label{obs:stepwise-wapg}
        Let's observe Definition \ref{def:stepwise-wapg} closely. 
        We make two crucial observations here. 
        We have $x_k, y_k, v_k$ lie on the same line because and they have the following equivalent equalities: 
        \begin{align*}
            (Q1): 
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k),
            \\
            (Q2): 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
    \end{observation}
        To see (Q1), observe 
        \begin{align*}
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k)
            \\
            \iff 
            -(\alpha_k \gamma\hat \gamma^{-1} + 1)y_k
            &= 
            - \alpha_k \gamma \hat \gamma^{-1}v_k - x_k
            \\
            \iff 
            y_k &= 
            \frac{
                \alpha_k \gamma \hat \gamma^{-1}v_k + x_k
            }{1 + \alpha_k \gamma \hat \gamma^{-1}}
            \\
            &=  
            \frac{\alpha_k \gamma v_k + \hat \gamma x_k}{\gamma + \alpha_k \mu}.
        \end{align*}
        On the first equality (Q1), we multiplied both side of the equation by $\hat\gamma/\alpha_k \gamma$ and then group $y_k$ on the LHS. 
        The last equality comes by multiplying both the numerator and denominator by $\hat \gamma$, leaving the numerator $\hat \gamma + \alpha_k \gamma = \gamma + \alpha_k \mu$. 
        We used the definition of $\hat \gamma$ here. 
        To see the second equality (Q2), consider: 
        \begin{align*}
            y_k &= (\gamma + \alpha_k \mu)^{-1}(\alpha_k \gamma v_k + \hat\gamma x_k)
            \\
            \iff
            y_k - x_k &= 
            (\gamma + \alpha_k \mu)^{-1}
            (\alpha_k \gamma_k v_k - (\gamma + \alpha_k \mu)x_k + \hat \gamma x_k)
            \\
            \iff 
            (\gamma + \alpha_k \mu)(y_k - x_k)
            &= 
            \alpha_k\gamma v_k + 
            (\hat \gamma - \gamma - \alpha_k \mu) x_k
            \\
            &= \alpha_k \gamma v_k - \alpha_k \gamma x_k 
            \\
            &= \alpha_k \gamma(v_k - x_k)
            \\
            \iff 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
        On the second equality that follows the second $\iff$ we just substituted $\hat\gamma = (1 - \alpha_k)\gamma + \alpha_k \mu$. 
        Therefore, we discover that $x_k, v_k, y_k$ lies on the same line because (Q1) indicates $y_k - v_k$ parallels to $-(y_k - x_k)$ and both vector shares the same head which anchors at $y_k$. 
    
    \begin{proposition}[Stepwise Lyapunov]\label{prop:stepwise-lyapunov}\;\\
        Fix any integer $k \ge0$, assume $v_{k + 1}, x_{k + 1}, y_k, g_k, \hat \gamma$ are produced by Definition \ref{def:stepwise-wapg}. 
        Given any $R_k$.
        Define: 
        \begin{align*}
            R_{k + 1}
            \defeq
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right). 
        \end{align*}
        Then it has for all $x^* \in \RR^n$ where $F^* = F(x^*)$, the inequality: 
        \begin{align*}
            F(x_{k + 1}) - F^* + R_{k + 1} + \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}
        Start by considering the first and the third term of the LHS of the inequality that we want to prove. 
        \begin{align*}
            F(x_{k + 1}) &= 
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2, 
            \\
            R_{k + 1}
            &= 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right), 
            \\
            \implies 
            F(x_{k + 1}) + R_{k + 1}
            &= 
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            \\
            &\quad 
                + 
                (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma_k}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right). 
        \tag{1*}
        \end{align*}
        Coefficient of $\epsilon_k$ and $\Vert g_k\Vert^2$ are grouped. 
        Next, we have: 
        \begin{align*}
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^* \Vert^2
            &= 
            \frac{\hat \gamma}{2}\Vert 
                \hat \gamma^{-1}
                (
                    \gamma_k(1 - \alpha_k)v_k - 
                    \alpha_k g_k + \mu \alpha_k y_k
                )
                - x^* 
            \Vert^2
            \\
            &=  
            \frac{\hat \gamma}{2}
            \Vert 
                \hat \gamma^{-1}
                (
                \hat \gamma v_k + \mu \alpha_k(y_k - v_k)
                    - \alpha_k g_k
                )
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert 
                v_k + \hat \gamma^{-1} \mu \alpha_k (y_k - v_k)
                - \hat \gamma^{-1}\alpha_k g_k
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert v_k - x^*\Vert^2 
            + 
            \frac{\alpha_k^2}{2\hat \gamma}\Vert \mu(y_k - v_k) - g_k\Vert^2 
            \\ &\quad 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            \\ &\quad
                + 
                \frac{\alpha_k^2}{2\hat \gamma}
                \Vert \mu(y_k - v_k) - g_k\Vert^2 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle. 
        \tag{2*}
        \end{align*}
        On the above derivation, the first equality is by definition of $v_{k + 1}$; the second equality is by $\hat \gamma = (1 - \alpha_k)\gamma + \mu \alpha_k$ the second equality comes by considering: 
        \begin{align*}
            \gamma(1 - \alpha_k) v_k &= 
            (\hat \gamma  - \mu \alpha_k)v_k
            = \hat \gamma v_k - \mu\alpha_k v_k
            \\
            \iff 
            \gamma(1 - \alpha_k) v_k + \mu \alpha_k y_k
            &= 
            \hat \gamma v_k + \mu \alpha_k(y_k - v_k). 
        \end{align*}
        Focusing on the last two terms by the end of expression (2*), we have  
        \begin{align*}
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            & = 
            \frac{\alpha_k^2\mu}{\hat \gamma}
            \left(
                \frac{\mu}{2}\Vert y_k - v_k\Vert^2 
                - \langle y_k - v_k, g_k\rangle
            \right)
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2, 
            \\
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            &= 
            \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle 
            - \alpha_k \langle v_k - x^*, g_k\rangle. 
        \tag{2.1*}
        \end{align*}
        Adding the LHS of both equations above together gives: 
        {\small
        \begin{align*}
            & \quad 
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            + 
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
            + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \end{align*}
        }
        With the above we can conclude that (2*) simplifies to 
        {\small
        \begin{align*} 
            & 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            + 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \tag{2.2*}
        \end{align*}
        }
        Recall from 
        \hyperref[obs:stepwise-wapg]{Observations \ref*{obs:stepwise-wapg}} that: 
        \begin{align*}
            (Q1): 
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k),
            \\
            (Q2): 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
        Ok that is a lot, we list the following equations to assist things: 
        \begin{align*}
            &  
            - \alpha_k(v_k - x^*) - \frac{\alpha_k^2 \mu}{\hat \gamma}(y_k - v_k) - (x_k - y_k)
            \\
            \text{use Q1 here } & =
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k^2\mu}{\hat \gamma}\frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k)
            - (x_k - y_k) 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k \mu}{\gamma}(x_k - y_k)
            - (x_k - y_k) 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \left(
                1 + \frac{\alpha_k \mu}{\gamma}
            \right)(x_k - y_k)
            \\
            \text{use Q2 here}
            &= 
            -\alpha_k(v_k - x^*) - 
            \frac{\alpha_k \mu + \gamma}{\gamma}
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(x_k - v_k)
            \\
            &= 
            -\alpha_k(v_k - x^*)
            - \alpha_k(x_k - v_k)
            \\
            &= \alpha_k(x^* - x_k). 
        \tag{Q3}
        \end{align*}
        Adding (2.2*) to (1*) gives: 
        \begin{align*}
            &
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\quad 
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
                + 
                \left\langle g_k, 
                    - \alpha_k(v_k - x^*) 
                    - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \epsilon_k 
            + \left\langle 
                g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                - (x_k - y_k)
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            \text{Use Q3}&= 
            F(x_k) - \epsilon_k 
            + \alpha_k\left\langle 
                g_k, 
                x^* - x_k
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k\epsilon_k + \alpha_k\langle g_k, x^* - x_k\rangle
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat\gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \tag{3*}
        \end{align*}
        From the left to the right on the first equality, coefficients of $\Vert g_k\Vert^2$ cancels out to zero and the inner product containing $g_k$ are grouped. 
        Going from the left to the right of the third equality, we applied (Q3) derived earlier to simplify the inner product term. 
        The last equalities re-arranged terms and grouped the coefficients of $\epsilon_k$ together. 
        Taking a page break, continuing on (3*) we have
        \begin{align*}
            &
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \left(
                    \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}
                    + 
                    \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
                \right)\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            & =
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \frac{\mu \alpha_k}{2}\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\ &=
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                +
                \frac{\mu\alpha_k}{2} \Vert y_k - x^*\Vert^2
            \\&= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
            \tag{3.1*}
        \end{align*}
        On the first equality, coefficients of $\Vert y_k - v_k\Vert^2$ are grouped together. 
        On the second to the third equality, the terms had been simplified by 
        \begin{align*}
            \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma} + 
            \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
            &= 
            \frac{\mu\alpha_k}{2}\left(
                \frac{(1 - \alpha_k)\gamma_k + \alpha_k \mu}{\hat \gamma}
            \right)
            \\
            &= \frac{\mu\alpha_k}{2}\left(
                \frac{\hat \gamma}{\hat \gamma}
            \right) = \frac{\mu\alpha_k}{2}. 
        \end{align*}
        (3.1*) was adding (1*) and (2.2*) together, which is the same as adding (1*) and (2*) together. 
        So that was all equal to (1*) + (2*) and it says: 
        \begin{align*}
            & F(x_{k + 1}) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            & \iff 
            \\
            & F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_k) - F(x^*) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &= (1 - \alpha_k)(F(x_k) - F(x^*))
            + \alpha_k\left(
                F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                (1 - \alpha_k)\left(
                    R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
                \right). 
        \tag{3.2*}
        \end{align*}
        Focusing on the second term, we simplify the multiplier inside: 
        {\small
        \begin{align*}
            & F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= 
            F(x_k) - F(x^*) - \left(
                F(x_k) - F(T_L y_k) - \langle g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2
            \right)- \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= F(T_L y_k) - F(x^*) + \langle g_k, x^* - y_k\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            + \frac{1}{2L}\Vert g_k\Vert^2 \le 0. 
        \tag{4*}    
        \end{align*}
        }
        On the last line, we expand the definition of $g_k$ and then used 
        \hyperref[thm:prox-grad-ineq]{Theorem \ref*{thm:prox-grad-ineq}}. 
        Therefore, we conclude that: 
        {\small
        \begin{align*}
            F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)\left(
                F(x_k) - F(x^*) + R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
        }
    \end{proof}
    \begin{remark}
        \textcolor{red}
        {
        Use $\mu = D_f(x^*, y_k)/\Vert y_k - x^*\Vert^2$ also works, $\mu$ is a pessimistic choice for the inequality above. 
        }
        But in general the choice of $\mu$ remains the strong convexity modulus or equivalently, any value that is smallar than the true strong convexity constant for claiming the convergence rate for all initial guesses. 
    \end{remark}

\section{Formulation of R-WAPG and its convergence rates}
    Definition \ref{def:wapg} is an algorithm which generates iterates $(x_k, y_k, v_k)$ and admits potential convergence claim using Proposition \ref{prop:stepwise-lyapunov}. 
    It is the backbone of multiple accelerated first order method in the literatures which fits all variants under the same convergence claim. 

    \begin{definition}[Relaxed weak accelerated proximal gradient (R-WAPG)]\label{def:wapg}
        \;\\
        Initialize any $\gamma_0 \in (0, 1)$, $(x_0, v_0)$ or equivalently: $(x_0, y_0), (y_0, v_0)$. 
        The algorithm generates a sequence of vector $(x_k, y_k, v_k)$ and auxiliary sequence $\alpha_k, \rho_k$ such that they satisfy for all $k\ge 0$: 
        $$
        \begin{aligned}
            \gamma_k &:= \left.\begin{cases}
                \rho_{k -1}L\alpha_{k - 1}^2 & k \ge 1,
                \\
                \gamma_0 & k = 0. 
            \end{cases}\right\rbrace,
            \\
            \text{find }&\alpha_k \in (0, 1), \rho_k > 0: L\alpha_k^2 = (1 - \alpha_k)\gamma_k + \mu \alpha_k  \;\wedge \; 0 < \rho_k \alpha_k^2 < 1, 
            \\
            \hat \gamma_{k + 1} &:= L\alpha_k^2, 
            \\
            y_k &= 
            (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k), 
            \\
            g_k &= \mathcal G_L y_k, 
            \\
            v_{k + 1} &= 
            \hat\gamma^{-1}_{k + 1}
            (\gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
            \\
            x_{k + 1} &= T_L y_k. 
        \end{aligned}
        $$
    \end{definition}
    \begin{observation}
        Observe that if $\rho_k = 1$ for all $k\ge 0$, then the above algorithm is similar to (2.2.7) in Nesterov's book \cite{nesterov_lectures_2018}. 
    \end{observation}
    \begin{observation}\label{obs:r-wapg-observation-1}
        If $\alpha_0 \in (0, 1)$, then the entire sequence $\alpha_k \in (0, 1)$. 
        Suppose inductively that $\alpha_{k - 1}, \rho_{k - 1}$ are given such that they satisfy $\alpha_{k -1} \in (0, 1)$ and $0 < \rho_{k - 1} \alpha_{k - 1}^2 < 1$. 
        Solving the quadratic $L\alpha_k^2=(1 - \alpha_k)\rho_{k - 1}L\alpha_{k - 1}^2 + \mu \alpha_k$ for $\alpha_k$ yields candidates. 
        \begin{align*}
            \alpha_k &= 
            \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
                \pm
                \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
            \right). 
        \end{align*}
        Choosing the positive sign which is the larger one of the roots of the quadratic. 
        We have $\alpha_k > 0$ because: 
        \begin{align*}
            \alpha_k &=
            \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
                +
                \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
            \right)
            \\
            &\ge 
            \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
                +
                \left|\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L\right|
            \right) + \alpha_{k - 1}\sqrt{\rho_{k - 1}}
            \\
            & > 0. 
        \end{align*}
        On the last strict inequality we used the fact that $\rho_{k - 1}> 0, \alpha_{k - 1} > 0$. 
        An upper bound can be identified by using inductive hypothesis and considering: 
        \begin{align*}
            \alpha_k &= 
            \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
                +
                \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
            \right)
            \\
            &<
            \frac{1}{2}\left(
                \frac{\mu}{L} + 
                \sup_{x \in (0, 1)}
                \left\lbrace
                    -x + \sqrt{(x - \mu/L)^2 + x}
                \right\rbrace
            \right)
            \\
            & \le \frac{1}{2}\left(
                \mu/L + \max\left(\mu/L, -1 + \sqrt{(1 - \mu/L)^2 + 1}\right)
            \right) \le 1. 
        \end{align*}
        Going to the first inequality, we used $\rho_{k - 1} \alpha_{k - 1}^2 < 1$ to get the strict inequality. 
        Going from the second to the third inequality, we maximized $\mu/L$ by monotone increasing of linear function and the square root function. 
        Therefore, inductively, $\alpha_k \in (0, 1)$ holds. 
        However, the limit of $\alpha_k$ could be 1. 
    \end{observation}
    
    \begin{proposition}[R-WAPG convergence claim]\label{prop:wagp-convergence}
        Let vectors $(x_k, y_k, v_k)_{k \ge0}$ and scalars $\alpha_k$ be generated by R-WAPG (Definition \ref{def:wapg}). 
        Define recursively for all $k\ge1$ by 
        \begin{align*}
        R_{k + 1}
        := 
        \frac{1}{2}\left(
            L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
        \right)\Vert g_k\Vert^2
        + 
        (1 - \alpha_k)
        \left(
            \epsilon_k + R_k + 
            \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
            \Vert v_k - y_k\Vert^2
        \right). 
        \end{align*}
        Choose $R_0= 0$ to be the base case. 
        Choose any $x^* \in \RR^n$ then
        \begin{align*}
            & F(x_{k + 1}) - F(x^*) + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 0}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_0) - F(x^*) + \frac{\gamma_0}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}  
        Recall the stepwise convergence theorem, since the choice of $\gamma, \hat \gamma$ is arbitrary choices for the theorem, we choose $\gamma = \rho_{k - 1} L \alpha_{k - 1}^2 > 0, \hat \gamma = L \alpha_k^2 > 0$ at each step. 
        Because $\alpha_k \in(0, 1)$ by Observation \ref{obs:r-wapg-observation-1}, we can use Proposition \ref{prop:stepwise-lyapunov}: 
        {\small
        \begin{align*}
            &F(x_{k + 1}) - F^* + R_{k + 1} + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{\rho_{k - 1}L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \max(1, \rho_{k - 1})\frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \max(1, \rho_{k - 1})(1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 0}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_0) - F^* + R_0 + \frac{\gamma_0}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
        }
        Additionally, the algorithm converges by considering $R_k$: 
        \begin{align*}
            R_{k + 1}
            &= 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &= (1 - \alpha_k)
            \left(
                \epsilon_k + R_k 
                + \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\ge 
            (1 - \alpha_k) R_k
            \\
            &\ge R_0 \prod_{i = 0}^{k} \left(1 - \alpha_i\right) = 0. 
        \end{align*}
        Going from the left to the right on the first equality, we used the fact that $\hat \gamma_{k + 1} = L \alpha_{k}^2$.
        This makes coefficient of $\Vert g_k\Vert^2$ zero. 
        The first inequality is by $\epsilon_k \ge 0$ and the non-negativity of the remaining terms. 
        The last equality is by the assumption that $R_0 = 0$. 
        Therefore: 
        {\small
        \begin{align*}
            & F(x_{k + 1}) - F^* +
            \frac{L\alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 0}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_0) - F^* + R_0 + \frac{\gamma_0}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
        }
    \end{proof}
    \begin{remark}        
        \textcolor{red}
        {
            The choice of $\rho_k$ allows for divergence to traditional choices of step sizes in the literatures. 
            As we can see from the above convergence claim, there exists choices of $\rho_k\ge 1$ where convergence remains possible. 
        }
    \end{remark}


\section{Equivalent representations of R-WAPG}
    This section reduces Definition \ref{def:wapg} into simpler forms that are comparable to what commonly appears in the literatures. 
    Proposition \ref{prop:wapg-first-equivalent-repr} simplifies Definition \ref{def:wapg} for the specific initial conditions $\gamma_0 \in(0, 1)$ and finds a representation without auxiliary sequence $\gamma_k, \hat \gamma_k$. 
    Definition \ref{def:r-wapg-intermediate} states the first simplified form of the R-WAPG algorithm which we call: ``R-WAPG intermediate form''. 
    Following a similar pattern, Proposition \ref{prop:wagp-st-form}, \ref{prop:wagp-st-form} states two more equivalent representations under R-WAPG intermediate form which are formulated as Definition \ref{def:r-wapg-st-form}, \ref{def:r-wapg-momentum-form}. 
    
    To start, the following proposition on ``abstract similar triangle form'' were made to simplify arguments and notations to make nicer proofs. 


    \begin{proposition}[Abstract similar triangle form]\label{prop:abs-st-form}\;\\
        For all $k\ge 0$, iterates $(w_t, y_t, x_{t + 1}, z_{t + 1})$ satisfies recursively that: 
        \begin{align*}
            y_k &= (1 + \tau_k)^{-1}(v_k + \tau_k x_k),
            \\
            v_{k + 1} &= (1 + \xi_k)^{-1}(v_k + \xi_k y_k) - (1 + \xi_k)^{-1}\delta_k g_k,
            \\
            x_{k + 1} &= y_k - L^{-1} g_k. 
        \end{align*}
        If $1 + \xi_t + \tau_t = L\delta_t$. Then 
        $$
            v_{k + 1} - x_{k + 1} = (1 + \xi_t)^{-1}\tau_t(x_{k + 1} - x_k). 
        $$
        Which makes the algorithm a similar triangle form. 
    \end{proposition}
    \begin{proof}
        We are interested in identifying the conditions required for the sequence of $\xi_t, \tau_t, \delta_t$ such that there exists $\theta_t$ satisfying: 
        \begin{align*}
            v_{t + 1} - x_{t + 1} 
            &= \theta_t(x_{t + 1} - x_t)
        \end{align*}
        To verify, do 
        \begin{align*}
            v_{t + 1} &= 
            (1 + \xi_t)^{-1}(v_t + \xi_t y_t - \delta_t \mathcal G_L(y_t))
            \\
            & \textcolor{gray}{
                v_t = (1 + \tau_t)y_t - \tau_t x_t
            }
            \\
            &= 
            (1 + \xi_t)^{-1}((1 + \tau_t)y_t - \tau_t x_t + \xi_t y_t - \delta_t \mathcal G_L(y_t))
            \\
            &= 
            (1 + \xi_t)^{-1}((1 + \tau_t + \xi_t)y_t - \tau_t x_t - \delta_t \mathcal G_L(y_t))
            \\
            v_{t + 1} - x_{t + 1}
            &= 
            (1 + \xi_t)^{-1}((1 + \tau_t + \xi_t)y_t - \tau_t x_t - \delta_t \mathcal G_L(y_t))
            - (y_t - L^{-1}\mathcal G_Ly_t)
            \\
            &= 
            (1 + \xi_t)^{-1}(\tau_ty_t - \tau_t x_t - \delta_t \mathcal G_L(y_t))
            + L^{-1}\mathcal G_Ly_t
            \\
            &= 
            (1 + \xi_t)^{-1}
            \left(
                \tau_ty_t - \tau_t x_t + (L^{-1}(1 + \xi_t) - \delta_t) \mathcal G_L(y_t)
            \right)
            \\
            &= 
            (1 + \xi_t)^{-1}\tau_t
            \left(
                y_t - x_t + 
                \tau_t^{-1}(L^{-1}(1 + \xi_t) - \delta_t) \mathcal G_L(y_t)
            \right)
        \end{align*}
        The RHS is can be verified through 
        \begin{align*}
            x_{t + 1} - x_t &= 
            y_t - L^{-1}\mathcal G_L(y_t) - x_t
            \\
            &= (y_t - x_t) - L^{-1}\mathcal G_L(y_t). 
        \end{align*}
        It necessitates the condition: 
        \begin{align*}
            \tau_t^{-1}(L^{-1}(1 + \xi_t) - \delta_t) 
            &= - L^{-1}
            \\
            (1 + \xi_t) - L\delta_t
            &= 
            - \tau_t
            \\
            1 + \xi_t + \tau_t
            &=
            L\delta_t. 
        \end{align*}
        Which allows for: 
        \begin{align*}
            v_{t + 1} - x_{t + 1} &= 
            (1 + \xi_t)^{-1}\tau_t
            \left(y_t - x_t - L^{-1}\mathcal G_L(y_t)\right) 
            = 
            (1 + \xi_t)^{-1}\tau_t(x_{t + 1} - x_t). 
        \end{align*}
    \end{proof}

    \subsection{Narrowing it down}
        \begin{proposition}[First equivalent representation of R-WAPG]\label{prop:wapg-first-equivalent-repr}\;\\
            Assume $\mu < L$. 
            Choose any $\gamma_0 \in (0, 1)$ as the initial condition for R-WAPG (Definition \ref{def:wapg}) to produce vectors sequence $(y_i)_{i \in \N}, (x_i)_{i \in \N}, (v_i)_{i \in \N}$ and $(\rho_k)_{i \in \N}, (\alpha_k)_{i \in \N}$. 
            Then the iterates can be expressed without $(\gamma_k)_{k \ge0}, (\rho_k)_{k\ge0}$, and for all $k\ge 0$ they are algebraically equivalent to
            \begin{align*}
                y_k &= 
                \left(
                    1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
                \right), 
                \\
                x_{k + 1} &= 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                v_{k + 1} &= 
                \left(
                    1 + \frac{\mu}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
        \end{proposition}
        \begin{proof}
            \begin{align*}
                y_{k} &= 
                (\gamma_k + \alpha_k \mu)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                (\hat \gamma_{k + 1} + \alpha_k \gamma_k)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                \left(
                    \frac{\hat \gamma_{k + 1}}{\alpha_k\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    1 + \frac{L\alpha_k^2}{\alpha_k \rho_{k - 1}L\alpha_{k - 1}^2} 
                \right)^{-1}
                \left(
                    v_k + \frac{L\alpha_k^2}{\alpha_k \rho_{k - 1}L\alpha_{k - 1}^2} x_k
                \right)
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k}{\rho_{k - 1}\alpha_{k - 1}^2}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k}{\rho_{k - 1}\alpha_{k - 1}^2} x_k
                \right)
                \\
                &= 
                \left(
                    1 + \frac{L - L \alpha_k}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{L - L \alpha_k}{L \alpha_k - \mu} x_k
                \right). 
            \end{align*}
            From the left to right of the second inequality, we used the fact that $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k\mu$. 
            Going from the left to the right of the second last inequality, we did the following: 
            \begin{align*}
                L\alpha_k^2 &= 
                (1 - \alpha_k)L\rho_{k- 1}\alpha_{k - 1}^2 + \alpha_k \mu 
                \\
                \iff 
                L \alpha_k^2 - \alpha_k\mu &= 
                (1 - \alpha_k)L \rho_{k - 1}\alpha_{k - 1}^2
                \\
                \iff 
                \rho_{k - 1}\alpha_{k - 1}^2
                &= 
                \frac{L \alpha_k^2 - \alpha_k\mu}{L (1 - \alpha_k)}
                \\
                \iff 
                \frac{1}{\rho_{k - 1}\alpha_{k - 1}^2}
                &= 
                \frac{L (1 - \alpha_k)}{L \alpha_k^2 - \alpha_k\mu}
                \\
                \iff 
                \frac{\alpha_k}{\rho_{k - 1}\alpha_{k - 1}^2}
                &= 
                \frac{L - L\alpha_k}{L\alpha_k - \mu}. 
            \end{align*}
            On the third $\iff$, we can assume $\alpha_k \neq \mu/L$ because $\alpha_k = \mu/L$ is impossible. 
            For contradiction, assume $a_k = \mu/L$. 
            Then
            \begin{align*}
                L \alpha_k^2 - \alpha_k\mu &= 
                (1 - \alpha_k)L \rho_{k - 1}\alpha_{k - 1}^2 = 0 \implies \alpha_{k - 1} = 0. 
            \end{align*}
            Which contradict $\alpha_{k - 1} \in (0, 1)$. 
            For $v_{k + 1}$ considers 
            \begin{align*}
                v_{k + 1} &= 
                \hat \gamma_{k + 1}^{-1}
                ((1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
                \left(
                    (1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k} y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k\mu}{(1 -\alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k\mu}{(1 - \alpha_k)\rho_{k - 1}L\alpha_{k - 1}^2} y_k
                \right)
                - \frac{1}{L\alpha_{k}}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu} y_k
                \right)
                - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
            Going from the left to the right of the second equality, we substitute $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu\alpha_k$. 
            Going from the left to right of the second last equality, we made the substitution $\hat \gamma_{k + 1} = L \alpha_k^2$, and $\gamma_k = \rho_{k - 1}L \alpha_{k - 1}^2$. 
            To arrive at the last equality, we considered substituting the denominator by the definition for $\hat\gamma_{k + 1}$. 
            \begin{align*}
                (1 - \alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2
                &= 
                \hat \gamma_{k + 1} - \mu \alpha_k
                = 
                L\alpha_{k}^2 - \alpha_k\mu. 
            \end{align*}
            The is now complete. 
            This form doesn't have $\rho_k, \gamma_k, \hat \gamma_k$ in it. 
            Finally, to see the inequality, consider the fact that $\rho_k \in [0, 1]$ so it has: 
            \begin{align*}
                \hat \gamma_{k + 1} = L\alpha_k^2 = (1 - \alpha_k)\rho_{k - 1}L \alpha_{k - 1}^2 + \alpha_k \mu 
                \le (1 - \alpha_k)L \alpha_{k - 1}^2 + \alpha_k \mu. 
            \end{align*}
            The only auxiliary sequence it needs is $\alpha_k$, which directly links to the convergence rate of the algorithm. 
        \end{proof}
        
        \begin{definition}[R-WAPG intermediate form]\label{def:r-wapg-intermediate}\;\\
            Given any initial $\alpha_0 \in (0, 1)$, $(x_0, y_0)$ or equivalently $(x_0, v_0), (y_0, v_0)$. 
            Assume $\mu < L$. 
            The algorithm generate sequence of vector iterates $(x_k, v_k, y_k)_{k \ge0}$ by the procedures: 
            \begin{align*}
                \text{find }& \rho_{k - 1}: 0 < \rho_{k - 1} < \alpha_{k - 1}^{-2},
                \\
                \alpha_k &= 
                \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
                +
                \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
                \right), 
                \\
                y_k &= 
                \left(
                    1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
                \right), 
                \\
                x_{k + 1} &= 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                v_{k + 1} &= 
                \left(
                    1 + \frac{\mu}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
        \end{definition}
        \begin{proposition}[Second equivalent representation of R-WAPG]\label{prop:wagp-st-form}\;\\
            Let iterates $(x_k, v_k, y_k)_{k \ge 0}$ and sequence $(\alpha_k, \rho_k)_{k \ge0}$ be given by Definition \ref{def:r-wapg-intermediate}. 
            Then for all $k \ge0$, iterate $(v_k)_{k \ge 0}, (x_k)_{k \ge 0}, (y_k)_{k \ge0}$ are algebraically equivalent to: 
            \begin{align*}
                y_k &= 
                \left(
                    1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
                \right), 
                \\
                x_{k + 1} &= 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                v_{k + 1} &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proposition}
        \begin{proof}
            From Definition \ref{def:r-wapg-intermediate}, set 
            \begin{align*}
                \tau_k &= \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}, 
                \\
                \xi_k &= \frac{\mu}{L \alpha_k - \mu},
                \\
                (1 + \xi_k)^{-1}\delta_k &= \frac{1}{L\alpha_k}
                \iff L \delta_k = \frac{1 + \xi_k}{\alpha_k}. 
            \end{align*}
            We claim that these parameters satisfy $L\delta_k = 1 + \tau_k + \xi_k$. 
            Hence $v_{k + 1} - x_{k + 1} = (1 + \xi_k)^{-1}(x_{k + 1} - x_k)$. 
            To see, we have 
            \begin{align*}
                1 + \tau_k + \xi_k &= 
                1 + \frac{L(1 - \alpha_k)}{L \alpha_k - \mu} 
                + \frac{\mu}{L \alpha_k - \mu}
                \\
                &= 
                1 + \frac{L - L \alpha_k + \mu}{L\alpha_k - \mu}
                \\
                &= 
                \frac{L - L \alpha_k + \mu + L \alpha_k - \mu}{L\alpha_k - \mu}
                \\
                &= \frac{L}{L\alpha_k - \mu}. 
            \end{align*}
            Next
            \begin{align*}
                1 + \tau_k + \xi_k &= 
                1 + \frac{L(1 - \alpha_k)}{L \alpha_k - \mu} 
                + \frac{\mu}{L \alpha_k - \mu}
                \\
                &= 
                1 + \frac{L - L \alpha_k + \mu}{L\alpha_k - \mu}
                \\
                &= 
                \frac{L - L \alpha_k + \mu + L \alpha_k - \mu}{L\alpha_k - \mu}
                \\
                &= \frac{L}{L\alpha_k - \mu}. 
            \end{align*}
            Therefore, substituting it gives: 
            \begin{align*}
                v_{k + 1} &= 
                x_{k + 1} + \left(
                    1 + \frac{\mu}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k - \mu}{L\alpha_k}
                \right)\left(
                    \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proof}
    \begin{definition}[R-WAPG similar triangle form]\label{def:r-wapg-st-form}
        Given any initial $\alpha_0 \in (0, 1)$, $(x_0, y_0)$ or equivalently $(x_0, v_0), (y_0, v_0)$. 
        Assume $\mu < L$. 
        The algorithm generate sequence of vector iterates $(x_k, v_k, y_k)_{k \ge0}$ by the procedures: 
        \begin{align*}
            \text{find }& \rho_{k - 1}: 0 < \rho_{k - 1} < \alpha_{k - 1}^{-2},
            \\
            \alpha_k &= 
            \frac{1}{2}\left(
            \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
            +
            \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
            \right), 
            \\
            y_k &= 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right), 
            \\
            x_{k + 1} &= 
            y_k - L^{-1} \mathcal G_L y_k, 
            \\
            v_{k + 1} &= 
            x_{k + 1} + (\alpha_k^{-1} -1)(x_{k + 1} - x_k). 
        \end{align*}
    \end{definition}
    
    \begin{proposition}[Third equivalent representation of R-WAPG]\label{prop:r-wapg-momentum-repr}
        \;\\
        Let sequence $(\alpha_k, \rho_k)_{k \ge0}$ and iterates $(x_k, v_k, y_k)_{k\ge 0}$ given by R-WAPG intermediate form (Definition \ref{def:r-wapg-st-form}). 
        Then the iterates are algebraically equivalent to: 
        \begin{align*}
            x_{k + 1} &= y_k - L^{-1}\mathcal G_Ly_k, 
            \\
            y_{k + 1} &= 
            x_{k + 1} + 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        In the special case when $\mu = 0$, the momentum term admits simpler representation 
        \begin{align*}
        \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
        & = \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proposition}

    \begin{proof}
        Start by considering the update rule for $v_k$ from the similar triangle form of WAPG which is: 
        \begin{align*}
            v_{k + 1} &= 
            x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \\
            \iff 
            (L \alpha_{k + 1} - \mu)v_{k + 1} 
            &= 
            (L \alpha_{k + 1} - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
        \end{align*}
        Next, we simplify the update for the iterates $y_{k}$ by the similar triangle form of R-WAPG. 
        \begin{align*}
            y_k &= 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \left(
            \frac{L - \mu}{L\alpha_k - \mu} 
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \frac{L\alpha_k - \mu}{L - \mu} v_k
            + 
            \frac{L - L \alpha_k}{L - \mu} x_k
            \\
            &= (L - \mu)^{-1}((L \alpha_k - \mu)v_k + (L - L \alpha_k)x_k). 
        \end{align*}
        Now considering substituting $v_{k + 1}$ into $y_{k + 1}$ so it has: 
        {\small
        \begin{align*}
            y_{k + 1} &= 
            (L - \mu)^{-1}((L\alpha_{k + 1} - \mu)v_{k + 1} + (L - L \alpha_{k + 1})x_{k + 1})
            \\
            &= (L - \mu)^{-1}
            \left(
                (L\alpha_{k + 1} - \mu)x_{k + 1} + 
                (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
                + (L - L \alpha_{k + 1})x_{k + 1}
            \right)
            \\
            &= 
            (L - \mu)^{-1}
            \left(
                (L - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \right)
            \\
            &= x_{k + 1} + \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}(x_{k + 1} - x_k). 
        \end{align*}
        }

        The coefficient for $(x_{k + 1} - x_k)$ needs some works to formulate it without the parameter $\mu, L$. 
        To do that we have: 
        \begin{align*}
            \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}
            &= \frac{(L\alpha_{k + 1} - \mu)\alpha_k(1 - \alpha_k)}{\alpha_k^2(L - \mu)}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \left(
                \frac{\alpha_k^2(L - \mu)}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \alpha_k(1 - \alpha_k)
            \left(
                \frac{L\alpha_k^2 - \mu\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \rho_k\left(
                \frac{L\rho_k\alpha_k^2 - \mu\rho_k\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \rho_k\alpha_k(1 - \alpha_k)
            \left(
                \frac{(L\alpha_{k + 1} - \mu)(\rho_k\alpha_k^2 + \alpha_{k + 1})}
                {L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}.
        \end{align*}
        Going from the left to right on the fourth equality, we used $L\alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_kL\alpha_k^2 + \mu \alpha_{k + 1}$ with 
        \begin{align*}
            L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2 
            &= 
            (1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            ((1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \mu \alpha_{k + 1}) - \mu\alpha_{k + 1} + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= L \alpha_{k + 1}^2 - \mu\alpha_{k + 1} + \alpha_{k + 1}L\rho_k\alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            L\alpha_{k + 1}(\alpha_{k + 1} + \rho_k \alpha_k^2) - \alpha_{k + 1}\mu - \mu \rho_k \alpha_k^2
            \\
            &= (L \alpha_{k + 1} - \mu)(\alpha_{k + 1} + \rho_k \alpha_k^2). 
        \end{align*}
        When $\mu = 0$, things simplify. 
        Consider that $\alpha_{k +1}^2 = (1 - \alpha_{k + 1})\rho_k\alpha_k^2$. 
        \begin{align*}
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \alpha_{k + 1}^2}
            \\
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \rho_k(1 - \alpha_{k + 1})\alpha_k^2}
            \\
            &= \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2}
            \\
            &= \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proof}

    \begin{definition}[R-WAPG momentum form]\label{def:r-wapg-momentum-form}
        Initialize any $\alpha_0 \in (0, 1)$. 
        The algorithm generates iterates $(v_k, x_k, y_k)$ and auxiliary sequence $(\rho_k)_{k \ge0}$ by the following procedures: 
        \begin{align*}
            \text{find }& \rho_{k - 1}: 0 < \rho_{k - 1} < \alpha_{k - 1}^{-2},
            \\
            \alpha_k &= 
            \frac{1}{2}\left(
            \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
            +
            \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
            \right), 
            \\
            x_{k + 1} &= y_k - L^{-1}\mathcal G_Ly_k, 
            \\
            y_{k + 1} &= 
            x_{k + 1} + 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        In the special case $\mu = 0$, the momentum term can be represented without relaxation parameter $\rho_k$: 
        $$
        \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}} 
        = \alpha_{k + 1}(\alpha_k^{-1} - 1).  
        $$
        
    \end{definition}
    \begin{remark}
        \textcolor{red}
        {
        There exists a choice of $\rho_k \in [0, 1), \mu = 0$ such that the above algorithm is equivalent to Chambolle Dossal 2015 \cite{chambolle_convergence_2015}. 
        When $\rho_k = 1$, this becomes just FISTA \cite{beck_fast_2009}. 
        When $L > \mu > 0$, then choosing $\rho_k = 1$ yields equation 
        \begin{align*}
            L \alpha_{k + 1}^2 = (1- \alpha_{k + 1})L \alpha_k^2 + \alpha_{k + 1} \mu. 
        \end{align*}
        Here, if we assume sequence $\alpha_k$ is constant, then $\alpha_k = \sqrt{\mu/L}$ is a solution to the above iterative equation. 
        Substituting it back to the momentum form of R-WAPG, we recovers the V-FISTA algorithm, which appears as (10.7.7) in Beck's Book \cite{beck_first-order_2017}. 
        }
    \end{remark}

\subsection{Representing algorithms in the literatures via R-WAPG}
\subsection{The method of Spectral Momentum}
\subsection{Numerical experiments and applications}


\bibliographystyle{siam}
\bibliography{references/Spectral_Momentum.bib}

\end{document}
