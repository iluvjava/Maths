\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{{\fontfamily{ptm}\selectfont 
        A Parameter Free Accelerated Proximal Gradient Method Without Restarting
    }}

\author{
    Hongda Li
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~Xianfu Wang
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
    and ~Heinz H. Bauschke~
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{heinz.bauschke@ubc.ca}.
    }
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    
    \cite{nesterov_lectures_2018}
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 65K10, 90c25, 90C30; Secondary 65Y20. 
\noindent{\bfseries Keywords: } Nesterov acceleration, Proximal point method. 
\section{Introduction}
    \subsection{Literature reviews}
    \subsection{Contributions}
        Our contributions are two folds. 
        
        % We present a theoretical unification through the formulation of ``Relaxed weak accelerated proximal gradient algorithm (R-WAPG)'' which we invented. 
        % It unifies convergence claim for accelerated method for the strongly convex objective and Lipschitz smooth but non-strongly convex case, and it relaxes the conditions on the momentum sequence. 
        
        % We connect the theories and practice by pointing out one practical exploits inspired the convergence proof of the algorithm which we called ``Spectral Momentum''. 
        % Numerical experiments are conducted to illustrate the theoretical convergence of R-WAPG and the performance of Spectral Momentum in with applications. 
        % We do net yet have a proof for the convergence of the algorithm in general. 

\section{Preliminaries}
    Throughout Section 1, 2, 3, we assume the problem of types $F = f + g$ where $f: \RR \rightarrow \RR$ is $L$ Lipschitz smooth and $\mu \ge 0$ strongly convex and $g: Q \rightarrow \overline \RR$ is convex. 
    We consider optimization problem of the form
    \begin{align*}
        \min_x \left\lbrace
            F(x) \defeq f(x) + g(x)
        \right\rbrace. 
    \end{align*}
    To start we define the following quantities
    \begin{enumerate}
        \item The proximal gradient model function: 
        $$
            \widetilde{\mathcal M}^{L^{-1}}
            (x; y) \defeq
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle 
            + \frac{L}{2}\Vert x - y\Vert^2. 
        $$
        \item The proximal point model function: 
        $$
            \mathcal M^{L^{-1}}(x; y) := F(x) + \frac{L}{2}\Vert x - y\Vert^2
        $$
        \item The proximal gradient operator $T_L = [I + L^{-1}\partial g]\circ [I - L^{-1}\nabla f]$. Since $L$ is fixed throughout, the subscript may be omitted and become $T$ instead to make the notations simpler. 
    \end{enumerate}
    To state the following lemma, we define the Bregman divergence of $f$ to be 
    \begin{align*}
        D_f(x, y): \RR^n \times \RR^n \rightarrow \RR 
        \defeq f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
    \end{align*}
    
    \begin{lemma}[Proximal gradient envelope] 
        Fix any $y$, we will have for all $x \in \RR^n$ that: 
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y)
            &= 
            \mathcal M^{L^{-1}}(x; y)- D_f(x, y) \le \mathcal M^{L^{-1}}(x; y). 
        \end{align*}
    \end{lemma}
    \begin{proof}
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) 
            &= 
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            g(x) + f(x) - f(x) + f(y) 
            + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            F(x) - D_f(x, y) + \frac{L}{2}\Vert x - y\Vert^2 
            \\
            &= \mathcal M^{L^{-1}}(x; y) - D_f(x, y). 
        \end{align*}
    \end{proof}
    
    \subsection{Proximal inequality}
    \begin{theorem}[Proximal inequality]\label{thm:prox-grad-ineq}
        Fix any $y$, we have for all $x$: 
        \begin{align*}
            F(x) - F(Ty) - \langle L(y - Ty), x - Ty\rangle
            &\ge  D_f(x, y). 
        \end{align*}
    \end{theorem}
    \begin{proof}
        $\widetilde {\mathcal M}(\cdot; x)$ is $L$ strongly convex with minimizer $Ty$
        {\small
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) - 
            \widetilde{\mathcal M}^{L^{-1}}(Ty; y)
            - 
            \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 
            0
            \\
            \iff
            \left(
                \mathcal M^{L^{-1}}(x; y) - D_f(x, y)
            \right) - 
            \mathcal M^{L^{-1}}(Ty; y) 
            - 
            \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                \mathcal M^{L^{-1}}(x; y)
                - 
                \mathcal M^{L^{-1}}(Ty; y)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}\Vert x - y\Vert^2 - 
                \frac{L}{2}\Vert Ty - y\Vert^2
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - Ty + Ty - y\Vert^2
                    - 
                    \Vert y - Ty\Vert^2
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - Ty\Vert^2 + 
                    2\langle x - Ty, Ty - y\rangle
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff
            \left(
                F(x) - F(Ty) + \frac{L}{2}\Vert x - Ty\Vert^2 
                - L\langle  x - Ty, y - Ty\rangle
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - Ty\Vert^2
            &\ge 0
            \\
            \iff 
            F(x) - F(Ty)
            - \langle L(y - Ty), x - Ty\rangle
            - D_f(x, y) 
            &\ge 0. 
        \end{align*}
        }
    \end{proof}
    \begin{remark}
        In our proof, we may use the following alternative representation of the above inequality which is 
        \begin{align*}
            F(x) - F(Ty) - \langle L(y - Ty), x - Ty\rangle - D_f(x, y) &\ge 0
            \\
            \iff
            F(x) - F(Ty)
            - \langle L(y - Ty), x - y + y - Ty\rangle - D_f(x, y) 
            &\ge 0
            \\
            \iff
            F(x) - F(Ty)
            - \langle L(y - Ty),x - y \rangle
            - L\Vert y - Ty\Vert^2
            - D_f(x, y) 
            &\ge 0
            \\
            \implies 
            F(x) - F(Ty)
            - \langle L(y - Ty),x - y \rangle
            - \frac{L}{2}\Vert y - Ty\Vert^2
            - D_f(x, y) 
            &\ge 0
            \\
            \implies
            F(x) - F(Ty)
            - \langle L(y - Ty),x - y \rangle
            - \frac{L}{2}\Vert y - Ty\Vert^2
            - \frac{\mu}{2}\Vert x - y\Vert^2
            &\ge 0. 
        \end{align*}
        
    \end{remark}

\section{Stepwise formulation of weak accelerated proximal gradient}
    In this section, we introduce the following new quantities and their notations. 
    It's there to make the notations simpler and easier to follow for the proofs that will come later. 
    The algorithm is parameterized by the sequence $y_k, x_k$. 
    We define for all $k \ge 0$ the following quantities: 
    \begin{assumption}
        Choose any integer $k\ge 0$. 
        Given $x_k, y_k, v_k$, we define the following quantities
        \begin{align}
            g_k &\defeq L(y_k - T_L y_k), 
            \label{eqn:grad-map}
            \\
            l_F(x; y_k) &\defeq F(T_Ly_k) + \langle g_k, x - y_k\rangle + \frac{1}{2L}\Vert g_k\Vert^2, 
            \label{eqn:lower-linearization}
            \\
            \epsilon_{k} &\defeq F(x_k) - l_F(x_k; y_k), 
            \label{eqn:regret}
        \end{align}
        Observe that by convexity of $F$, $\epsilon_k \ge 0$ for all $x_k, L > 0$. 
        To see, consider the last inequality in the remark of Theorem \ref{thm:prox-grad-ineq} with $y = y_k, x = x_k$: 
        \begin{align*}
            F(x_k) - F(T_Ly_k)
            - \langle L(y_k - T_Ly_k),x_k - y_k \rangle
            - \frac{L}{2}\Vert y_k - T_Ly_k\Vert^2
            - \frac{\mu}{2}\Vert x_k - y_k\Vert^2
            &\ge 0
            \\
            \implies 
            F(x_k) - F(T_Ly_k)
            - \langle g_k,x_k - y_k \rangle
            - \frac{1}{2L}\Vert g_k\Vert^2
            &\ge 0. 
        \end{align*}
    \end{assumption}
    
    \begin{definition}[Stepwise weak accelerated proximal gradient]\label{def:stepwise-wapg}\;\\
        Assume $0 \le \mu < L$.  
        Fix any $k \in \mathbb Z$. 
        For any $(v_k, x_k), \alpha_k \in (0, 1), \gamma > 0$, let $\hat \gamma$, and vectors $y_k, v_{k + 1}, x_{k + 1}$ be given by: 
        \begin{align*}
            \hat \gamma &= (1 - \alpha_k)\gamma + \mu \alpha_k, 
            \\
            y_k &= 
            (\gamma + \alpha_k \mu)^{-1}(\alpha_k \gamma v_k + \hat\gamma x_k), 
            \\
            g_k &= \mathcal G_L y_k, 
            \\
            v_{k + 1} &= \hat\gamma^{-1}
            (\gamma(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
            \\
            x_{k + 1} &= T_L y_k. 
        \end{align*}
    \end{definition}
    \begin{observation}\label{obs:stepwise-wapg}
        Let's observe Definition \ref{def:stepwise-wapg} closely. 
        We make two crucial observations here. 
        We have $x_k, y_k, v_k$ lie on the same line because and they have the following equivalent equalities: 
        \begin{align*}
            (Q1): 
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k),
            \\
            (Q2): 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
    \end{observation}
        To see (Q1), observe 
        \begin{align*}
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k)
            \\
            \iff 
            -(\alpha_k \gamma\hat \gamma^{-1} + 1)y_k
            &= 
            - \alpha_k \gamma \hat \gamma^{-1}v_k - x_k
            \\
            \iff 
            y_k &= 
            \frac{
                \alpha_k \gamma \hat \gamma^{-1}v_k + x_k
            }{1 + \alpha_k \gamma \hat \gamma^{-1}}
            \\
            &=  
            \frac{\alpha_k \gamma v_k + \hat \gamma x_k}{\gamma + \alpha_k \mu}.
        \end{align*}
        On the first equality (Q1), we multiplied both side of the equation by $\hat\gamma/\alpha_k \gamma$ and then group $y_k$ on the LHS. 
        The last equality comes by multiplying both the numerator and denominator by $\hat \gamma$, leaving the numerator $\hat \gamma + \alpha_k \gamma = \gamma + \alpha_k \mu$. 
        We used the definition of $\hat \gamma$ here. 
        To see the second equality (Q2), consider: 
        \begin{align*}
            y_k &= (\gamma + \alpha_k \mu)^{-1}(\alpha_k \gamma v_k + \hat\gamma x_k)
            \\
            \iff
            y_k - x_k &= 
            (\gamma + \alpha_k \mu)^{-1}
            (\alpha_k \gamma v_k - (\gamma + \alpha_k \mu)x_k + \hat \gamma x_k)
            \\
            \iff 
            (\gamma + \alpha_k \mu)(y_k - x_k)
            &= 
            \alpha_k\gamma v_k + 
            (\hat \gamma - \gamma - \alpha_k \mu) x_k
            \\
            &= \alpha_k \gamma v_k - \alpha_k \gamma x_k 
            \\
            &= \alpha_k \gamma(v_k - x_k)
            \\
            \iff 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
        On the second equality that follows the second $\iff$ we just substituted $\hat\gamma = (1 - \alpha_k)\gamma + \alpha_k \mu$. 
        Therefore, we discover that $x_k, v_k, y_k$ lies on the same line because (Q1) indicates $y_k - v_k$ parallels to $-(y_k - x_k)$ and both vector shares the same head which anchors at $y_k$. 
    
    \begin{proposition}[Stepwise Lyapunov]\label{prop:stepwise-lyapunov}\;\\
        Fix any integer $k \in \mathbb Z$, for any $v_k, x_k$, let $v_{k + 1}, x_{k + 1}, y_k, v_k, x_k, \hat \gamma$ be given by Definition \ref{def:stepwise-wapg}. 
        Choose any arbitrary $R_k \in \RR$.
        Define: 
        \begin{align*}
            R_{k + 1}
            \defeq
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right). 
        \end{align*}
        Then it has for all $x^* \in \RR^n$ where $F^* = F(x^*)$, the inequality: 
        \begin{align*}
            F(x_{k + 1}) - F^* + R_{k + 1} + \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}
        Start by considering the first and the third term of the LHS of the inequality that we want to prove. 
        \begin{align*}
            F(x_{k + 1}) &= 
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2, 
            \\
            R_{k + 1}
            &= 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right), 
            \\
            \implies 
            F(x_{k + 1}) + R_{k + 1}
            &= 
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            \\
            &\quad 
                + 
                (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right). 
        \tag{1*}
        \end{align*}
        Coefficient of $\epsilon_k$ and $\Vert g_k\Vert^2$ are grouped. 
        Next, we have: 
        \begin{align*}
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^* \Vert^2
            &= 
            \frac{\hat \gamma}{2}\Vert 
                \hat \gamma^{-1}
                (
                    \gamma(1 - \alpha_k)v_k - 
                    \alpha_k g_k + \mu \alpha_k y_k
                )
                - x^* 
            \Vert^2
            \\
            &=  
            \frac{\hat \gamma}{2}
            \Vert 
                \hat \gamma^{-1}
                (
                \hat \gamma v_k + \mu \alpha_k(y_k - v_k)
                    - \alpha_k g_k
                )
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert 
                v_k + \hat \gamma^{-1} \mu \alpha_k (y_k - v_k)
                - \hat \gamma^{-1}\alpha_k g_k
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert v_k - x^*\Vert^2 
            + 
            \frac{\alpha_k^2}{2\hat \gamma}\Vert \mu(y_k - v_k) - g_k\Vert^2 
            \\ &\quad 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            \\ &\quad
                + 
                \frac{\alpha_k^2}{2\hat \gamma}
                \Vert \mu(y_k - v_k) - g_k\Vert^2 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle. 
        \tag{2*}
        \end{align*}
        On the above derivation, the first equality is by definition of $v_{k + 1}$; the second equality is by $\hat \gamma = (1 - \alpha_k)\gamma + \mu \alpha_k$ the second equality comes by considering: 
        \begin{align*}
            \gamma(1 - \alpha_k) v_k &= 
            (\hat \gamma  - \mu \alpha_k)v_k
            = \hat \gamma v_k - \mu\alpha_k v_k
            \\
            \iff 
            \gamma(1 - \alpha_k) v_k + \mu \alpha_k y_k
            &= 
            \hat \gamma v_k + \mu \alpha_k(y_k - v_k). 
        \end{align*}
        Focusing on the last two terms by the end of expression (2*), we have  
        \begin{align*}
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            & = 
            \frac{\alpha_k^2\mu}{\hat \gamma}
            \left(
                \frac{\mu}{2}\Vert y_k - v_k\Vert^2 
                - \langle y_k - v_k, g_k\rangle
            \right)
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2, 
            \\
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            &= 
            \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle 
            - \alpha_k \langle v_k - x^*, g_k\rangle. 
        \tag{2.1*}
        \end{align*}
        Adding the LHS of both equations above together gives: 
        {\small
        \begin{align*}
            & \quad 
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            + 
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
            + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \end{align*}
        }
        With the above we can conclude that (2*) simplifies to 
        {\small
        \begin{align*} 
            & 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            + 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \tag{2.2*}
        \end{align*}
        }
        Recall from 
        \hyperref[obs:stepwise-wapg]{Observations \ref*{obs:stepwise-wapg}} that: 
        \begin{align*}
            (Q1): 
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k),
            \\
            (Q2): 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
        Ok that is a lot, we list the following equations to assist things: 
        \begin{align*}
            &  
            - \alpha_k(v_k - x^*) - \frac{\alpha_k^2 \mu}{\hat \gamma}(y_k - v_k) - (x_k - y_k)
            \\
            & =
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k^2\mu}{\hat \gamma}\frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k)
            - (x_k - y_k) 
            &\text{by Q1} 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k \mu}{\gamma}(x_k - y_k)
            - (x_k - y_k) 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \left(
                1 + \frac{\alpha_k \mu}{\gamma}
            \right)(x_k - y_k)
            \\
            &= 
            -\alpha_k(v_k - x^*) - 
            \frac{\alpha_k \mu + \gamma}{\gamma}
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(x_k - v_k)
            &\text{by Q2}
            \\
            &= 
            -\alpha_k(v_k - x^*)
            - \alpha_k(x_k - v_k)
            \\
            &= \alpha_k(x^* - x_k). 
        \tag{Q3}
        \end{align*}
        Adding (2.2*) to (1*) gives: 
        \begin{align*}
            &
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\quad 
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
                + 
                \left\langle g_k, 
                    - \alpha_k(v_k - x^*) 
                    - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \epsilon_k 
            + \left\langle 
                g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                - (x_k - y_k)
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            \text{Use Q3}&= 
            F(x_k) - \epsilon_k 
            + \alpha_k\left\langle 
                g_k, 
                x^* - x_k
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k\epsilon_k + \alpha_k\langle g_k, x^* - x_k\rangle
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat\gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \tag{3*}
        \end{align*}
        From the left to the right on the first equality, coefficients of $\Vert g_k\Vert^2$ cancels out to zero and the inner product containing $g_k$ are grouped. 
        Going from the left to the right of the third equality, we applied (Q3) derived earlier to simplify the inner product term. 
        The last equalities re-arranged terms and grouped the coefficients of $\epsilon_k$ together. 
        Taking a page break, continuing on (3*) we have
        \begin{align*}
            &
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \left(
                    \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}
                    + 
                    \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
                \right)\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            & =
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \frac{\mu \alpha_k}{2}\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\ &=
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                +
                \frac{\mu\alpha_k}{2} \Vert y_k - x^*\Vert^2
            \\&= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
            \tag{3.1*}
        \end{align*}
        On the first equality, coefficients of $\Vert y_k - v_k\Vert^2$ are grouped together. 
        On the second to the third equality, the terms had been simplified by 
        \begin{align*}
            \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma} + 
            \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
            &= 
            \frac{\mu\alpha_k}{2}\left(
                \frac{(1 - \alpha_k)\gamma_k + \alpha_k \mu}{\hat \gamma}
            \right)
            \\
            &= \frac{\mu\alpha_k}{2}\left(
                \frac{\hat \gamma}{\hat \gamma}
            \right) = \frac{\mu\alpha_k}{2}. 
        \end{align*}
        (3.1*) was adding (1*) and (2.2*) together, which is the same as adding (1*) and (2*) together. 
        So that was all equal to (1*) + (2*) and it says: 
        \begin{align*}
            & F(x_{k + 1}) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            & \iff 
            \\
            & F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_k) - F(x^*) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &= (1 - \alpha_k)(F(x_k) - F(x^*))
            + \alpha_k\left(
                F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                (1 - \alpha_k)\left(
                    R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
                \right). 
        \tag{3.2*}
        \end{align*}
        Focusing on the second term, we simplify the multiplier inside: 
        {\small
        \begin{align*}
            & F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= 
            F(x_k) - F(x^*) - \left(
                F(x_k) - F(T_L y_k) - \langle g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2
            \right)- \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= F(T_L y_k) - F(x^*) + \langle g_k, x^* - y_k\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            + \frac{1}{2L}\Vert g_k\Vert^2 \le 0. 
        \tag{4*}    
        \end{align*}
        }
        On the last line, we expand the definition of $g_k$ and then used the
        \hyperref[thm:prox-grad-ineq]{remark of Theorem \ref*{thm:prox-grad-ineq}}. 
        Therefore, we conclude that: 
        {\small
        \begin{align*}
            F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)\left(
                F(x_k) - F(x^*) + R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
        }
    \end{proof}
    \begin{remark}
        \textcolor{red}
        {
        Given $y_k$, we may choose to increase $\mu_\kappa = 2D_f(x^*, y_k)/\Vert y_k - x^*\Vert^2 \ge \mu$ which also works on (4*), $\mu$ is a pessimistic choice for the inequality above. 
        }
        But in general the choice of $\mu$ remains the strong convexity modulus or equivalently, any value that is smaller than the true strong convexity constant for claiming the convergence rate for all initial guesses. 
    \end{remark}

\section{Formulation of R-WAPG and its convergence rates}
    Definition \ref{def:wapg} is an algorithm which generates iterates $(x_k, y_k, v_k)$ and admits potential convergence claim using Proposition \ref{prop:stepwise-lyapunov}. 
    It is the backbone of multiple accelerated first order method in the literatures which fits all variants under the same convergence claim. 

    \begin{definition}[Relaxed weak accelerated proximal gradient (R-WAPG)]\label{def:wapg}
        \;\\
        Initialize any $\gamma_1 \in (0, L]$, $(x_1, v_1)$. 
        Given auxiliary sequences $(\alpha_k)_{k \ge 1}, (\rho_k)_{k \ge 1}$ satisfying for all $k \ge 2$ the conditions: 
        \begin{align*}
            &\alpha_k \in (0, 1), 
            \\
            &\rho_k > 0, 
            \\
            &L\alpha_k^2 = (1 - \alpha_k)\rho_{k - 1}L \alpha_{k - 1}^2 + \mu \alpha_k , 
            \\
            &0 < \rho_k \alpha_k^2 < 1. 
        \end{align*}
        With base case $k = 1$ given by $\alpha_1 = (1 - \alpha_1)\gamma_1 + \mu \alpha_1$. 
        The algorithm generates a sequence of vector $(y_k, x_{k + 1}, v_{k + 1})_{k \ge 1}$ such that they satisfy for all $k\ge 1$: 
        \begin{align*}
            \gamma_k &:= \left.\begin{cases}
                \rho_{k -1}L\alpha_{k - 1}^2 & k \ge 2,
                \\
                \gamma_1 & k = 1. 
            \end{cases}\right\rbrace,
            \\
            \hat \gamma_{k + 1} &:= L\alpha_k^2, 
            \\
            y_k &= 
            (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k), 
            \\
            g_k &= \mathcal G_L y_k, 
            \\
            v_{k + 1} &= 
            \hat\gamma^{-1}_{k + 1}
            (\gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
            \\
            x_{k + 1} &= T_L y_k. 
        \end{align*}
    \end{definition}
    \begin{observation}
        Observe that if $\rho_k = 1$ for all $k\ge 0$, then the above algorithm is similar to (2.2.7) in Nesterov's book \cite{nesterov_lectures_2018}. 
        The initial condition is generic, so it can describe accelerated proximal gradient method in the middle of its executions. 
        \par
        Alternatively, any two of the vectors among $x_1, y_1, v_1$ can be used to initialize the algorithm because given $(v_1, y_1), (v_1, x_1)$, or $(x_1, y_1)$ we can solve 
        $y_1 = (\gamma_1 + \alpha_1\mu)^{-1}(\alpha_1 \gamma_1 v_1 + \hat \gamma_{2}x_1)$ for $x_1, y_1$ or $v_1$. 
        % \par
        % If one initial condition $x_0$ is given. 
        % Then $v_1 = x_0, x_1 = T_L x_0$ is considered a valid initial conditions. 
    \end{observation}

    \begin{observation}\label{obs:r-wapg-observation-1}
        If $\alpha_1 \in (0, 1)$ which is true by $\gamma_1 \in (0, L]$, then the entire sequence $\alpha_k \in (0, 1)$. 
        Suppose inductively that $\alpha_{k - 1}, \rho_{k - 1}$ are given such that they satisfy $\alpha_{k -1} \in (0, 1)$ and $0 < \rho_{k - 1} \alpha_{k - 1}^2 < 1$. 
        Solving the quadratic $L\alpha_k^2=(1 - \alpha_k)\rho_{k - 1}L\alpha_{k - 1}^2 + \mu \alpha_k$ for $\alpha_k$ yields candidates. 
        \begin{align*}
            \alpha_k &= 
            \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
                \pm
                \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
            \right). 
        \end{align*}
        Choosing the positive sign which is the larger one of the roots of the quadratic. 
        We have $\alpha_k > 0$ because: 
        \begin{align*}
            \alpha_k &=
            \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
                +
                \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
            \right)
            \\
            &\ge 
            \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
                +
                \left|\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L\right|
            \right) + \alpha_{k - 1}\sqrt{\rho_{k - 1}}
            \\
            & > 0. 
        \end{align*}
        On the last strict inequality we used the fact that $\rho_{k - 1}> 0, \alpha_{k - 1} > 0$. 
        An upper bound can be identified by using inductive hypothesis and considering: 
        \begin{align*}
            \alpha_k &= 
            \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
                +
                \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
            \right)
            \\
            &<
            \frac{1}{2}\left(
                \frac{\mu}{L} + 
                \sup_{x \in (0, 1)}
                \left\lbrace
                    -x + \sqrt{(x - \mu/L)^2 + 4x}
                \right\rbrace
            \right)
            \\
            & \le \frac{1}{2}\left(
                \mu/L + \max\left(\mu/L, -1 + \sqrt{(1 - \mu/L)^2 + 4}\right)
            \right) \le 1. 
        \end{align*}
        Going to the first inequality, we used $\rho_{k - 1} \alpha_{k - 1}^2 < 1$ to get the strict inequality. 
        Going from the second to the third inequality, we maximized $\mu/L$ by monotone increasing of linear function and the square root function. 
        Therefore, inductively, $\alpha_k \in (0, 1)$ holds. 
        However, the limit of $\alpha_k$ could be 1. 
    \end{observation}
    
    \begin{proposition}[R-WAPG convergence claim]\label{prop:wagp-convergence}\; \\
        Given any $(v_1, x_1)$, let vectors $(y_k, v_{k + 1}, x_{k + 1})_{k \ge 1}$ and scalars sequence $(\alpha_k)_{k \ge 1}, (\rho_k)_{k \ge 1}$ be generated by R-WAPG (Definition \ref{def:wapg}). 
        Define recursively for all $k\ge1$ by 
        \begin{align*}
            R_{k + 1}
            := 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right). 
        \end{align*}
        Choose $R_1= 0$ to be the base case. 
        Choose any $x^* \in \RR^n$ then
        \begin{align*}
            & F(x_{k + 1}) - F(x^*) + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 1}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F(x^*) + \frac{\gamma_1}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}  
        Recall the stepwise convergence theorem, since the choice of $\gamma, \hat \gamma$ is satisfies is strictly positive and for $i=2, 3, \cdots, k$, by Definition \ref{def:wapg} it has $\gamma = \rho_{i - 1} L \alpha_{i - 1}^2 > 0, \hat \gamma = L \alpha_i^2 > 0$ at each step, with the base case $i = 1$: $\gamma \in (0, L], \hat \gamma = (1 - \alpha_1)\gamma + \mu \alpha_1$. 
        Since $\alpha_i \in(0, 1)$ for all $i = 1, \cdots, k$, because of Observation \ref{obs:r-wapg-observation-1}, we can use Proposition \ref{prop:stepwise-lyapunov}, to show that for all $k \ge 1$: 
        {\small
        \begin{align*}
            &F(x_{k + 1}) - F^* + R_{k + 1} + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{\rho_{k - 1}L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \max(1, \rho_{k - 1})\frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \max(1, \rho_{k - 1})(1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \left(
                \prod_{i = 1}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F^* + R_1 + \frac{\gamma_1}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
        }
        Additionally, for all $k \ge 1$ the algorithm converges by considering $R_{k + 1}$: 
        \begin{align*}
            R_{k + 1}
            &= 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &= (1 - \alpha_k)
            \left(
                \epsilon_k + R_k 
                + \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\ge 
            (1 - \alpha_k) R_k
            \\
            &\ge R_1 \prod_{i = 1}^{k} \left(1 - \alpha_i\right) = 0. 
        \end{align*}
        Going from the left to the right on the first equality, we used the fact that $\hat \gamma_{k + 1} = L \alpha_{k}^2$.
        This makes coefficient of $\Vert g_k\Vert^2$ zero. 
        The first inequality is by $\epsilon_k \ge 0$ and the non
        -negativity of the remaining terms. 
        The last equality is by the assumption that $R_1 = 0$. 
        Therefore: 
        {\small
        \begin{align*}
            & F(x_{k + 1}) - F^* +
            \frac{L\alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 1}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F^* + \frac{\gamma_1}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
        }
    \end{proof}
    \begin{remark}        
        \textcolor{red}
        {
            The choice of $\rho_k$ allows for nontraditional choices of step sizes in the literatures. 
            As we can see from the above convergence claim, there exists choices of $\rho_k\ge 1$ where convergence remains possible. 
        }
    \end{remark}


\section{Equivalent representations of R-WAPG}
    This section reduces Definition \ref{def:wapg} into simpler forms that are comparable to what commonly appears in the literatures. 
    Proposition \ref{prop:wapg-first-equivalent-repr} simplifies Definition \ref{def:wapg} for the specific initial conditions $\gamma_0 \in(0, 1)$ and finds a representation without auxiliary sequence $\gamma_k, \hat \gamma_k$. 
    Definition \ref{def:r-wapg-intermediate} states the first simplified form of the R-WAPG algorithm which we call: ``R-WAPG intermediate form''. 
    Following a similar pattern, Proposition \ref{prop:wagp-st-form}, \ref{prop:wagp-st-form} states two more equivalent representations under R-WAPG intermediate form which are formulated as Definition \ref{def:r-wapg-st-form}, \ref{def:r-wapg-momentum-form}. 
    Proposition \ref{prop:wagp-convergence} applies to all these equivalent forms. 
    
    To start, the following proposition on ``abstract similar triangle form'' were made to simplify arguments and notations to make nicer proofs. 

    \begin{proposition}[Abstract similar triangle form]\label{prop:abs-st-form}\;\\
        Given any initial $(x_1, v_1)$, and a sequence $(\tau_k)_{k \ge 1}, (\xi_k)_{k \ge 1}$ such that for all $k \ge 1$ $\tau_k \in (0, 1), \xi_k \in (0, 1)$. 
        for all $k\ge 1$, iterates $(y_k, z_{k + 1}, x_{k + 1})_{k \ge 1}$ satisfies recursively that: 
        \begin{align*}
            y_k &= (1 + \tau_k)^{-1}(v_k + \tau_k x_k),
            \\
            v_{k + 1} &= (1 + \xi_k)^{-1}(v_k + \xi_k y_k) - (1 + \xi_k)^{-1}\delta_k g_k,
            \\
            x_{k + 1} &= y_k - L^{-1} g_k. 
        \end{align*}
        If $1 + \xi_k + \tau_k = L\delta_k, \tau_k \neq 0\; \forall k \ge 1$. 
        Then for all $k \ge 1$: 
        $$
            v_{k + 1} - x_{k + 1} = (1 + \xi_k)^{-1}\tau_k(x_{k + 1} - x_k). 
        $$
        Which makes the algorithm a similar triangle form. 
    \end{proposition}
    \begin{proof}
        We are interested in identifying the conditions required for the sequence of $\xi_k, \tau_k, \delta_k$ such that there exists $\theta_k$ satisfying: 
        \begin{align*}
            v_{k + 1} - x_{k + 1} 
            &= \theta_k(x_{k + 1} - x_k).
        \end{align*}
        To verify, do 
        \begin{align*}
            v_{k + 1} &= 
            (1 + \xi_k)^{-1}(v_k + \xi_t y_k - \delta_k \mathcal G_L(y_k))
            \\
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k)y_k - \tau_t x_k + \xi_k y_k - \delta_k \mathcal G_L(y_k))
            \\
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k + \xi_k)y_k - \tau_k x_k - \delta_k \mathcal G_L(y_k))
            \\
            v_{k + 1} - x_{k + 1}
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k + \xi_k)y_t - \tau_k x_k - \delta_t \mathcal G_L(y_k))
            - (y_k - L^{-1}\mathcal G_Ly_k)
            \\
            &= 
            (1 + \xi_k)^{-1}(\tau_ky_k - \tau_k x_k - \delta_k \mathcal G_L(y_k))
            + L^{-1}\mathcal G_Ly_k
            \\
            &= 
            (1 + \xi_k)^{-1}
            \left(
                \tau_ty_k - \tau_t x_k + (L^{-1}(1 + \xi_k) - \delta_k) \mathcal G_L(y_k)
            \right)
            \\
            &= 
            (1 + \xi_k)^{-1}\tau_k
            \left(
                y_k - x_k + 
                \tau_k^{-1}(L^{-1}(1 + \xi_k) - \delta_k) \mathcal G_L(y_k)
            \right).
        \end{align*}
        Going between the first and second inequality we used $v_k = (1 + \tau_k)y_k - \tau_k x_k$ which is rearranged $y_k = (1 + \tau_k)^{-1}(v_k + \tau_k x_k)$. 
        The RHS is can be verified through 
        \begin{align*}
            x_{k + 1} - x_k &= 
            y_t - L^{-1}\mathcal G_L(y_k) - x_k
            \\
            &= (y_k - x_k) - L^{-1}\mathcal G_L(y_k). 
        \end{align*}
        It necessitates the condition: 
        \begin{align*}
            \tau_k^{-1}(L^{-1}(1 + \xi_k) - \delta_k) 
            &= - L^{-1}
            \\
            (1 + \xi_k) - L\delta_k
            &= 
            - \tau_k
            \\
            1 + \xi_k + \tau_k
            &=
            L\delta_k. 
        \end{align*}
        Which allows for: 
        \begin{align*}
            v_{k + 1} - x_{k + 1} &= 
            (1 + \xi_k)^{-1}\tau_t
            \left(y_k - x_k - L^{-1}\mathcal G_L(y_k)\right) 
            = 
            (1 + \xi_k)^{-1}\tau_k(x_{k + 1} - x_k). 
        \end{align*}
    \end{proof}

    \subsection{Narrowing it down}
        \begin{proposition}[First equivalent representation of R-WAPG]\label{prop:wapg-first-equivalent-repr}\;\\
            Assume $\mu < L$. 
            Choose any $\gamma_1 \in (0, L], x_1, v_1$ as the initial condition for R-WAPG (Definition \ref{def:wapg}) to produce vectors sequence $(y_k, v_{k + 1}, x_{k + 1})_{k \ge 1}$. 
            Then the iterates can be expressed without $(\gamma_k)_{k \ge1},(\hat \gamma_k)_{k \ge1}$, and for all $k\ge 1$ they are algebraically equivalent to
            \begin{align*}
                & 
                y_{k} = 
                \left(
                    1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                \right)^{-1}
                \left(
                    v_{k} + 
                    \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                \right), 
                \\
                & x_{k + 1} = 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                & v_{k + 1} = 
                \left(
                    1 + \frac{\mu}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
            
        \end{proposition}
        \begin{proof}
            For all $k \ge 1$, by R-WAPG (Definition \ref{def:wapg}), it has: 
            \begin{align*}
                y_{k} &= 
                (\gamma_k + \alpha_k \mu)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                (\hat \gamma_{k + 1} + \alpha_k \gamma_k)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                \left(
                    \frac{\hat \gamma_{k + 1}}{\alpha_k\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    \frac{L\alpha_k^2}{\alpha_k\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{L\alpha_k^2}{\alpha_k \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    \frac{L\alpha_k}{\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{L\alpha_k}{ \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    1 + \frac{L - L \alpha_k}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{L - L \alpha_k}{L \alpha_k - \mu} x_k
                \right). 
            \end{align*}
            From the left to right of the second inequality, we used the fact that $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k\mu$. 
            Going from the left to the right of the second last inequality, we did the following: 
            \begin{align*}
                L\alpha_k^2 &= 
                (1 - \alpha_k)\gamma_k + \alpha_k \mu 
                \\
                \iff 
                L \alpha_k^2 - \alpha_k\mu &= 
                (1 - \alpha_k)\gamma_k
                \\
                \iff 
                \gamma_k/L
                &= 
                \frac{L \alpha_k^2 - \alpha_k\mu}{L (1 - \alpha_k)}
                \\
                \iff 
                L/\gamma_k
                &= 
                \frac{L (1 - \alpha_k)}{L \alpha_k^2 - \alpha_k\mu}
                \\
                \iff 
                L\alpha_k/\gamma_k
                &= 
                \frac{L - L\alpha_k}{L\alpha_k - \mu}. 
            \end{align*}
            On the third $\iff$, we can assume $\alpha_k \neq \mu/L\; \forall k \ge 2$ because if $\alpha_k = \mu/L$ were impossible, it then has
            \begin{align*}
                L \alpha_k^2 - \alpha_k\mu &= 
                (1 - \alpha_k)L \rho_{k - 1}\alpha_{k - 1}^2 = 0 \implies \alpha_{k - 1} = 0. 
            \end{align*}
            Which contradict $\alpha_{k - 1} \in (0, 1)\; \forall k \ge 2$ because $\alpha_1 \in (0, 1)$ is given to satisfy the condition in the base case, and $\alpha_k \in (0, 1)\; \forall k \ge 2$ by justifications in Observation \ref{obs:r-wapg-observation-1}. 
            \\
            For all $k \ge 1$, $v_{k + 1}$ has: 
            \begin{align*}
                v_{k + 1} &= 
                \hat \gamma_{k + 1}^{-1}
                ((1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
                \left(
                    (1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k} y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                % \\
                % &= 
                % \left(
                %     1 + \frac{\alpha_k\mu}{(1 -\alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2}
                % \right)^{-1}
                % \left(
                %     v_k + 
                %     \frac{\alpha_k\mu}{(1 - \alpha_k)\rho_{k - 1}L\alpha_{k - 1}^2} y_k
                % \right)
                % - \frac{1}{L\alpha_{k}}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu} y_k
                \right)
                - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
            Going from the left to the right of the second equality, we substitute $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu\alpha_k$. 
            At the end, recall that for all $k \ge 1$, it has $\hat \gamma_{k + 1} = L \alpha_k^2 = (1 - \alpha_k)\gamma_k + \alpha_k \mu$, so: 
            \begin{align*}
                (1 - \alpha_k)\gamma_k
                &= 
                \hat \gamma_{k + 1} - \mu \alpha_k
                = 
                L\alpha_{k}^2 - \alpha_k\mu. 
            \end{align*}
            The is now complete. 
            This form doesn't have $\rho_k, \gamma_k, \hat \gamma_k$ in it. 
        \end{proof}
        \begin{definition}[R-WAPG intermediate form]\label{def:r-wapg-intermediate}\;\\
            Given any $\gamma_1 \in (0, L]$, $(x_1, v_1)$. 
            Assume $\mu < L$. 
            Define 
            \begin{align*}
                \alpha_1 = \frac{1}{2}\left(
                    \frac{\mu}{L} - \frac{\gamma_1}{L} + 
                    \sqrt{
                        \left(
                            \frac{\gamma_1}{L} - \frac{\mu}{L}
                        \right)^2 + 
                        \frac{4\gamma_1}{L}
                    }
                \right). 
            \end{align*}
            For $k = 1, 2, \cdots $, the algorithm generates sequence of vector iterates $(y_{k}, v_{k + 1}, x_{k + 1})_{k \ge 1}$ and auxiliary sequence $(\alpha_{k + 1}, \rho_k)_{k \ge 1}$ by the procedures: 
            \begin{align*} 
                & y_{k} = 
                \left(
                    1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                \right)^{-1}
                \left(
                    v_{k + 1} + 
                    \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                \right)
                \\
                & x_{k + 1} = 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                & v_{k + 1} = 
                \left(
                    1 + \frac{\mu}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k
                \\
                & \text{choose any } \rho_{k}: 0 < \rho_{k} < \alpha_{k}^{-2}, 
                \\
                & \alpha_{k + 1} := 
                \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k}\alpha_{k}^2 
                +
                \sqrt{(\rho_{k}\alpha_{k}^2 - \mu/L)^2 + 4\rho_{k}\alpha_{k}^2}
                \right). 
            \end{align*}
        \end{definition}
        \begin{proposition}[Second equivalent representation of R-WAPG]\label{prop:wagp-st-form}\;\\
            Let iterates $(y_k, x_{k + 1}, v_{k + 1})_{k \ge 1}$ and sequence $(\alpha_k, \rho_k)_{k \ge1}$ be given by Definition \ref{def:r-wapg-intermediate}. 
            Then for all $k \ge 1$, iterate $y_k, x_{k + 1}, v_{k + 1}$
            satisfy: 
            \begin{align*}
                y_{k} &= 
                \left(
                    1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                \right)^{-1}
                \left(
                    v_{k} + 
                    \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                \right), 
                \\
                x_{k + 1} &= 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                v_{k + 1} &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proposition}
        \begin{proof}
            From Definition \ref{def:r-wapg-intermediate}, define $(\tau_k, \xi_k, \delta_k)_{k \ge 1}$ in Proposition \ref{prop:abs-st-form} to be
            \begin{align*}
                (\forall k \ge 1) \quad \tau_k &= \frac{L(1 - \alpha_k)}{L\alpha_k - \mu},
                \\
                (\forall k \ge 1)\quad 
                \xi_k &= \frac{\mu}{L \alpha_k - \mu},
                \\
                (\forall k \ge 1)\quad 
                \delta_k &\defeq \frac{1 + \xi_k}{L\alpha_k}. 
            \end{align*}
            We claim that these parameters satisfy $L\delta_k = 1 + \tau_k + \xi_k$. 
            To see, we have forall $k\ge 1$: 
            \begin{align*}
                1 + \tau_k + \xi_k &= 
                1 + \frac{L(1 - \alpha_k)}{L \alpha_k - \mu} 
                + \frac{\mu}{L \alpha_k - \mu}
                \\
                &= 
                1 + \frac{L - L \alpha_k + \mu}{L\alpha_k - \mu}
                \\
                &= 
                \frac{L - L \alpha_k + \mu + L \alpha_k - \mu}{L\alpha_k - \mu}
                \\
                &= \frac{L}{L\alpha_k - \mu}. 
            \end{align*}
            Next, it also has for all $k \ge 1$: 
            \begin{align*}
                \frac{1 + \xi_k}{\alpha_k}
                &= 
                \frac{1 + \frac{\mu}{L\alpha_k - \mu}}{\alpha_k}
                = 
                \frac{\frac{L\alpha_k - \mu + \mu}{L \alpha_k - \mu}}{\alpha_k}
                = 
                \frac{L}{L\alpha_k - \mu}.
            \end{align*}
            Hence,  $v_{k + 1} - x_{k + 1} = (1 + \xi_k)^{-1}(x_{k + 1} - x_k)\; \forall k \ge 1$ by Proposition \ref{prop:abs-st-form}.
            Therefore, substituting it gives: 
            \begin{align*}
                v_{k + 1} &= 
                x_{k + 1} + \left(
                    1 + \frac{\mu}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k - \mu}{L\alpha_k}
                \right)\left(
                    \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proof}
        \begin{remark}
            We note that $x_1, v_1, y_1$ may not satisfy the inequalities since, 
        \end{remark}
    \begin{definition}[R-WAPG similar triangle form]\label{def:r-wapg-st-form} \; \\
        Given any initial $L \in (0, L]$, $(x_1, v_1)$. 
        Assume $\mu < L$.
        Define $\alpha_1$ by 
        \begin{align*}
            \alpha_1 = \frac{1}{2}\left(
                \frac{\mu}{L} - \frac{\gamma_1}{L} + 
                \sqrt{
                    \left(
                        \frac{\gamma_1}{L} - \frac{\mu}{L}
                    \right)^2 + 
                    \frac{4\gamma_1}{L}
                }
            \right). 
        \end{align*}
        For $k = 1, 2, \cdots $, the algorithm generates sequences of vector iterates $(y_k, v_{k + 1}, x_{k + 1})_{k \ge 1}$ by the procedures: 
        \begin{align*}
            & y_k = 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right), 
            \\
            & x_{k + 1} = 
            y_k - L^{-1} \mathcal G_L y_k, 
            \\
            & v_{k + 1} = 
            x_{k + 1} + (\alpha_k^{-1} -1)(x_{k + 1} - x_k), 
            \\
            & \text{choose any }   \rho_{k}: 0 < \rho_{k} < \alpha_{k}^{-2}, 
            \\
            &\alpha_{k + 1} = 
            \frac{1}{2}\left(
            \frac{\mu}{L} - \rho_{k}\alpha_{k}^2 
            +
            \sqrt{(\rho_{k}\alpha_{k}^2 - \mu/L)^2 + 4\rho_{k}\alpha_{k}^2}
            \right). 
        \end{align*}
    \end{definition}
    
    \begin{proposition}[Third equivalent representation of R-WAPG]\label{prop:r-wapg-momentum-repr}
        \;\\
        Let sequence $(\alpha_k, \rho_k)_{k \ge 1}$ and iterates $(x_k, v_k, y_k)_{k\ge 1}$ given by R-WAPG intermediate form (Definition \ref{def:r-wapg-st-form}). 
        Then for all $k \ge 1$, the iterates $(x_{k + 1}, y_{k + 1})_{k \ge 1}$ are algebraically equivalent to: 
        \begin{align*}
            x_{k + 1} &= y_k - L^{-1}\mathcal G_Ly_k, 
            \\
            y_{k + 1} &= 
            x_{k + 1} + 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        If in addition, $v_1 = x_1$ then 
        \begin{align*}
            y_1 = \left(
                1 + \frac{L - L \alpha_1}{L\alpha_1 - \mu}
            \right)^{-1}\left(
                v_1 + \left(
                    \frac{L - L \alpha_1}{L \alpha_1 - \mu}
                \right)x_1
            \right) = x_1. 
        \end{align*}
        In the special case when $\mu = 0$, the momentum term admits simpler representation 
        \begin{align*}
            (\forall k \ge 1) \quad 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
            & = \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proposition}

    \begin{proof}
        Start by considering the update rule for $v_k$ from the Definition \ref{def:r-wapg-st-form} which has for all $k \ge 1$: 
        \begin{align*}
            v_{k + 1} &= 
            x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \\
            \iff 
            (L \alpha_{k + 1} - \mu)v_{k + 1} 
            &= 
            (L \alpha_{k + 1} - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
        \end{align*}
        Next, we simplify the update $y_{k}$ by Definition \ref{def:r-wapg-st-form} for all $k \ge 1$: 
        \begin{align*}
            y_k &= 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \left(
            \frac{L - \mu}{L\alpha_k - \mu} 
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \frac{L\alpha_k - \mu}{L - \mu} v_k
            + 
            \frac{L - L \alpha_k}{L - \mu} x_k
            \\
            &= (L - \mu)^{-1}((L \alpha_k - \mu)v_k + (L - L \alpha_k)x_k). 
        \end{align*}
        We have $y_1 = x_1$ when $v_1 = x_1$ because substituting $v_1 = x_1$ for how $y_1$ is defined it has: 
        \begin{align*}
            y_1 &= (L - \mu)^{-1}(L(\alpha_1 - \mu)x_1 + (L - L\alpha_1)x_1)
            \\
            &= (L - \mu)^{-1}(L(\alpha_1 - \mu)x_1 + (L - L \alpha_1)x_1)
            \\
            &= (L - \mu)^{-1}((L - \mu)x_1) = x_1. 
        \end{align*}
        Now substituting $v_{k + 1}$ into $y_{k + 1}$ so it has for all $k\ge 1$ : 
        {\small
        \begin{align*}
            y_{k + 1} &= 
            (L - \mu)^{-1}((L\alpha_{k + 1} - \mu)v_{k + 1} + (L - L \alpha_{k + 1})x_{k + 1})
            \\
            &= (L - \mu)^{-1}
            \left(
                (L\alpha_{k + 1} - \mu)x_{k + 1} + 
                (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
                + (L - L \alpha_{k + 1})x_{k + 1}
            \right)
            \\
            &= 
            (L - \mu)^{-1}
            \left(
                (L\alpha_{k + 1} - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \right)
            \\
            &= x_{k + 1} + \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}(x_{k + 1} - x_k). 
        \end{align*}
        }
        The coefficient for $(x_{k + 1} - x_k)$ needs some works to formulate it without the parameter $\mu, L$. 
        To do that we have: 
        \begin{align*}
            \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}
            &= \frac{(L\alpha_{k + 1} - \mu)\alpha_k(1 - \alpha_k)}{\alpha_k^2(L - \mu)}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \left(
                \frac{\alpha_k^2(L - \mu)}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \alpha_k(1 - \alpha_k)
            \left(
                \frac{L\alpha_k^2 - \mu\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \rho_k\left(
                \frac{L\rho_k\alpha_k^2 - \mu\rho_k\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \rho_k\alpha_k(1 - \alpha_k)
            \left(
                \frac{(L\alpha_{k + 1} - \mu)(\rho_k\alpha_k^2 + \alpha_{k + 1})}
                {L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}.
        \end{align*}
        Going from the left to right on the fourth equality, we used $L\alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_kL\alpha_k^2 + \mu \alpha_{k + 1}$ with 
        \begin{align*}
            L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2 
            &= 
            (1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            ((1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \mu \alpha_{k + 1}) - \mu\alpha_{k + 1} + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= L \alpha_{k + 1}^2 - \mu\alpha_{k + 1} + \alpha_{k + 1}L\rho_k\alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            L\alpha_{k + 1}(\alpha_{k + 1} + \rho_k \alpha_k^2) - \alpha_{k + 1}\mu - \mu \rho_k \alpha_k^2
            \\
            &= (L \alpha_{k + 1} - \mu)(\alpha_{k + 1} + \rho_k \alpha_k^2). 
        \end{align*}
        When $\mu = 0$, things simplify. 
        Consider that $\forall k \ge 1: \alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_k\alpha_k^2$. 
        \begin{align*}
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \alpha_{k + 1}^2}
            \\
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \rho_k(1 - \alpha_{k + 1})\alpha_k^2}
            \\
            &= \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2}
            \\
            &= \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proof}

    \begin{definition}[R-WAPG momentum form]\label{def:r-wapg-momentum-form}
        Let $y_1 = x_1$. 
        Given any $\gamma_1 \in (0, L]$, initialize any $\alpha_1 \in (0, 1)$ by: 
        \begin{align*}
            \alpha_1 = \frac{1}{2}\left(
                \frac{\mu}{L} - \frac{\gamma_1}{L} + 
                \sqrt{
                    \left(
                        \frac{\gamma_1}{L} - \frac{\mu}{L}
                    \right)^2 + 
                    \frac{4\gamma_1}{L}
                }
            \right). 
        \end{align*}
        For $k = 1, 2, \cdots $, the algorithm generates iterates $x_{k + 1}, y_{k + 1}$ and auxiliary sequence $(\rho_k)_{k \ge 1}, (\alpha_k)_{k \ge 2}$ by the procedures: 
        \begin{align*}
            & x_{k + 1} = y_k - L^{-1}\mathcal G_Ly_k, 
            \\
            & 
            y_{k + 1} = 
            x_{k + 1} + 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k), 
            \\
            & \text{find } \rho_{k}: 0 < \rho_{k} < \alpha_{k}^{-2}, 
            \\
            & 
            \alpha_{k + 1} = 
            \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k}\alpha_{k}^2 
                +
                \sqrt{(\rho_{k}\alpha_{k}^2 - \mu/L)^2 + 4\rho_{k}\alpha_{k}^2}
            \right). 
        \end{align*}
        In the special case $\mu = 0$, the momentum term can be represented without relaxation parameter $\rho_k$: 
        $$
            (\forall k \ge 1)\quad \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}} 
            = \alpha_{k + 1}(\alpha_k^{-1} - 1).  
        $$
    \end{definition}
    \begin{remark}
        % \textcolor{red}
        % {
        % There exists a choice of $\rho_k \in (0, 1], \mu = 0$ such that the above algorithm is equivalent to Chambolle, Dossal 2015 \cite{chambolle_convergence_2015}. 
        % When $\rho_k = 1$, this becomes just FISTA \cite{beck_fast_2009}. 
        % When $L > \mu > 0$, then choosing $\rho_k = 1$ yields equation 
        % \begin{align*}
        %     L \alpha_{k + 1}^2 = (1- \alpha_{k + 1})L \alpha_k^2 + \alpha_{k + 1} \mu. 
        % \end{align*}
        % Here, if we assume sequence $\alpha_k$ is constant, then $\alpha_k = \sqrt{\mu/L}$ is a solution to the above iterative equation. 
        % Substituting it back to the momentum form of R-WAPG, we recover the V-FISTA algorithm, which appears as (10.7.7) in Beck's Book \cite{beck_first-order_2017}. 
        % }
    \end{remark}

\subsection{Representing algorithms in the literatures via R-WAPG}


\subsection{The method of Inexact Momentum}
    This section introduces an algorithm of our creation through the remark of Proposition \ref*{prop:stepwise-lyapunov}. 
    The algorithm estimates the $\mu$ constant as the algorithm executes and pools the information using the Bregman Divergence of the smooth part function $f$. 


\subsection{Numerical experiments and applications}

\bibliographystyle{siam}
\bibliography{references/Inexact_Momentum.bib}

\end{document}
