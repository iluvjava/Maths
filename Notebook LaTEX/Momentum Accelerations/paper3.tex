\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}


\begin{document}


\title{{\fontfamily{ptm}\selectfont 
        A Parameter Free Accelerated Proximal Gradient Method Without Restarting
    }}

\author{
    Hongda Li
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~ and ~Xianfu Wang
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
    % and ~Heinz H. Bauschke~
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{heinz.bauschke@ubc.ca}.
    % }
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    
    \todo{Write this abstract when finishing. }
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 65K10, 90c25, 90C30; Secondary 65Y20. 
\noindent{\bfseries Keywords: } Nesterov acceleration, Proximal point method. 
\section{Introduction}
    Suppose we are minimizing a differentiable convex function $F: \RR^n \rightarrow \RR$ for simplicity. 
    Let $F$ be $L$-Lipschitz smooth and $\mu \ge 0$ strongly convex meaning that there exists $L \in \RR$ such that for all $x \in \RR^n, y\in \RR^n$ it has $\mu\Vert x - y\Vert\le \Vert \nabla F(x) - \nabla F(y)\Vert \le L \Vert x - y \Vert$. 

    \par 
    A good choice of a minimization algorithm is the Nesterov's acceleration scheme which was originally proposed in 1983 \cite{nesterov_method_1983}. 
    Initialize $x_1 = y_1$ and $t_0 = 1$, the algorithm finds $(x_k)_{k \ge 1}$ for all $k \ge 1$ by: 
    \begin{align}
        & x_{k + 1} = y_k - L^{-1}\nabla F(y_k), 
        \\
        & t_{k + 1} = 1/2\left(1 + \sqrt{1 + 4t_{k}^2}\right), 
        \\
        & \theta_{k + 1} = (t_{k} - 1)/t_{k + 1}, 
        \\
        & y_{k + 1} = x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k). 
    \end{align}\label{eqn:example_algorithm}
    It's well known that if the minimizer $x^*$ exists, then the optimality gap $F(x_k) - F(x^*)$ decreases at a rate of $\mathcal O(1/k^2)$, faster than the gradient descent method which is $\mathcal O(1/k)$, and it's optimal.
    See Chapter 2 of Nesterov's book \cite{nesterov_lectures_2018}. 
    However, everything is not sunshine and rainbow because the above scheme is associated with an issue. 
    When $F$ is $\mu > 0$ strongly convex, the optimality gap $F(x_k) - F(x^*)$ oscillates and converges sub-linearly, making it worse than gradient descent. See page 9 of Su et al. \cite{su_differential_2016}. 
    \par
    This issue motivated a vast amount of literatures aims at improving, interpreting, and extending this method. 
    Restarting is a popular solution to address the issue of obtaining faster convergence rate when the objective function is strongly convex. 
    Beck and Toubelle \cite{beck_fast_2009} mitigate the issue by restarting and showed that it $\mathcal O(1/k^2)$ convergence rate is kept, and it performs empirically better. 
    See \cite{necoara_linear_2019}\cite{aujol_parameter-free_2024} and references within for recent advancements in restarting accelerated proximal gradient algorithm. 
    \par
    Inspired by prior works and the method of Nestrov's estimating sequence \cite{nesterov_lectures_2018}, we present to you firstly, an alternative to restarting that performs well empirically. 
    Secondly, a unified framework of Accelerated Proximal Gradient (APG) which we call Relaxed Weak Accelerated Proximal Gradient (R-WAPG). 
    The R-WAPG has a convergence claim of $F(x_k)- F(x^*)$ for a much more flexible choice of $(\theta_k)_{k \ge 1}$. 
    It also has descriptive power to describe several variants of FISTA in the literatures.  

    \subsection{Contributions}
        Our contributions are two folds, theoretical and practical. 
        Our results is based the assumption $F = f + g$ where $g:Q \rightarrow \bar\RR^n$ is convex, and $f$ is an $L$-Lipschitz smooth and $\mu \ge 0$ strongly convex function. 
        \par
        Nesterov's acceleration extrapolate $y_{k + 1} = x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k)$ where $\theta_{k + 1} = (t_{k} - 1)/t_{k + 1} \in (0, 1)$ is the ``momentum''. 
        The choices for $\theta_k$ varies for different variants of the accelerated proximal gradient algorithm. 
        In Chamboolle, Dossal \cite{chambolle_convergence_2015}, it has $t_k = (n + a - 1)/a$ for all $a > 2$ which gives weak convergence of the iterates $x_k$ in Hilbert space. 
        In Chapter 10 of Beck's Book \cite{beck_first-order_2017}, a variant called V-FISTA can achieve the faster linear convergence rate: $\mathcal O((1 - \sqrt{\mu/L})^k)$ on the optimality gap for $\mu > 0$ strongly convex $F$. 
        V-FISTA has $\theta_t = (\sqrt{\kappa} - 1)/(\sqrt{\kappa} + 1)$ where $\kappa = \mu/L$. 
        \par
        We relax the traditional choice of the sequence $\theta_k$ in Equation \ref{eqn:example_algorithm} and showed an upper bound of the optimal gap. 
        Let $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ be two sequences that satisfy
        \begin{align*}
            \alpha_0 &\in (0, 1], 
            \\
            \alpha_k &\in (\mu/L, 1) \quad (\forall k \ge 1), 
            \\
            \rho_k &:= \frac{\alpha_{k + 1}^2 - (\mu/L)\alpha_{k + 1}}{(1 - \alpha_{k + 1})\alpha_k^2} \quad \forall (k \ge 0). 
        \end{align*}
        Our first main result shows that if $\theta_{k + 1} = (\rho_k\alpha_k(1 - \alpha_k)/(\rho_k\alpha_k^2 + \alpha_{k + 1}))$, using the R-WAPG we proposed in Definition \ref{def:wapg} with Proposition \ref{prop:wagp-convergence}, \ref{prop:r-wapg-momentum-repr}, we can show that the gap $F(x_k) - F(x^*)$ is bounded by:
        \begin{align*}
            \mathcal O\left(
                \left(
                    \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
                \right)
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right). 
        \end{align*}
        Our second main result shows that there exists $\rho_k > 1$ such that our R-WAPG reduces to a variant of FISTA proposed in Chambolle, Dossal \cite{chambolle_convergence_2015}, and we are able to show the same convergence rate. 
        And when $\rho_k = 1, \mu = 0$, R-WAPG reduces perfectly to FISTA by Beck \cite{beck_first-order_2017}, if $\mu > 0$, it reduces to the V-FISTA by Beck \cite{beck_first-order_2017}, which is an optimal algorithm for a strongly convex $F$. 
        R-WAPG gives the convergence claim, and it's consistent with the literatures. 
        \par
        Our practical contribution is an algorithm inspired by a detail in our convergence proof which we call it ``Parameter Free R-WAPG''. 
        The algorithm is parameter free, meaning that it doesn't require knowing $L, \mu$ in advance, and it determines the value of $\theta_t$ by estimating the local concavity using iterates $y_{k}, y_{k + 1}$ with minimal computational cost. 
        We conducted ample amount of numerical experiments to show that it has a favorable convergence rate in practice and behaves similarly to the FISTA with monotone restart. 
    \subsection{Prior works}
        \todoinline{
            Here is a list of papers that are relevant to generalizing the convergence claim for APG beyond just the traditional choices for the momentum sequence. 
            \begin{enumerate}
                \item Attouch H., Cabot A.: ``Convergence Rates of Inertial Forward-Backward Algorithms'', SIAM. 
                \item Vassilis A., et al.: ``Convergence rate of inertial Forward-Backward algorithm
                beyond Nesterov's rule"
            \end{enumerate}
            The following papers are related to stabiling, improving the acceleration scheme without monotone restarting. 
            \begin{enumerate}
                \item Attouch H., et al.: ``First-order optimization algorithms via inertial systems with Hessian driven damping'', Math Programming. 
            \end{enumerate}
        }
        
    \subsection{Organizations}
        \todoinline{This will be written once most of the contents in the paper are settled. }
    
       

\section{Preliminaries}
    Throughout, we define $F := f + g$ where $f: \RR^n \rightarrow \RR$ is $L$ Lipschitz smooth and $\mu \ge 0$ strongly convex and $g: Q \rightarrow \overline \RR$ is convex. 
    We consider optimization problem: $\min_x \left\lbrace f(x) + g(x)\right\rbrace$.
    Fix any arbitrary $y \in \RR^n$, define for all $x \in Q$: 
    \begin{align*}
        \widetilde{\mathcal M}^{L^{-1}}
        (x; y) 
        &:=
        g(x) + f(y) + \langle \nabla f(y), x - y\rangle 
        + \frac{L}{2}\Vert x - y\Vert^2, 
        \\
        \mathcal M^{L^{-1}}(x; y) &:= F(x) + \frac{L}{2}\Vert x - y\Vert^2, 
    \end{align*}
    and the proximal gradient operator $T_L := [I + L^{-1}\partial g]^{-1}\circ [I - L^{-1}\nabla f]$. 
    The proximal gradient operator $T_L$ admits 
    $$
        T_Ly = \argmin_x \left\lbrace
            g(x) + \langle \nabla f(y), x\rangle + L/2\Vert x - y\Vert^2
        \right\rbrace. 
    $$
    It is a single-valued mapping, and it has its domain on the entire $\RR^n$. 
    Another operator often associated with $T_L$ is the gradient mapping operator $\mathcal G_L:= L(I - T_L)$. 
    Finally, define the Bregman divergence of $f$: 
    \begin{align*}
        D_f(x, y): \RR^n \times \RR^n \rightarrow \RR 
        \defeq f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
    \end{align*}
    The lemma follows explains equates between $\mathcal M(\cdot; y), \widetilde{\mathcal M}(\cdot; y)$. 
    \begin{lemma}[Proximal gradient envelope] 
        Fix any $y \in \RR^n$, we will have for all $x \in Q$ that: 
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y)
            &= 
            \mathcal M^{L^{-1}}(x; y)- D_f(x, y) \le \mathcal M^{L^{-1}}(x; y). 
        \end{align*}
    \end{lemma}
    \begin{proof}
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) 
            &= 
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            g(x) + f(x) - f(x) + f(y) 
            + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            F(x) - D_f(x, y) + \frac{L}{2}\Vert x - y\Vert^2 
            \\
            &= \mathcal M^{L^{-1}}(x; y) - D_f(x, y). 
        \end{align*}
    \end{proof}
    \subsection{Proximal inequality}
    \begin{theorem}[Proximal inequality]\label{thm:prox-grad-ineq}
        Fix any $y\in \RR^n$, we have for all $x \in Q$: 
        \begin{align*}
            F(x) - F(T_Ly) - \langle L(y - T_Ly), x - y\rangle
            - L\Vert y - T_L y\Vert^2
            - D_f(x, y)
            &\ge  
            0. 
        \end{align*}
        When $f$ is $\mu \ge 0$ strongly convex, it implies that 
        \begin{align*}
            F(x) - F(T_Ly) - \langle L(y - T_Ly), x - y\rangle
            - \frac{L}{2}\Vert y - T_L y\Vert^2
            - \frac{\mu}{2}\Vert x - y\Vert^2
            &\ge  
            0.
        \end{align*}
    \end{theorem}
    \begin{proof}
        Observe that the function $\widetilde {\mathcal M}(\cdot; y)$ is $L$ strongly convex with minimizer $T_Ly$. 
        For all $x \in Q$, it has quadratic growth condition at its minimizer $T_Ly$ which gives these equivalent inequalities: 
        {\small
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) - 
            \widetilde{\mathcal M}^{L^{-1}}(T_Ly; y)
            - 
            \frac{L}{2}\Vert x - T_Ly\Vert^2
            &\ge 
            0
            \\
            \iff
            \left(
                \mathcal M^{L^{-1}}(x; y) - D_f(x, y)
            \right) - 
            \mathcal M^{L^{-1}}(T_Ly; y) 
            - 
            \frac{L}{2}\Vert x - T_Ly\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                \mathcal M^{L^{-1}}(x; y)
                - 
                \mathcal M^{L^{-1}}(T_Ly; y)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - T_Ly\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(T_Ly) 
                + 
                \frac{L}{2}\Vert x - y\Vert^2 - 
                \frac{L}{2}\Vert T_Ly - y\Vert^2
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - T_Ly\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(T_Ly) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - T_Ly + T_Ly - y\Vert^2
                    - 
                    \Vert y - T_Ly\Vert^2
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - T_Ly\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(T_Ly) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - T_Ly\Vert^2 + 
                    2\langle x - T_Ly, T_Ly - y\rangle
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - T_Ly\Vert^2
            &\ge 0
            \\
            \iff
            \left(
                F(x) - F(T_Ly) + \frac{L}{2}\Vert x - T_Ly\Vert^2 
                - L\langle  x - T_Ly, y - T_Ly\rangle
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - T_Ly\Vert^2
            &\ge 0
            \\
            \iff 
            F(x) - F(T_Ly)
            - \langle L(y - T_Ly), x - T_Ly\rangle
            - D_f(x, y) 
            &\ge 0. 
        \end{align*}
        }
        Continuing, it has: 
        \begin{align*}
            F(x) - F(T_Ly) - \langle L(y - T_Ly), x - T_Ly\rangle - D_f(x, y) &\ge 0
            \\
            \iff
            F(x) - F(T_Ly)
            - \langle L(y - T_Ly), x - y + y - T_Ly\rangle - D_f(x, y) 
            &\ge 0
            \\
            \iff
            F(x) - F(T_Ly)
            - \langle L(y - T_Ly),x - y \rangle
            - L\Vert y - T_Ly\Vert^2
            - D_f(x, y) 
            &\ge 0
            \\
            \implies 
            F(x) - F(T_Ly)
            - \langle L(y - T_Ly),x - y \rangle
            - \frac{L}{2}\Vert y - T_Ly\Vert^2
            - D_f(x, y) 
            &\ge 0
            \\
            \implies
            F(x) - F(T_Ly)
            - \langle L(y - T_Ly),x - y \rangle
            - \frac{L}{2}\Vert y - T_Ly\Vert^2
            - \frac{\mu}{2}\Vert x - y\Vert^2
            &\ge 0. 
        \end{align*}
        Going from the second last inequality to the last inequality, we used the fact that $f$ is strongly convex hence for all $x, y$, it's true that $D_f(x, y) \ge \mu/2 \Vert x - y\Vert^2$. 
    \end{proof}

\section{Stepwise formulation of weak accelerated proximal gradient}
    This entire section is building up the R-WAPG algorithm which is described in the Definition \ref{def:wapg} of the next section. 
    \par 
    Definition \ref{def:stepwise-wapg} which describes what happens at a single iteration of the R-WAPG algorithm. 
    It defines a procedure of generating $x_{k + 1}, v_{k + 1}$ given any $x_k, v_k$. 
    Proposition \ref{prop:stepwise-lyapunov} states the inequality that describes a decreasing quantity that involves $F(x_k), F(x_{k + 1})$ at each single iteration. 
    \begin{assumption}
        Choose any integer $k\ge 0$. 
        Given $x_k, y_k, v_k$, we define the following quantities
        \begin{align}
            g_k &\defeq L(y_k - T_L y_k), 
            \label{eqn:grad-map}
            \\
            l_F(x; y_k) &\defeq F(T_Ly_k) + \langle g_k, x - y_k\rangle + \frac{1}{2L}\Vert g_k\Vert^2, 
            \label{eqn:lower-linearization}
            \\
            \epsilon_{k} &\defeq F(x_k) - l_F(x_k; y_k), 
            \label{eqn:regret}
        \end{align}
        Observe that by convexity of $F$, $\epsilon_k \ge 0$ for all $x_k, L > 0$. 
        To see, use Theorem \ref{thm:prox-grad-ineq} and let $y = y_k, x = x_k$ which gives: 
        \begin{align*}
            F(x_k) - F(T_Ly_k)
            - \langle L(y_k - T_Ly_k),x_k - y_k \rangle
            - \frac{L}{2}\Vert y_k - T_Ly_k\Vert^2
            - \frac{\mu}{2}\Vert x_k - y_k\Vert^2
            &\ge 0
            \\
            \iff 
            F(x_k) - F(T_Ly_k)
            - \langle g_k,x_k - y_k \rangle
            - \frac{1}{2L}\Vert g_k\Vert^2
            &\ge 0. 
        \end{align*}
    \end{assumption}
    
    \begin{definition}[Stepwise weak accelerated proximal gradient]\label{def:stepwise-wapg}\;\\
        Assume $0 \le \mu < L$.
        Fix any $k \in \mathbb Z$. 
        For any $(v_k, x_k), \alpha_k \in (0, 1), \gamma_k > 0$, let $\hat \gamma_{k + 1}$, and vectors $y_k, v_{k + 1}, x_{k + 1}$ be given by: 
        \begin{align*}
            \hat \gamma_{k + 1} &= (1 - \alpha_k)\gamma_k + \mu \alpha_k, 
            \\
            y_k &= 
            (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k), 
            \\
            g_k &= \mathcal G_L y_k, 
            \\
            v_{k + 1} &= \hat\gamma^{-1}_{k + 1}
            (\gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
            \\
            x_{k + 1} &= T_L y_k. 
        \end{align*}
    \end{definition}
    \begin{observation}\label{obs:stepwise-wapg}
        Let's observe Definition \ref{def:stepwise-wapg} closely. 
        We make two crucial observations here. 
        We have $x_k, y_k, v_k$ lie on the same line because and they have the following equivalent equalities: 
        \begin{align*}
            (Q1): 
            y_k - v_k &= 
            \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k}(x_k - y_k),
            \\
            (Q2): 
            y_k - x_k &= 
            \frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
    \end{observation}
        To see (Q1), observe 
        \begin{align*}
            y_k - v_k &= 
            \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k}(x_k - y_k)
            \\
            \iff 
            -(\alpha_k \gamma_k \hat \gamma^{-1}_{k + 1} + 1)y_k
            &= 
            - \alpha_k \gamma_k \hat \gamma^{-1}_{k + 1}v_k - x_k
            \\
            \iff 
            y_k &= 
            \frac{
                \alpha_k \gamma_k \hat \gamma_{k + 1}^{-1}v_k + x_k
            }{1 + \alpha_k \gamma_k \hat \gamma_{k + 1}^{-1}}
            \\
            &=  
            \frac{\alpha_k \gamma_k v_k + \hat \gamma_{k + 1} x_k}{\gamma_k + \alpha_k \mu}.
        \end{align*}
        On the first equality (Q1), we multiplied both side of the equation by $\hat\gamma_{k + 1}/\alpha_k \gamma_k$ and then group $y_k$ on the LHS. 
        The last equality comes by multiplying both the numerator and denominator by $\hat \gamma_{k + 1}$, leaving the numerator $\hat \gamma_{k + 1} + \alpha_k \gamma_k = \gamma_k + \alpha_k \mu$. 
        We used the definition of $\hat \gamma_{k + 1}$ here. 
        To see the second equality (Q2), consider: 
        \begin{align*}
            y_k &= (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k)
            \\
            \iff
            y_k - x_k &= 
            (\gamma_k + \alpha_k \mu)^{-1}
            (\alpha_k \gamma_k v_k - (\gamma_k + \alpha_k \mu)x_k + \hat \gamma_{k + 1} x_k)
            \\
            \iff 
            (\gamma_k + \alpha_k \mu)(y_k - x_k)
            &= 
            \alpha_k\gamma _kv_k + 
            (\hat \gamma_k - \gamma_k - \alpha_k \mu) x_k
            \\
            &= \alpha_k \gamma_k v_k - \alpha_k \gamma_k x_k 
            \\
            &= \alpha_k \gamma_k(v_k - x_k)
            \\
            \iff 
            y_k - x_k &= 
            \frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
        On the second equality that follows the second $\iff$ we just substituted $\hat\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$. 
        Therefore, we discover that $x_k, v_k, y_k$ lies on the same line because (Q1) indicates $y_k - v_k$ parallels to $-(y_k - x_k)$ and both vector shares the same head which anchors at $y_k$. 
    
    \begin{proposition}[Stepwise Lyapunov]\label{prop:stepwise-lyapunov}\;\\
        Fix any integer $k \in \mathbb Z$.
        Given any $v_k, x_k$ and $\gamma_k > 0$, invoke Definition \ref{def:stepwise-wapg} to obtain $v_{k + 1}, x_{k + 1}, y_k, \hat \gamma_{k + 1}$. 
        Fix any arbitrary $R_k \in \RR$.
        Define: 
        \begin{align*}
            R_{k + 1}
            \defeq
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right). 
        \end{align*}
        Then it has for all $x^* \in \RR^n$ where $F^* = F(x^*)$, the inequality: 
        \begin{align*}
            F(x_{k + 1}) - F^* + R_{k + 1} + \frac{\hat \gamma_{k + 1}}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{\gamma_{k}}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}
        The proof is a direct proof. 
        Here we drop the subscript $k$, $k + 1$ on $\gamma_{k}, \hat \gamma_{k + 1}$ because they are fixed throughout the proof, and it makes for better readability! 
        \par 
        We started with $F(x_{k + 1}) + R_{k + 1}$ as Equation (1*), and $\hat \gamma/2\Vert v_{k + 1} - x^*\Vert^2$ as (2*). 
        In the proof that follows, we show (2*) equals to (2.2*). 
        We added (2*), (2.2*) to produce (3*). 
        Then we show (3*) equals to (3.1*). 
        The content of the entire LHS of the inequality we want to prove is in (3.2*). 
        Going from (3.2*) to (4*) is an inequality which we used Theorem \ref{thm:prox-grad-ineq}. 
        \par
        Start by considering the first and the third term of the LHS of the inequality that we want to prove. 
        \begin{align*}
            F(x_{k + 1}) &= 
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2, 
            \\
            R_{k + 1}
            &= 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right), 
            \\
            \implies 
            F(x_{k + 1}) + R_{k + 1}
            &= 
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            \\
            &\quad 
                + 
                (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right). 
        \tag{1*}
        \end{align*}
        Coefficient of $\epsilon_k$ and $\Vert g_k\Vert^2$ are grouped. 
        Next, we have: 
        \begin{align*}
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^* \Vert^2
            &= 
            \frac{\hat \gamma}{2}\Vert 
                \hat \gamma^{-1}
                (
                    \gamma(1 - \alpha_k)v_k - 
                    \alpha_k g_k + \mu \alpha_k y_k
                )
                - x^* 
            \Vert^2
            \\
            &=  
            \frac{\hat \gamma}{2}
            \Vert 
                \hat \gamma^{-1}
                (
                \hat \gamma v_k + \mu \alpha_k(y_k - v_k)
                    - \alpha_k g_k
                )
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert 
                v_k + \hat \gamma^{-1} \mu \alpha_k (y_k - v_k)
                - \hat \gamma^{-1}\alpha_k g_k
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert v_k - x^*\Vert^2 
            + 
            \frac{\alpha_k^2}{2\hat \gamma}\Vert \mu(y_k - v_k) - g_k\Vert^2 
            \\ &\quad 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            \\ &\quad
                + 
                \frac{\alpha_k^2}{2\hat \gamma}
                \Vert \mu(y_k - v_k) - g_k\Vert^2 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle. 
        \tag{2*}
        \end{align*}
        On the above derivation, the first equality is by definition of $v_{k + 1}$; the second equality is by $\hat \gamma = (1 - \alpha_k)\gamma + \mu \alpha_k$ the second equality comes by considering: 
        \begin{align*}
            \gamma(1 - \alpha_k) v_k &= 
            (\hat \gamma  - \mu \alpha_k)v_k
            = \hat \gamma v_k - \mu\alpha_k v_k
            \\
            \iff 
            \gamma(1 - \alpha_k) v_k + \mu \alpha_k y_k
            &= 
            \hat \gamma v_k + \mu \alpha_k(y_k - v_k). 
        \end{align*}
        Focusing on the last two terms by the end of expression (2*), we have  
        \begin{align*}
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            & = 
            \frac{\alpha_k^2\mu}{\hat \gamma}
            \left(
                \frac{\mu}{2}\Vert y_k - v_k\Vert^2 
                - \langle y_k - v_k, g_k\rangle
            \right)
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2, 
            \\
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            &= 
            \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle 
            - \alpha_k \langle v_k - x^*, g_k\rangle. 
        \tag{2.1*}
        \end{align*}
        Adding the LHS of both equations above together gives: 
        {\small
        \begin{align*}
            & \quad 
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            + 
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
            + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \end{align*}
        }
        With the above we can conclude that (2*) simplifies to 
        {\small
        \begin{align*} 
            & 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            + 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \tag{2.2*}
        \end{align*}
        }
        Recall from 
        \hyperref[obs:stepwise-wapg]{Observations \ref*{obs:stepwise-wapg}} that: 
        \begin{align*}
            (Q1): 
            y_k - v_k &= 
            \frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k),
            \\
            (Q2): 
            y_k - x_k &= 
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
        Ok that is a lot, we list the following equations to assist things: 
        \begin{align*}
            &  
            - \alpha_k(v_k - x^*) - \frac{\alpha_k^2 \mu}{\hat \gamma}(y_k - v_k) - (x_k - y_k)
            \\
            & =
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k^2\mu}{\hat \gamma}\frac{\hat \gamma}{\alpha_k \gamma}(x_k - y_k)
            - (x_k - y_k) 
            &\text{by Q1} 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k \mu}{\gamma}(x_k - y_k)
            - (x_k - y_k) 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \left(
                1 + \frac{\alpha_k \mu}{\gamma}
            \right)(x_k - y_k)
            \\
            &= 
            -\alpha_k(v_k - x^*) - 
            \frac{\alpha_k \mu + \gamma}{\gamma}
            \frac{\alpha_k \gamma}{\gamma + \alpha_k \mu}(x_k - v_k)
            &\text{by Q2}
            \\
            &= 
            -\alpha_k(v_k - x^*)
            - \alpha_k(x_k - v_k)
            \\
            &= \alpha_k(x^* - x_k). 
        \tag{Q3}
        \end{align*}
        Adding (2.2*) to (1*) gives: 
        \begin{align*}
            &
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\quad 
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
                + 
                \left\langle g_k, 
                    - \alpha_k(v_k - x^*) 
                    - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \epsilon_k 
            + \left\langle 
                g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                - (x_k - y_k)
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            \text{Use Q3}&= 
            F(x_k) - \epsilon_k 
            + \alpha_k\left\langle 
                g_k, 
                x^* - x_k
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k\epsilon_k + \alpha_k\langle g_k, x^* - x_k\rangle
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat\gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \tag{3*}
        \end{align*}
        From the left to the right on the first equality, coefficients of $\Vert g_k\Vert^2$ cancels out to zero and the inner product containing $g_k$ are grouped. 
        Going from the left to the right of the third equality, we applied (Q3) derived earlier to simplify the inner product term. 
        The last equalities re-arranged terms and grouped the coefficients of $\epsilon_k$ together. 
        Taking a page break, continuing on (3*) we have
        \begin{align*}
            &
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \left(
                    \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}
                    + 
                    \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
                \right)\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            & =
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \frac{\mu \alpha_k}{2}\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\ &=
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                +
                \frac{\mu\alpha_k}{2} \Vert y_k - x^*\Vert^2
            \\&= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
            \tag{3.1*}
        \end{align*}
        On the first equality, coefficients of $\Vert y_k - v_k\Vert^2$ are grouped together. 
        On the second to the third equality, the terms had been simplified by 
        \begin{align*}
            \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma} + 
            \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
            &= 
            \frac{\mu\alpha_k}{2}\left(
                \frac{(1 - \alpha_k)\gamma_k + \alpha_k \mu}{\hat \gamma}
            \right)
            \\
            &= \frac{\mu\alpha_k}{2}\left(
                \frac{\hat \gamma}{\hat \gamma}
            \right) = \frac{\mu\alpha_k}{2}. 
        \end{align*}
        (3.1*) was adding (1*) and (2.2*) together, which is the same as adding (1*) and (2*) together. 
        So that was all equal to (1*) + (2*) and it says: 
        \begin{align*}
            & F(x_{k + 1}) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            & \iff 
            \\
            & F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_k) - F(x^*) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &= (1 - \alpha_k)(F(x_k) - F(x^*))
            + \alpha_k\left(
                F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                (1 - \alpha_k)\left(
                    R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
                \right). 
        \tag{3.2*}
        \end{align*}
        Focusing on the second term, we simplify the multiplier inside: 
        {\small
        \begin{align*}
            & F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= 
            F(x_k) - F(x^*) - \left(
                F(x_k) - F(T_L y_k) - \langle g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2
            \right)- \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= F(T_L y_k) - F(x^*) + \langle g_k, x^* - y_k\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            + \frac{1}{2L}\Vert g_k\Vert^2 \le 0. 
        \tag{4*}    
        \end{align*}
        }
        On the last line, we expand the definition of $g_k$ and then used the
        \hyperref[thm:prox-grad-ineq]{Theorem \ref*{thm:prox-grad-ineq}}. 
        Therefore, we conclude that: 
        {\small
        \begin{align*}
            F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)\left(
                F(x_k) - F(x^*) + R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
        }
        Adding back the $k$, $k + 1$ subscript for $\gamma$, $\hat \gamma$, it's proven. 
    \end{proof}
    \begin{remark}
        \textcolor{red}
        {
        Given $y_k$, we may choose to increase $\mu_\kappa = 2D_f(x^*, y_k)/\Vert y_k - x^*\Vert^2 \ge \mu$ which also works on (4*), $\mu$ is a pessimistic choice for the inequality above. 
        }
        But in general the choice of $\mu$ remains the strong convexity modulus or equivalently, any value that is smaller than the true strong convexity constant for claiming the convergence rate for all initial guesses. 
    \end{remark}

\section{Formulation of R-WAPG and its convergence rates}
    The sole focus of this section is Definition \ref{def:wapg}.
    It describes the R-WAPG algorithm which generates iterates $(x_k, y_k, v_k)$ and admits potential convergence claim using Proposition \ref{prop:stepwise-lyapunov}. 
    Definition \ref{def:rwapg-seq} introduces the concept of an R-WAPG sequences which is crucially important. 
    The sequence formulates the R-WAPG algorithm stated in Definition \ref{def:wapg}, it connects the step-wise formulation of R-WAPG (Definition \ref{def:stepwise-wapg}) and describes the convergence claim of R-WAPG in Proposition \ref{prop:wagp-convergence}. 
    In the next section, it continues to play a crucial role in describing several equivalent forms of the R-WAPG algorithm, and their convergence claim. 
    \begin{definition}[R-WAPG sequences]\label{def:rwapg-seq}\;\\
        Assume $0 \le \mu < L$. 
        The sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge1}$ are sequences parameterized by $\mu, L$. 
        They are valid for R-WAPG if all the following holds: 
        \begin{align*}
            \alpha_0 &\in (0, 1], 
            \\
            \alpha_k &\in (\mu/L, 1) \quad (\forall k \ge 1), 
            \\
            \rho_k &:= \frac{\alpha_{k + 1}^2 - (\mu/L)\alpha_{k + 1}}{(1 - \alpha_{k + 1})\alpha_k^2} \quad \forall (k \ge 0). 
        \end{align*}
        We call $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ the \textbf{R-WAPG Sequences}. 
    \end{definition}
    \begin{observation}\label{obs:r-wapg-observation-1}
        The following is true: 
        \begin{enumerate}
            \item $\rho_k > 0$ for all $k \ge 0$. 
            \item $\forall k \ge 1: L\alpha_k^2 = (1 - \alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2 +\mu\alpha_k$. 
        \end{enumerate}
        
        % If $\alpha_1 \in (0, 1)$ which is true by $\gamma_1 \in (0, L]$, then the entire sequence $\alpha_k \in (0, 1)$. 
        % Suppose inductively that $\alpha_{k - 1}, \rho_{k - 1}$ are given such that they satisfy $\alpha_{k -1} \in (0, 1)$ and $0 < \rho_{k - 1} \alpha_{k - 1}^2 < 1$. 
        % Solving the quadratic $L\alpha_k^2=(1 - \alpha_k)\rho_{k - 1}L\alpha_{k - 1}^2 + \mu \alpha_k$ for $\alpha_k$ yields candidates. 
        % \begin{align*}
        %     \alpha_k &= 
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
        %         \pm
        %         \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
        %     \right). 
        % \end{align*}
        % Choosing the positive sign which is the larger one of the roots of the quadratic. 
        % We have $\alpha_k > 0$ because: 
        % \begin{align*}
        %     \alpha_k &=
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
        %         +
        %         \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
        %     \right)
        %     \\
        %     &\ge 
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
        %         +
        %         \left|\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L\right|
        %     \right) + \alpha_{k - 1}\sqrt{\rho_{k - 1}}
        %     \\
        %     & > 0. 
        % \end{align*}
        % On the last strict inequality we used the fact that $\rho_{k - 1}> 0, \alpha_{k - 1} > 0$. 
        % An upper bound can be identified by using inductive hypothesis and considering: 
        % \begin{align*}
        %     \alpha_k &= 
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
        %         +
        %         \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
        %     \right)
        %     \\
        %     &<
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} + 
        %         \sup_{x \in (0, 1)}
        %         \left\lbrace
        %             -x + \sqrt{(x - \mu/L)^2 + 4x}
        %         \right\rbrace
        %     \right)
        %     \\
        %     & \le \frac{1}{2}\left(
        %         \mu/L + \max\left(\mu/L, -1 + \sqrt{(1 - \mu/L)^2 + 4}\right)
        %     \right) \le 1. 
        % \end{align*}
        % Going to the first inequality, we used $\rho_{k - 1} \alpha_{k - 1}^2 < 1$ to get the strict inequality. 
        % Going from the second to the third inequality, we maximized $\mu/L$ by monotone increasing of linear function and the square root function. 
        % Therefore, inductively, $\alpha_k \in (0, 1)$ holds. 
        % However, the limit of $\alpha_k$ could be 1. 
    \end{observation}

    \begin{definition}[Relaxed weak accelerated proximal gradient (R-WAPG)]\label{def:wapg}\;\\
        Choose any $x_1 \in \RR^n, v_1 \in \RR^n$. 
        Let $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ be given by Definition \ref{def:rwapg-seq}. 
        The algorithm generates a sequence of vector $(y_k, x_{k + 1}, v_{k + 1})_{k \ge 1}$ for $k\ge 1$ by the procedures:  
        \begin{tcolorbox}
            For $k=1, 2, 3, \cdots$
            \begin{align*}
                \gamma_k &:= \rho_{k -1}L\alpha_{k - 1}^2, 
                \\
                \hat \gamma_{k + 1} & := (1 - \alpha_k)\gamma_k + \mu \alpha_k = L\alpha_k^2, 
                \\
                y_k &= 
                (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k), 
                \\
                g_k &= \mathcal G_L y_k, 
                \\
                v_{k + 1} &= 
                \hat\gamma^{-1}_{k + 1}
                (\gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
                \\
                x_{k + 1} &= T_L y_k. 
            \end{align*}    
        \end{tcolorbox}
    \end{definition}
    \begin{observation}\label{obs:r-wapg-observation-2}
        Observe that if $\rho_k = 1$ for all $k\ge 0$, then the above algorithm is similar to (2.2.7) in Nesterov's book \cite{nesterov_lectures_2018} because $L\alpha_k^{2} = (1 - \alpha_k)L\alpha_{k - 1}^2 + \mu \alpha_k$. 
        \par
        For all $k \ge 1$, the algorithm chains together the sequence of $(\hat \gamma_{k+1})_{k \ge 1}$ and $(\gamma_k)_{k \ge 1}$ in Definition \ref{prop:stepwise-lyapunov} through the equalities: 
        \begin{align*}
            \hat \gamma_{k + 1} 
            &= L\alpha_k^2 
            \\
            &= (1 - \alpha_k)\gamma_k + \alpha_k \mu
            \\
            &= (1 - \alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2 + \alpha_k \mu
            \\
            &= (1 - \alpha_k)\rho_{k - 1}\hat \gamma_{k} + \alpha_k \mu. 
        \end{align*}
        The third equality from Observation \ref{obs:r-wapg-observation-1}. 
    \end{observation}
    
    \begin{proposition}[R-WAPG convergence claim]\label{prop:wagp-convergence}\; \\
        Fix any arbitrary $x^* \in \RR^n, N \in \mathbb N$. 
        Let vector sequence $(y_k, v_{k}, x_{k})_{k \ge 1}$ and R-WAPG sequences $\alpha_k, \rho_k$ be given by Definition \ref{def:wapg}. 
        Define $R_1 = 0$ and suppose that for $k = 1, 2, \cdots, N$, we have $R_k$ recursively given by: 
        \begin{align*}
            R_{k + 1}
            := 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right). 
        \end{align*} 
        Then for all $k = 1, 2, \cdots, N$: 
        \begin{align*}
            & F(x_{k + 1}) - F(x^*) + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}  
        $\hat \gamma_2 = (1 - \alpha_1)\gamma_1 + \mu \alpha_1$ is true by Observation \ref{obs:r-wapg-observation-1}. 
        Observe that $R_2$ is defined in the same manner as in Proposition \ref{prop:stepwise-lyapunov} with k = 1 so we can invoke Proposition \ref{prop:stepwise-lyapunov}, and it gives: 
        \begin{align*}
            & F(x_{2}) - F(x^*) + R_2 + \frac{L \alpha_1^2}{2}\Vert v_{2} - x^*\Vert^2
            \\
            & = F(x_{2}) - F(x^*) + R_2 + \frac{\hat \gamma_{2}}{2}\Vert v_{2} - x^*\Vert^2
            & \hat \gamma_2 = L \alpha_1^2 , \text{ Definition \ref{def:wapg}}
            \\
            &\le 
            (1 - \alpha_1)\left(
                F(x_1) - F(x^*) + R_1 + \frac{\gamma_1}{2}\Vert v_1 - x^*\Vert^2
            \right) 
            & \quad \text{By Proposition \ref{prop:stepwise-lyapunov}}
            \\
            &= 
            (1 - \alpha_1)\left(
                F(x_1) - F(x^*) + R_1 + \frac{L\rho_{0}\alpha_{0}^2}{2}\Vert v_1 - x^*\Vert^2
            \right) 
            \\
            &= \max\left(\rho_0, 1\right)
            (1 - \alpha_1)\left(
                F(x_1) - F(x^*) + R_1 + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
        We had demonstrated the base case. 
        \par 
        Next, for all $k = 2, 3, \cdots, N$, from Observation \ref{obs:r-wapg-observation-2}: 
        \begin{align*}
            \hat \gamma_{k + 1} = L\alpha_{k}^2 
            &=(1 - \alpha_k)\rho_{k - 1}L\alpha_{k - 1}^2 + \mu\alpha_k
            \\
            &= (1 - \alpha_k)\gamma_k + \mu\alpha_k. 
        \end{align*}
        So $\gamma_k, \hat \gamma_{k + 1}, \alpha_k$ fits Definition \ref{def:stepwise-wapg}. 
        The definition of $R_k$ given is the same as in Proposition \ref{prop:stepwise-lyapunov} hence we can use Proposition \ref{prop:stepwise-lyapunov} to unroll recursively for all $k \ge 1$: 
        {\small
        \begin{align*}
            &
            F(x_{k + 1}) - F^* + R_{k + 1} + \frac{\hat\gamma_{k + 1}}{2}\Vert v_{k + 1} - x^*\Vert^2 
            \\
            &=
            F(x_{k + 1}) - F^* + R_{k + 1} + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{\rho_{k - 1}L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \max(1, \rho_{k - 1})\frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \max(1, \rho_{k - 1})(1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F^* + R_1 + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
        }
        We had demonstrated in the inductive case for $k=1, 2, \cdots, N$. 
        Additionally, for all $k = 1, 2, \cdots, N$ it has $R_{k + 1} \ge 0$ because: 
        \begin{align*}
            R_{k + 1}
            &= 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &= (1 - \alpha_k)
            \left(
                \epsilon_k + R_k 
                + \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\ge 
            (1 - \alpha_k) R_k
            \\
            &\ge R_1 \prod_{i = 1}^{k} \left(1 - \alpha_i\right) = 0. 
        \end{align*}
        Going from the left to the right on the first equality, we used the fact that $\hat \gamma_{k + 1} = L \alpha_{k}^2$.
        This makes coefficient of $\Vert g_k\Vert^2$ zero. 
        The first inequality is by $\epsilon_k \ge 0$ and the non-negativity of the remaining terms. 
        The last equality is by the assumption that $R_1 = 0$. 
        Therefore: 
        {\small
        \begin{align*}
            & 
            F(x_{k + 1}) - F^* +
            \frac{L\alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F^* + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
        }
    \end{proof}
    \begin{remark}        
        \textcolor{red}
        {
            The choice of $\rho_k$ allows for nontraditional choices of step sizes in the literatures. 
            As we can see from the above convergence claim, there exists choices of $\rho_k\ge 1$ where convergence remains possible. 
        }
    \end{remark}


\section{Equivalent representations of R-WAPG}
    This section reduces Definition \ref{def:wapg} into simpler forms that are comparable to what commonly appears in the literatures. 
    In the literatures, variants of Accelerated Proximal Gradient algorithm such as FISTA, V-FISTA has different representations. 
    This section carries out the tedious but necessary works for demonstrating the descriptive power of R-WAPG. 
    We will formulate it into three generic forms that are commonly seemed in the literatures. 
    These equivalent representations are listed in Definition \ref{def:r-wapg-intermediate}, \ref{def:r-wapg-st-form} and \ref{def:r-wapg-momentum-form}. 
    \par 
    Proposition \ref{prop:wapg-first-equivalent-repr} simplifies Definition \ref{def:wapg} and finds a representation without using auxiliary sequence $\gamma_k, \hat \gamma_k$. 
    Definition \ref{def:r-wapg-intermediate} states the first simplified form of the R-WAPG algorithm which we call: ``R-WAPG intermediate form''. 
    Following a similar pattern, Proposition \ref{prop:wagp-st-form}, \ref{prop:r-wapg-momentum-repr} demonstrates two more equivalent representations of the R-WAPG intermediate form (Definition \ref{def:r-wapg-intermediate}) which are formulated into Definition \ref{def:r-wapg-st-form}, \ref{def:r-wapg-momentum-form}. 
    Convergence results from Proposition \ref{prop:wagp-convergence} applies to all these equivalent forms of R-WAPG. 
    In brief, the relations of these proposition and definitions can be summarized by
    \begin{align*}
        & \text{Definition } \ref{def:wapg} 
        \\
        & \iff 
        \text{Definition } \ref{def:r-wapg-intermediate} & \text{By Proposition }\ref{prop:wapg-first-equivalent-repr}.
        \\
        & \iff \text{Definition }\ref{def:r-wapg-st-form} & \text{By Proposition }\ref{def:r-wapg-st-form}.
        \\
        & \;\implies 
        \text{Definition }\ref{def:r-wapg-momentum-form}. 
        & \text{By Proposition }\ref{prop:r-wapg-momentum-repr}.
    \end{align*}
    
    To start, the following proposition on ``abstract similar triangle form'' were made to simplify arguments and notations to make nicer proofs. 

    \begin{proposition}[Abstract similar triangle form]\label{prop:abs-st-form}\;\\
        Given any initial $(x_1, v_1)$, and a sequence $(\tau_k)_{k \ge 1}, (\xi_k)_{k \ge 1}$ such that for all $k \ge 1$ $\tau_k \in (0, 1), \xi_k \in (0, 1)$. 
        For all $k\ge 1$, iterates $(y_k, z_{k + 1}, x_{k + 1})_{k \ge 1}$ satisfies recursively that: 
        \begin{align*}
            y_k &= (1 + \tau_k)^{-1}(v_k + \tau_k x_k),
            \\
            v_{k + 1} &= (1 + \xi_k)^{-1}(v_k + \xi_k y_k) - (1 + \xi_k)^{-1}\delta_k g_k,
            \\
            x_{k + 1} &= y_k - L^{-1} g_k. 
        \end{align*}
        If $1 + \xi_k + \tau_k = L\delta_k, \tau_k \neq 0\; \forall k \ge 1$. 
        Then for all $k \ge 1$: 
        $$
            v_{k + 1} - x_{k + 1} = (1 + \xi_k)^{-1}\tau_k(x_{k + 1} - x_k). 
        $$
        Which makes the algorithm a similar triangle form. 
    \end{proposition}
    \begin{proof}
        We are interested in identifying the conditions required for the sequence of $\xi_k, \tau_k, \delta_k$ such that there exists $\theta_k$ satisfying: 
        \begin{align*}
            v_{k + 1} - x_{k + 1} 
            &= \theta_k(x_{k + 1} - x_k).
        \end{align*}
        To verify, do 
        \begin{align*}
            v_{k + 1} &= 
            (1 + \xi_k)^{-1}(v_k + \xi_t y_k - \delta_k \mathcal G_L(y_k))
            \\
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k)y_k - \tau_t x_k + \xi_k y_k - \delta_k \mathcal G_L(y_k))
            \\
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k + \xi_k)y_k - \tau_k x_k - \delta_k \mathcal G_L(y_k))
            \\
            v_{k + 1} - x_{k + 1}
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k + \xi_k)y_t - \tau_k x_k - \delta_t \mathcal G_L(y_k))
            - (y_k - L^{-1}\mathcal G_Ly_k)
            \\
            &= 
            (1 + \xi_k)^{-1}(\tau_ky_k - \tau_k x_k - \delta_k \mathcal G_L(y_k))
            + L^{-1}\mathcal G_Ly_k
            \\
            &= 
            (1 + \xi_k)^{-1}
            \left(
                \tau_ty_k - \tau_t x_k + (L^{-1}(1 + \xi_k) - \delta_k) \mathcal G_L(y_k)
            \right)
            \\
            &= 
            (1 + \xi_k)^{-1}\tau_k
            \left(
                y_k - x_k + 
                \tau_k^{-1}(L^{-1}(1 + \xi_k) - \delta_k) \mathcal G_L(y_k)
            \right).
        \end{align*}
        Going between the first and second inequality we used $v_k = (1 + \tau_k)y_k - \tau_k x_k$ which is rearranged $y_k = (1 + \tau_k)^{-1}(v_k + \tau_k x_k)$. 
        The RHS is can be verified through 
        \begin{align*}
            x_{k + 1} - x_k &= 
            y_t - L^{-1}\mathcal G_L(y_k) - x_k
            \\
            &= (y_k - x_k) - L^{-1}\mathcal G_L(y_k). 
        \end{align*}
        It necessitates the condition: 
        \begin{align*}
            \tau_k^{-1}(L^{-1}(1 + \xi_k) - \delta_k) 
            &= - L^{-1}
            \\
            (1 + \xi_k) - L\delta_k
            &= 
            - \tau_k
            \\
            1 + \xi_k + \tau_k
            &=
            L\delta_k. 
        \end{align*}
        Which allows for: 
        \begin{align*}
            v_{k + 1} - x_{k + 1} &= 
            (1 + \xi_k)^{-1}\tau_t
            \left(y_k - x_k - L^{-1}\mathcal G_L(y_k)\right) 
            = 
            (1 + \xi_k)^{-1}\tau_k(x_{k + 1} - x_k). 
        \end{align*}
    \end{proof}
    \subsection{A list of equivalent representations of R-WAPG}
        This section lists three equivalent representations of the R-WAPG algorithm. 
        They are comparable to existing APG algorithms in the literatures. 
        \begin{definition}[R-WAPG intermediate form]\label{def:r-wapg-intermediate}\;\\
            Assume $\mu < L$ and let $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ given by Definition \ref{def:rwapg-seq}. 
            Initialize any $x_1, v_1$ in $\RR^n$. 
            For $k \ge 1$, the algorithm generates sequence of vector iterates $(y_{k}, v_{k + 1}, x_{k + 1})_{k \ge 1}$ by the procedures: 
            \begin{tcolorbox}
                For $k = 1, 2, \cdots$
                \begin{align*} 
                    & y_{k} = 
                    \left(
                        1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                    \right)^{-1}
                    \left(
                        v_{k + 1} + 
                        \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                    \right), 
                    \\
                    & x_{k + 1} = 
                    y_k - L^{-1} \mathcal G_L y_k, 
                    \\
                    & v_{k + 1} = 
                    \left(
                        1 + \frac{\mu}{L \alpha_k - \mu}
                    \right)^{-1}
                    \left(
                        v_k + 
                        \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                    \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
                \end{align*}
            \end{tcolorbox}
        \end{definition}
        \begin{definition}[R-WAPG similar triangle form]\label{def:r-wapg-st-form} \; \\
            Given any $(x_1, v_1)$ in $\RR^n$. 
            Assume $\mu < L$.
            Let the sequence $(\alpha_k)_{k \ge 0}, (\rho_k)_{k\ge 0}$ be given by Definition \ref{def:rwapg-seq}. 
            For $k \ge 1$, the algorithm generates sequences of vector iterates $(y_k, v_{k + 1}, x_{k + 1})_{k \ge 1}$ by the procedures: 
            \begin{tcolorbox}
                For $k=1, 2, \cdots $
                \begin{align*}
                    & y_k = 
                    \left(
                        1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
                    \right)^{-1}
                    \left(
                        v_k + 
                        \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
                    \right), 
                    \\
                    & x_{k + 1} = 
                    y_k - L^{-1} \mathcal G_L y_k, 
                    \\
                    & v_{k + 1} = 
                    x_{k + 1} + (\alpha_k^{-1} -1)(x_{k + 1} - x_k). 
                \end{align*}    
            \end{tcolorbox}
        \end{definition}
        \begin{definition}[R-WAPG momentum form]\label{def:r-wapg-momentum-form}
            Given any $y_1 = x_1 \in \RR^n$, and sequences $(\rho_k)_{k \ge 0}, (\alpha_k)_{k\ge 0}$ Definition \ref{def:rwapg-seq}. 
            The algorithm generates iterates $x_{k + 1}, y_{k + 1}$ For $k = 1, 2, \cdots $ by the procedures: 
            \begin{tcolorbox}
                For $k=1, 2,\cdots $
                \begin{align*}
                    & x_{k + 1} = y_k - L^{-1}\mathcal G_Ly_k, 
                    \\
                    & 
                    y_{k + 1} = 
                    x_{k + 1} + 
                    \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
                \end{align*}    
            \end{tcolorbox}
            In the special case where $\mu = 0$, the momentum term can be represented without relaxation parameter $\rho_k$: 
            $$
                (\forall k \ge 1)\quad \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}} 
                = \alpha_{k + 1}(\alpha_k^{-1} - 1).  
            $$
        \end{definition}
        \begin{remark}
           \todo[inline]{Consider adding a remark here on how this is similar in format to the Nesterov's accelerated gradient (2.2.19) in his book. }
        \end{remark}

    \subsection{Proving the equivalent representations of R-WAPG}
        \begin{proposition}[First equivalent representation of R-WAPG]\label{prop:wapg-first-equivalent-repr}\;\\
            If the sequence $(y_k, v_k, x_k)_{k \ge 1}$ is produced by R-WAPG (Definition \ref{def:wapg}), 
            then the iterates can be expressed without $(\gamma_k)_{k \ge1},(\hat \gamma_k)_{k \ge 2}$, and for all $k\ge 1$ they are algebraically equivalent to
            \begin{align*}
                & 
                y_{k} = 
                \left(
                    1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                \right)^{-1}
                \left(
                    v_{k} + 
                    \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                \right), 
                \\
                & x_{k + 1} = 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                & v_{k + 1} = 
                \left(
                    1 + \frac{\mu}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
        \end{proposition}
        \begin{proof}
            For all $k \ge 1$, by R-WAPG (Definition \ref{def:wapg}), it has: 
            \begin{align*}
                y_{k} &= 
                (\gamma_k + \alpha_k \mu)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                (\hat \gamma_{k + 1} + \alpha_k \gamma_k)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                \left(
                    \frac{\hat \gamma_{k + 1}}{\alpha_k\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    \frac{L\alpha_k^2}{\alpha_k\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{L\alpha_k^2}{\alpha_k \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    \frac{L\alpha_k}{\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{L\alpha_k}{ \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    1 + \frac{L - L \alpha_k}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{L - L \alpha_k}{L \alpha_k - \mu} x_k
                \right). 
            \end{align*}
            From the left to right of the second inequality, we used the fact that $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k\mu$. 
            Going from the left to the right of the second last inequality, we did the following: 
            \begin{align*}
                L\alpha_k^2 &= 
                (1 - \alpha_k)\gamma_k + \alpha_k \mu 
                \\
                \iff 
                L \alpha_k^2 - \alpha_k\mu &= 
                (1 - \alpha_k)\gamma_k
                \\
                \iff 
                \gamma_k/L
                &= 
                \frac{L \alpha_k^2 - \alpha_k\mu}{L (1 - \alpha_k)}
                \\
                \iff 
                L/\gamma_k
                &= 
                \frac{L (1 - \alpha_k)}{L \alpha_k^2 - \alpha_k\mu}
                \\
                \iff 
                L\alpha_k/\gamma_k
                &= 
                \frac{L - L\alpha_k}{L\alpha_k - \mu}. 
            \end{align*}
            On the third $\iff$, we can assume $\alpha_k \neq \mu/L\;  \forall k \ge 1$ because from Definition \ref{def:rwapg-seq}: $\alpha_k \in (\mu/L, 1)$ for all $k \ge 1$. 
            % On the third $\iff$, we can assume $\alpha_k \neq \mu/L\; \forall k \ge 2$ because if $\alpha_k = \mu/L$ were impossible, it then has
            % \begin{align*}
            %     L \alpha_k^2 - \alpha_k\mu &= 
            %     (1 - \alpha_k)L \rho_{k - 1}\alpha_{k - 1}^2 = 0 \implies \alpha_{k - 1} = 0. 
            % \end{align*}
            % Which contradict $\alpha_{k - 1} \in (0, 1)\; \forall k \ge 2$ because $\alpha_1 \in (0, 1)$ is given to satisfy the condition in the base case, and $\alpha_k \in (0, 1)\; \forall k \ge 2$ by justifications in Observation \ref{obs:r-wapg-observation-1}. 
            \\
            For all $k \ge 1$, $v_{k + 1}$ has: 
            \begin{align*}
                v_{k + 1} &= 
                \hat \gamma_{k + 1}^{-1}
                ((1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
                \left(
                    (1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k} y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu} y_k
                \right)
                - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
            Going from the left to the right of the second equality, we substitute $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu\alpha_k$. 
            At the end, recall that for all $k \ge 1$, it has $\hat \gamma_{k + 1} = L \alpha_k^2 = (1 - \alpha_k)\gamma_k + \alpha_k \mu$, so: 
            \begin{align*}
                (1 - \alpha_k)\gamma_k
                &= 
                \hat \gamma_{k + 1} - \mu \alpha_k
                = 
                L\alpha_{k}^2 - \alpha_k\mu. 
            \end{align*}
            The is now complete. 
            This form doesn't have $\rho_k, \gamma_k, \hat \gamma_k$ in it. 
        \end{proof}
        
        \begin{proposition}[Second equivalent representation of R-WAPG]\label{prop:wagp-st-form}\;\\
            Let iterates $(y_k, x_{k}, v_{k})_{k \ge 1}$ and sequence $(\alpha_k, \rho_k)_{k \ge 0}$ be given by Definition \ref{def:r-wapg-intermediate}. 
            Then for all $k \ge 1$, iterate $y_k, x_{k + 1}, v_{k + 1}$
            satisfy: 
            \begin{align*}
                y_{k} &= 
                \left(
                    1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                \right)^{-1}
                \left(
                    v_{k} + 
                    \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                \right), 
                \\
                x_{k + 1} &= 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                v_{k + 1} &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proposition}
        \begin{proof}
            From Definition \ref{def:r-wapg-intermediate}, define $(\tau_k, \xi_k, \delta_k)_{k \ge 1}$ in Proposition \ref{prop:abs-st-form} to be
            \begin{align*}
                (\forall k \ge 1) \quad \tau_k &= \frac{L(1 - \alpha_k)}{L\alpha_k - \mu},
                \\
                (\forall k \ge 1)\quad 
                \xi_k &= \frac{\mu}{L \alpha_k - \mu},
                \\
                (\forall k \ge 1)\quad 
                \delta_k &\defeq \frac{1 + \xi_k}{L\alpha_k}. 
            \end{align*}
            We claim that these parameters satisfy $L\delta_k = 1 + \tau_k + \xi_k$. 
            To see, we have forall $k\ge 1$: 
            \begin{align*}
                1 + \tau_k + \xi_k &= 
                1 + \frac{L(1 - \alpha_k)}{L \alpha_k - \mu} 
                + \frac{\mu}{L \alpha_k - \mu}
                \\
                &= 
                1 + \frac{L - L \alpha_k + \mu}{L\alpha_k - \mu}
                \\
                &= 
                \frac{L - L \alpha_k + \mu + L \alpha_k - \mu}{L\alpha_k - \mu}
                \\
                &= \frac{L}{L\alpha_k - \mu}. 
            \end{align*}
            Next, it also has for all $k \ge 1$: 
            \begin{align*}
                \frac{1 + \xi_k}{\alpha_k}
                &= 
                \frac{1 + \frac{\mu}{L\alpha_k - \mu}}{\alpha_k}
                = 
                \frac{\frac{L\alpha_k - \mu + \mu}{L \alpha_k - \mu}}{\alpha_k}
                = 
                \frac{L}{L\alpha_k - \mu}.
            \end{align*}
            Hence,  $v_{k + 1} - x_{k + 1} = (1 + \xi_k)^{-1}(x_{k + 1} - x_k)\; \forall k \ge 1$ by Proposition \ref{prop:abs-st-form}.
            Therefore, substituting it gives: 
            \begin{align*}
                v_{k + 1} &= 
                x_{k + 1} + \left(
                    1 + \frac{\mu}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k - \mu}{L\alpha_k}
                \right)\left(
                    \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proof}
    
    \begin{proposition}[Third equivalent representation of R-WAPG]\label{prop:r-wapg-momentum-repr}
        \;\\
        Let sequence $(\alpha_k, \rho_k)_{k \ge 0}$ and iterates $(x_k, v_k, y_k)_{k\ge 1}$ given by R-WAPG intermediate form (Definition \ref{def:r-wapg-st-form}). 
        Then for all $k \ge 1$, the iterates $(x_{k + 1}, y_{k + 1})_{k \ge 1}$ are algebraically equivalent to: 
        \begin{align*}
            x_{k + 1} &= y_k - L^{-1}\mathcal G_Ly_k, 
            \\
            y_{k + 1} &= 
            x_{k + 1} + 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        If in addition, $v_1 = x_1$ then 
        \begin{align*}
            y_1 = \left(
                1 + \frac{L - L \alpha_1}{L\alpha_1 - \mu}
            \right)^{-1}\left(
                v_1 + \left(
                    \frac{L - L \alpha_1}{L \alpha_1 - \mu}
                \right)x_1
            \right) = x_1. 
        \end{align*}
        In the special case when $\mu = 0$, the momentum term admits simpler representation 
        \begin{align*}
            (\forall k \ge 1) \quad 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
            & = \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proposition}
    \begin{proof}
        Start by considering the update rule for $v_k$ from the Definition \ref{def:r-wapg-st-form} which has for all $k \ge 1$: 
        \begin{align*}
            v_{k + 1} &= 
            x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \\
            \iff 
            (L \alpha_{k + 1} - \mu)v_{k + 1} 
            &= 
            (L \alpha_{k + 1} - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
        \end{align*}
        Next, we simplify the update $y_{k}$ by Definition \ref{def:r-wapg-st-form} for all $k \ge 1$: 
        \begin{align*}
            y_k &= 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \left(
            \frac{L - \mu}{L\alpha_k - \mu} 
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \frac{L\alpha_k - \mu}{L - \mu} v_k
            + 
            \frac{L - L \alpha_k}{L - \mu} x_k
            \\
            &= (L - \mu)^{-1}((L \alpha_k - \mu)v_k + (L - L \alpha_k)x_k). 
        \end{align*}
        We have $y_1 = x_1$ when $v_1 = x_1$ because substituting $v_1 = x_1$ for how $y_1$ is defined it has: 
        \begin{align*}
            y_1 &= (L - \mu)^{-1}(L(\alpha_1 - \mu)x_1 + (L - L\alpha_1)x_1)
            \\
            &= (L - \mu)^{-1}(L(\alpha_1 - \mu)x_1 + (L - L \alpha_1)x_1)
            \\
            &= (L - \mu)^{-1}((L - \mu)x_1) = x_1. 
        \end{align*}
        Now substituting $v_{k + 1}$ into $y_{k + 1}$, so it has for all $k\ge 1$: 
        {\small
        \begin{align*}
            y_{k + 1} &= 
            (L - \mu)^{-1}((L\alpha_{k + 1} - \mu)v_{k + 1} + (L - L \alpha_{k + 1})x_{k + 1})
            \\
            &= (L - \mu)^{-1}
            \left(
                (L\alpha_{k + 1} - \mu)x_{k + 1} + 
                (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
                + (L - L \alpha_{k + 1})x_{k + 1}
            \right)
            \\
            &= 
            (L - \mu)^{-1}
            \left(
                (L\alpha_{k + 1} - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \right)
            \\
            &= x_{k + 1} + \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}(x_{k + 1} - x_k). 
        \end{align*}
        }
        The coefficient for $(x_{k + 1} - x_k)$ needs some works to formulate it without the parameter $\mu, L$. 
        To do that we have: 
        \begin{align*}
            \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}
            &= \frac{(L\alpha_{k + 1} - \mu)\alpha_k(1 - \alpha_k)}{\alpha_k^2(L - \mu)}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \left(
                \frac{\alpha_k^2(L - \mu)}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \alpha_k(1 - \alpha_k)
            \left(
                \frac{L\alpha_k^2 - \mu\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \rho_k\left(
                \frac{L\rho_k\alpha_k^2 - \mu\rho_k\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \rho_k\alpha_k(1 - \alpha_k)
            \left(
                \frac{(L\alpha_{k + 1} - \mu)(\rho_k\alpha_k^2 + \alpha_{k + 1})}
                {L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}.
        \end{align*}
        Going from the left to right on the fourth equality, we used $L\alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_kL\alpha_k^2 + \mu \alpha_{k + 1}$ with 
        \begin{align*}
            L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2 
            &= 
            (1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            ((1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \mu \alpha_{k + 1}) - \mu\alpha_{k + 1} + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= L \alpha_{k + 1}^2 - \mu\alpha_{k + 1} + \alpha_{k + 1}L\rho_k\alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            L\alpha_{k + 1}(\alpha_{k + 1} + \rho_k \alpha_k^2) - \alpha_{k + 1}\mu - \mu \rho_k \alpha_k^2
            \\
            &= (L \alpha_{k + 1} - \mu)(\alpha_{k + 1} + \rho_k \alpha_k^2). 
        \end{align*}
        When $\mu = 0$, things simplify. 
        Consider that $\forall k \ge 1: \alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_k\alpha_k^2$. 
        \begin{align*}
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \alpha_{k + 1}^2}
            \\
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \rho_k(1 - \alpha_{k + 1})\alpha_k^2}
            \\
            &= \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2}
            \\
            &= \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proof}


\section{R-WAPG describes existing accelerations scheme in the literatures}
    In addition to various equivalent forms of the R-WAPG algorithm, the R-WAPG sequences are descriptive. 
    They generalize many existing sequences used in accelerated proximal gradient schemes. 
    \par
    This section demonstrates that several variants of FISTA in the literatures reduces to the R-WAPG method by setting up the R-WAPG sequences with additional assumptions. 
    This section will also demonstrate that the convergence claim (Proposition \ref{prop:wagp-convergence}) holds, and it derives convergence rates consistent with results in the literatures.  
    \begin{lemma}[R-WAPG sequences as inverted FISTA sequence]\label{lemma:inverted-fista-seq}
        Let R-WAPG sequence $(\rho_k)_{k \ge 0}, (\alpha_k)_{k \ge 0}$ given by Definition \ref{def:rwapg-seq}. 
        If $\mu = 0, \rho_k \ge 1\; \forall k \ge 0$, and $\alpha_0 = 1$, then: 
        \begin{enumerate}
            \item $\alpha_k^{-2} \ge \alpha_{k + 1}^{-2} - \alpha_{k + 1}^{-1}\; \forall k \ge 0$
            \item Let $t_k := \alpha_k^{-1}$, then $0 < t_{k + 1} \le (1/2)\left(1 + \sqrt{1 + 4t_k^2}\right)\;\forall k\ge 0$, hence the name: ``Inverted FISTA sequence''. 
            \item $\prod_{i = 1}^k\max(1, \rho_{k - 1})(1 - \alpha_k) = \alpha_k^2 \quad (\forall k \ge 1)$. 
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        We start proving (i). 
        For all $k \ge 1$: 
        \begin{align*}
            \alpha_{k + 1}^2 
            &= (1 - \alpha_{k + 1})\rho_k\alpha_k^2 + (\mu/L) \alpha_k
            \\
            &= (1 - \alpha_{k + 1})\rho_k\alpha_k^2 & \mu = 0 \text{ assumed } 
            \\
            \implies 
            \alpha_{k + 1}^2 
            & \ge (1 - \alpha_{k + 1})\alpha_k^2 
            &  \rho_k \ge 1
            \\
            \iff 
            \alpha_k^{-1} 
            &\ge 
            \alpha_{k + 1}^{-2} + \alpha_{k + 1}^{-1}. 
        \end{align*}
        When $k = 0$, it has $\alpha_0 = 1$. 
        By definition of $\rho_0$ we have 
        \begin{align*}
            \alpha_1^2 &= 
            (1 - \alpha_1)\alpha_0^2\rho_0 \underset{\mu = 0}{=}
            \frac{\alpha_1^2}{\alpha_0^2(1 - \alpha_1)}(1 - \alpha_1)\alpha_0^2 = \alpha_1^2. 
        \end{align*}
        Therefore (i) holds. 
        (i) is proved. 
        \par
        (i) $\implies$ (ii) because substituting $\alpha_k^{-1} = t_k$ changes (i) into $t_{k + 1}^2 - t_{k + 1} - t_{k}^2 \le 0$ for all $k \ge 0$. 
        Solving the equality yields $t_{k + 1} = (1/2)\left(1 \pm \sqrt{1 + 4 t_k^2}\right)$ for all $k \ge 0$. 
        Since $\alpha_k \in (0, 1)$, it means $t_k > 0$, hence the valid root is the larger root which gives the upper bound for $t_{k + 1}$ so: 
        \begin{align*}
            t_{k + 1} \in \left(
                0, \frac{1}{2}\left(1 + \sqrt{1 + 4t_k^2}\right) 
            \right]
        \end{align*}
        To prove (iii), because $\rho_k \ge 1$, we have $\prod_{i = 1}^{k} \max(1, \rho_{k - 1})(1 - 
        \alpha_k)$ for all $k \ge 1 = \prod_{i = 1}^{k}\rho_{k - 1}(1 - \alpha_k)$. 
        By definition $(\alpha_k)_{k \ge 1}, (\rho_k)_{k \ge 1}$ sequences they have for all $k \ge 1$: 
        \begin{align*}
            \alpha_k^2 &= \rho_{k - 1}(1 - \alpha_k)\alpha_{k - 1}^2
            \\
            \iff 
            \alpha_k^2/\alpha_{k - 1}^2 &= \rho_{k - 1}(1 - \alpha_k)
            \\
            \prod_{i = 1}^{k}\rho_{k - 1}(1 - \alpha_k)
            &= 
            \prod_{i = 1}^{k}\alpha_k^2 /\alpha_{k - 1}^2= \alpha_k^2/\alpha_0. 
        \end{align*}
        Because $\alpha_0 = 1$, (iii) is justified. 
    \end{proof}
    \begin{remark}
        The sequence $t_k$ is exactly the same as in Theorem 3.1 of Chmabolle, Dossal \cite{chambolle_convergence_2015}. 
    \end{remark}

    \begin{lemma}[Constant R-WAPG sequences]\label{lemma:constant-rwapg-seq}
        Suppose $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ are R-WAPG sequences given by Definition \ref{def:rwapg-seq} and assume $L > \mu > 0$.
        Define $q := \mu/L$. 
        Then $\forall r \in \left(\sqrt{q},\sqrt{q^{-1}}\right)$, the constant sequence $\alpha_k := r \sqrt{q}$. 
        \begin{enumerate}
            \item Fix any $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$ then the constant sequence $\alpha_k := \alpha \in (q, 1)$ and\\
            $\rho_k := \rho=\left(1-r^{-1}\sqrt{q}\right)\left(1 - r \sqrt{q}\right)^{-1} > 0$, hence it's a pair of valid R-WAPG sequence. 
            \item The momentum term in Definition \ref{def:r-wapg-momentum-form}, which we denoted by $\theta$ has:\\ $\theta = (1 - r^{-1}\sqrt{q})(1 - r\sqrt{q})(1- q)^{-1}$. 
            \item When $r = 1$, $\theta = (1- \sqrt{q})(1 + \sqrt{q})^{-1}$. 
            \item For all $r \in \left(1, \sqrt{q^{-1}}\right)$, $\rho > 1$; for all $r \in \left(\sqrt{q}, 1\right]$ $\rho \le 1$. 
            \item For all $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$, $\max(\rho, 1)(1 - \alpha) = \max\left(1 - r\sqrt{q}, 1 - r^{-1}q\right)$. 
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        To see (i), fix any $\sqrt{q} < r < \sqrt{q^{-1}}$, so
        \begin{align*}
            r &\in \left(\sqrt{q}, \sqrt{q^{-1}}\right)
            \iff 
            r\sqrt{q} \in 
            \left(
                q, 1
            \right). 
        \end{align*}
        Therefore, $\alpha_k \in (\mu/L, 1)$. 
        To see $\rho_k$, by definition it has 
        \begin{align*}
            \rho_k &= \frac{\alpha_{k + 1}^2 - q \alpha_{k + 1}}{(1 - \alpha_{k + 1})\alpha_k^2} 
            = \frac{\alpha^2 - q \alpha}{(1 - \alpha)\alpha^2} 
            \\
            &= \frac{1 - q\alpha^{-1}}{1 - \alpha}
            \\
            &= \frac{1 - q r^{-1}\sqrt{q^{-1}}}{1 - r \sqrt{q}}
            \\
            &= \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}} > 0. 
        \end{align*}
        Simple algebra can show (ii). 
        To start, we substitute the constant sequence $\alpha, \rho$ for $\alpha_k, \rho_k$ into the definition of $\theta$: 
        \begin{align*}
            \theta &= \frac{\rho\alpha(1 - \alpha)}{\rho \alpha^2 + \alpha}
            = (\rho\alpha)\frac{1 - \alpha}{\rho \alpha^2 + \alpha} = \rho \frac{1 - \alpha}{\rho \alpha + 1}
            \\
            &= 
            \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}}
            (1 - r\sqrt{q})
            \left(
                1 + r\sqrt{q}\frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}}
            \right)^{-1}
            \\
            &= (1 - r^{-1} \sqrt{q})\left(
                1 + \frac{r \sqrt{q} - q}{1 - r \sqrt{q}}
            \right)^{-1}
            \\
            &= \left(1 - r^{-1} \sqrt{q}\right)\left(
                \frac{1 - q}{1 - r \sqrt{q}}
            \right)^{-1} = \frac{(1 - r^{-1}\sqrt{q})(1 - r \sqrt{q})}{1 - q}. 
        \end{align*}
        Now it's the perfect opportunity to show (iii) by substituting $r = 1$ which has 
        \begin{align*}
            \theta &= \frac{(1 - \sqrt{q})^2}{1 - q}
            =
            \frac{(1 - \sqrt{q})}{(1 - \sqrt{q})(1 + \sqrt{q})}
            = \frac{1 - \sqrt{q}}{1 + \sqrt{q}}. 
        \end{align*}
        Next for (iv), we determine when $\rho$ switches from $> 1$ to $ \le 1$. 
        For any $r \in \left(1, \sqrt{q^{-1}}\right)$, we show $\rho > 1$: 
        \begin{align*}
            \rho &= \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}} 
            > \frac{1 - \sqrt{q}}{1 - r \sqrt{q}} > \frac{1 - \sqrt{q}}{1 - \sqrt{q}} = 1
        \end{align*}
        The first inequality comes from $r > 1 \iff -r^{-1} > -1$, the second inequality comes from $r > 1 \iff -r < 1$. 
        For any $r \in \left(\sqrt{q}, 1\right]$, we have $\rho \le 1$ by 
        \begin{align*}
            \rho &= \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}} 
            \le \frac{1 - \sqrt{q}}{1 - r \sqrt{q}} \le \frac{1 - \sqrt{q}}{1 - \sqrt{q}} = 1. 
        \end{align*}
        The first inequality comes from $-r^{-1} \le - 1$; the second inequality comes from $-r \ge -1$. 
        \par
        Finally, to show (v), we consider by cases: 
        \begin{enumerate}
            \item[A:] When $r \in \left(1, \sqrt{q^{-1}}\right)$, we have $\rho > 1$ by (iii), meaning that $1 - r^{-1}\sqrt{q} > 1 - r \sqrt{q}$. $\rho > 1 \implies \max(\rho, 1) = \rho$ hence 
                \begin{align*}
                    \max(\rho, 1)(1 - \alpha) &= \rho(1- \alpha)
                    = \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}}(1 - r \sqrt{q}) = 1 - r^{-1}\sqrt{q}. 
                \end{align*}
                In this case, the larger one of the two values: $\left\lbrace1 - \rho^{-1}\sqrt{q}, 1 - r\sqrt{q}\right\rbrace$ is taken. 
            \item[B:] When $r \in \left(\sqrt{q}, 1\right]$, we have $\rho \le 1$ by (iii); hence $1 - r^{-1}\sqrt{q} \le 1 - r \sqrt{q}$ and $\max(1,\rho) = 1$ therefore: 
            \begin{align*}
                \max(1, \rho)(1 - \alpha) = (1 - \alpha) = 1 - r \sqrt{q}. 
            \end{align*}
            Once again, the larger one of the quantity: $1 - r \sqrt{q}$ among $\{1 - r^{-1}\sqrt{q}, 1 - r\sqrt{q}\}$ is taken. 
        \end{enumerate}
        Therefore, for all vales of $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$ are completed by cases: (A), (B).
        The larger of the value among $\{1 - r \sqrt{q}, 1 - r^{-1}\sqrt{q}\}$ is always taken. 
        Therefore it has: 
        \begin{align*}
            \max(1, \rho)(1 - \alpha) = \max\left(1 - r\sqrt{q}, 1 - r^{-1}\sqrt{q}\right). 
        \end{align*}

    \end{proof}

    \begin{theorem}[FISTA first variant Chambolle, Dossal 2015]\;\\
        Fix arbitrary $a \ge 2$.
        Define $\forall k \ge 1$ the sequence $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ by 
        \begin{align*}
            \alpha_k &= a/(k + a), 
            \\
            \rho_k &= \frac{(k + a)^2}{(k + 1)(k + a + 1)}. 
        \end{align*}
        Consider the algorithm given by: 
        \begin{tcolorbox}
            Initialize any $y_1 = x_1$. 
            \\
            For $k = 1, 2, \cdots$, update: 
            \begin{align*}
                & x_{k + 1} := y_k + L^{-1}\mathcal G_L(y_k), 
                \\
                & \theta_{t + 1} := \alpha_{k + 1}(\alpha_k^{-1} - 1),
                \\
                & y_{k + 1} := x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k). 
            \end{align*}    
        \end{tcolorbox}
        If $\mu = 0$, then $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ is a valid pair of R-WAPG sequence from Definition \ref{def:rwapg-seq} and the above algorithm is a valid form of R-WAPG. 
        \par
        Assume minimizer $x^*$ exists for function $F$. 
        Then algorithm produces $(x_k)_{k \ge0}$ such that $F(x) - F(x^*)$ convergences at a rate of $\mathcal O(1/k^2)$. 
    \end{theorem}
    \begin{proof}
        We only need to show that $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ are valid R-WAPG sequences. 
        To do that we verify equality $\alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_k \alpha_k^2$ because it's assumed $\mu = 0$ in this case. 
        For all $k \ge 0$, the right side of the equality evaluates to 
        \begin{align*}
            \alpha_k^2 \rho_k(1 - \alpha_{k + 1}) &= 
            \left(
                \frac{\alpha}{k + 1}
            \right)^2 \left(
                \frac{(k + a)^2}{(k + 1)(k + a + 1)}
            \right)
            \left(
                1 - \frac{a}{k + 1 + a}
            \right)
            \\
            &= \left(
                \frac{\alpha}{k + 1}
            \right)^2 \left(
                \frac{(k + a)^2}{(k + 1)(k + a + 1)}
            \right)
            \left(
                \frac{k + 1}{k + 1 + a}
            \right)
            \\
            &= \frac{a^2}{(k + a + 1)^2} = \alpha_{k + 1}^2. 
        \end{align*}
        Therefore, $(\rho_k)_{k \ge 0}, (\alpha_k)_{k \ge 0}$ is a pair of valid R-WAPG sequence. 
        So, it can be used in the R-WAPG algorithm and represent it in R-WAPG Momentum Form in Definition \ref{def:r-wapg-momentum-form}. 
        Using $\mu = 0$, it simplifies the momentum term in Definition \ref{def:r-wapg-momentum-form} and gives the rules of updates in the theorem statement. 
        \par
        Next, we show the convergence rate of the algorithm. 
        Observe that $(\forall k \ge 0)(\forall a \ge 2)$, $\rho_k$ has 
        \begin{align*}
            \rho_k &= \frac{(k + a)^2}{(k + 1)(k + a + 1)} 
            \\
            &> \frac{(k + 1)^2}{(k + 1)(k + a + 1)} 
            & \text{By } a \ge 2. 
            \\
            &= \frac{(k + 1)}{(k + a + 1)} 
            \\
            & > 1.   & \text{By } a > 2. 
        \end{align*}
        Hence (iii) in Lemma \ref{lemma:inverted-fista-seq} applies. 
        We may use Proposition \ref{prop:wagp-convergence} by the validity of our $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ sequences, we have:
        \begin{align*}
            & F(x_{k + 1}) - F(x^*) + \frac{L\alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            & = F(x_{k + 1}) - F(x^*) + \frac{L\alpha_k^2}{2}\Vert x_{k + 1} - x^* + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k)\Vert^2
            \\
            & \le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right)
            \\
            &= 
            \alpha_k^2
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right) 
            & \text{By Lemma \ref{lemma:inverted-fista-seq} (iii). }
            \\
            &= 
            \left(\frac{a}{k + a}\right)^2
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert x_1 - x^*\Vert^2
            \right). 
        \end{align*}
        We can replace $v_1$ to be $x_1$ by Proposition \ref{prop:r-wapg-momentum-repr}. 
    \end{proof}
    \begin{remark}
        This algorithm described here is exactly the same algorithm being analyzed in the paper by Chambolle, Dossal \cite{chambolle_convergence_2015}. 

    \end{remark}

    \begin{theorem}[Fixed momentum FISTA]\label{thm:fixed-momentum-fista}
        Suppose a constant 
    \end{theorem}


\subsection{The method of Free R-WAPG}
    This section introduces an algorithm of our creation inspired by the remark of Proposition \ref{prop:stepwise-lyapunov}. 
    Algorithm \ref{alg:free-rwapg} estimates the $\mu$ constant as the algorithm executes and pools the information using the Bregman Divergence of the smooth part function $f$. 
    \begin{algorithm}
        \begin{algorithmic}[1]
        {\footnotesize
        \STATE{\textbf{Input: } $f, g, x_0, L > \mu \ge 0, \in \RR^n, N \in \N$}
        \STATE{\textbf{Initialize: }$y_0 := x_0;L := 1; \mu := 1/2; \alpha_0 = 1;$}
        \STATE{\textbf{Compute: } $f(y_k)$; }
        \FOR{$k = 0, 1, 2, \cdot, N$}
            \STATE{\textbf{Compute: }$\nabla f(y_k); x^+:= [I + L^{-1}\partial g](y_k - L^{-1}\nabla f(y_k))$;}
            \WHILE{$L/2\Vert x^+ - y\Vert^2 < D_f(x^+, y)$}
                \STATE{$L:= 2L$;}
                \STATE{$x^+ = [I + L^{-1}\partial g](y_k - L^{-1}\nabla f(y_k))$; }
            \ENDWHILE
            \STATE{$x_{k + 1} := x^+$;}
            \STATE{$\alpha_{k + 1} := (1/2)\left(\mu/L - \alpha_{k}^2 + \sqrt{(\mu/L - \alpha_{k}^2)^2 + 4\alpha_{k}^2}\right)$;}
            \STATE{$\theta_{k + 1} := \alpha_k(1 - \alpha_k)/(\alpha_k^2 + \alpha_{k + 1})$;}
            \STATE{$y_{k + 1}:= x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k)$; }
            \STATE{\textbf{Compute: } $f(y_{k + 1})$}
            \STATE{$\mu := (1/2)(2D_f(y_{k + 1}, y_{k})/\Vert y_{k + 1} - y_k\Vert^2) + (1/2)\mu$;}
        \ENDFOR
        }
        \end{algorithmic}
        \caption{Free R-WAPG}
        \label{alg:free-rwapg}
    \end{algorithm}
    \par
    Line 5-8 estimates upper bound for the Lipschitz constant and find $x^+$, the next iterates produced by proximal gradient descent on previous $y_k$.
    Line 9 updates $x_{k + 1}$ to be $x^+$, a successful iterate identified by the Lipschitz line search routine. 
    Line 10 updates the R-WAPG sequence $\alpha_k$ for the iterates $y_{k + 1}$. 
    Line 13 updates $\mu$ using the Bregman Divergence of $f$ from iterates $y_{k + 1}, y_k$. 
    \par
    Assume $L$ given is an upper bound of the Lipschitz smoothness constant of $f$, the algorithm computes $\nabla f(y_k)$ for $x^+$, $f(y_{k + 1})$ for Bregman Divergence and $f(y_{k})$ is evaluated from the previous iteration. 
    We note that $f(y_0)$ is computed before the start of the for loop. 
    And finally, it evaluates proximal of $g$ at $y_k - L^{-1}\nabla f(y_k)$ once. 



\subsection{Numerical experiments}



\bibliographystyle{siam}
\bibliography{references/Inexact_Momentum.bib}

\appendix

\end{document}
