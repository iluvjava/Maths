
\subsection{Introduction}
    Recent works from Ahn and Sra \cite{ahn_understanding_2022} and Nesterov \cite{nesterov_lectures_2018} inspired content in this section.
    Ahn explored the interpretation of Nesterov acceleration via PPM. 
    They proposed the idea of ``similar triangle" for unifying all varieties of Nesterov accelerated gradient. 
    They used PPM to derive several variations of the Nesterov accelerated gradient algorithms. 
    Finally, they refurnished \hyperref[thm:lower_approx_ppm_convergence]{theorem \ref*{thm:lower_approx_ppm_convergence}} for the proof of convergence rate for the accelerated gradient.
    Their analysis results in relatively simple arguments that exhibits powerful extensions to several variants of the Nesterov accelerated gradient. 

    \par\noindent
    Interestingly, the Nesterov accelerated gradient applies to PPM too; Guler \cite{guler_new_1992} did it two decades ago. 
    He uses the idea of a Nesterov acceleration sequence faithfully. 
    One recent development of the accelerated PPM is an algorithmic framework named: ``Universal Catalyst acceleration", proposed by Lin et al \cite{lin_universal_2015}. 
    It is an application of Guler's work in the context of variance-reduction stochastic gradient algorithms for machine learning. 

    \par\noindent
    \textbf{We state our contributions}. 
    In \hyperref[sec:AG_varieties]{section \ref*{sec:AG_varieties}} 
    We stated different forms of accelerated gradient that appeared in the literatures and the equivalences between their forms. 
    They are abstract because the choice of stepsize parameters in the algorithms are unspecified. 
    Abstract forms related to the proximal gradient method were our own inventions inspired by discussions in the literatures, which includes 
    \hyperref[def:ag_prox_grad_ppm]{definition \ref*{def:ag_prox_grad_ppm}}, and 
    \hyperref[def:ag_prox_grad_generic]{definition \ref*{def:ag_prox_grad_generic}}. 
    In \hyperref[sec:AG_useful_information]{section \ref*{sec:AG_useful_information}}
    we prove and interpret some of the unproved claims made by Ahn, Sra \cite{ahn_understanding_2022} as good exercises and for the peace of the mind. 
    In
    \hyperref[sec:generic_ag_ppm_lyapunov_analysis]{section \ref*{sec:generic_ag_ppm_lyapunov_analysis}},
    we state two theorems for the derivation of convergences rate for various type of algorithm using an upper bound of a Lyapunov function. 
    This part is inspired by works from Ahn, Sra \cite{ahn_understanding_2022}, but with the gradient mapping and proximal gradient added into the picture, making it distinct from all algorithms discussed in Ahn and Sra's paper. 
    \hyperref[sec:generic_ag_ppm_lyapunov_analysis]
        {Section \ref*{sec:generic_ag_ppm_lyapunov_analysis}} 
    derives the convergence rate and determine the parameters for the step sizes needed to achieve an optimal convergence rate. 
    In each section, we recover a specific variant of the Accelerated gradient and identify their appearances in the literature. 


\subsection{Preliminaries}
    We introduce some additional lemma that are crucial to the derivations of non-smooth accelerated gradient method. 
    \begin{definition}[The Gradient Mapping]
        \label{def:gradient_mapping}
        Let $g = f + g$ where $f$ is $L$-Lipschitz smooth and convex, $g$ is convex. 
        Define the proximal gradient operator
        $$
            \mathcal T_L(x) := \hprox_{L^{-1}g}(x - L^{-1}\nabla f(x)),
        $$
        then the gradient mapping is defined as
        $$
            \mathcal G_L(x) = L(x - \mathcal T_L(x)). 
        $$
    \end{definition}
    \begin{remark}
        The name ``gradient mapping" comes from \cite[(2.2.54)]{nesterov_lectures_2018}, however, Nesterov was referring to only the case when $g$ is an indicator function of a convex set in his writing. 
        Of course, in Amir Beck \cite[10.3.2]{beck_first-order_nodate}, it has the exact same definition for gradient mapping as the above. 
    \end{remark}

    \begin{lemma}[Gradient Mapping Approximates Subgradient]
        \label{lemma:grad_map_lemma_first}
        Continue from 
        \hyperref[def:gradient_mapping]{definition \ref*{def:gradient_mapping}}, 
        the gradient mapping satisfies
        \begin{align*}
            x^+ &= \mathcal T_L(x), 
            \\
            L(x - x^+) &\in  \nabla f(x) + \partial g(x^+) \ni \mathcal G_L(x). 
        \end{align*}
    \end{lemma}
    \begin{proof}
        \begin{align*}
            x^+ &= [I + L^{-1}\partial g]^{-1}\circ [I - L^{-1}\nabla f](x)
            \\
            [I + L^{-1}\partial g](x^+) &\ni [I - L^{-1}\nabla f](x)
            \\
            x^+ + L^{-1}\partial g(x^+) &\ni x - L^{-1}\nabla f(x)
            \\
            x^+ - x + L^{-1}\partial g(x^+) &\ni L^{-1}\nabla f(x)
            \\
            L(x^+ - x) + \partial g(x^+) &\ni - \nabla f(x)
            \\
            L(x - x^+) &\in \nabla f(x) + \partial g(x^+)
            \\
            \mathcal G_L(x) &\in \nabla f(x) + \partial g(x^+). 
        \end{align*}
    \end{proof}

    \begin{lemma}[Linearized Gradient Mapping Lower Bound]
    \label{lemma:grad_map_linearization}
        Continue from 
        \hyperref[lemma:grad_map_lemma_first]{definition \ref*{lemma:grad_map_lemma_first}}, 
        with $x^+ = \mathcal T_L(x)$, the gradient mapping satisfies the inequality for all $z$: 
        \begin{align*}
            h(z) &\ge
            h(z) - \frac{L}{2}\Vert x - x^+\Vert^2
            \ge h(x^+) + \langle \mathcal G_L(x), z - x\rangle. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Directly from the $L$-smoothness of $f$, convexity of $g, f$, we have the list of inequalities: 
        \begin{align*}
            &f(x^+) \le 
            f(x) + \langle \nabla f(x), x^+ - x\rangle
            + \frac{L}{2}\Vert x - x^+\Vert^2, 
            \\
            &f(x) + \langle \nabla f(x), z - x\rangle 
            \le f(z), 
            \\
            &g(x^+) \le 
            g(z) + \langle \partial g(x^+), x^+ - z\rangle. 
        \end{align*}
        Now, consider adding $g(x^+)$ to the first inequality from above we get 
        {\footnotesize 
        \begin{align*}
            f(x^+) + g(x^+) 
            &\le 
            f(x) + g(x^+) + \langle \nabla f(x), x^+ - x\rangle 
            + \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &\le 
            (f(z) - \langle \nabla f(x), z - x\rangle) + 
            \left(g(z) - \langle \partial g(x^+), x^+ - z\rangle\right)
            + 
            \langle \nabla f(x), x^+ - x\rangle + \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &= f(z) + g(z) + \langle \nabla f(x), x - z + x^+ - x\rangle
            + 
            \langle \partial g(x^+), x^+ - z\rangle + \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &= 
            h(z) + \langle \nabla f(x), x^+ - z\rangle + 
            \langle \partial g(x^+), x^+ - z\rangle
            + \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &= h(z) + \langle \nabla f(x) + \partial g(x^+), x^+ - z\rangle 
            + \frac{L}{2}\Vert x - x^+\Vert^2. 
        \end{align*}
        }
        By $\mathcal G_L(x) = L(x - x^+) \in \nabla f(x) + \partial g(x^+)$ from previous dicussion, we have 
        \begin{align*}
            h(x^+) &\le 
            h(z) + \langle \mathcal G_L(x), x^+ - z\rangle + \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &= h(z) - \langle L(x^+ - x), x^+ - x + x - z \rangle 
            + 
            \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &= 
            h(z) - L\Vert x^+ - x\Vert^2 
            + L \langle x^+ - x, x - z\rangle
            + \frac{L}{2}\Vert x - x^+\Vert^2
            \\
            &= h(z) + \langle \mathcal G_L(x), x - z\rangle - 
            \frac{L}{2}\Vert x - x^+\Vert^2. 
        \end{align*}
        Therefore, the inequality is justified. 
    \end{proof}
    \begin{remark}
        Observe that the linearization $h(x^+) + \langle \mathcal G_L(x), z - x\rangle$ is anchored at $x^+$, instead of $x$. 
        Geometrically, it's tilted and it ``prefers" the sharp corners of a convex function, if, $x$ is close to a sharp corner. 
    \end{remark}

\subsection{Varieties of Nesterov accelerated gradient}\label{sec:AG_varieties}
    Here, the acrynonym: ``AG" stands for accelerated gradient. 
    \subsubsection{AG Abstract Forms}
        In this section, we list different varieties of the Nesterov accelerated method. 
        \begin{definition}[Nestrov 2.2.7]\label{def:Nes2.2.7}
            Let $f$ be a $L$ Lipschitz smooth and $\mu\ge 0$ strongly convex function. 
            Choose $x_0$, $\gamma_0 > 0$, set $v_0 = x_0$, for iteration $k\ge 0$, it
            \begin{enumerate}
                \item[1.] computes $\alpha_k \in (0, 1)$ by solving $L\alpha_k^2 = (1 - \alpha_k)\gamma_k + \alpha_k \mu$; 
                \item[2.] sets $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$;
                \item[3.] chooses $y_k = (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k)$. Compute $f(y_k)$ and $\nabla f(y_k)$; 
                \item[4.] finds $x_{k + 1}$ such that $f(x_{k + 1}) \le f(y_k) - (2L)^{-1} \Vert \nabla f(y_k)\Vert^2$; 
                \item[5.] sets $v_{k + 1} = \gamma_{k+1}^{-1}((1 - \alpha_k)\gamma_kv_k + \alpha_k \mu y_k - \alpha_k \nabla f(y_k))$. 
            \end{enumerate}
        \end{definition}
        \begin{remark}
            This is in Nesterov's book \cite[(2.2.7)]{nesterov_lectures_2018}. 
            For more context, the sequence $\alpha_k \in (0, 1)$ such that $\sum_{k = 1}^{\infty}\alpha_k = \infty$, and recursively we have $\lambda_{k + 1} = (1 - \alpha_k)\lambda_k$. 
            the sequence $\lambda_k$ is called an Nesterov estimating sequence. 
            It is the most generic algorithm in his book about accelerated gradient method. 
            The genericity of the algorithm is provided by item 4., which is the a special case of the smooth descent lemma. 

        \end{remark}

        \begin{definition}[Ahn Sra 6.24]\label{def:agg_ppm}
            With $f$ $\mu \ge 0$ strongly convex and $L$-Lipschitz smooth, the generic PPM form is formulated for strictly positive stepsizes $\tilde \eta_i,\eta_i$: 
            \begin{align*}
                x_{t + 1} &= \argmin_{x} \left\lbrace
                    l_f(x; y_t) 
                    + 
                    \frac{\mu}{2}\Vert x - y_t\Vert^2
                    + 
                    \frac{1}{2\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2
                \right\rbrace, 
                \\
                y_{t + 1} &= \argmin_{x} 
                \left\lbrace
                    l_f(x; y_t) + \frac{L}{2}\Vert x - y_t\Vert^2 + 
                    \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
                \right\rbrace. 
            \end{align*}
        \end{definition}
        \begin{remark}
            This algorithm is the same as algorithm (6.24) described by Ahn and Sra \cite{ahn_understanding_2022}. 
            Observe that by setting $\mu = 0, \tilde \eta_{t + 1} = \eta_{t + 1}$ this recovers algorithm (4.8) described in Ahn and Sra \cite{ahn_understanding_2022}. 
            Finally, Ahn, Sra 6.24 has the same form as Nesterov 2.2.7. 
            See 
            \hyperref[prop:Nes2.2.7_via_ahn_sra_6.24]{proposition \ref*{prop:Nes2.2.7_via_ahn_sra_6.24}} for a demonstration. 
        \end{remark}

        \begin{definition}[AG Proximal Gradient PPM Generic Form]
            \label{def:ag_prox_grad_ppm}
            Let $h=f + g$ be the sum of convex function $g$ and convex differentiable $f$ with $L$-Lipschitz gradient. 
            With $\mathcal T_L, \mathcal G_L$ being the proximal gradient, and the gradient mapping operator. 
            Define the linear lower bounding function for $h$ at $y$, for all $x$: 
            $$
            \begin{aligned}
                l_h(x; y) &= h(\mathcal T_L y) + \langle \mathcal G_L(y), x - y \rangle \le h(x), 
            \end{aligned}
            $$
            with that we define the algorithm:
            $$
            \begin{aligned}
                x_{t + 1} &= \argmin_{x} \left\lbrace
                    l_h(x; y_t) + \frac{1}{2\tilde \eta_{t + 1}} 
                    \Vert x - x_t\Vert^2
                \right\rbrace,
                \\
                y_{t + 1}&= 
                \argmin_{x}
                \left\lbrace
                    l_h(x; y_t) + \frac{L}{2}\Vert x - y_t\Vert^2 + 
                    \frac{1}{2\eta_{t + 1}} \Vert x - x_{t + 1}\Vert^2
                \right\rbrace.
            \end{aligned}
            $$
        \end{definition}
        \begin{observation}
            With $g \equiv 0$, the above definition reduces to 
            \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}}
            where $\mu = 0$. 
        \end{observation}

        \begin{definition}[AG Proximal Gradient Generic Form]
        \label{def:ag_prox_grad_generic}
            With $h = f + g$, where $g$ is convex, $f$ is convex and $L$-Lipschitz smooth. 
            Define proximal gradient and gradient mapping operator 
            $$
            \begin{aligned}
                \mathcal T_L(x) 
                &:= \hprox_{L^{-1}g}(x - L^{-1}\nabla f(x)), 
                \\
                x^+ &= \mathcal T_L(x), 
                \\
                \mathcal G_L(x) 
                &:= L(x -  x^+). 
            \end{aligned}
            $$
            Then the algorithm updates $(y_t, x_{t + 1}, z_{t + 1})$ with expression: 
            $$
            \begin{aligned}
                y_t^+ &= \mathcal T_L(y_t)
                \\
                y_t &= (1 + L\eta_t)^{-1}(x_t + L\eta_t z_t)
                \\
                x_{t + 1} &= x_t - \tilde \eta \mathcal G_L(y_t)
                \\
                z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t)
            \end{aligned}
            $$
            for all $t\in \mathbb N$ where the base case has $y_0 = x_0$. 
        \end{definition}
        \begin{remark}
            Observe that $z_{t + 1} = y_t^+$. 
        \end{remark}

        \begin{definition}[Generic Form]\label{def:agg_tri}
            With $f$ be $L$-Lipschitz \\ 
            smooth and convex, choose any $y_0 = x_0=z_0$ and a non-negative sequence $\eta_t, \tilde\eta_t$, the algorithm admits form: 
            \begin{align*}
                x_{t + 1} &= x_t - \tilde \eta_{t + 1} \nabla f(y_t) 
                \\
                z_{t + 1} &= y_t - L^{-1} \nabla f(y_t) 
                \\
                y_{t + 1} &= 
                (1 + L\eta_{t + 1})^{-1}
                (
                x_{t + 1} + L\eta_{t + 1}z_{t + 1}
                ). 
            \end{align*}
        \end{definition}
        \begin{remark}
            The parameter $\eta_t, \tilde\eta_{t + 1}$ are exactly the same as Ahn, Sra \cite[(6.24)]{ahn_understanding_2022}, but with the choice of $\mu = 0$. 
            This form is not explictly stated by Ahn and Sra but it's trivial to realize its presence regardless. 
            They are the same parameters for both algorithms and they are equivalent. 
            For a demonstration, see 
            \hyperref[prop:tri_form_via_ppm]{proposition \ref*{prop:tri_form_via_ppm}}. 
        \end{remark}

        \begin{definition}[Strongly Convex Generic Form]
            \quad \\
            With $f$ being $L$-Lipschitz smooth and $\mu > 0$ strongly convex then the Strongly convex generic form has updates of for iterates
            \begin{align*}
                x_{t + 1} &= (1 + \mu\tilde \eta_{t + 1})^{-1}(x_t + \mu\tilde\eta_{t + 1}(y_t - \mu^{-1}\nabla f(y_t))), 
                \\
                z_{t + 1} &= y_t - L^{-1}\nabla f(y_t), 
                \\
                y_{t + 1} &= (1 + L\eta_{t + 1})^{-1}(x_{t + 1} + L\eta_{t + 1}z_{t +1}). 
            \end{align*}
        \end{definition}
        \begin{remark}
            This algorithm is equivalent to 
            \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}}. 
            See \ref*{prop:tri_scvx_from_ahn_sra_6.24} for a demonstration. 
        \end{remark}

    \subsubsection{AG Concrete Examples}
        In this section we state variants of the Nesterov accelerated gradient method found in the literatures which are not generic. 
        
        \begin{definition}[AG Ryu 12.1]
        \label{def:ag_ryu_12.1}
            State in chapter 12 in Ryu's writing \cite{ryu_large-scale_2022} is the following variant of accelerated gradient algorithm: 
            \begin{align*}
                z_{t + 1} &= y_k + L^{-1}\nabla f(y_t), 
                \\
                y_{t + 1} &= z_{t + 1} + \frac{t - 1}{t + 2}\left(
                    z_{k + 1} - z_k
                \right). 
            \end{align*}
            An equivalent form of the above is also provided by Ryu: 
            \begin{align*}
                z_{t + 1} &= y_t + L^{-1}\nabla f(y_t), 
                \\
                x_{t + 1} &= x_t + (t + 1)(2L)^{-1}\nabla f(y_t), 
                \\
                y_{t + 1} &= \left(
                    1 - \frac{2}{t + 2} 
                \right)z_{t + 1} + 
                \left(
                    \frac{2}{t + 2}
                \right)x_{t + 1}. 
            \end{align*}
        \end{definition}
        \begin{observation}
            The base case requires cares since it's not unique, there are more representations  depending on how the base case is define, 
            Observe that the second equivalent form is an example of \hyperref[def:agg_tri]{definition \ref*{def:agg_tri}}. 
            \par\noindent
            It is stated in Ryu's writing that the base case is $x_0 = y_0 = z_0$, 
            we think this is not the only options. 
            We remind the reader that there is not an obvious choice of $t\in \mathbb Z$ such that $x_t = y_t = z_t$ where the above algorithm remains consistent for all $t \ge -1$. 
            The choice of base case presented in Ryu's writing is a choice and not a necessity. 
            We state that one of the base case scenario that only requires knowing $y_1$. 

            \par\noindent 
            We need to observe the case where $t = 0$ to understand the base case of the algorithm. 
            At each iteration $t$, Given $(x_t, y_t)$, the above formula maps to $(x_{t + 1}, y_{t + 1}, z_{t + 1})$. 
            When $t= 0$, it has $y_1 = x_1$. 
            Therefore, knowing only either $y_1$, or $x_1$ can initiate the algorithm. 
            This is an alternative scenario of the base case. 
            If, we wish to write $y_0 = x_0$ at $t = 0$ for the base case instead, then it creates the following algorithm 
            \begin{align*}
                y_{t} &= \left(
                1 - \frac{2}{t + 2} 
                \right)z_{t} + 
                \left(
                    \frac{2}{t + 2}
                \right)x_{t}, 
                \\
                z_{t + 1} &= y_t + L^{-1}\nabla f(y_t), 
                \\
                x_{t + 1} &= x_t + (t + 1)(2L)^{-1}\nabla f(y_t). 
            \end{align*}
            It has a different representations due to a specific choice of the base case. 
            \par\noindent
            Finally, by the observation that given $(z_t, y_t)$ for any $t\ge -1$, we can solve for $x_t$ \newline via $y_t = (1 - 2/(1 + t))z_t + 1/(t + 1)x_t$, obtaining $(x_t, y_t, z_t)$, hence the algorithm can be reduced to a representations with only $(z_t, y_t)$ and an updates using only $(z_t, y_t)$ to $(z_{t + 1}, y_{t + 1})$. 
            That is the momentum form presented above. 
        \end{observation}
        \begin{remark}
            We advise the reader to read the Biliography notes by Ryu \cite[chapter 12]{ryu_large-scale_2022} of his book. 
            It goes over the full context and history for this particular variant of gradient acceleration method. 
            At the time of composing this notes, the writer is still reading on the topic of accelerated gradient.     
        \end{remark}

        \begin{definition}[Proximal Gradient variant of Ryu 12.1]
        \label{def:ag_tri_pt_form_E}
            The method appeared in Ryu's text is a special case of 
            \hyperref[def:ag_prox_grad_generic]{definition \ref*{def:ag_prox_grad_generic}}
            with the condition that $\tilde \eta_t = \eta_t$ for all $t \in \mathbb N$ and $g \equiv 0$ for the non-smooth part of the objective. 
            As a consequence, the algorithm has the following updates for iterates $(y_t, x_{t + 1}, z_{t + 1})$: 
            \begin{align*}
                y_t &= (1 + L\eta_t)^{-1}(x_t + L\eta_t z_t)
                \\
                x_{t + 1} &= x_t - \eta_t \mathcal G_L(y_t)
                \\
                z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t).
            \end{align*}
        \end{definition}

        \begin{definition}[Similar Triangle Form I]
        \label{def:ag_form_similar_tria_I}
            The similar triangle form is equivalent to the classic nesterov accelerated gradient proposed back in 1983. 
            It updates iterates $(y_t, x_{t + 1}, z_{t + 1})$ using 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1} \mathcal G_L(y_t) 
                \\
                x_{t + 1} &= z_{t + 1} + L\eta_t (z_{t + 1} - z_t)
                \\
                y_{t + 1} &= 
                (1 + L\eta_{t + 1})^{-1}
                (x_{t + 1} + L\eta_{t + 1}z_{t + 1}). 
            \end{align*}
        \end{definition}
        \begin{observation}
            There are a lot of observations about this form to unpack. 
            Let's list the following observations
            \begin{enumerate}
                \item Observe that $y_t = (1 + L\eta_t)^{-1}(x_t + L\eta_t z_t)$ is a convex combination between $x_t, z_t$. Hence $z_t, y_t, x_t$ are three collinear points. Additionally, we have $y_t - x_t = L\eta_t (z_t - y_t)$, hence $\Vert y_t - x_t\Vert/\Vert z_t - y_t\Vert = L\eta_t$. 
                \item By the algorithm, we have $z_{t + 1} - y_t = - L^{-1} \mathcal G_L(y_t)$, and $x_{t + 1} - x_t = - \tilde \eta_{t + 1} \mathcal G_L(y_t)$, hence vector $z_{t + 1} - y_t$ parallels to $x_{t + 1} - x_t$. 
                \item Finally, with update $x_{t +1} - z_{t + 1} = L\eta_t (z_{t +1} - z_t)$, we have three colinear points $(z_t, z_{t + 1}, x_t)$ with $\Vert z_{t + 1} - z_t\Vert/\Vert z_{t +1 } - z_t\Vert = L\eta_t$. 
            \end{enumerate}
            From the above results, we can conclude that triangle $(y_t, z_t, z_{t + 1})$ is similar to $(x_t, z_t, x_{t + 1})$ because they share colinear points $z_t, y_t, x_t$ and $z_t, z_{t + 1}, x_{t + 1}$, and their sides $z_{t + 1} - y_t$ parallels to $x_{t + 1} - x_t$. 
        \end{observation}
        \begin{remark}
            The above algorithm is a special case of 
            \hyperref[def:ag_prox_grad_generic]{definition \ref*{def:ag_prox_grad_generic}}, 
            see 
            \hyperref[thm:ag_generic_stepsize_constrants]{section \ref*{thm:ag_generic_stepsize_constrants}} 
            for more information. 
        \end{remark}

        
        \begin{definition}[Similar Triangle Form II]
        \label{def:ag_form_similar_tria_II}
            
        \end{definition}


    \subsubsection{Useful Observations about these Generic forms}
    \label{sec:AG_useful_information}

        In this section we list some incredibaly useful information about these forms of the Nestrov type accelerated gradient algorithm. 
        \hyperref[prop:tri_form_via_ppm]{Proposition \ref*{prop:tri_form_via_ppm}}, 
        \hyperref[prop:tri_scvx_from_ahn_sra_6.24]{proposition \ref*{prop:tri_scvx_from_ahn_sra_6.24}}, 
        \hyperref[prop:Nes2.2.7_via_ahn_sra_6.24]{proposition \ref*{prop:Nes2.2.7_via_ahn_sra_6.24}}
        are results assumed as true in Ahn, Sra. 
        They are implicitly left as necessary exercises for the readers. 
        Here, we fill out the blanks with our own interpretations. 
        \hyperref[prop:derive_ag_prox_grad_tript]{Proposition \ref*{prop:derive_ag_prox_grad_tript}} 
        is our contribution where we intorduce non-smooth ness to the objective function while still retaining the PPM interpretation of accelerated gradient. 
        
        \begin{proposition}[Tri-points generic form via Ahn Sra 6.24]
            \label{prop:tri_form_via_ppm}
            with $f$ being \\
            $L$-Lipschitz smooth, 
            let $l_f(x, \bar x) = f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle$, we can show that the AG generic triangular form is a consequence of th AG generic PPM form. 
        \end{proposition}
        \begin{proof}
            Solving the optimality on the first PPM yields: 
            \begin{align*}
                \mathbf 0 &= \nabla f(y_t) + 
                \frac{1}{\tilde \eta_{t + 1}} (x - x_t)
                \\
                x &= x_t - \tilde \eta_{t + 1} \nabla f(y_t).
            \end{align*}
            Therefore, $x_{t + 1} = x_t - \tilde \eta_{t + 1}\nabla f(y_y)$. 
            Similarly, for the updates of $y_{t + 1}$, we have optimality condition of 
            \begin{align*}
                \mathbf 0 &= \nabla f (y_t) + L (x - y_t) + \eta_{t + 1}^{-1} (x - x_{t + 1})
                \\
                \mathbf 0 &= \eta_{t + 1}\nabla f (y_t) + \eta_{t + 1}L (x - y_t) + x - x_{t + 1}
                \\
                \mathbf 0 &= 
                \eta_{t + 1}\nabla f(y_t) -\eta_{t + 1} Ly_t + (\eta_{t + 1}L + 1)x - x_{t + 1}
                \\
                (1 + \eta_{t + 1}L)x
                &= 
                x_{t + 1} - \eta_{t + 1}\nabla f(y_t) + \eta_{t + 1}L y_t
                \\
                \text{define: } y_{t + 1} &:= x. 
            \end{align*}
            In the above expression, it hides a step of gradient descent, continuing it we have 
            \begin{align*}
                (1 + \eta_{t + 1}L)y_{t + 1} &= 
                x_{t + 1}  + \eta_{t + 1}L (-L^{-1}\nabla f(y_t) + y_t)
                \\
                \text{let: } z_{t + 1} &= y_t - L^{-1}\nabla f(y_t), \text{ so, }
                \\
                (1 + \eta_{t + 1}L)y_{t + 1} &= 
                x_{t + 1} + L\eta_{t + 1}z_{t + 1}. 
            \end{align*}
            Combining it yields the tree points update format 
            \begin{align*}
                x_{t + 1} &= x_t - \tilde \eta_{t + 1} \nabla f(y_t) 
                \\
                z_{t + 1} &= y_t - L^{-1} \nabla f(y_t) 
                \\
                y_{t + 1} &= 
                (1 + L\eta_{t + 1})^{-1}
                (
                x_{t + 1} + L\eta_{t + 1}z_{t + 1}
                ), 
            \end{align*}
            the ordering of $x_{t +1}, z_{t + 1}$ can be permuted. 
            The base case is when $t = 0$, so then $x_0 = y_0$ for the initial guess.
        \end{proof}
        \begin{remark}
            It is quite obvious to us that the choice of $z_{t + 1} = y_t - L^{-1}\nabla f(y_t)$ is deliberate. 
            In fact, it's true that $z_{t + 1}$ is just a term such that it makes $y_{t +1}$ a convex combination between the vector $x_{t + 1}, z_{t + 1}$, and the choice here would be unique because of the consideration that 
            {\footnotesize
            \begin{align*}
                & \quad 
                l_f(x; y_t) + \frac{L}{2}\Vert x - y_t\Vert^2 
                \\
                &= 
                f(y_t) + \langle \nabla f(y_t), x - y_t\rangle 
                + 
                \frac{L}{2}
                \left\Vert 
                    x - (y_t - L^{-1}\nabla f(y_t)) - L^{-1}\nabla f(y_t)
                \right\Vert^2
                \\
                &= 
                f(y_t) + 
                \langle \nabla f(y_t), x - (y_t - L^{-1}\nabla f(y_t)) - L^{-1}\nabla f(y_t)\rangle 
                + 
                \frac{L}{2}
                \left\Vert 
                    x - (y_t - L^{-1}\nabla f(y_t)) - L^{-1}\nabla f(y_t)
                \right\Vert^2
                \\
                &\quad  \text{Let }z_{t + 1} 
                = y_t - L^{-1}\nabla f(y_t)
                \\
                &= f(y_t) + \langle \nabla f(y_t), x - z_{t + 1} - L^{-1}\nabla f(y_t)\rangle
                + 
                \frac{L}{2}\left\Vert
                    x - z_{t + 1} - L^{-1}\nabla f(y_t)
                \right\Vert^2
                \\
                &= 
                f(y_t) + \langle \nabla f(y_t), x - z_{t + 1}\rangle 
                - L^{-1}\Vert \nabla f(y_t)\Vert^2
                + 
                \frac{L}{2}\Vert x - z_{t + 1}\Vert^2 + 
                \frac{1}{2L}\Vert \nabla f(y_t)\Vert^2 - 
                L\langle L^{-1}\nabla f(y_t), x - z_{t + 1}\rangle
                \\
                &= f(y_t) + (1/(2L)- L^{-1})\Vert \nabla f(y_t) \Vert^2 + 
                \frac{L}{2}\Vert x - z_{t + 1}\Vert^2
                \\
                &= f(y_t) - \frac{1}{2L}\Vert \nabla f(y_t)\Vert^2 + \frac{L}{2}\Vert x - z_{t + 1}\Vert^2. 
            \end{align*}
            }
            Therefore 
            \begin{align*}
                y_{t + 1} &= \argmin_{x}\left\lbrace
                    \frac{L}{2}\Vert x - z_{t + 1}\Vert^2 + 
                    \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
                \right\rbrace
                \\
                &= \argmin_{x}\left\lbrace
                    \left\Vert x - 
                        \frac{Lz_{t + 1} + \eta_{t + 1}^{-1}x_{t + 1}}{
                            L + \eta_{t + 1}^{-1}
                        }
                    \right\Vert^2
                \right\rbrace
                \\
                &= 
                \frac{
                    L\eta_{t + 1}z_{t + 1} + 
                    x_{t + 1}
                }{
                    1 + L \eta_{t + 1}
                }. 
            \end{align*}
        \end{remark}

        \begin{proposition}[Tri-points Strongly Convex Form via Ahn Sra 6.24]\label{prop:tri_scvx_from_ahn_sra_6.24}
            Coninue \\ 
            from \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}} with but with $\mu > 0$, under the same set of assumptions it recovers a tri-points algorithm with the following updates: 
            \begin{align*}
                y_t^+ &= y_t - \mu^{-1}\nabla f(y_t)
                \\
                x_{t + 1} &= \argmin_{x} 
                \left\lbrace
                    l_f (x; y_t) + \frac{\mu}{2} \Vert x - y_t\Vert^2 + 
                    \frac{1}{2\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2 
                \right\rbrace
                \\
                &= \argmin_{x}
                \left\lbrace
                    f(y_t) - \frac{1}{2\mu} \Vert \nabla f(y_t)\Vert^2 
                    + 
                    \frac{\mu
                    }{2}\Vert x - y_t^+\Vert^2
                    + 
                    \frac{1}{2\tilde \eta_{t + 1}}\Vert x - x_t\Vert^2
                \right\rbrace
                \\
                &= (1 + \mu\tilde \eta_{t + 1})^{-1}
                \left(
                    x_t + \mu \tilde \eta_{t + 1}y_t^{+}
                \right)
                \\
                z_{t + 1} &= y_t - L^{-1} \nabla f(y_t)
                \\
                y_{t + 1} &= (1 + L\eta_{t + 1})^{-1}(x_{t + 1} + L \eta_{t + 1} z_{t + 1})
            \end{align*}
        \end{proposition}
        \begin{proof}
            By a similar argument made in the remarks of \hyperref[prop:tri_form_via_ppm]{proposition \ref*{prop:tri_form_via_ppm}} we have 
            \begin{align*}
                l_f(x; y_t) + \frac{\mu}{2}\Vert x - y_t\Vert^2
                &= 
                f(y_t) - \frac{1}{2\mu}\Vert \nabla f(y_t)\Vert^2 + \frac{\mu}{2}\Vert x - y_t^+\Vert^2
                \\
                \implies 
                x_{t + 1} &= 
                \argmin_{x} 
                \left\lbrace
                    f(y_t) - \frac{1}{2\mu}\Vert \nabla f(y_t)\Vert^2 + \frac{\mu}{2}\Vert x - y_t^+\Vert^2
                    + 
                    \frac{1}{2\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2
                \right\rbrace
                \\
                &= 
                \argmin_{x}
                \left\lbrace
                    \frac{\mu}{2}\Vert x - y_t^+\Vert^2 + 
                    \frac{2}{\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2
                \right\rbrace
                \\
                &= (\mu + \tilde \eta_{t + 1})^{-1}
                (\mu y_t^+ + \tilde \eta_{t + 1}x_t)
                \\
                &= (1 + \tilde \eta_{t + 1}\mu)^{-1}
                (x_t + \mu\tilde \eta_{t + 1}y_t^+). 
            \end{align*}
            The updates for $y_{t + 1}, z_{t + 1}$ is the same as before because $\mu > 0$ only alters the update of the term $x_{t + 1}$ in \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}}. 
        \end{proof}
        
        \begin{proposition}[Nestrov 2.2.7 is Equivlant to Ahn Sra 6.24 in Form]
        \label{prop:Nes2.2.7_via_ahn_sra_6.24}
            The \hyperref[def:Nes2.2.7]{algorithm \ref*{def:Nes2.2.7}} is equivalent to \hyperref[def:agg_ppm]{\ref*{def:agg_ppm}} in form. 
            So the updates for $y_{k + 1}, x_{k + 1}, v_{k + 1}$: 
            \begin{align*}
                \text{find } &
                \alpha_k \in (0, 1) 
                \text{ s.t: } L\alpha_k^2 
                = (1 - \alpha_k)\gamma_k + \alpha_k \mu = \gamma_{k + 1} 
                \\
                y_k &= 
                \left(
                    \gamma_k + \alpha_k \mu
                \right)^{-1} \left(
                    \alpha_k \gamma_k v_k + \gamma_{k + 1}x_k
                \right)
                \\
                \text{find } & x_{k + 1} \text{ s.t: }
                f(x_{k + 1})
                = f(y_k) - (2L)^{-1}\Vert \nabla f(y_k)\Vert^2
                \\
                v_{k+1} &= 
                \gamma_{k + 1}^{-1} 
                \left(
                    (1 - \alpha_k) \gamma_k v_k + 
                    \alpha_k \mu y_k 
                    - \alpha_k \nabla f(y_k)
                \right). 
            \end{align*}
            Is the same as $y_{t + 1}, z_{t + 1}, x_{t + 1}$ from below: 
            \begin{align*}
                x_{t + 1} 
                & = 
                (1 + \tilde \eta_{t + 1}\mu)^{-1}
                (x_t + \mu\tilde \eta_{t + 1}y_t - \tilde \eta_{t + 1}\nabla f(y_t))
                \\
                z_{t + 1} &= y_t - L^{-1} \nabla f(y_t)
                \\
                y_{t + 1} &= 
                (1 + L\eta_{t + 1})^{-1}
                (x_{t + 1} + L \eta_{t + 1} z_{t + 1}).
            \end{align*}
            which is an equivalent representation of \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}} as showned in \hyperref[prop:tri_scvx_from_ahn_sra_6.24]{propostion \ref*{prop:tri_scvx_from_ahn_sra_6.24}}. 
        \end{proposition}
        \begin{proof}
            We simplify the Nesterov form into the Strongly convex Generic Triangular Form. 
            Consider update for $v_{k + 1}$ by substituting $\gamma_{k+1} = (1 - \alpha_k) \gamma_k + \alpha_k \mu$ as informed by the first step of the algorithm, we have 
            \begin{align*}
                v_{k + 1} &= 
                ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
                \left(
                    (1 - \alpha_k)\gamma_k v_k + \alpha_k \mu y_k - \alpha_k \nabla f(y_k)
                \right)
                \\
                &= ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
                (
                    (1 - \alpha_k)\gamma_k v_k + \alpha_k \mu(y_k - \alpha_k \mu^{-1}\nabla f(y_k))
                )
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k \mu}{(1 - \alpha_k)\gamma_k}
                \right)^{-1}
                \left(
                    v_k
                    + 
                    \left(
                        \frac{\alpha_k \mu}{(1 - \alpha_k)\gamma_k} 
                    \right)
                    \left(
                        y_k 
                        - \alpha_k \mu^{-1}\nabla f(y_k)
                    \right)
                \right). 
            \end{align*}
            Notice that the right hand size has the same form as $x_{t + 1}$. 
            This is true by the observation that 
            \begin{align*}
                x_{t + 1} &= 
                (1 + \tilde\eta_{t + 1}\mu)^{-1}
                \left( 
                    x_t + \mu\tilde \eta_{t + 1}
                    \left(y_t - \mu^{-1}\nabla f(y_t)\right)
                \right). 
            \end{align*}
            Similarly, when $\mu = 0$, we have from the first step that 
            \begin{align*}
                v_{k + 1} 
                &= ((1 - \alpha_k)\gamma_k)^{-1}
                (
                    (1 - \alpha_k)\gamma_k v_k
                    + \alpha_k \mu y_k - \alpha_k \nabla f(y_k)
                )
                \\
                &= 
                v_k - \alpha_k((1 - \alpha_k)\gamma_k)^{-1}\nabla f(y_k)
            \end{align*}
            which is the same as the AG generic PPM form where 
            $$
                x_{t + 1} = x_t - \tilde \eta_{t + 1}\nabla f(y_t). 
            $$
            Next, we consider $y_{k}$ iterate of Nesterov 2.2.7. 
            We want to write it as a convex combination of the vector $v_k,x_k$. 
            To show that the updates $y_k$ can are of the same form, start by considering that 
            \begin{align*}
                \gamma_{k + 1 } &= (1 - \alpha_k)\gamma_k + \alpha_k \mu
                \\
                &= (\gamma_k + \alpha_k \mu) - \alpha_k \gamma_k, 
            \end{align*}
            with the grouping we have
            \begin{align*}
                y_k &= \left(
                    \gamma_k + \alpha_k \mu
                    \right)^{-1} \left(
                        \alpha_k \gamma_k v_k + \gamma_{k + 1}x_k
                    \right)
                \\
                &= 
                \left(
                    \gamma_k + \alpha_k \mu
                \right)^{-1}
                \left(
                    \alpha_k \gamma_k v_k 
                    + 
                    ((\gamma_k + \alpha_k\mu) - \alpha_k\gamma_k)x_k
                \right)
                \\
                &= 
                \left(
                    \frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu}
                \right)v_k
                + 
                \left(
                    1 - \frac{\alpha_k\gamma_k }{\gamma_k + \alpha_k \mu}
                \right)x_k, 
            \end{align*}

            which is indeed, a convex combination of $v_k, x_k$, if, we assume that $(\alpha_k\gamma_k)(\gamma_k + \alpha_k \mu)^{-1}$ is in the interval $(0, 1)$. 
            We can assume it by considering $\gamma_k = L\alpha_{k-1}^2$, simplifying so that 
            \begin{align*}
                \frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu}
                &= 
                \frac{L\alpha_k \alpha_{k - 1}^2}{L\alpha_{k - 1}^2 + \mu \alpha_k}
                \\
                &= \frac{\alpha_k \alpha_{k - 1}^2}{1 + q_f \alpha_k \alpha_{k - 1}^{-2}} \in (0, 1), 
            \end{align*}
            where $q_f = \mu / L \in (0, 1)$ and we recall the fact that the sequence $(\alpha_k)_{k \in \NN}$ has $\alpha_k \in (0, 1)$ and $\sum_{i = 1}^{\infty} \alpha_k = \infty$. 
            However, it would require more works to express $\eta_t, \tilde\eta$ from Ahn Sra 6.24 using $\gamma_k, \alpha_k$ from Nesterov 2.2.7. 
        \end{proof}

        \begin{proposition}[Deriving Proximal Gradient Generic Form]
        \label{prop:derive_ag_prox_grad_tript}
            \quad \\
            Continue from the proximal point interpretation of the proximal gradient 
            (\hyperref[def:ag_prox_grad_ppm]{definition \ref*{def:ag_prox_grad_ppm}})
            we have the equalities 
            \begin{align*}
                x_{t + 1} &= \argmin_{x}
                \left\lbrace
                    l_h(x, y_t) + \frac{1}{2\tilde \eta_{t + 1}} \Vert x - x_t\Vert^2
                \right\rbrace
                \\
                &= x_t - \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                \\
                y_{t + 1} &= \argmin_{x}
                \left\lbrace
                        h(y_t^+) + \langle \mathcal G_L(y_t), x - y_t\rangle + \frac{L}{2}\Vert x -y_t\Vert^2 + \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
                \right\rbrace
                \\
                &= (1 + L\eta_{t + 1})^{-1}
                (x_{t + 1} + L\eta_{t + 1}(y_t - L^{-1}\mathcal  G_L(y_t))). 
            \end{align*}
            Therefore, the algorithm 
            \hyperref[def:ag_prox_grad_ppm]{definition \ref*{def:ag_prox_grad_ppm}}
            is equivalent to 
            \hyperref[def:ag_prox_grad_generic]{definition \ref*{def:ag_prox_grad_generic}}
        \end{proposition}
        \begin{proof}
            Let $y_t^+ = \mathcal T_L(y_t)$, recall that $l_h(x; y_t) = h(y_t^+) + \langle \mathcal G_L(y_t), x -y_t\rangle \le f(x)$ by the definition of algorithm. 
            Since $l_h(x; y_t)$ is a simple linear function wrt $x$, it's not hard to minimize the quandratic where we can get $x_{t + 1} = x_t - \tilde\eta_{t + 1} \mathcal G_L(y_t)$. 
            For $y_{t + 1}$, observe that we can complete the square on the second and the third terms: 
            \begin{align*}
                & \frac{L}{2}\left(
                    2\langle L^{-1}\mathcal G_L(y_t), x - y_t\rangle + 
                    \Vert x - y_t\Vert^2
                \right)
                \\
                &= 
                \frac{L}{2}
                \left(
                    - \Vert L^{-1} \mathcal G_L(y_t)\Vert^2  
                    + \Vert L^{-1} \mathcal G_L(y_t)\Vert^2 
                    + 
                    2\langle L^{-1} \mathcal G_L(y_t), x - y_t\rangle + 
                    \Vert x - y_t\Vert^2
                \right)
                \\
                &= \frac{L}{2}\left(
                    - \Vert L^{-1}\mathcal G_L(y_t)\Vert^2  
                    + \Vert x - (y_t - L^{-1}\mathcal G_L(y_t))
                    \Vert^2
                \right), 
            \end{align*}
            therefore it transforms into 
            \begin{align*}
                y_{t + 1} &=\argmin_{x} \left\lbrace
                \frac{L}{2}\left\Vert 
                    x - (y_t - L^{-1}\mathcal G_L(y_t))
                \right\Vert^2
                + \frac{1}{2\eta_{t + 1}}\Vert x - x_{t + 1}\Vert^2
                \right\rbrace
                \\
                &=
                \frac{
                    \left(y_t - L^{-1}\mathcal G_L(y_t)\right) + x_{t + 1}
                }{L + \eta_{t + 1}^{-1}}.
            \end{align*}
            Define $z_{t + 1} = y_t - \mathcal G_L(y_t) = \mathcal T_L(y_t)$, which is the proximal gradient set, then the above expression simplifies to 
            $$
            y_{t + 1} = (1 + L\eta_{t +1})^{-1}(x_{t + 1}+ L\eta_{t + 1}z_{t + 1}). 
            $$
            As a sanity checks, notice that when 
            $h \equiv 0$, 
            $\mathcal G_L(x) = \nabla f(x)$ 
            by definition, making it the same as 
            \hyperref[def:agg_tri]{definition \ref*{def:agg_tri}}. 
        \end{proof}
        \begin{remark}
            $y_{t + 1}$ is the minimizer of a simple quadratic. 
            Given that the original function $h$ is potentially non-smooth, therefore it's not always an upper bound of $h(x)$. 
            The upper bound interpretation of the smooth case as proposed by Ahn, Sra for the update of $y_{t + 1}$ fails when $h$ is non-smooth! 
        \end{remark}

        \begin{lemma}[Deriving Similar Triangle Form I]
            \quad \\
            With the choice of stepszie $\tilde \eta_{t + 1} = \eta_t + L^{-1}$ 
            in 
            \hyperref[def:ag_prox_grad_generic]{definition \ref*{def:ag_prox_grad_generic}}
            We can derive similar triangle form I as stated back in
            \hyperref[def:ag_form_similar_tria_I]{definition \ref*{def:ag_form_similar_tria_I}}. 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1} \mathcal G_L(y_t)
                \\
                x_{t + 1} &= z_{t + 1} + L\eta_t (z_{t + 1} - z_t)
                \\
                y_{t + 1} &= 
                (1 + L\eta_{t + 1})^{-1}
                (
                x_{t + 1} + L\eta_{t + 1}z_{t + 1}
                ). 
            \end{align*}
            It also has an equivalent momentum form 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t)
                \\
                y_{t + 1} &= z_{t + 1} + (1 + L\eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            To do, we show that updates sequence $x_{t + 1} = z_{t + 1} + L\eta_t (z_{t + 1} - z_t)$ is equivalent to $x_{t + 1} = x_t + \tilde\eta_{t + 1}\nabla f(y_t)$. 
            Starting with the former, susbtitute definition of $z_{t + 1}$, $z_{t + 1} = z_t + L^{-1}\nabla f(y_t)$ expanding: 
            \begin{align*}
                x_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t) 
                + L \eta_t y_t - \eta_t \mathcal G_L(y_t) - L\eta_t z_t
                \\
                &= 
                (1 + L\eta_t)y_t - (\eta_t + L^{-1})\mathcal G_L(y_t) - L\eta_t z_t
                \\
                &= \eta_t Lz_t + x_t -(\eta_t + L^{-1}) \mathcal G_L(y_t)  - L\eta_t z_t
                \\
                &= x_t - (\eta_t + L^{-1})\mathcal G_L(y_t). 
            \end{align*}
            So $x_{t + 1} = x_t + \tilde \eta_{t + 1}\mathcal G_L(y_t)$, by assumption $\tilde \eta_{t + 1} = \eta_t + L^{-1}$
            Reducing it to the classic momentum form starts by 
            \begin{align*}
                y_{t + 1} &= (1 + L\eta_{t + 1})^{-1} (x_{t + 1} + L\eta_{t + 1}z_{t + 1})
                \\
                &= (1 + L\eta_{t + 1})^{-1} (
                    z_{t + 1} + L\eta_t (z_{t + 1} - z_t) + L\eta_{t + 1} z_{t + 1}
                )
                \\
                &= 
                (1 + L\eta_{t + 1})^{-1} (
                    (1 + L\eta_{t + 1})z_{t + 1} + L\eta_t(z_{t + 1} - z_t)
                )
                \\
                &= z_{t + 1} + (1 + L\eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t), 
            \end{align*}
            it negates the $x_t$ variables, therefore we have 
            \begin{align*}
                z_{t + 1} &= y_t - L^{-1} \mathcal G_L (y_t)
                \\
                y_{t + 1} &= z_{t + 1} + (1 + L\eta_{t + 1})^{-1}L\eta_t (z_{t + 1} - z_t).
            \end{align*}
        \end{proof}
        \begin{remark}
            In this remark we clarify the name ``similar triangle" as given in the literatures. 
            We think it is a fitting name, becaues it has a similar triangle in it. 
            We list the following observations
            \begin{enumerate}
                \item 
                The updates for $y_{t}$ from the algorithm has 
                $$
                    y_t = (1 + L\eta_t)^{-1} x_t + L\eta_t(1 + L\eta_t)^{-1} z_t, 
                $$
                therefore, $y_t$ is a convex combinations of $x_t, z_t$, so $z_t, y_t, x_t$ are three collinear points. 
                We have the ratio $\Vert y_t - x_t\Vert/\Vert z_t - y_t\Vert = L\eta_t$. 
                \item 
                The updates for $z_{t + 1}, x_{t + 1}$ are based on $y_t, x_t$ displaced by $L^{-1} \mathcal G_L(y_t), \tilde\eta_{t +1} \mathcal G_L(y_t)$, therefore vector $z_{t + 1} - y_t$ parallels to $x_{t + 1} - x_t$. 
                \item The updates for $x_{t + 1}$ has $x_{t + 1} - z_{t + 1} = L\eta_t \left(z_{t + 1} - z_t\right)$, therefore, the three points $z_t, x_{t + 1}, z_{t + 1}$ are collinear. 
                The ratio between line segment has $\Vert x_{t + 1} - z_{t + 1}\Vert/\Vert z_{t + 1} - z_t\Vert = L\eta_t$. 
            \end{enumerate}
            By these tree observations, the triangle $z_{t}, z_{t + 1}, y_t$ similar to triangle $z_t, x_{t + 1}, x_t$. 
            Finally, similar remarks about the similar triangle form can be found in Ahn, Sra's paper \cite{ahn_understanding_2022} as well. 
            
        \end{remark}
        

\subsection{Generic Lyapunov analysis for Accelerated gradient via PPM}
\label{sec:generic_ag_ppm_lyapunov_analysis}
    The lemma theorem states targets
    \hyperref[prop:derive_ag_prox_grad_tript]
    {definition \ref*{prop:derive_ag_prox_grad_tript}}. 

    \begin{lemma}[Nonsmooth Generic AG Lyapunov Analysis]
    \label{lemma:nsmooth_agg_lyapunov_upper_bound}
        Continue from 
        \hyperref[prop:derive_ag_prox_grad_tript]
        {definition \ref*{prop:derive_ag_prox_grad_tript}},
        we derive the non-smooth analogous case of the Lyapunov upper bound for all $t \in \mathbb N \cup \{0\}$: 
        \begin{align*}
            \Upsilon_{1, t + 1}^\text{AG}
            &= 
            \tilde\eta_{t + 1} (h(z_{t + 1}) - h(x_*)) + 
            \frac{1}{2} (
                \Vert x_{t + 1} - x_*\Vert^2
                - 
                \Vert x_t - x_*\Vert^2
            )
            \\
            &\le 
            - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 
            + \frac{\tilde\eta_{t + 1}L}{2}\Vert z_{t + 1} - y_t\Vert^2
            - \langle 
                \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                x_{t + 1} - z_{t + 1}
            \rangle
            \\
            \Upsilon_{2, t + 1}^\text{AG}
            &= 
            h(z_{t + 1}) - h(z_t) 
            \le 
            \langle \mathcal G_L(y_t), z_{t + 1} - z_t\rangle + 
            \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Define 
        \begin{align*}
            \phi_t(u) &:= 
            \tilde \eta_{t + 1} 
            \left(
                f(y_t) + g(y_t) + \langle \nabla f(y_t) + \partial g(y_t^+), u - y_t\rangle
                % + 
                % \frac{1}{2\tilde \eta_{t + 1}}\Vert u - x_t\Vert^2
            \right)\\
            &= 
            \tilde \eta_{t + 1} \left(
                f(y_t) + \langle \nabla f(y_t), u - y_t\rangle + 
                % \frac{1}{2\tilde \eta_{t + 1}}\Vert u - x_t\Vert^2
                % + 
                g(y_t) + \langle \partial g(y_t^+), u - y_t\rangle
            \right). 
        \end{align*}
        Then the update in the Generic Tri-Point algorithm 
        (\hyperref[def:ag_prox_grad_ppm]
            {definition \ref*{def:ag_prox_grad_ppm}}) 
        has $x_{t + 1} = \hprox_{\phi_t}(x_t)$ if we ignore the difference between the constant terms $h(y_t^+)$, $f(y_t) + g(y_t)$ from above. 
        Therefore we can use the Lyapunov inequality for PPM (\hyperref[thm:ppm_descent_ineq]
            {theorem \ref*{thm:ppm_descent_ineq}})
        which gives for all $x_*$: 
        \begin{align*}
            & \phi_t(x_{t + 1}) - \phi_t(x_*) + 
            \frac{1}{2}\Vert x_{t+1} - x_*\Vert^2 - 
            \frac{1}{2}\Vert x_t - x_*\Vert^2
            \le \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2, 
            \\
            & \text{where }
            \phi_t(x_{t + 1}) =
            \\
            & \quad 
            \tilde \eta_{t + 1}
            \left(
                f(y_t) + \langle \nabla f(y_t), x_{t + 1} - y_t\rangle + 
                % \frac{1}{2\tilde \eta_{t + 1}}\Vert x_{t + 1} - x_t\Vert^2
                % + 
                g(y_t) + \langle \partial g(y_t^+), x_{t + 1} - y_t\rangle
            \right). 
        \end{align*}
        to simplify, consider using the $L$-smoothness of $f$, we have the inequality: 
        \begin{align*}
            \begin{aligned}
                f(y_t) + \langle \nabla f(y_t), x_{t + 1} - y_t\rangle
                &=
                f(y_t) + \langle \nabla f(y_t), (x_{t +1} - z_{t + 1}) + (z_{t + 1} - y_t) \rangle
                \\
                &\ge 
                f(z_{t + 1}) - \frac{L}{2} \Vert z_{t + 1} - y_t\Vert^2 + 
                \langle \nabla f(y_t), x_{t +1} - z_{t + 1}\rangle
                \\
                &= 
                f(y_t^+) - \frac{L}{2}\Vert y_t^+ - y_t\Vert^2
                + \langle \nabla f(y_t), x_{t + 1} - y_t^+\rangle.     
            \end{aligned}
            \tag{$[*]$}
        \end{align*}
        further consider
        \begin{align*}
            \begin{aligned}
                g(y_t) + \langle \partial g(y_t^+), x_{t + 1} - y_t\rangle 
                &= g(y_t) + 
                \langle \partial g(y_t^+), 
                x_{t + 1} - y_t^+ + y_t^+ - y_t
                \rangle
                \\
                &= g(y_t) 
                + \langle \partial g (y_t^+),
                    x_{t + 1} - y_t^+
                \rangle
                + 
                \langle 
                    \partial g(y_t^+), y_t^+ - y_t
                \rangle
                \\
                g \text{ convex }\implies 
                &\ge 
                g(y_t^+) + 
                \langle \partial g(y_t^+), x_{t + 1} - y_t^+\rangle. 
            \end{aligned}
            \tag{$[\star]$}
        \end{align*}
        In the $([\star])$ derivation, we used the convexity of $g$ where 
        \begin{align*}
            \langle \partial g(y_t^+), y_t - y_t^+\rangle
            &\le g(y_t) - g(y_t^+)
            \\
            \langle \partial g (y_t^+), y_t^+ - y_t \rangle
            &\le 
            g(y_t^+) - g(y_t). 
        \end{align*}
        Now, adding $([*]), ([\star])$, multiply their sum by $\tilde\eta_{t + 1}$ makes it equals to $\phi_t(x_{t +1 })$ , we can establish a lower bound for $\phi_t(x_{t + 1})$, yielding inequality 
        {\small
            \begin{align*}
                \phi_t(x_{t + 1})
                &\ge 
                \tilde\eta_{t + 1}
                \left(
                    f(y_t^+) - \frac{L}{2}\Vert y_t^+ - y_t\Vert^2 
                    +
                    \langle \nabla f(y_t), x_{t + 1} - y_t^+\rangle
                    + 
                    g(y_t^+) + 
                    \langle \partial g(y_t^+), x_{t + 1} - y_t^+\rangle
                \right) 
                \\
                &= 
                \tilde\eta_{t + 1}
                \left(
                    h(y_t^+) - \frac{L}{2}\Vert y_t^+ - y_t\Vert^2 
                    + 
                    \langle \partial g(y_t^+) + \nabla f(y_t), x_{t + 1} - y_t^+\rangle
                \right).
            \end{align*}
        }
        Apply
        \hyperref[lemma:grad_map_lemma_first]{lemma \ref*{lemma:grad_map_lemma_first}} 
        we have $\partial g(y_t^+) + \nabla f(y_t) \ni \mathcal G_L(y_t)$, and by the algorithm updates $y_t^+ = z_{t + 1}$, therefore the above inequality simplifies to 
        \begin{align*}
            \phi_t(x_{t + 1}) &\ge 
            \tilde\eta_{t + 1} 
            \left(
                h(z_{t + 1}) - 
                \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
                + 
                \langle \mathcal G_L(y_t), x_{t +1} - z_{t + 1}\rangle
            \right). 
        \end{align*}
        Finally, from (\hyperref[lemma:grad_map_linearization]
            {lemma \ref*{lemma:grad_map_linearization}})
        we have $\phi_t(u) \le \tilde\eta_{t + 1}h(u)$ for all $u$. 
        With that we lower bound the LHS of the PPM Lyapunov inequality of $\phi_t(u)$, making: 
        {\footnotesize
        \begin{align*}
            & \phi_t(x_{t + 1}) - \phi_t(x_*) + 
            \frac{1}{2}\Vert x_{t+1} - x_*\Vert^2 - 
            \frac{1}{2}\Vert x_t - x_*\Vert^2
            \le
            \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 
            \\
            \implies &
            \tilde \eta_{t + 1}
            \left(
                h(z_{t + 1}) - 
                \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
                + 
                \langle \mathcal G_L(y_t), x_{t +1} - z_{t + 1}\rangle
            \right) - \tilde \eta_{t + 1} h(x_*)
            +
            \frac{1}{2}\Vert x_{t+1} - x_*\Vert^2 - 
            \frac{1}{2}\Vert x_t - x_*\Vert^2
            \\
            & \quad \le
            \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 
            \\
            \iff &
            \tilde\eta_{t + 1}(h(z_{t + 1}) - h(x_*))
            + \frac{1}{2}\left(
                \Vert x_{t + 1} - x_*\Vert^2 
                - 
                \Vert x_t - x_*\Vert^2
            \right) =: \Upsilon_{1, t + 1}^\text{AG}
            \\
            &\quad \le 
            \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 
            - 
            \tilde\eta_{t + 1}
            \langle \mathcal G_L(y_t), x_{t +1} - z_{t + 1}\rangle
            + 
            \frac{L \tilde\eta_{t + 1}}{2}\Vert z_{t + 1} - y_t\Vert^2. 
        \end{align*}
        }
        It established the first inequality that we wish to prove. 
        Next, the smoothness of $f$ establish inequality: 
        \begin{align*}
            \begin{aligned}
                f(z_{t + 1}) - f(z_t) &= f(z_{t + 1}) - f(y_t) + f(y_t) - f(z_t) 
                \\
                &\le 
                \langle \nabla f(y_t), z_{t + 1} - y_t\rangle + \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2 
                + 
                \langle \nabla f(y_t), y_t - z_t\rangle
                \\
                &= 
                \langle \nabla f(y_t), z_{t + 1} - z_t\rangle + \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2. 
            \end{aligned}
            \tag{$[**]$}
        \end{align*}
        The convexity of $h$ establish 
        \begin{align*}
            \begin{aligned}
                g(z_{t + 1}) + 
                \langle \partial g(z_{t + 1}), z_t - z_{t + 1}\rangle
                &\le g(z_t)
                \\
                g(z_{t + 1}) - g(z_t)
                &\le 
                \langle 
                    \partial g (z_{t +1}), 
                    z_{t + 1} - z_t
                \rangle. 
            \end{aligned}
            \tag{$[\star *]$}    
        \end{align*}
        Adding $([**]), ([\star *])$ yields 
        \begin{align*}
            \begin{aligned}
                \Upsilon_{2, t + 1}^{\text{AG}} := 
                h(z_{t + 1}) - h(z_t) 
                &\le 
                \langle 
                    \nabla f(y_t) + \partial g(z_{t+1}), 
                    z_{t + 1} - z_t
                \rangle + 
                \frac{L}{2}
                \Vert 
                    z_{t + 1} - y_t
                \Vert^2. 
            \end{aligned}
        \end{align*}
        Using the property of gradient mapping (\hyperref[lemma:grad_map_lemma_first]
            {lemma \ref*{lemma:grad_map_lemma_first}}), 
        $\mathcal G_L(y_t) \in \nabla f(y_t) + \partial g(z_{t + 1})$, substituting it, we proved all we want to show. 
    \end{proof}

    \begin{theorem}[Generic AG Convergence]
    \label{thm:generic_ag_convergence}
        Define the Lyapunov function $\Phi_t\; \forall t \in \{0\}\cup \N$: 
        \begin{align*}
            \Phi_t &= \left(
                \sum_{i = 1}^{t} \tilde\eta_{i}
            \right) (f(z_t) - f(x_*)) + \frac{1}{2}\Vert x_t - x_*\Vert^2 \quad \forall t \in \NN
            \\
            \Phi_0 &= \frac{1}{2}\Vert x_0 - x_*\Vert^2, 
        \end{align*}
        if there is a choice of $\eta_i, \tilde \eta_i$ in \hyperref[def:agg_ppm]{definition \ref*{def:agg_ppm}} such that we have 
        \begin{align*}
            & \Phi_{t + 1} - \Phi_{t}\left(
                \sum_{i = 1}^{t} \tilde \eta_i
            \right) =
            \Upsilon_{2, t + 1}^{\text{AG}} + 
            \Upsilon_{1, t + 1}^{\text{AG}} 
            \le \delta_{t + 1} \quad 
            \forall t \in \mathbb N, 
            \\
            & \Upsilon_{1, 1}^{\text{AG}} \le \delta_1. 
        \end{align*}
        Then with
        \begin{align*}
            S_t &:= \sum_{i = 1}^{t} \delta_i
            \\
            \sigma_t &:= \sum_{i = 1}^{t}\tilde \eta_i
        \end{align*}
        we have $\Phi_{t + 1} - \Phi_t \le \delta_{t + 1}$, it allows for a convergence rate of $\mathcal O \left(\sigma_T^{-1}S_T\right)$ of $f(z_T) - f(x_*)$ when $S_T \neq 0$ and a rate of $\mathcal O(\sigma_T^{-1})$ when $S_T = 0$. 
    \end{theorem}
    \begin{proof}
        By definition we have
        {\footnotesize
        \begin{align*}
            \Phi_{t + 1} - \Phi_t 
            &= 
            \left(
                \sum_{i = 1}^{t+1} \tilde\eta_{i}
            \right) (f(z_{t + 1}) - f(x_*)) 
            - 
            \left(
                \sum_{i = 1}^{t} \tilde\eta_{i}
            \right) (f(z_{t}) - f(x_*)) 
            + \frac{1}{2}\Vert x_t - x_*\Vert^2
            - \frac{1}{2}\Vert x_{t + 1} - x_*\Vert^2
            \\
            &= 
            \tilde \eta_{t + 1} (f(z_{t + 1}) - f(z_*))
            +
            \left(
                \sum_{i = 1}^{t} \tilde \eta_i
            \right)(f(z_{t + 1}) - f(z_t))
            + \frac{1}{2}\Vert x_t - x_*\Vert^2
            - \frac{1}{2}\Vert x_{t + 1} - x_*\Vert^2
            \\
            &= \left(
                \sum_{i = 1}^{t} \tilde \eta_i
            \right)\Upsilon_{2, t + 1}^{\text{AG}} + \Upsilon_{1, t + 1}^{\text{AG}} \le \delta_{t + 1}. 
        \end{align*}
        }
        Telescoping we have 
        \begin{align*}
            \Phi_{t + 1} - \Phi_t &\le \delta_{t + 1}
            \\
            \sum_{i = 0}^{T - 1}\Phi_{i + 1} - \Phi_i &\le \sum_{i = 0}^{T - 1}\delta_{i + 1}
            \\
            \Phi_T - \Phi_0 &\le 
            \sum_{i = 1}^{T}\delta_i = S_{T}. 
        \end{align*}
        So then $\Phi_T - \Phi_0$ yields: 
        $$
        \begin{aligned}
            \sigma_T (f(z_t) - f(x_*)) 
            + \frac{1}{2}\Vert x_t - x_*\Vert^2 
            - \frac{1}{2}\Vert x_0 - x_*\Vert^2 
            &\le S_{T}
            \\
            \implies 
            \sigma_T(f(z_t) - f(x_*))
            &\le 
            S_T + \frac{1}{2}\Vert x_0 - x_*\Vert^2
            \\
            f(z_t) - f(x_*) &\le 
            \sigma_T^{-1}\left(
                S_{T} + \frac{1}{2}\Vert x_0 - x_*\Vert^2
            \right),
        \end{aligned}
        $$
        which yields a convergence rate $\mathcal O(\sigma_T^{-1}S_{T})$. 
        When $S_T = 0$, the convergence rate is $O(\sigma_T^{-1})$ instead. 
    \end{proof}

    \begin{theorem}[Constraints on the PPM stepsize sequence]
        \label{thm:ag_generic_stepsize_constrants}
        \;\\
        Algorithm formulated in 
        \hyperref[def:ag_prox_grad_generic]{definition \ref*{def:ag_prox_grad_generic}}
        converges if the stepsize sequence $\eta_i, \tilde \eta_{i}$ satisfies
        \begin{align*}
            \begin{cases}
                \tilde \eta_{t + 1} (\tilde \eta_{t + 1} - L^{-1})
                - L^{-1} \sum_{i= 1}^{t}\tilde \eta_i 
                = 
                \epsilon_{t + 1} \tilde \eta_{t + 1}
                & \forall t \in \mathbb N, 
                \\
                L \eta_t \tilde \eta_{t + 1} = \sum_{i=1}^{t}\tilde \eta_i 
                & \forall t \in \mathbb N. 
            \end{cases}
        \end{align*}
        Here $\epsilon_{t + 1} = \tilde \eta_{t + 1} - \eta_t - L^{-1}$ is a sequence characterizing the error between $\tilde \eta_{t + 1}, \eta_t - L^{-1}$. 
        Then the convergence rate can be formulated by
        \begin{align*}
            \Phi_{t + 1} - \Phi_t =
            \Upsilon_{1, t + 1}^\text{AG} + 
            \sigma_t\Upsilon_{1, t + 1}^{\text{AG}} 
            &\le \epsilon_{t + 1}\tilde\eta_{t + 1} \Vert \mathcal G_L(y_t)\Vert^2 \le \delta_{t + 1}.
        \end{align*}
        When $\epsilon_t = 0 \;\forall t\in \mathbb N$, the sequence $a_t:= a_t = 1 + L \eta_t = \tilde \eta_{t + 1} - \epsilon_{t + 1}$, the relation simplies to: 
        \begin{align*}
            \begin{cases}
                a_{t + 1} = (1/2)\left(
                1 + \sqrt{1 + 4 c_{t + 1}}
                \right), 
                \\
                c_{t + 1} = a_t^2.     
            \end{cases}
        \end{align*}
        As a consequence: 
        \begin{enumerate}
            \item It makes $\delta_{t} = 0$ in the Lypunov analysis; 
            \item it simplifies the algorithm to
            \hyperref[def:ag_form_similar_tria_I]{definition\ref*{def:ag_form_similar_tria_I}}; 
            \item and it is the famous Nesterov momentum sequence. 
        \end{enumerate}
        
        
    \end{theorem}
    \begin{proof}
        With $t \in \mathbb N \cup \{0\}$ fixed, 
        recall that for the proximal gradient PPM generic form 
        (as formulated in 
        \hyperref[def:ag_prox_grad_generic]{definition \ref*{def:ag_prox_grad_generic}}
        ) for $t\in \mathbb N$ it has: 
        \begin{align*}
            y_t &= (1 + L\eta_t)^{-1}(x_t + L\eta_t z_t)
            \\
            x_{t + 1} &= x_t - \tilde \eta_{t + 1} \mathcal G_L(y_t)
            \\
            z_{t + 1} &= y_t - L^{-1}\mathcal G_L(y_t). 
        \end{align*}
        Recall the upper bounds from 
        \ref*{thm:generic_ag_convergence}, 
        it has 
        \begin{align*}
            \Upsilon_{1, t + 1}^\text{AG}
            &= 
            \tilde\eta_{t + 1} (h(z_{t + 1}) - h(x_*)) + 
            \frac{1}{2} (
                \Vert x_{t + 1} - x_*\Vert^2
                - 
                \Vert x_t - x_*\Vert^2
            )
            \\
            &\le 
            - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 
            + \frac{\tilde\eta_{t + 1}L}{2}\Vert z_{t + 1} - y_t\Vert^2
            - \langle 
                \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                x_{t + 1} - z_{t + 1}
            \rangle
            \\
            \Upsilon_{2, t + 1}^\text{AG}
            &= 
            h(z_{t + 1}) - h(z_t) 
            \le 
            \langle \mathcal G_L(y_t), z_{t + 1} - z_t\rangle + 
            \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2. 
        \end{align*}
        By the updates, vector $x_{t + 1} - x_t$ and $z_{t + 1} - y_t$ are parallel by observations: 
        \begin{align*}
            x_{t + 1} - x_t &= -\tilde\eta_{t + 1}\mathcal G_L(y_t), 
            \\
            z_{t + 1} - y_t &= -L^{-1}\mathcal G_L(y_t). 
        \end{align*}
        This allows for 
        \begin{align*}
            \Upsilon_{1, t + 1}^{\text{AG}} 
            &\le 
            - \frac{1}{2}\Vert x_{t + 1} - x_t\Vert^2 + 
            \frac{\tilde\eta_{t + 1}L}{2}\Vert z_{t + 1} - y_t\Vert^2 
            - 
            \langle \tilde\eta_{t + 1}\mathcal G_L (y_t), x_{t + 1} - z_{t + 1} \rangle
            \\
            &= 
            - \frac{1}{2}\Vert \tilde\eta_{t + 1} \mathcal G_L\Vert^2 + 
            \frac{\tilde\eta_{t + 1}L}{2}\Vert L^{-1} \mathcal G_L(y_t)\Vert^2
            - 
            \langle \tilde\eta_{t + 1} \mathcal G_L(y_t), x_{t + 1} - z_{t + 1} \rangle
            \\
            &= 
            \frac{1}{2}\left(
                - \tilde\eta_{t + 1}^2 + 
                L^{-1}\tilde\eta_{t + 1}
            \right)\Vert \mathcal G_L(y_t)\Vert^2
            - 
            \langle 
                \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                (x_{t + 1} - x_{t}) + x_t
                + (y_t - z_{t + 1}) - y_t
            \rangle
            \\
            &= 
            \frac{1}{2}\left(
                L^{-1}\tilde\eta_{t + 1}
                - \tilde\eta_{t + 1}^2
            \right)\Vert \mathcal G_L(y_t)\Vert^2
            - 
            \langle 
                \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                -\tilde\eta_{t + 1}\mathcal G_L(y_t) + x_t 
                + L^{-1}\mathcal G_L(y_t) - y_t
            \rangle
            \\
            &= 
            \frac{1}{2}\left(
                L^{-1}\tilde\eta_{t + 1}
                - \tilde\eta_{t + 1}^2
            \right)\Vert \mathcal G_L(y_t)\Vert^2
            - \langle 
                \tilde\eta_{t +1}\mathcal G_L(y_t), 
                (L^{-1} - \tilde\eta_{t + 1})\mathcal G_L(y_t) + x_t - y_t
            \rangle
            \\
            &= \frac{1}{2}\left(
                L^{-1}\tilde\eta_{t + 1} - \tilde\eta_{t + 1}^2 
                + 2 \tilde\eta_{t + 1}^2 - 2\tilde\eta_{t + 1}L^{-1}
            \right)\Vert \mathcal G_L(y_t)\Vert^2
            - 
            \langle 
                \tilde\eta_{t + 1} \mathcal G_L(y_t), 
                x_t - y_t
            \rangle
            \\
            &= 
            \frac{1}{2}\left(
                \tilde\eta_{t + 1}^2 - \tilde\eta_{t + 1}L^{-1}
            \right)\Vert \mathcal G_L(y_t)\Vert^2 
            + \langle \tilde\eta_{t + 1} \mathcal G_L(y_t), y_t - x_t\rangle.
        \end{align*}
        Similarly 
        \begin{align*}
            \Upsilon_{2, t + 1}^{\text{AG}} 
            &= 
            \langle \mathcal G_L(y_t), z_{t + 1} - z_t\rangle + 
            \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
            \\
            &= 
            \langle \mathcal G_L(y_t), z_{t + 1} - y_t + y_t - z_t\rangle
            + \frac{L}{2}\Vert z_{t + 1} - y_t\Vert^2
            \\
            &= 
            \langle \mathcal G_L(y_t), - L^{-1} \mathcal G_L(y_t) + y_t - z_t\rangle
            + 
            \frac{L}{2}\Vert L^{-1}\mathcal G_L(y_t)\Vert^2
            \\
            &= 
            -L^{-1}\Vert \mathcal G_L(y_t)\Vert^2 
            + 
            (1/2)L^{-1}\Vert \mathcal G_L(y_t)\Vert^2 
            + 
            \langle \mathcal G_L(y_t), y_t - z_t\rangle
            \\
            &= 
            -(1/2)L^{-1}\Vert \mathcal G_L(y_t)\Vert^2
            + 
            \langle \mathcal G_L(y_t), y_t - z_t\rangle. 
        \end{align*}
        Observe that the cross product term for $\Upsilon_{1, t + 1}^\text{AG}, \Upsilon_{2, t + 1}^\text{AG}$ doesn't match. 
        Hence let's consider the update for $y_t$, which can be written as $y_t - x_t = L \eta_t (z_t - y_t)$. We make the choice to do surgery on upper bound of $\Upsilon_{2, t + 1}^\text{AG}$, so $\langle \mathcal G_L(y_t), y_t - x_t\rangle = \langle \mathcal G_L(y_t), L \eta_t (z_t - y_t)\rangle$. 
        With this in mind, RHS of $\phi_{t + 1} - \phi_t$ yields: 
        {\footnotesize
        \begin{align*}
            &\Upsilon_{1, t + 1}^\text{AG} + 
            \left(
                \sum_{i = 1}^{t}\tilde\eta_i 
            \right)\Upsilon_{1, t + 1}^{\text{AG}}
            \\
            &\le 
            \frac{1}{2}\left(
                \tilde\eta_{t + 1}^2 - \tilde\eta_{t + 1}L^{-1}
            \right)\Vert \mathcal G_L(y_t)\Vert^2 
            + 
            \langle \tilde\eta_{t + 1} \mathcal G_L(y_t), L\eta_t(z_t - y_t)\rangle
            \\ 
            &\quad 
            + 
            \left(
                \sum_{i = 1}^{t}\tilde\eta_i 
            \right)\left(
                -(1/2)L^{-1}\Vert \mathcal G_L(y_t)\Vert^2
                + 
                \langle \mathcal G_L(y_t), y_t - z_t\rangle
            \right)
            \\
            &= 
            \left(
                \frac{1}{2}\tilde\eta_{t + 1}\left(
                    \tilde \eta_{t +1} - L^{-1}
                \right)
                - 
                \frac{1}{2L}\sum_{i = 1}^{t}\tilde \eta_i
            \right)\Vert \mathcal G_L(y_t)\Vert^2 + 
            \left(
                L\eta_t \tilde \eta_{t + 1} - \sum_{i = 1}^{t}\tilde \eta_i
            \right)\langle \mathcal G_L(y_t), z_t - y_t\rangle. 
        \end{align*}
        }
        The non-negativity of $\Vert \mathcal G_L(y_t) \Vert^2$ characterize the culmulative error $\delta_{t + 1}$ in the Lypunov analysis through sequence $\epsilon_i$. 
        Setting the coefficient of $\Vert \mathcal G_L(y_t) \Vert^2$ to be $\epsilon_{t + 1}\tilde \eta_{t + 1}$, it yields a system of inequality: 
        \begin{align*}
            \begin{cases}
                \tilde \eta_{t + 1} (\tilde \eta_{t + 1} - L^{-1})
                - L^{-1} \sum_{i= 1}^{t}\tilde \eta_i 
                = 
                \epsilon_{t + 1} \tilde \eta_{t + 1}
                & \forall t \in \N, 
                \\
                L \eta_t \tilde \eta_{t + 1} = \sum_{i=1}^{t}\tilde \eta_i 
                & \forall t \in \N. 
            \end{cases}
        \end{align*}
        It requires base case $L\eta_0\tilde\eta_{1} = 0$, assume $\sigma_0 = 0$. 
        We use $\sum_{i = 1}^t \tilde \eta_i = \sigma_t$, simplifying the first equation we have 
        \begin{align*}
            \tilde \eta_{t + 1} (\tilde \eta_{t + 1} - L^{-1})
            - L^{-1} \sigma_t
            &= 
            \epsilon_{t + 1} \tilde \eta_{t + 1}
            \\
            \iff 
            \tilde \eta_{t + 1} ^2 - L \tilde \eta_{t + 1} 
            &= 
            \epsilon_{t + 1} \tilde \eta_{t + 1} + L^{-1} \sigma_t
            \\
            &= 
            \epsilon_{t + 1} \tilde \eta_{t + 1} 
            + L^{-1}(L \eta_t \tilde \eta_{t + 1})
            \\
            \iff 
            \tilde \eta_{t + 1} &= \epsilon_{t + 1} + \eta_t + L^{-1}. 
        \end{align*}
        At the last step, we divided both side of the equation by $\tilde \eta_{t + 1} > 0$.
        The parameter gives relation $\epsilon_{t+1} = \tilde \eta_{t + 1} - \eta_t - L^{-1}$. 
        Hence, it gives us the following system of equality to work with 
        \begin{align*}
            \forall t \in \N: 
            \begin{cases}
                \tilde \eta_{t + 1} = \epsilon_{t + 1} + \eta_t + L^{-1}, 
                \\
                L \eta_t \tilde \eta_{t + 1} = \sigma_t.     
            \end{cases}
        \end{align*}
        With that, we can solve a relation between $\eta_{t + 1}$ in terms of the sequence $\epsilon$ and $\eta_t$.
        Consider the equality 

        \begin{align*}
            L \sigma_{t + 1} &= L \tilde \eta_{t + 1} + L \sigma_t
            \\
            &=
            L \tilde \eta_{t + 1}  + L (L \eta_t \tilde \eta_{t + 1})
            \\
            &= L \tilde \eta_{t+ 1} + L \eta_t (L \tilde \eta_{t + 1})
            \\
            &=  L \tilde \eta_{t+ 1} + L \eta_t (L \epsilon_{t + 1} + L \eta_t + 1)
            \\
            &=  L \tilde \eta_{t+ 1} + L \eta_t (L \eta_t + 1) + L^2 \eta_t \epsilon_{t + 1}
            \\
            &= L (\epsilon_{t +1} + \eta_t + L^{-1}) + L\eta_t(L \eta_t + 1) + L^2\eta_t \epsilon_{t + 1}
            \\
            &= L \epsilon_{t + 1} + (L\eta_t + 1)^2 + L^2\eta_t \epsilon_{t + 1}
            \\
            &= 
            L \epsilon_{t + 1}(1 + L \eta_t) + (1 + L \eta_t)^2. 
        \end{align*}
        At the same time we have 
        \begin{align*}
            L \sigma_{t + 1} &= L^2 \eta_{t + 1}\tilde \eta_{t + 1} 
            \\
            &= L\eta_{t + 1}(1 + L \eta_{t + 1} + \epsilon_{t + 2})
            \\
            &= L\eta_{t + 1}(1 + L \eta_{t + 1}) + \epsilon_{t + 2}L\eta_{t + 1}. 
        \end{align*}
        Therefore, it generates the following equation: 
        \begin{align*}
            L\eta_{t + 1} (1 + L \eta_{t + 1}) 
            + 
            \epsilon_{t + 2} L \eta_{t + 1} 
            &= 
            L\epsilon_{t + 1}(1 + L \eta_t)  + (1 + L\eta_t)^2
            \\
            (L\eta_{t + 1} + L^2\eta_{t + 1}^2)
            + 
            \epsilon_{t + 2} L \eta_{t + 1} 
            + 
            \frac{1}{4}
            &= 
            L\epsilon_{t + 1}(1 + L \eta_t)  + (1 + L\eta_t)^2
            + 
            \frac{1}{4}
            \\
            (L\eta_{t + 1} + L^2\eta_{t + 1}^2 + 1/4)
            + 
            \epsilon_{t + 2} L \eta_{t + 1} 
            + \epsilon_{t + 2}
            &= 
            L\epsilon_{t + 1}(1 + L \eta_t)  + (1 + L\eta_t)^2
            + \frac{1}{4}
            + \epsilon_{t + 2}
            \\
            (L\eta_{t + 1} + 1/2)^2 + \epsilon_{t + 2}(L \eta_{t + 1} + 1)
            &= 
            L \epsilon_{t + 1}(1 + L \eta_t) + (1 + L\eta_t)^2
            + \frac{1}{4} + \epsilon_{t + 2}
            \\
            \text{ with: } & a_t = 1 + L \eta_t = \tilde \eta_{t + 1} - \epsilon_{t + 1}
            \\
            (a_{t + 1} - 1/2)^2 + \epsilon_{t + 2}a_{t + 1}
            &= 
            L \epsilon_{t + 1}a_t + a_t^2 + 1/4 + \epsilon_{t + 1}
            \\
            a_{t + 1}^2 + 1/4 - a_{t + 1} + \epsilon_{t + 2}a_{t + 1}
            &= 
            L \epsilon_{t + 1}a_t + a_t^2 + 1/4 + \epsilon_{t + 1}
            \\
            a_{t + 1}^2 + a_{t + 1}(\epsilon_{t + 2} - 1)
            &= 
            \underbrace{
                a_t(L \epsilon_{t + 1} + a_t) + \epsilon_{t + 1}
            }_{c_{t + 1}}. 
        \end{align*}
        Solving reveals the relations: 
        \begin{align*}
            \begin{cases}
                a_{t + 1} = (1/2)\left(
                1 - \epsilon_{t + 2} + \sqrt{(1 - \epsilon_{t + 2}) + 4 c_{t + 2}}
                \right), 
                \\
                c_{t + 1} = a_t (L \epsilon_{t + 1} + a_t) + \epsilon_{t + 1}. 
            \end{cases}
        \end{align*}
        Observe that in the case where we choose $\epsilon_t = 0 \forall t \in \N$, the above relation simplifies to 
        \begin{align*}
            a_{t + 1} &= (1/2)\left(
                1 + \sqrt{1 + 4 c_{t + 1}}
            \right), 
            \\
            c_{t + 1} &= a_t^2. 
        \end{align*}
        This relation is the Famous Nesterov momentum sequence. 
        At the same time, we can analyize the convergence rate of the algorithm by the abstract convergence lemma, producing: 
        \begin{align*}
            \Phi_{t + 1} - \Phi_t =
            \Upsilon_{1, t + 1}^\text{AG} + 
            \sigma_t\Upsilon_{1, t + 1}^{\text{AG}} 
            &\le \epsilon_{t + 1}\eta_{t + 1} \Vert \mathcal G_L(y_t)\Vert^2 \le \delta_{t + 1}
        \end{align*}
        Telescoping yields: 
        \begin{align*}
            S_{T} = 
            \sum_{i = 0}^{T- 1} \delta_i 
            &= 
            \sum_{i = 0}^{T - 1} \epsilon_{i + 1}\tilde\eta_{i + 1}\Vert \mathcal G_L(y_i)\Vert^2
            \\
            &\le \sum_{i = 0}^{T - 1}\max(\epsilon_{i + 1} \tilde\eta_{i + 1}\Vert \mathcal G_L(y_i)\Vert^2, 0). 
        \end{align*}
        Under an ideal case where we wish to attain accelerations, we want $\lim_{T \rightarrow \infty} S_T < \infty$. 
        One way to accomplish is choose the error sequence $\epsilon_i, i \in \N$ to be bounded by for all $i \in \N$, $\epsilon_i$ should satisfy: 
        \begin{align*}
            \epsilon_{i + 1}\tilde \eta_{i + 1}
            \Vert \mathcal G_L(y_i)\Vert^2 
            &\le \delta_{i + 1}
            \\
            \iff 
            \epsilon_{i + 1}
            &\le 
            \frac{\delta_{i + 1} }{\tilde\eta_{t + 1}\Vert \mathcal G_L(y_i)\Vert^2}. 
        \end{align*}
        for any $\sum_{i = 0}^{T - 1}\delta_{i + 1}$ converges to a limit as $T \rightarrow \infty$. 

    \end{proof}

