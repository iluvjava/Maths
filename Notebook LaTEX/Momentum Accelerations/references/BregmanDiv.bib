
@misc{allen-zhu_linear_2016,
	title = {Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent},
	url = {http://arxiv.org/abs/1407.1537},
	doi = {10.48550/arXiv.1407.1537},
	shorttitle = {Linear Coupling},
	abstract = {First-order methods play a central role in large-scale machine learning. Even though many variations exist, each suited to a particular problem, almost all such methods fundamentally rely on two types of algorithmic steps: gradient descent, which yields primal progress, and mirror descent, which yields dual progress. We observe that the performances of gradient and mirror descent are complementary, so that faster algorithms can be designed by {LINEARLY} {COUPLING} the two. We show how to reconstruct Nesterov's accelerated gradient methods using linear coupling, which gives a cleaner interpretation than Nesterov's original proofs. We also discuss the power of linear coupling by extending it to many other settings that Nesterov's methods cannot apply to.},
	number = {{arXiv}:1407.1537},
	publisher = {{arXiv}},
	author = {Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
	urldate = {2023-10-20},
	date = {2016-11-07},
	eprinttype = {arxiv},
	eprint = {1407.1537 [cs, math, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Machine Learning, Numerical Analysis, Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/BG8VCZLK/Allen-Zhu and Orecchia - 2016 - Linear Coupling An Ultimate Unification of Gradie.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/YXTMC655/1407.html:text/html},
}

@incollection{amir_first-order_nodate,
	title = {First-Order Methods in Optimization {\textbar} Chapter 9: Mirror Descent},
	isbn = {978-1-61197-499-7},
	url = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/epdf/10.1137/1.9781611974997.ch9},
	shorttitle = {Chapter 9},
	author = {Amir, Beck},
	urldate = {2023-10-23},
	langid = {english},
	file = {Chapter 9 Mirror Descent.pdf:/Users/hongdali/Zotero/storage/EV2BPJKQ/Chapter 9 Mirror Descent.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/EV6SJ2U7/1.9781611974997.html:text/html},
}

@article{bauschke_descent_2017,
	title = {A Descent Lemma Beyond Lipschitz Gradient Continuity: First-Order Methods Revisited and Applications},
	volume = {42},
	issn = {0364-765X, 1526-5471},
	url = {https://pubsonline.informs.org/doi/10.1287/moor.2016.0817},
	doi = {10.1287/moor.2016.0817},
	shorttitle = {A Descent Lemma Beyond Lipschitz Gradient Continuity},
	abstract = {The proximal gradient and its variants is one of the most attractive first-order algorithm for minimizing the sum of two convex functions, with one being nonsmooth. However, it requires the differentiable part of the objective to have a Lipschitz continuous gradient, thus precluding its use in many applications. In this paper we introduce a framework which allows to circumvent the intricate question of Lipschitz continuity of gradients by using an elegant and easy to check convexity condition which captures the geometry of the constraints. This condition translates into a new descent lemma which in turn leads to a natural derivation of the proximal-gradient scheme with Bregman distances. We then identify a new notion of asymmetry measure for Bregman distances, which is central in determining the relevant step-size. These novelties allow to prove a global sublinear rate of convergence, and as a by-product, global pointwise convergence is obtained. This provides a new path to a broad spectrum of problems arising in key applications which were, until now, considered as out of reach via proximal gradient methods. We illustrate this potential by showing how our results can be applied to build new and simple schemes for Poisson inverse problems.},
	pages = {330--348},
	number = {2},
	journaltitle = {Mathematics of Operations Research},
	shortjournal = {Mathematics of {OR}},
	author = {Bauschke, Heinz H. and Bolte, Jérôme and Teboulle, Marc},
	urldate = {2023-12-11},
	date = {2017-05},
	langid = {english},
	keywords = {First-order Methods},
	file = {A descent lemma beyond lip smoothness.pdf:/Users/hongdali/Zotero/storage/AN94F2WG/A descent lemma beyond lip smoothness.pdf:application/pdf},
}

@unpublished{dhillon_learning_nodate,
	location = {University of Texas at Austin},
	title = {Learning with Bregman Divergences},
	url = {https://www.cs.utexas.edu/users/inderjit/Talks/bregtut.pdf},
	author = {Dhillon, Inderjit S},
	langid = {english},
	file = {Dhillon - Learning with Bregman Divergences.pdf:/Users/hongdali/Zotero/storage/DFPL5SJI/Dhillon - Learning with Bregman Divergences.pdf:application/pdf},
}

@article{dragomir_optimal_2022,
	title = {Optimal complexity and certification of Bregman first-order methods},
	volume = {194},
	issn = {0025-5610, 1436-4646},
	url = {https://link.springer.com/10.1007/s10107-021-01618-1},
	doi = {10.1007/s10107-021-01618-1},
	abstract = {We provide a lower bound showing that the O(1/k) convergence rate of the {NoLips} method (a.k.a. Bregman Gradient or Mirror Descent) is optimal for the class of problems satisfying the relative smoothness assumption. This assumption appeared in the recent developments around the Bregman Gradient method, where acceleration remained an open issue.},
	pages = {41--83},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Dragomir, Radu-Alexandru and Taylor, Adrien B. and d’Aspremont, Alexandre and Bolte, Jérôme},
	urldate = {2023-12-02},
	date = {2022-07},
	langid = {english},
	file = {Dragomir et al. - 2022 - Optimal complexity and certification of Bregman first-order methods.pdf:/Users/hongdali/Zotero/storage/4UA78G9T/Dragomir et al. - 2022 - Optimal complexity and certification of Bregman first-order methods.pdf:application/pdf},
}

@unpublished{fawzi_lecture_2022,
	location = {Cambridge University Department of Applied Mathematics and Physics},
	title = {Lecture 11 Bregman gradient methods},
	url = {https://www.damtp.cam.ac.uk/user/hf323/L22-III-OPT/lecture11.pdf},
	abstract = {Topics in Convex Optimization lecture notes, Lent 2022},
	author = {Fawzi, Hamza},
	date = {2022},
	langid = {english},
	file = {Fawzi - 11 Bregman gradient methods.pdf:/Users/hongdali/Zotero/storage/XUR87TVM/Fawzi - 11 Bregman gradient methods.pdf:application/pdf},
}

@inproceedings{krichene_efficient_2015,
	location = {Osaka},
	title = {Efficient Bregman projections onto the simplex},
	isbn = {978-1-4799-7886-1},
	url = {http://ieeexplore.ieee.org/document/7402714/},
	doi = {10.1109/CDC.2015.7402714},
	eventtitle = {2015 54th {IEEE} Conference on Decision and Control ({CDC})},
	pages = {3291--3298},
	booktitle = {2015 54th {IEEE} Conference on Decision and Control ({CDC})},
	publisher = {{IEEE}},
	author = {Krichene, Walid and Krichene, Syrine and Bayen, Alexandre},
	urldate = {2024-01-10},
	date = {2015-12},
	langid = {english},
	file = {Krichene et al. - 2015 - Efficient Bregman projections onto the simplex.pdf:/Users/hongdali/Zotero/storage/J4KRTQQS/Krichene et al. - 2015 - Efficient Bregman projections onto the simplex.pdf:application/pdf},
}
