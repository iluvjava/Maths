
@misc{allen-zhu_linear_2016,
	title = {Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent},
	url = {http://arxiv.org/abs/1407.1537},
	doi = {10.48550/arXiv.1407.1537},
	shorttitle = {Linear Coupling},
	abstract = {First-order methods play a central role in large-scale machine learning. Even though many variations exist, each suited to a particular problem, almost all such methods fundamentally rely on two types of algorithmic steps: gradient descent, which yields primal progress, and mirror descent, which yields dual progress. We observe that the performances of gradient and mirror descent are complementary, so that faster algorithms can be designed by {LINEARLY} {COUPLING} the two. We show how to reconstruct Nesterov's accelerated gradient methods using linear coupling, which gives a cleaner interpretation than Nesterov's original proofs. We also discuss the power of linear coupling by extending it to many other settings that Nesterov's methods cannot apply to.},
	number = {{arXiv}:1407.1537},
	publisher = {{arXiv}},
	author = {Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
	urldate = {2023-10-20},
	date = {2016-11-07},
	eprinttype = {arxiv},
	eprint = {1407.1537 [cs, math, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Machine Learning, Numerical Analysis, Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/BG8VCZLK/Allen-Zhu and Orecchia - 2016 - Linear Coupling An Ultimate Unification of Gradie.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/YXTMC655/1407.html:text/html},
}

@inproceedings{alamo_gradient_2019,
	title = {Gradient Based Restart {FISTA}},
	url = {https://ieeexplore.ieee.org/document/9029983},
	doi = {10.1109/CDC40024.2019.9029983},
	abstract = {Fast gradient methods ({FGM}) are very popular in the field of large scale convex optimization problems. Recently, it has been shown that restart strategies can guarantee global linear convergence for non-strongly convex optimization problems if a quadratic functional growth condition is satisfied [1], [2]. In this context, a novel restart {FGM} algorithm with global linear convergence is proposed in this paper. The main advantages of the algorithm with respect to other linearly convergent restart {FGM} algorithms are its simplicity and that it does not require prior knowledge of the optimal value of the objective function or of the quadratic functional growth parameter. We present some numerical simulations that illustrate the performance of the algorithm.},
	eventtitle = {2019 {IEEE} 58th Conference on Decision and Control ({CDC})},
	pages = {3936--3941},
	booktitle = {2019 {IEEE} 58th Conference on Decision and Control ({CDC})},
	author = {Alamo, Teodoro and Krupa, Pablo and Limon, Daniel},
	urldate = {2023-10-18},
	date = {2019-12},
	note = {{ISSN}: 2576-2370},
	file = {IEEE Xplore Abstract Record:/Users/hongdali/Zotero/storage/W8B3RR7L/9029983.html:text/html;IEEE Xplore Full Text PDF:/Users/hongdali/Zotero/storage/79P9V3R6/Alamo et al. - 2019 - Gradient Based Restart FISTA.pdf:application/pdf},
}

@unpublished{fazel_ee_2014,
	location = {University of Washington, Paul Allen Center, Room {CSE} 230.},
	title = {{EE} 546 at University of Wasthington Spring 2014, Nesterov Estimating Sequences Method},
	url = {https://class.ece.uw.edu/546/2014spr/lectures/optimal.pdf},
	note = {{EE} 546 Lecture},
	author = {Fazel, Maryam},
	urldate = {2023-10-14},
	date = {2014},
	langid = {english},
	file = {optimal.pdf:/Users/hongdali/Zotero/storage/V2Q3HN2U/optimal.pdf:application/pdf},
}

@incollection{fornasier_introduction_2010,
	title = {An Introduction to Total Variation for Image Analysis},
	isbn = {978-3-11-022614-0},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110226157.263/html},
	abstract = {These notes address various theoretical and practical topics related to Total Variationbased image reconstruction. It focuses ﬁrst on some theoretical results on functions which minimize the total variation, and in a second part, describes a few standard and less standard algorithms to minimize the total variation in a ﬁnite-differences setting, with a series of applications from simple denoising to stereo, or deconvolution issues, and even more exotic uses like the minimization of minimal partition problems.},
	pages = {263--340},
	booktitle = {Theoretical Foundations and Numerical Methods for Sparse Recovery},
	publisher = {{DE} {GRUYTER}},
	author = {Chambolle, Antonin and Caselles, Vicent and Cremers, Daniel and Novaga, Matteo and Pock, Thomas},
	editor = {Fornasier, Massimo},
	urldate = {2023-10-15},
	date = {2010-07-16},
	langid = {english},
	doi = {10.1515/9783110226157.263},
	keywords = {Image Reconstruction, Splitting Algorithm, total variation},
	file = {Chambolle et al. - 2010 - An Introduction to Total Variation for Image Analy.pdf:/Users/hongdali/Zotero/storage/36HV9FQJ/Chambolle et al. - 2010 - An Introduction to Total Variation for Image Analy.pdf:application/pdf},
}

@incollection{nesterov_lecture_2018,
	location = {Cham},
	title = {Lecture on Convex Optimizations Chapter 2, Smooth Convex Optimization},
	isbn = {978-3-319-91578-4},
	url = {https://doi.org/10.1007/978-3-319-91578-4_2},
	series = {Springer Optimization and Its Applications},
	abstract = {In this chapter, we study the complexity of solving optimization problems formed by differentiable convex components. We start by establishing the main properties of such functions and deriving the lower complexity bounds, which are valid for all natural optimization methods. After that, we prove the worst-case performance guarantees for the Gradient Method. Since these bounds are quite far from the lower complexity bounds, we develop a special technique, based on the notion of estimating sequences, which allows us to justify the Fast Gradient Methods. These methods appear to be optimal for smooth convex problems. We also obtain performance guarantees for these methods targeting on generating points with small norm of the gradient. In order to treat problems with set constraints, we introduce the notion of a Gradient Mapping. This allows an automatic extension of methods for unconstrained minimization to the constrained case. In the last section, we consider methods for solving smooth optimization problems, defined by several functional components.},
	pages = {59--137},
	booktitle = {Lectures on Convex Optimization},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	editor = {Nesterov, Yurii},
	urldate = {2023-10-11},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-91578-4_2},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/5LJMV866/Nesterov - 2018 - Smooth Convex Optimization.pdf:application/pdf},
}

@online{yu_acceleration_nodate,
	title = {Acceleration Course Notes for {CS}794 at University of Waterloo},
	url = {https://cs.uwaterloo.ca/~y328yu/mycourses/794/794-note-apg.pdf},
	author = {Yu, Yaoliang},
	urldate = {2023-10-11},
	keywords = {Lecture Notes},
	file = {794-note-apg.pdf:/Users/hongdali/Zotero/storage/8TGD3FNG/794-note-apg.pdf:application/pdf},
}

@article{nesterov_smooth_2005,
	title = {Smooth minimization of non-smooth functions},
	volume = {103},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-004-0552-5},
	doi = {10.1007/s10107-004-0552-5},
	abstract = {In this paper we propose a new approach for constructing efficient schemes for non-smooth convex optimization. It is based on a special smoothing technique, which can be applied to functions with explicit max-structure. Our approach can be considered as an alternative to black-box minimization. From the viewpoint of efficiency estimates, we manage to improve the traditional bounds on the number of iterations of the gradient schemes from keeping basically the complexity of each iteration unchanged.},
	pages = {127--152},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Nesterov, Yu.},
	urldate = {2023-11-23},
	date = {2005-05-01},
	langid = {english},
	keywords = {Complexity Theory, Convex Optimization, Non-smooth Optimization, Optimal Methods, Structural Optimization},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/VRX6E68U/Nesterov - 2005 - Smooth minimization of non-smooth functions.pdf:application/pdf},
}

@book{nesterov_lectures_2018,
	location = {Cham},
	title = {Lectures on Convex Optimization},
	volume = {137},
	isbn = {978-3-319-91577-7 978-3-319-91578-4},
	url = {http://link.springer.com/10.1007/978-3-319-91578-4},
	series = {Springer Optimization and Its Applications},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	urldate = {2023-10-11},
	date = {2018},
	doi = {10.1007/978-3-319-91578-4},
	keywords = {Optimization, Fast Gradient Methods, Algorithmic Complexity, Interior-Point Methods, Numerical Optimization, Optimization in Relative Scale, Self-Concordant Functions, Smoothing Technique},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/HSCCPYL9/Nesterov - 2018 - Lectures on Convex Optimization.pdf:application/pdf},
}

@book{beck_first-order_nodate,
	title = {First-Order Methods in Optimization {\textbar} {SIAM} Publications Library},
	isbn = {978-1-61197-498-0},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974997},
	series = {{MOS}-{SIAM} Series in Optimization},
	publisher = {{SIAM}},
	author = {Beck, Amir},
	urldate = {2023-10-19},
	langid = {english},
	keywords = {Optimization, First-order Methods, Non-smooth Optimization, Numerical Optimization},
	file = {First-Order Methods in Optimization  SIAM Publication.pdf:/Users/hongdali/Zotero/storage/P2HFAVVQ/First-Order Methods in Optimization  SIAM Publication.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/88BHKZ6Y/1.html:text/html},
}

@article{aujol_convergence_2023,
	title = {Convergence rates of the Heavy-Ball method under the Łojasiewicz property},
	volume = {198},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-022-01770-2},
	doi = {10.1007/s10107-022-01770-2},
	abstract = {In this paper, a joint study of the behavior of solutions of the Heavy Ball {ODE} and Heavy Ball type algorithms is given. Since the pioneering work of Polyak ({USSR} Comput Math Math Phys 4(5):1–17, 1964), it is well known that such a scheme is very efficient for \$\$C{\textasciicircum}2\$\$strongly convex functions with Lipschitz gradient. But much less is known when only growth conditions are considered. Depending on the geometry of the function to minimize, convergence rates for convex functions, with some additional regularity such as quasi-strong convexity, or strong convexity, were recently obtained in Aujol et al. (Convergence rates of the Heavy-Ball method for quasi-strongly convex optimization, 2020). Convergence results with much weaker assumptions are given in the present paper: namely, linear convergence rates when assuming a growth condition (which amounts to a Łojasiewicz property in the convex case). This analysis is firstly performed in continuous time for the {ODE}, and then transposed for discrete optimization schemes. In particular, a variant of the Heavy Ball algorithm is proposed, which converges geometrically whatever the parameters choice, and which has the best state of the art convergence rate for first order methods to minimize composite non smooth convex functions satisfying a Łojasiewicz property.},
	pages = {195--254},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Aujol, J.-F. and Dossal, Ch. and Rondepierre, A.},
	urldate = {2023-10-19},
	date = {2023-03-01},
	langid = {english},
	keywords = {Optimization, Rate of Convergence, Heavy Ball method, Łojasiewicz property, Lyapunov Function, {ODEs}},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/URSACZIH/Aujol et al. - 2023 - Convergence rates of the Heavy-Ball method under t.pdf:application/pdf},
}

@article{polyak_methods_1964,
	title = {Some methods of speeding up the convergence of iteration methods},
	volume = {4},
	issn = {00415553},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0041555364901375},
	doi = {10.1016/0041-5553(64)90137-5},
	pages = {1--17},
	number = {5},
	journaltitle = {{USSR} Computational Mathematics and Mathematical Physics},
	shortjournal = {{USSR} Computational Mathematics and Mathematical Physics},
	author = {Polyak, B.T.},
	urldate = {2023-10-11},
	date = {1964-01},
	langid = {english},
	file = {Polyak - 1964 - Some methods of speeding up the convergence of ite.pdf:/Users/hongdali/Zotero/storage/LFZIZLCJ/Polyak - 1964 - Some methods of speeding up the convergence of ite.pdf:application/pdf},
}

@misc{jang_computer-assisted_2023,
	title = {Computer-assisted design of accelerated composite optimization methods: {OptISTA}},
	url = {http://arxiv.org/abs/2305.15704},
	doi = {10.48550/arXiv.2305.15704},
	shorttitle = {Computer-Assisted Design of Accelerated Composite Optimization Methods},
	abstract = {The accelerated composite optimization method {FISTA} (Beck, Teboulle 2009) is suboptimal, and we present a new method {OptISTA} that improves upon it by a factor of 2. The performance estimation problem ({PEP}) has recently been introduced as a new computer-assisted paradigm for designing optimal first-order methods, but the methodology was largely limited to unconstrained optimization with a single function. In this work, we present a novel double-function stepsize-optimization {PEP} methodology that poses the optimization over fixed-step first-order methods for composite optimization as a finite-dimensional nonconvex {QCQP}, which can be practically solved through spatial branch-and-bound algorithms, and use it to design the exact optimal method {OptISTA} for the composite optimization setup. We then establish the exact optimality of {OptISTA} with a novel lower-bound construction that extends the semi-interpolated zero-chain construction (Drori, Taylor 2022) to the double-function setup of composite optimization. By establishing exact optimality, our work concludes the search for the fastest first-order methods for the proximal, projected-gradient, and proximal-gradient setups.},
	number = {{arXiv}:2305.15704},
	publisher = {{arXiv}},
	author = {Jang, Uijeong and Gupta, Shuvomoy Das and Ryu, Ernest K.},
	urldate = {2023-10-13},
	date = {2023-05-25},
	eprinttype = {arxiv},
	eprint = {2305.15704 [math]},
	keywords = {Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/72I9QL34/Jang et al. - 2023 - Computer-Assisted Design of Accelerated Composite .pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/RFS7VLTM/2305.html:text/html},
}

@misc{goldstein_field_2016,
	title = {A field guide to forward-backward splitting with a {FASTA} implementation},
	url = {http://arxiv.org/abs/1411.3406},
	abstract = {Non-differentiable and constrained optimization play a key role in machine learning, signal and image processing, communications, and beyond. For high-dimensional minimization problems involving large datasets or many unknowns, the forward-backward splitting method provides a simple, practical solver. Despite its apparently simplicity, the performance of the forward-backward splitting is highly sensitive to implementation details. This article is an introductory review of forward-backward splitting with a special emphasis on practical implementation concerns. Issues like stepsize selection, acceleration, stopping conditions, and initialization are considered. Numerical experiments are used to compare the effectiveness of different approaches. Many variations of forward-backward splitting are implemented in the solver {FASTA} (short for Fast Adaptive Shrinkage/Thresholding Algorithm). {FASTA} provides a simple interface for applying forward-backward splitting to a broad range of problems.},
	number = {{arXiv}:1411.3406},
	publisher = {{arXiv}},
	author = {Goldstein, Tom and Studer, Christoph and Baraniuk, Richard},
	urldate = {2023-11-09},
	date = {2016-12-27},
	eprinttype = {arxiv},
	eprint = {1411.3406 [cs]},
	keywords = {Numerical Analysis},
	file = {arXiv.org Snapshot:/Users/hongdali/Zotero/storage/PZZLNWLU/1411.html:text/html;Full Text PDF:/Users/hongdali/Zotero/storage/LZW8GXGH/Goldstein et al. - 2016 - A Field Guide to Forward-Backward Splitting with a.pdf:application/pdf;Goldstein et al. - 2016 - A Field Guide to Forward-Backward Splitting with a.pdf:/Users/hongdali/Zotero/storage/HPWHH37K/Goldstein et al. - 2016 - A Field Guide to Forward-Backward Splitting with a.pdf:application/pdf},
}

@misc{alamo_restart_2019,
	title = {Restart {FISTA} with global linear convergence},
	url = {http://arxiv.org/abs/1906.09126},
	doi = {10.48550/arXiv.1906.09126},
	abstract = {Fast Iterative Shrinking-Threshold Algorithm ({FISTA}) is a popular fast gradient descent method ({FGM}) in the field of large scale convex optimization problems. However, it can exhibit undesirable periodic oscillatory behaviour in some applications that slows its convergence. Restart schemes seek to improve the convergence of {FGM} algorithms by suppressing the oscillatory behaviour. Recently, a restart scheme for {FGM} has been proposed that provides linear convergence for non strongly convex optimization problems that satisfy a quadratic functional growth condition. However, the proposed algorithm requires prior knowledge of the optimal value of the objective function or of the quadratic functional growth parameter. In this paper we present a restart scheme for {FISTA} algorithm, with global linear convergence, for non strongly convex optimization problems that satisfy the quadratic growth condition without requiring the aforementioned values. We present some numerical simulations that suggest that the proposed approach outperforms other restart {FISTA} schemes.},
	number = {{arXiv}:1906.09126},
	publisher = {{arXiv}},
	author = {Alamo, Teodoro and Krupa, Pablo and Limon, Daniel},
	urldate = {2023-10-18},
	date = {2019-12-24},
	eprinttype = {arxiv},
	eprint = {1906.09126 [math]},
	keywords = {Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/B79AUCM7/Alamo et al. - 2019 - Restart FISTA with Global Linear Convergence.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/LZ7ISHFW/1906.html:text/html},
}

@misc{ahn_understanding_2022,
	title = {Understanding nesterov's acceleration via proximal point method},
	url = {http://arxiv.org/abs/2005.08304},
	doi = {10.48550/arXiv.2005.08304},
	abstract = {The proximal point method ({PPM}) is a fundamental method in optimization that is often used as a building block for designing optimization algorithms. In this work, we use the {PPM} method to provide conceptually simple derivations along with convergence analyses of different versions of Nesterov's accelerated gradient method ({AGM}). The key observation is that {AGM} is a simple approximation of {PPM}, which results in an elementary derivation of the update equations and stepsizes of {AGM}. This view also leads to a transparent and conceptually simple analysis of {AGM}'s convergence by using the analysis of {PPM}. The derivations also naturally extend to the strongly convex case. Ultimately, the results presented in this paper are of both didactic and conceptual value; they unify and explain existing variants of {AGM} while motivating other accelerated methods for practically relevant settings.},
	number = {{arXiv}:2005.08304},
	publisher = {{arXiv}},
	author = {Ahn, Kwangjun and Sra, Suvrit},
	urldate = {2023-11-04},
	date = {2022-06-02},
	eprinttype = {arxiv},
	eprint = {2005.08304 [cs, math]},
	keywords = {Computer Science - Machine Learning, Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/PZBWUWW5/Ahn and Sra - 2022 - Understanding Nesterov's Acceleration via Proximal.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/WSSGK9Q4/2005.html:text/html},
}

@article{tseng_accelerated_nodate,
	title = {On accelerated proximal gradient methods for convex-concave optimization},
	url = {https://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf},
	abstract = {Recently there has been active interest in accelerated proximal gradient methods for large-scale convex-concave optimization, as studied by Nesterov, Nemirovski, and others. We present a unified treatment of these methods, including new variants that perform either one or two projections per iteration, and give simple analyses of their iteration complexity. These methods are compared on a matrix game example.},
	author = {Tseng, Paul},
	file = {apgm.pdf:/Users/hongdali/Zotero/storage/7LAU6JSU/apgm.pdf:application/pdf},
}

@article{su_differential_2015,
	title = {A differential equation for modeling nesterov's accelerated gradient method: Theory and Insights},
	url = {https://arxiv.org/abs/1503.01243v2},
	shorttitle = {A Differential Equation for Modeling Nesterov's Accelerated Gradient Method},
	abstract = {We derive a second-order ordinary differential equation ({ODE}) which is the limit of Nesterov's accelerated gradient method. This {ODE} exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time {ODE} allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The {ODE} interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
	journaltitle = {{arXiv}.org},
	author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
	urldate = {2023-10-09},
	date = {2015-03-04},
	langid = {english},
	keywords = {{ODEs}, dynamical system},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/ANWIF5NT/Su et al. - 2015 - A Differential Equation for Modeling Nesterov's Ac.pdf:application/pdf},
}

@article{rudin_nonlinear_1992,
	title = {Nonlinear total variation based noise removal algorithms},
	volume = {60},
	issn = {0167-2789},
	url = {https://www.sciencedirect.com/science/article/pii/016727899290242F},
	doi = {10.1016/0167-2789(92)90242-F},
	abstract = {A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lanrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t → ∞ the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set.},
	pages = {259--268},
	number = {1},
	journaltitle = {Physica D: Nonlinear Phenomena},
	shortjournal = {Physica D: Nonlinear Phenomena},
	author = {Rudin, Leonid I. and Osher, Stanley and Fatemi, Emad},
	urldate = {2023-10-11},
	date = {1992-11-01},
	keywords = {total variation},
	file = {Rudin et al. - 1992 - Nonlinear total variation based noise removal algo.pdf:/Users/hongdali/Zotero/storage/VIG4E6VH/Rudin et al. - 1992 - Nonlinear total variation based noise removal algo.pdf:application/pdf},
}

@article{ochs_adaptive_2019,
	title = {Adaptive {FISTA} for nonconvex optimization},
	volume = {29},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/17M1156678},
	doi = {10.1137/17M1156678},
	abstract = {Backtracking line-search is an old yet powerful strategy for finding better step sizes to be used in proximal gradient algorithms. The main principle is to locally find a simple convex upper bound of the objective function, which in turn controls the step size that is used. In case of inertial proximal gradient algorithms, the situation becomes much more difficult and usually leads to very restrictive rules on the extrapolation parameter. In this paper, we show that the extrapolation parameter can be controlled by also locally finding a simple concave lower bound of the objective function. This gives rise to a double convex-concave backtracking procedure which allows for an adaptive choice of both the step size and extrapolation parameters. We apply this procedure to the class of inertial Bregman proximal gradient methods, and prove that any sequence generated by these algorithms converges globally to a critical point of the function at hand. Numerical experiments on a number of challenging nonconvex problems in image processing and machine learning were conducted and show the power of combining inertial step and double backtracking strategy in achieving improved performances.},
	pages = {2482--2503},
	number = {4},
	journaltitle = {{SIAM} Journal on Optimization},
	shortjournal = {{SIAM} J. Optim.},
	author = {Ochs, Peter and Pock, Thomas},
	urldate = {2023-10-09},
	date = {2019-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Submitted Version:/Users/hongdali/Zotero/storage/VW687RLR/Ochs and Pock - 2019 - Adaptive FISTA for Nonconvex Optimization.pdf:application/pdf},
}

@article{noel_nesterovs_nodate,
	title = {Nesterov's method for convex optimization},
	volume = {65},
	url = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/epdf/10.1137/21M1390037},
	doi = {10.1137/21M1390037},
	abstract = {While Nesterov's algorithm for computing the minimum of a convex function is now over forty years old, it is rarely presented in texts for a first course in optimization. This is unfortunate since for many problems this algorithm is superior to the ubiquitous steepest descent algorithm, and it is equally simple to implement. This article presents an elementary analysis of Nesterov's algorithm that parallels that of steepest descent. It is envisioned that this presentation of Nesterov's algorithm could easily be covered in a few lectures following the introductory material on convex functions and steepest descent included in every course on optimization.},
	pages = {539--562},
	number = {2},
	journaltitle = {{SIAM} Review},
	author = {Noel, Walkington},
	urldate = {2023-10-09},
	langid = {english},
	doi = {10.1137/21M1390037},
	file = {Nesterov's Method for Convex Optimization.pdf:/Users/hongdali/Zotero/storage/BZC6TFHX/Nesterov's Method for Convex Optimization.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/M8MDL6E3/21M1390037.html:text/html},
}

@article{chambolle_introduction_2016,
	title = {An introduction to continuous optimization for imaging},
	volume = {25},
	url = {https://hal.science/hal-01346507},
	doi = {10.1017/S096249291600009X},
	series = {Acta Numerica},
	abstract = {A large number of imaging problems reduce to the optimization of a cost function , with typical structural properties. The aim of this paper is to describe the state of the art in continuous optimization methods for such problems, and present the most successful approaches and their interconnections. We place particular emphasis on optimal first-order schemes that can deal with typical non-smooth and large-scale objective functions used in imaging problems. We illustrate and compare the different algorithms using classical non-smooth problems in imaging, such as denoising and deblurring. Moreover, we present applications of the algorithms to more advanced problems, such as magnetic resonance imaging, multilabel image segmentation, optical flow estimation, stereo matching, and classification.},
	pages = {161--319},
	journaltitle = {Acta Numerica},
	author = {Chambolle, Antonin and Pock, Thomas},
	urldate = {2023-10-19},
	date = {2016},
	note = {Publisher: Cambridge University Press ({CUP})},
	keywords = {convex analysis, Nonsmooth Optimization},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/WYGGWYVC/Chambolle and Pock - 2016 - An introduction to continuous optimization for ima.pdf:application/pdf},
}

@article{bubeck_convex_2015,
	title = {Convex optimization: algorithms and complexity},
	url = {http://arxiv.org/abs/1405.4980},
	doi = {10.48550/arXiv.1405.4980},
	shorttitle = {Convex Optimization},
	abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with {FISTA} (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
	author = {Bubeck, Sébastien},
	urldate = {2023-10-12},
	date = {2015-11-16},
	eprinttype = {arxiv},
	eprint = {1405.4980 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Machine Learning, Numerical Analysis, Optimization and Control, Computer Science - Computational Complexity},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/7JTU9RKT/Bubeck - 2015 - Convex Optimization Algorithms and Complexity.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/8CADXSB9/1405.html:text/html},
}

@article{beck_fast_2009,
	title = {Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems},
	volume = {18},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/document/5173518},
	doi = {10.1109/TIP.2009.2028250},
	abstract = {This paper studies gradient-based schemes for image denoising and deblurring problems based on the discretized total variation ({TV}) minimization model with constraints. We derive a fast algorithm for the constrained {TV}-based image deburring problem. To achieve this task, we combine an acceleration of the well known dual approach to the denoising problem with a novel monotone version of a fast iterative shrinkage/thresholding algorithm ({FISTA}) we have recently introduced. The resulting gradient-based algorithm shares a remarkable simplicity together with a proven global rate of convergence which is significantly better than currently known gradient projections-based methods. Our results are applicable to both the anisotropic and isotropic discretized {TV} functionals. Initial numerical results demonstrate the viability and efficiency of the proposed algorithms on image deblurring problems with box constraints.},
	pages = {2419--2434},
	number = {11},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Beck, Amir and Teboulle, Marc},
	urldate = {2023-10-19},
	date = {2009-11},
	note = {Conference Name: {IEEE} Transactions on Image Processing},
	file = {IEEE Xplore Abstract Record:/Users/hongdali/Zotero/storage/G4ZXMUZG/5173518.html:text/html;IEEE Xplore Full Text PDF:/Users/hongdali/Zotero/storage/J2T9LVVK/Beck and Teboulle - 2009 - Fast Gradient-Based Algorithms for Constrained Tot.pdf:application/pdf},
}

@article{beck_fast_2009-1,
	title = {A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
	volume = {2},
	issn = {1936-4954},
	url = {http://epubs.siam.org/doi/10.1137/080716542},
	doi = {10.1137/080716542},
	abstract = {We consider the class of iterative shrinkage-thresholding algorithms ({ISTA}) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm ({FISTA}) which preserves the computational simplicity of {ISTA} but with a global rate of convergence which is proven to be signiﬁcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of {FISTA} which is shown to be faster than {ISTA} by several orders of magnitude.},
	pages = {183--202},
	number = {1},
	journaltitle = {{SIAM} Journal on Imaging Sciences},
	shortjournal = {{SIAM} J. Imaging Sci.},
	author = {Beck, Amir and Teboulle, Marc},
	urldate = {2023-11-16},
	date = {2009-01},
	langid = {english},
	file = {Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:/Users/hongdali/Zotero/storage/H7CGKLL3/Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:application/pdf},
}

@article{aujol_parameter-free_2023,
	title = {Parameter-Free {FISTA} by adaptive restart and backtracking},
	url = {https://arxiv.org/abs/2307.14323v1},
	abstract = {We consider a combined restarting and adaptive backtracking strategy for the popular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for accelerating the convergence speed of large-scale structured convex optimization problems. Several variants of {FISTA} enjoy a provable linear convergence rate for the function values \$F(x\_n)\$ of the form \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}{\textasciitilde}n\})\$ under the prior knowledge of problem conditioning, i.e. of the ratio between the ({\textbackslash}L ojasiewicz) parameter \${\textbackslash}mu\$ determining the growth of the objective function and the Lipschitz constant \$L\$ of its smooth component. These parameters are nonetheless hard to estimate in many practical cases. Recent works address the problem by estimating either parameter via suitable adaptive strategies. In our work both parameters can be estimated at the same time by means of an algorithmic restarting scheme where, at each restart, a non-monotone estimation of \$L\$ is performed. For this scheme, theoretical convergence results are proved, showing that a \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}n\})\$ convergence speed can still be achieved along with quantitative estimates of the conditioning. The resulting Free-{FISTA} algorithm is therefore parameter-free. Several numerical results are reported to confirm the practical interest of its use in many exemplar problems.},
	journaltitle = {{arXiv}.org},
	author = {Aujol, Jean-François and Calatroni, Luca and Dossal, Charles and Labarrière, Hippolyte and Rondepierre, Aude},
	urldate = {2023-10-09},
	date = {2023-07-26},
	langid = {english},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/3YBXJH4F/Aujol et al. - 2023 - Parameter-Free FISTA by Adaptive Restart and Backt.pdf:application/pdf},
}

@article{attouch_rate_2016,
	title = {The rate of convergence of nesterov's accelerated forward-backward method is actually aaster than \$1/k{\textasciicircum}2\$},
	volume = {26},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/15M1046095},
	doi = {10.1137/15M1046095},
	abstract = {In a Hilbert space \${\textbackslash}mathcal H\$, assuming \$({\textbackslash}alpha\_k)\$ a general sequence of nonnegative numbers, we analyze the convergence properties of the inertial forward-backward algorithm \$({IFB}){\textbackslash}\{{\textbackslash}begin\{array\}\{l\}  y\_k=x\_k+{\textbackslash}alpha\_k(x\_k-x\_\{k-1\}), x\_\{k+1\}=\{{\textbackslash}rm prox\}\_\{s{\textbackslash}Psi\}(y\_k-s{\textbackslash}nabla {\textbackslash}Phi(y\_k)) {\textbackslash}end\{array\},\$ where \${\textbackslash}Psi: {\textbackslash}mathcal H {\textbackslash}to {\textbackslash}mathbb R {\textbackslash}cup {\textbackslash}lbrace + {\textbackslash}infty {\textbackslash}rbrace \$ is a proper lower-semicontinuous convex function, and \${\textbackslash}Phi: {\textbackslash}mathcal H {\textbackslash}to {\textbackslash}mathbb R\$ is a differentiable convex function, whose gradient is Lipschitz continuous. Various options for the sequence \$({\textbackslash}alpha\_k)\$ are considered in the literature. Among them, the Nesterov choice leads to the {FISTA} algorithm and accelerates convergence from \${\textbackslash}mathcal\{O\}(1/k)\$ to \${\textbackslash}mathcal\{O\}(1/k{\textasciicircum}2)\$ for the values. Several variants are used to guarantee the convergence of the iterates or to improve the rate of convergence for the values. For the design of fast optimization methods, the tuning of the sequence \$({\textbackslash}alpha\_k)\$ is a subtle issue, which we deal with in this paper in general. We show that the convergence rate of the algorithm can be obtained simply by analyzing the sequence of positive real numbers \$({\textbackslash}alpha\_k)\$. In addition to the case \${\textbackslash}alpha\_k= 1 -{\textbackslash}frac\{{\textbackslash}alpha\}\{k\} \$ with \${\textbackslash}alpha{\textbackslash}geq 3\$, our results apply equally well to \${\textbackslash}alpha\_k = 1- {\textbackslash}frac\{{\textbackslash}alpha\}\{k{\textasciicircum}r\}\$, with an exponent \$0{\textless}r{\textless}1\$, and to Polyak's heavy ball method. Thus, we unify most of the existing results based on the accelerated gradient method of Nesterov. In the process, we improve some of them and discover new ones.},
	pages = {1824--1834},
	number = {3},
	journaltitle = {{SIAM} Journal on Optimization},
	shortjournal = {{SIAM} J. Optim.},
	author = {Attouch, Hedy and Peypouquet, Juan},
	urldate = {2023-10-11},
	date = {2016-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Submitted Version:/Users/hongdali/Zotero/storage/VIWDS354/Attouch and Peypouquet - 2016 - The Rate of Convergence of Nesterov's Accelerated .pdf:application/pdf},
}

@article{an_enhanced_2023,
	title = {Enhanced total variation minimization for stable image reconstruction},
	volume = {39},
	issn = {0266-5611, 1361-6420},
	url = {http://arxiv.org/abs/2201.02979},
	doi = {10.1088/1361-6420/acd4e1},
	abstract = {The total variation ({TV}) regularization has phenomenally boosted various variational models for image processing tasks. We propose to combine the backward diffusion process in the earlier literature of image enhancement with the {TV} regularization, and show that the resulting enhanced {TV} minimization model is particularly effective for reducing the loss of contrast. The main purpose of this paper is to establish stable reconstruction guarantees for the enhanced {TV} model from noisy subsampled measurements with two sampling strategies, non-adaptive sampling for general linear measurements and variable-density sampling for Fourier measurements. In particular, under some weaker restricted isometry property conditions, the enhanced {TV} minimization model is shown to have tighter reconstruction error bounds than various {TV}-based models for the scenario where the level of noise is significant and the amount of measurements is limited. Advantages of the enhanced {TV} model are also numerically validated by preliminary experiments on the reconstruction of some synthetic, natural, and medical images.},
	pages = {075005},
	number = {7},
	journaltitle = {Inverse Problems},
	shortjournal = {Inverse Problems},
	author = {An, Congpei and Wu, Hao-Ning and Yuan, Xiaoming},
	urldate = {2023-11-16},
	date = {2023-07-01},
	eprinttype = {arxiv},
	eprint = {2201.02979 [cs, eess, math]},
	keywords = {Numerical Analysis, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/YH2I69YG/An et al. - 2023 - Enhanced total variation minimization for stable i.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/46QKSWIM/2201.html:text/html},
}

@article{an_springback_2022,
	title = {The springback penalty for robust signal recovery},
	volume = {61},
	issn = {10635203},
	url = {http://arxiv.org/abs/2110.06754},
	doi = {10.1016/j.acha.2022.07.002},
	abstract = {We propose a new penalty, the springback penalty, for constructing models to recover an unknown signal from incomplete and inaccurate measurements. Mathematically, the springback penalty is a weakly convex function. It bears various theoretical and computational advantages of both the benchmark convex \${\textbackslash}ell\_1\$ penalty and many of its non-convex surrogates that have been well studied in the literature. We establish the exact and stable recovery theory for the recovery model using the springback penalty for both sparse and nearly sparse signals, respectively, and derive an easily implementable difference-of-convex algorithm. In particular, we show its theoretical superiority to some existing models with a sharper recovery bound for some scenarios where the level of measurement noise is large or the amount of measurements is limited. We also demonstrate its numerical robustness regardless of the varying coherence of the sensing matrix. The springback penalty is particularly favorable for the scenario where the incomplete and inaccurate measurements are collected by coherence-hidden or -static sensing hardware due to its theoretical guarantee of recovery with severe measurements, computational tractability, and numerical robustness for ill-conditioned sensing matrices.},
	pages = {319--346},
	journaltitle = {Applied and Computational Harmonic Analysis},
	shortjournal = {Applied and Computational Harmonic Analysis},
	author = {An, Congpei and Wu, Hao-Ning and Yuan, Xiaoming},
	urldate = {2023-11-16},
	date = {2022-11},
	eprinttype = {arxiv},
	eprint = {2110.06754 [cs, math]},
	keywords = {Numerical Analysis, Optimization and Control, Computer Science - Information Theory},
	file = {arXiv.org Snapshot:/Users/hongdali/Zotero/storage/I2NI3ARM/2110.html:text/html;Full Text PDF:/Users/hongdali/Zotero/storage/PGAKYHJW/An et al. - 2022 - The springback penalty for robust signal recovery.pdf:application/pdf},
}

@misc{necoara_linear_2015,
	title = {Linear convergence of first order methods for Non-strongly convex optimization (draft)},
	url = {https://arxiv.org/abs/1504.06298v4},
	abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
	author = {Necoara, I. and Nesterov, Yu and Glineur, F.},
	urldate = {2023-10-09},
	date = {2015-04-23},
	langid = {english},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/5Q6738AN/Necoara et al. - 2015 - Linear convergence of first order methods for non-.pdf:application/pdf},
}

@article{aujol_fista_2022,
	title = {{FISTA} restart rsing an automatic estimation of the growth parameter},
	url = {https://hal.science/hal-03153525},
	abstract = {In this paper, we propose a restart scheme for {FISTA} (Fast Iterative Shrinking-Threshold Algorithm). This method which is a generalization of Nesterov's accelerated gradient algorithm is widely used in the field of large convex optimization problems and it provides fast convergence results under a strong convexity assumption. These convergence rates can be extended for weaker hypotheses such as the {\textbackslash}L\{\}ojasiewicz property but it requires prior knowledge on the function of interest. In particular, most of the schemes providing a fast convergence for non-strongly convex functions satisfying a quadratic growth condition involve the growth parameter which is generally not known. Recent works show that restarting {FISTA} could ensure a fast convergence for this class of functions without requiring any knowledge on the growth parameter. We improve these restart schemes by providing a better asymptotical convergence rate and by requiring a lower computation cost. We present numerical results emphasizing the efficiency of this method.},
	author = {Aujol, Jean-François and Dossal, Charles H and Labarrière, Hippolyte and Rondepierre, Aude},
	urldate = {2023-10-09},
	date = {2022-05},
	file = {HAL PDF Full Text:/Users/hongdali/Zotero/storage/VQV7T5EP/Aujol et al. - 2022 - FISTA restart using an automatic estimation of the.pdf:application/pdf},
}

@article{aujol_fista_2023,
	title = {{FISTA} is an automatic geometrically optimized algorithm for strongly convex functions},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-023-01960-6},
	doi = {10.1007/s10107-023-01960-6},
	abstract = {In this work, we are interested in the famous {FISTA} algorithm. We show that {FISTA} is an automatic geometrically optimized algorithm for functions satisfying a quadratic growth assumption. This explains why {FISTA} works better than the standard Forward-Backward algorithm ({FB}) in such a case, although {FISTA} is known to have a polynomial asymptotic convergence rate while {FB} is exponential. We provide a simple rule to tune the \$\${\textbackslash}alpha \$\$parameter within the {FISTA} algorithm to reach an \$\${\textbackslash}varepsilon \$\$-solution with an optimal number of iterations. These new results highlight the efficiency of {FISTA} algorithms, and they rely on new non asymptotic bounds for {FISTA}.},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Aujol, J.-F. and Dossal, Ch. and Rondepierre, A.},
	urldate = {2023-10-09},
	date = {2023-04-08},
	langid = {english},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/JP7BSENL/Aujol et al. - 2023 - FISTA is an automatic geometrically optimized algo.pdf:application/pdf},
}

@article{calatroni_backtracking_2019,
	title = {Backtracking strategies for accelerated descent methods with smooth composite objectives},
	volume = {29},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/17M1149390},
	doi = {10.1137/17M1149390},
	abstract = {Motivated by big data applications, first-order methods have been extremely popular in recent years. However, naive gradient methods generally converge slowly. Hence, much effort has been made to accelerate various first-order methods. This paper proposes two accelerated methods towards solving structured linearly constrained convex programming, for which we assume composite convex objective that is the sum of a differentiable function and a possibly nondifferentiable one. The first method is the accelerated linearized augmented Lagrangian method ({LALM}). At each update to the primal variable, it allows linearization to the differentiable function and also the augmented term, and thus it enables easy subproblems. Assuming merely convexity, we show that {LALM} owns \$O(1/t)\$ convergence if parameters are kept fixed during all the iterations and can be accelerated to  \$O(1/t{\textasciicircum}2)\$ if the parameters are adapted, where \$t\$ is the number of total iterations. The second method is the accelerated linearized alternating direction method of multipliers ({LADMM}). In addition to the composite convexity, it further assumes two-block structure on the objective. Different from classic alternating direction method of multipliers, our method allows linearization to the objective and also augmented term to make the update simple. Assuming strong convexity on one block variable, we show that {LADMM} also enjoys \$O(1/t{\textasciicircum}2)\$ convergence with adaptive parameters.  This result is a significant improvement over that in [Goldstein et. al, {SIAM} J. Imag. Sci., 7 (2014), pp. 1588--1623], which requires strong convexity on both block variables and no linearization to the objective or augmented term. Numerical experiments are performed on quadratic programming, image denoising, and support vector machine. The proposed accelerated methods are compared to nonaccelerated ones and also existing accelerated methods. The results demonstrate the validity of acceleration and superior performance of the proposed methods over existing ones.},
	pages = {1772--1798},
	number = {3},
	journaltitle = {{SIAM} Journal on Optimization},
	shortjournal = {{SIAM} J. Optim.},
	author = {Calatroni, Luca and Chambolle, Antonin},
	urldate = {2023-10-19},
	date = {2019-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Submitted Version:/Users/hongdali/Zotero/storage/PA25ST8M/Calatroni and Chambolle - 2019 - Backtracking Strategies for Accelerated Descent Me.pdf:application/pdf},
}

@article{fercoq_adaptive_2019,
	title = {Adaptive restart of accelerated gradient methods under local quadratic growth condition},
	volume = {39},
	issn = {0272-4979, 1464-3642},
	url = {http://arxiv.org/abs/1709.02300},
	doi = {10.1093/imanum/drz007},
	abstract = {By analyzing accelerated proximal gradient methods under a local quadratic growth condition, we show that restarting these algorithms at any frequency gives a globally linearly convergent algorithm. This result was previously known only for long enough frequencies. Then, as the rate of convergence depends on the match between the frequency and the quadratic error bound, we design a scheme to automatically adapt the frequency of restart from the observed decrease of the norm of the gradient mapping. Our algorithm has a better theoretical bound than previously proposed methods for the adaptation to the quadratic error bound of the objective. We illustrate the efficiency of the algorithm on a Lasso problem and on a regularized logistic regression problem.},
	pages = {2069--2095},
	number = {4},
	journaltitle = {{IMA} Journal of Numerical Analysis},
	author = {Fercoq, Olivier and Qu, Zheng},
	urldate = {2023-11-08},
	date = {2019-10-16},
	eprinttype = {arxiv},
	eprint = {1709.02300 [math]},
	keywords = {Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/83L7V5YY/Fercoq and Qu - 2019 - Adaptive restart of accelerated gradient methods u.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/26N2YTES/1709.html:text/html},
}

@article{dragomir_optimal_2022,
	title = {Optimal complexity and certification of Bregman first-order methods},
	volume = {194},
	issn = {0025-5610, 1436-4646},
	url = {https://link.springer.com/10.1007/s10107-021-01618-1},
	doi = {10.1007/s10107-021-01618-1},
	abstract = {We provide a lower bound showing that the O(1/k) convergence rate of the {NoLips} method (a.k.a. Bregman Gradient or Mirror Descent) is optimal for the class of problems satisfying the relative smoothness assumption. This assumption appeared in the recent developments around the Bregman Gradient method, where acceleration remained an open issue.},
	pages = {41--83},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Dragomir, Radu-Alexandru and Taylor, Adrien B. and d’Aspremont, Alexandre and Bolte, Jérôme},
	urldate = {2023-12-02},
	date = {2022-07},
	langid = {english},
	file = {Dragomir et al. - 2022 - Optimal complexity and certification of Bregman first-order methods.pdf:/Users/hongdali/Zotero/storage/4UA78G9T/Dragomir et al. - 2022 - Optimal complexity and certification of Bregman first-order methods.pdf:application/pdf},
}

@article{bezanson_julia_2017,
	title = {Julia: A Fresh Approach to Numerical Computing},
	volume = {59},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/10.1137/141000671},
	doi = {10.1137/141000671},
	shorttitle = {Julia},
	abstract = {This is the third in a series of papers on aspects of modern computing environments that are relevant to statistical data analysis. In this paper, we discuss programming environments. In particular, we argue that integrated programming environments (for example, Lisp and Smalltalk environments) are more appropriate as a base for data analysis than conventional operating systems (for example, Unix).},
	pages = {65--98},
	number = {1},
	journaltitle = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
	urldate = {2023-11-20},
	date = {2017-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Full Text:/Users/hongdali/Zotero/storage/UTXT9Y9J/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:application/pdf},
}

@book{nesterov_introductory_2004,
	location = {Boston, {MA}},
	title = {Introductory Lectures on Convex Optimization},
	volume = {87},
	isbn = {978-1-4613-4691-3 978-1-4419-8853-9},
	url = {http://link.springer.com/10.1007/978-1-4419-8853-9},
	series = {Applied Optimization},
	publisher = {Springer {US}},
	author = {Nesterov, Yurii},
	editorb = {Pardalos, Panos M. and Hearn, Donald W.},
	editorbtype = {redactor},
	urldate = {2023-11-19},
	date = {2004},
	doi = {10.1007/978-1-4419-8853-9},
	keywords = {Optimization, complexity theory},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/TS6388FE/Nesterov - 2004 - Introductory Lectures on Convex Optimization.pdf:application/pdf},
}

@article{chambolle_convergence_2015,
	title = {On the Convergence of the Iterates of the “Fast Iterative Shrinkage/Thresholding Algorithm”},
	volume = {166},
	issn = {1573-2878},
	url = {https://doi.org/10.1007/s10957-015-0746-4},
	doi = {10.1007/s10957-015-0746-4},
	abstract = {We discuss here the convergence of the iterates of the “Fast Iterative Shrinkage/Thresholding Algorithm,” which is an algorithm proposed by Beck and Teboulle for minimizing the sum of two convex, lower-semicontinuous, and proper functions (defined in a Euclidean or Hilbert space), such that one is differentiable with Lipschitz gradient, and the proximity operator of the second is easy to compute. It builds a sequence of iterates for which the objective is controlled, up to a (nearly optimal) constant, by the inverse of the square of the iteration number. However, the convergence of the iterates themselves is not known. We show here that with a small modification, we can ensure the same upper bound for the decay of the energy, as well as the convergence of the iterates to a minimizer.},
	pages = {968--982},
	number = {3},
	journaltitle = {Journal of Optimization Theory and Applications},
	shortjournal = {J Optim Theory Appl},
	author = {Chambolle, A. and Dossal, Ch.},
	urldate = {2023-11-18},
	date = {2015-09-01},
	langid = {english},
	keywords = {Optimization, First-order Methods, Heavy Ball Momentum, Forward–backward Splitting},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/P7LSJUWM/Chambolle and Dossal - 2015 - On the Convergence of the Iterates of the “Fast It.pdf:application/pdf},
}

@misc{patrinos_douglas-rachford_2014,
	title = {Douglas-Rachford Splitting: Complexity Estimates and Accelerated Variants},
	url = {http://arxiv.org/abs/1407.6723},
	shorttitle = {Douglas-Rachford Splitting},
	abstract = {We propose a new approach for analyzing convergence of the Douglas-Rachford splitting method for solving convex composite optimization problems. The approach is based on a continuously differentiable function, the Douglas-Rachford Envelope ({DRE}), whose stationary points correspond to the solutions of the original (possibly nonsmooth) problem. By proving the equivalence between the Douglas-Rachford splitting method and a scaled gradient method applied to the {DRE}, results from smooth unconstrained optimization are employed to analyze convergence properties of {DRS}, to tune the method and to derive an accelerated version of it.},
	number = {{arXiv}:1407.6723},
	publisher = {{arXiv}},
	author = {Patrinos, Panagiotis and Stella, Lorenzo and Bemporad, Alberto},
	urldate = {2023-11-18},
	date = {2014-09-20},
	eprinttype = {arxiv},
	eprint = {1407.6723 [math]},
	keywords = {Optimization and Control},
	file = {arXiv.org Snapshot:/Users/hongdali/Zotero/storage/54N82FCP/1407.html:text/html;Full Text PDF:/Users/hongdali/Zotero/storage/YGBFNQAB/Patrinos et al. - 2014 - Douglas-Rachford Splitting Complexity Estimates a.pdf:application/pdf},
}

@unpublished{ang_heavy_nodate,
	location = {Mathe  ́matique et recherche op ́erationnelle {UMONS}, Belgium},
	title = {Heavy Ball Method on Quadratic Problems},
	url = {https://angms.science/doc/CVX/CVX_HBM.pdf},
	author = {Ang, Andersen},
	langid = {english},
	keywords = {Lecture Notes},
	file = {Ang - Math´ematique et recherche op´erationnelle UMONS, .pdf:/Users/hongdali/Zotero/storage/YU63848N/Ang - Math´ematique et recherche op´erationnelle UMONS, .pdf:application/pdf},
}

@article{necoara_linear_2019,
	title = {Linear convergence of first order methods for non-strongly convex optimization},
	volume = {175},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-018-1232-1},
	doi = {10.1007/s10107-018-1232-1},
	abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
	pages = {69--107},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Necoara, I. and Nesterov, Yu. and Glineur, F.},
	urldate = {2023-10-11},
	date = {2019-05-01},
	langid = {english},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/7X79PGLC/Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:application/pdf},
}

@article{aujol_convergence_2022,
	title = {Convergence rates of the Heavy-Ball method for quasi-strongly convex optimization},
	url = {https://hal.science/hal-02545245/document},
	doi = {10.1137/21M1403990},
	abstract = {In this paper, we discuss the convex optimization problem over the fixed point set of a nonexpansive mapping. The main objective of the paper is to accelerate the hybrid steepest descent method for the problem. To this goal, we present a new iterative scheme that utilizes the conjugate gradient direction. Its convergence to the solution is guaranteed under certain assumptions. In order to demonstrate the effectiveness, performance, and convergence of our proposed algorithm, we present numerical comparisons of the algorithm with the existing algorithm.},
	journaltitle = {{HAL} open science},
	author = {Aujol, J.-F. and Dossal, Ch. and Rondepierre, A.},
	urldate = {2023-10-11},
	date = {2022-09-30},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {Heavy Ball Momentum, Lyapunov Function, {ODEs}},
	file = {aujol-et-al-2022-convergence-rates-of-the-heavy-ball-method-for-quasi-strongly-convex-optimization.pdf:/Users/hongdali/Zotero/storage/EGIRXZMP/aujol-et-al-2022-convergence-rates-of-the-heavy-ball-method-for-quasi-strongly-convex-optimization.pdf:application/pdf;Submitted Version:/Users/hongdali/Zotero/storage/LFFE3WXB/Aujol et al. - 2022 - Convergence Rates of the Heavy Ball Method for Qua.pdf:application/pdf},
}

@inproceedings{lin_universal_2015,
	title = {A Universal Catalyst for First-Order Optimization},
	url = {https://inria.hal.science/hal-01160728},
	abstract = {We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, {SAG}, {SAGA}, {SDCA}, {SVRG}, Finito/{MISO}, and their proximal variants. For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill conditioned problems where we measure significant improvements.},
	eventtitle = {{NIPS} - Advances in Neural Information Processing Systems},
	pages = {3384},
	publisher = {{MIT} Press},
	author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
	urldate = {2024-05-31},
	date = {2015-12-07},
	langid = {english},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/8ILQU6TZ/Lin et al. - 2015 - A Universal Catalyst for First-Order Optimization.pdf:application/pdf},
}
