
@article{calatroni_backtracking_2019,
	title = {Backtracking strategies for accelerated descent methods with smooth composite objectives},
	volume = {29},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/17M1149390},
	doi = {10.1137/17M1149390},
	abstract = {Motivated by big data applications, first-order methods have been extremely popular in recent years. However, naive gradient methods generally converge slowly. Hence, much effort has been made to accelerate various first-order methods. This paper proposes two accelerated methods towards solving structured linearly constrained convex programming, for which we assume composite convex objective that is the sum of a differentiable function and a possibly nondifferentiable one. The first method is the accelerated linearized augmented Lagrangian method (LALM). At each update to the primal variable, it allows linearization to the differentiable function and also the augmented term, and thus it enables easy subproblems. Assuming merely convexity, we show that LALM owns \$O(1/t)\$ convergence if parameters are kept fixed during all the iterations and can be accelerated to  \$O(1/t{\textasciicircum}2)\$ if the parameters are adapted, where \$t\$ is the number of total iterations. The second method is the accelerated linearized alternating direction method of multipliers (LADMM). In addition to the composite convexity, it further assumes two-block structure on the objective. Different from classic alternating direction method of multipliers, our method allows linearization to the objective and also augmented term to make the update simple. Assuming strong convexity on one block variable, we show that LADMM also enjoys \$O(1/t{\textasciicircum}2)\$ convergence with adaptive parameters.  This result is a significant improvement over that in [Goldstein et. al, SIAM J. Imag. Sci., 7 (2014), pp. 1588--1623], which requires strong convexity on both block variables and no linearization to the objective or augmented term. Numerical experiments are performed on quadratic programming, image denoising, and support vector machine. The proposed accelerated methods are compared to nonaccelerated ones and also existing accelerated methods. The results demonstrate the validity of acceleration and superior performance of the proposed methods over existing ones.},
	number = {3},
	urldate = {2023-10-19},
	journal = {SIAM Journal on Optimization},
	author = {Calatroni, Luca and Chambolle, Antonin},
	month = jan,
	year = {2019},
	pages = {1772--1798},
	file = {Backtracking strategies for accelerated descent methods with smooth composite objectives, SIAM:/Volumes/T6/Zotero Library/storage/RKZLQL7H/17m1149390.pdf:application/pdf},
}

@misc{alamo_restart_2019,
	title = {Restart {FISTA} with global linear convergence},
	url = {http://arxiv.org/abs/1906.09126},
	doi = {10.48550/arXiv.1906.09126},
	abstract = {Fast Iterative Shrinking-Threshold Algorithm (FISTA) is a popular fast gradient descent method (FGM) in the field of large scale convex optimization problems. However, it can exhibit undesirable periodic oscillatory behaviour in some applications that slows its convergence. Restart schemes seek to improve the convergence of FGM algorithms by suppressing the oscillatory behaviour. Recently, a restart scheme for FGM has been proposed that provides linear convergence for non strongly convex optimization problems that satisfy a quadratic functional growth condition. However, the proposed algorithm requires prior knowledge of the optimal value of the objective function or of the quadratic functional growth parameter. In this paper we present a restart scheme for FISTA algorithm, with global linear convergence, for non strongly convex optimization problems that satisfy the quadratic growth condition without requiring the aforementioned values. We present some numerical simulations that suggest that the proposed approach outperforms other restart FISTA schemes.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Alamo, Teodoro and Krupa, Pablo and Limon, Daniel},
	month = dec,
	year = {2019},
	keywords = {Optimization and Control},
	file = {arXiv Fulltext PDF:/Volumes/T6/Zotero Library/storage/B79AUCM7/Alamo et al. - 2019 - Restart FISTA with Global Linear Convergence.pdf:application/pdf;arXiv.org Snapshot:/Volumes/T6/Zotero Library/storage/LZ7ISHFW/1906.html:text/html},
}

@book{bauschke_convex_2017,
	address = {Cham},
	series = {{CMS} {Books} in {Mathematics}},
	title = {Convex {Analysis} and {Monotone} {Operator} {Theory} in {Hilbert} {Spaces}},
	isbn = {978-3-319-48310-8 978-3-319-48311-5},
	url = {https://link.springer.com/10.1007/978-3-319-48311-5},
	language = {en},
	urldate = {2023-11-29},
	publisher = {Springer International Publishing},
	author = {Bauschke, Heinz H. and Combettes, Patrick L.},
	year = {2017},
	keywords = {Convex analysis, Monotone Operator, Nonexpansive Operator, Operator Splitting, Proximal Algorithm},
	file = {Bauschke and Combettes - 2017 - Convex Analysis and Monotone Operator Theory in Hi.pdf:/Volumes/T6/Zotero Library/storage/57IWCTZJ/Bauschke and Combettes - 2017 - Convex Analysis and Monotone Operator Theory in Hi.pdf:application/pdf},
}

@article{guler_new_1992,
	title = {New proximal point algorithms for convex minimization},
	volume = {2},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/0802032},
	doi = {10.1137/0802032},
	abstract = {The proximal point algorithm (PPA) for the convex minimization problem minx∈Hf(x), where f:H→R∪\{∞\} is a proper, lower semicontinuous (lsc) function in a Hilbert space H is considered. Under this minimal assumption on f, it is proved that the PPA, with positive parameters \{λk\}k=1∞, converges in general if and only if σn=∑k=1nλk→∞. Global convergence rate estimates for the residual f(xn)−f(u), where xn is the nth iterate of the PPA and u∈H is arbitrary are given. An open question of Rockafellar is settled by giving an example of a PPA for which xn converges weakly but not strongly to a minimizes of f.},
	number = {4},
	urldate = {2023-11-30},
	journal = {SIAM Journal on Optimization},
	author = {Güler, Osman},
	month = nov,
	year = {1992},
	pages = {649--664},
	file = {Güler - 1992 - New Proximal Point Algorithms for Convex Minimization.pdf:/Volumes/T6/Zotero Library/storage/G9LCY67Q/Güler - 1992 - New Proximal Point Algorithms for Convex Minimization.pdf:application/pdf},
}

@article{aujol_parameter-free_2024,
	title = {Parameter-{Free} {FISTA} by adaptive restart and backtracking},
	url = {https://epubs.siam.org/doi/10.1137/23M158961X},
	abstract = {We consider a combined restarting and adaptive backtracking strategy for the popular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for accelerating the convergence speed of large-scale structured convex optimization problems. Several variants of FISTA enjoy a provable linear convergence rate for the function values \$F(x\_n)\$ of the form \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}{\textasciitilde}n\})\$ under the prior knowledge of problem conditioning, i.e. of the ratio between the ({\textbackslash}L ojasiewicz) parameter \${\textbackslash}mu\$ determining the growth of the objective function and the Lipschitz constant \$L\$ of its smooth component. These parameters are nonetheless hard to estimate in many practical cases. Recent works address the problem by estimating either parameter via suitable adaptive strategies. In our work both parameters can be estimated at the same time by means of an algorithmic restarting scheme where, at each restart, a non-monotone estimation of \$L\$ is performed. For this scheme, theoretical convergence results are proved, showing that a \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}n\})\$ convergence speed can still be achieved along with quantitative estimates of the conditioning. The resulting Free-FISTA algorithm is therefore parameter-free. Several numerical results are reported to confirm the practical interest of its use in many exemplar problems.},
	language = {en},
	urldate = {2023-10-09},
	journal = {SIAM Journal on Optimization},
	author = {Aujol, Jean-François and Calatroni, Luca and Dossal, Charles and Labarrière, Hippolyte and Rondepierre, Aude},
	month = dec,
	year = {2024},
	pages = {3259--3285},
	file = {Full Text PDF:/Volumes/T6/Zotero Library/storage/3YBXJH4F/Aujol et al. - 2023 - Parameter-Free FISTA by Adaptive Restart and Backt.pdf:application/pdf},
}

@article{beck_fast_2009,
	title = {A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
	volume = {2},
	issn = {1936-4954},
	url = {http://epubs.siam.org/doi/10.1137/080716542},
	doi = {10.1137/080716542},
	abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be signiﬁcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
	language = {en},
	number = {1},
	urldate = {2023-11-16},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Beck, Amir and Teboulle, Marc},
	month = jan,
	year = {2009},
	pages = {183--202},
	file = {Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:/Volumes/T6/Zotero Library/storage/H7CGKLL3/Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:application/pdf},
}

@article{beck_fast_2009-1,
	title = {Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems},
	volume = {18},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/document/5173518},
	doi = {10.1109/TIP.2009.2028250},
	abstract = {This paper studies gradient-based schemes for image denoising and deblurring problems based on the discretized total variation (TV) minimization model with constraints. We derive a fast algorithm for the constrained TV-based image deburring problem. To achieve this task, we combine an acceleration of the well known dual approach to the denoising problem with a novel monotone version of a fast iterative shrinkage/thresholding algorithm (FISTA) we have recently introduced. The resulting gradient-based algorithm shares a remarkable simplicity together with a proven global rate of convergence which is significantly better than currently known gradient projections-based methods. Our results are applicable to both the anisotropic and isotropic discretized TV functionals. Initial numerical results demonstrate the viability and efficiency of the proposed algorithms on image deblurring problems with box constraints.},
	number = {11},
	urldate = {2023-10-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Beck, Amir and Teboulle, Marc},
	month = nov,
	year = {2009},
	pages = {2419--2434},
	file = {Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems:/Volumes/T6/Zotero Library/storage/J2T9LVVK/Beck and Teboulle - 2009 - Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems.pdf:application/pdf;IEEE Xplore Abstract Record:/Volumes/T6/Zotero Library/storage/G4ZXMUZG/5173518.html:text/html},
}

@article{necoara_linear_2019,
	title = {Linear convergence of first order methods for non-strongly convex optimization},
	volume = {175},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-018-1232-1},
	doi = {10.1007/s10107-018-1232-1},
	abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
	language = {en},
	number = {1},
	urldate = {2023-10-11},
	journal = {Mathematical Programming},
	author = {Necoara, I. and Nesterov, Yu. and Glineur, F.},
	month = may,
	year = {2019},
	pages = {69--107},
	file = {Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:/Volumes/T6/Zotero Library/storage/7X79PGLC/Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:application/pdf},
}

@book{nesterov_lectures_2018,
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	volume = {137},
	isbn = {978-3-319-91577-7 978-3-319-91578-4},
	url = {http://link.springer.com/10.1007/978-3-319-91578-4},
	urldate = {2023-10-11},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	keywords = {Optimization, Fast Gradient Methods, Algorithmic Complexity, Interior-Point Methods, Numerical Optimization, Optimization in Relative Scale, Self-Concordant Functions, Smoothing Technique},
	file = {Nesterov - 2018 - Lectures on Convex Optimization.pdf:/Volumes/T6/Zotero Library/storage/HSCCPYL9/Nesterov - 2018 - Lectures on Convex Optimization.pdf:application/pdf},
}

@book{beck_first-order_2017,
	series = {{MOS}-{SIAM} {Series} in {Optimization}},
	title = {First-order {Methods} in {Optimization}},
	isbn = {978-1-61197-498-0},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974997},
	language = {en},
	urldate = {2023-10-19},
	publisher = {SIAM},
	author = {Beck, Amir},
	year = {2017},
	keywords = {Optimization, First-order Methods, Numerical Optimization, Non-smooth Optimization},
	file = {First-Order Methods in Optimization  SIAM Publication.pdf:/Volumes/T6/Zotero Library/storage/P2HFAVVQ/First-Order Methods in Optimization  SIAM Publication.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/88BHKZ6Y/1.html:text/html},
}

@misc{drusvyatskiy_error_2016,
	title = {Error bounds, quadratic growth, and linear convergence of proximal methods},
	url = {http://arxiv.org/abs/1602.06661},
	doi = {10.48550/arXiv.1602.06661},
	abstract = {The proximal gradient algorithm for minimizing the sum of a smooth and a nonsmooth convex function often converges linearly even without strong convexity. One common reason is that a multiple of the step length at each iteration may linearly bound the "error" -- the distance to the solution set. We explain the observed linear convergence intuitively by proving the equivalence of such an error bound to a natural quadratic growth condition. Our approach generalizes to linear convergence analysis for proximal methods (of Gauss-Newton type) for minimizing compositions of nonsmooth functions with smooth mappings. We observe incidentally that short step-lengths in the algorithm indicate near-stationarity, suggesting a reliable termination criterion.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Drusvyatskiy, Dmitriy and Lewis, Adrian S.},
	month = jun,
	year = {2016},
	note = {arXiv:1602.06661 [math]},
	keywords = {Mathematics - Optimization and Control, 90C25, 90C31, 90C55, 65K10},
	file = {arXiv Fulltext PDF:/Volumes/T6/Zotero Library/storage/MLYSVWJ6/Drusvyatskiy and Lewis - 2016 - Error bounds, quadratic growth, and linear convergence of proximal methods.pdf:application/pdf;arXiv.org Snapshot:/Volumes/T6/Zotero Library/storage/GK4FBD5X/1602.html:text/html;Snapshot:/Volumes/T6/Zotero Library/storage/5BFJ49ND/1602.html:text/html},
}

@article{aragon_artacho_characterization_2024,
	title = {Characterization of metric regularity of subdifferentials},
	url = {https://www.researchgate.net/publication/39522449_Characterization_of_Metric_Regularity_of_Subdifferentials},
	abstract = {PDF {\textbar} We study regularity properties of the subdifferential of proper lower semicontinuous convex functions in Hilbert spaces. More precisely, we... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-03-13},
	journal = {Journal of Convex Analysis},
	author = {Aragon Artacho, Francisco and Geoffroy, Michel},
	month = nov,
	year = {2024},
	file = {Characterization of metric regularity of subdifferentials:/Volumes/T6/Zotero Library/storage/2IFNRMAE/2024 - Characterization of metric regularity of subdifferentials.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/EMIW9EDD/39522449_Characterization_of_Metric_Regularity_of_Subdifferentials.html:text/html},
}

@article{guler_approximations_1995,
	title = {Approximations to solutions to systems of linear {Inequalities}},
	volume = {16},
	copyright = {[Copyright] © 1995 Society for Industrial and Applied Mathematics},
	issn = {08954798},
	url = {https://www.proquest.com/docview/923769468/abstract/18DCC1DA6BEB4E83PQ/1},
	doi = {10.1137/S0895479892237744},
	abstract = {In this paper we consider a result of Hoffman [J. Res. Nat. Bur. Stand., 49 (1952) pp. 263-265] about approximate solutions to systems of linear inequalities. We obtain a new representation for a corresponding Lipschitz bound via singular values. We also provide geometric representations of these bounds via extreme points. The latter have been developed independently by Bergthaller and Singer [Linear Algebra Appl., 169 (1992), pp. 111-129] and Li [Linear Algebra Appl., 187 (1993), pp. 15-40], but, our proofs are simpler. We obtain a particularly simple proof of Hoffman's existence result which relies only on linear programming duality.},
	language = {English},
	number = {2},
	urldate = {2025-03-14},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Guler, Osman and Hoffman, Alan J. and Rothblum, Uriel G.},
	month = apr,
	year = {1995},
	note = {Num Pages: 9
Place: Philadelphia, United States
Publisher: Society for Industrial and Applied Mathematics},
	keywords = {Algorithms, Approximation, Linear algebra, Linear programming, Norms},
	pages = {9},
	file = {Approximations to Solutions to Systems of Linear Inequalities:/Volumes/T6/Zotero Library/storage/KQERI5VS/Guler et al. - 1995 - Approximations to Solutions to Systems of Linear Inequalities.pdf:application/pdf},
}

@article{burke_unified_1996,
	title = {A unified analysis of {Hoffman}’s {Bound} via {Fenchel} {Duality}},
	volume = {6},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/0806015},
	doi = {10.1137/0806015},
	abstract = {By considering the epigraphs of conjugate functions, we extend the Fenchel duality, applicable to a (possibly infinite) family of proper lower semicontinuous convex functions on a Banach space. Applications are given in providing fuzzy KKT conditions for semi-infinite programming.},
	number = {2},
	urldate = {2025-06-05},
	journal = {SIAM Journal on Optimization},
	author = {Burke, James V. and Tseng, Paul},
	month = may,
	year = {1996},
	pages = {265--282},
	file = {Unified Analysis of Hoffman’s Bound via Fenchel Duality:/Volumes/T6/Zotero Library/storage/NIZIYGF4/Burke and Tseng - 1996 - A Unified Analysis of Hoffman’s Bound via Fenchel Duality.pdf:application/pdf},
}

@article{hoffman_approximate_1952,
	title = {On approximate solutions of systems of linear inequalities},
	volume = {49},
	issn = {0091-0635},
	url = {https://nvlpubs.nist.gov/nistpubs/jres/049/jresv49n4p263_A1b.pdf},
	doi = {10.6028/jres.049.027},
	language = {en},
	number = {4},
	urldate = {2025-06-16},
	journal = {Journal of Research of the National Bureau of Standards},
	author = {Hoffman, A.J.},
	month = oct,
	year = {1952},
	pages = {263},
	file = {On approximate solutions of systems of linear inequalities.pdf:/Volumes/T6/Zotero Library/storage/XPGZTXML/Hoffman - 1952 - On approximate solutions of systems of linear inequalities.pdf:application/pdf},
}
