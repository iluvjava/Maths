\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
% \input{presets/julia_lstlisting.tex}

\begin{document}

\newcommand{\dist}{\ensuremath{\operatorname{dist}}}

\title{{\fontfamily{ptm}\selectfont Linear Convergence of Accelerated Gradient without Restart}}

\author{
    Author 1 Name, Author 2 Name
    \thanks{
        Subject type, Some Department of Some University, Location of the University,
        Country. E-mail: \texttt{author.name@university.edu}.
    }
}

\date{March 2, 2020}

\maketitle
\todoinline{This paper is currently in draft mode. Check source to change options. \cite{bauschke_convex_2017}}
% \vskip 8mm

\begin{abstract} 
    This paper gives definitive answers to open problems in Necoara et al. \cite{necoara_linear_2019}. 
    Linear converence of Nesterov's accelerated gradient method is possible in a boarder context of functions. 
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}


\section{Introduction}
    \textbf{Notations.}
    Unless specified, our ambient space is $\RR^n$ with Euclidean norm $\Vert \cdot\Vert$.
    Let $C\subseteq \RR^n$, $\Pi_C(\cdot)$ denotes the projection onto the set $C$, i.e: the closest point in $C$ to another point in $\RR^n$. 
    We denote $\delta_C$ to be the indicator function for the set $C$. 
    For a function of $F = f + g$, and a $B\ge 0$ where $f$ is $\mathcal C^1$ differentiable, and $g$ is l.s.c, we consider the proximal gradient operator: 
    \begin{align*}
        T_B(x) &= \argmin_{z} \left\lbrace
            g(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert x - z\Vert^2
        \right\rbrace
        \\
        &= \hprox_{B^{-1}g}(x - B^{-1}\nabla f(x)). 
    \end{align*}
    We also define the gradient mapping operator $\mathcal G_B(x) = B^{-1}(x - T_B(x))$. 

\section{Precursors materials for our proofs of linear convergence}
    The following two definitions defines the accelerated proximal gradient algorithm. 
    % DEFINITION =======================================================================================================
    \begin{definition}[similar triangle form of accelerated proximal gradient]\;\label{def:st-apg}\\
        The definition is about $((\alpha_k)_{k \ge 0}, (q_k)_{k \ge 0}, (B_k)_{k \ge 0}, (y_k)_{k \ge 0}, (x_k)_{k \ge -1}, (v_k)_{k \ge -1})$. 
        These sequences satisfy:
        \begin{enumerate}[nosep]
            \item $x_{-1}, y_{- 1}\in \RR^n$ are arbitrary initial condition of the algorithm;
            \item $(q_k)_{k \ge 1}$ be a sequence such that $q_k \in [0, 1)$ for all $k \ge 1$;
            \item $(\alpha_k)_{k \ge 1}$ be a sequence such that $\alpha_0 \in (0, 1]$, and for all $k \ge 1$ it has $\alpha_k \in (q_k, 1)$;
            \item $(B_k)_{k \ge 0}$ has $B_k \ge 0$, it's a nonnegative sequence. 
        \end{enumerate}
        Then an algorithm satisfies the similar triangle form of Nesterov's accelerated gradient if it generates iterates $(y_k, x_k, v_k)_{k \ge 1}$ such that for all $k\ge 0$: 
        \begin{align*}
            y_k &= \left(\frac{\alpha_k - q_k}{1 - q_k}\right)v_{k - 1} 
            + \left(\frac{1 - \alpha_k}{1 - q_k}\right) x_{k - 1},
            \\
            x_k &= T_{B_k}(y_k), D_f(x_k, y_k) \le \frac{B_k}{2}\Vert x_k - y_k\Vert^2, 
            \\
            v_k &= x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}). 
        \end{align*}
    \end{definition}
    % DEFINITION =======================================================================================================
    \begin{definition}[relaxed momentum sequence]\label{def:rlx-momentum-seq}
        The following definition is about sequences $((\alpha_k)_{k \ge 0}, (q_k)_{k \ge 0}, (\rho_k)_{k \ge 0})$. 
        Let 
        \begin{enumerate}[nosep]
            \item $(q_k)_{k \ge 0}$ is a sequence such that $q_k \in [0, 1)$ for all $k \ge 0$;
            \item $(\alpha_k)_{k \ge 0}$ be such that $\alpha_0 \in (0, 1]$, and for all $k \ge 1$ it has $\alpha_k \in (q_k, 1)$;
            \item $(\rho_k)_{k \ge 0}$ is a strictly positive sequence for all $k \ge 1$. 
        \end{enumerate}
        The sequences $q_k, \alpha_k$ are considered relaxed momentum sequence if for all $k \ge 1$ it satisfies the relation that: 
        \begin{align*}
            \rho_{k - 1} &= \frac{\alpha_k(\alpha_k - q_k)}{(1 - \alpha_{k})\alpha_{k - 1}^2}. 
        \end{align*}
    \end{definition}
    % DEFINITION =======================================================================================================
    \begin{definition}[proximal gradient gap]\label{def:pg-gap}
        Let $F = f + g$ where $f$ is $L$ Lipschitz smooth and $g$ is convex. 
        Then the proximal gradient mapping $T_B(x) = \hprox_{B^{-1}g}(x -  B^{-1}\nabla f(x))$ is a singleton, and $\dom T_B = \RR^n$. 
        Let $\mu, B$ be parameters such that $B > \mu \ge 0$. 
        We define the proximal gradient gap $\mathcal E(z, y, \mu, B)$ is the mapping: 
        \begin{align*}
            \mathcal E(z, y, \mu, B) &:= 
            F(z) - F(T_B(y)) 
            - \left\langle B(y - T_B(y)), z - y\right\rangle
            - \frac{\mu}{2}\Vert z - y\Vert^2
            - \frac{B}{2}\Vert y - T_B(y)\Vert^2. 
        \end{align*}
    \end{definition}
    \begin{remark}
        This expression is the same as the proximal gradient inequality up to a negative sign, after moving everything to one side. 
    \end{remark}
    
\section{Deriving the convergence rate}
    To derive the convergence rate of algorithm satisfying Definition \ref{def:st-apg}, \ref{def:rlx-momentum-seq}, we leverage Definition \ref{def:pg-gap}. 
    The first two subsections prepare for the results and the the convergence results are derived by the end. 
    \par
    The following assumption is about the Proximal Gradient gap, it's required to obtain our first result. 
    % ASSUMPTION =======================================================================================================
    \begin{assumption}[generic assumptions for convergence]\label{ass:for-cnvg}
        The following assumption is about $(F, f, g, \mathcal E, \mu, L)$, it is the configuration needed to derive the convergence rate of algorithms that satisfy Definition \ref{def:st-apg}. 
        We assume that there exists $\mu \ge 0$ such that the followings are true: 
        \begin{enumerate}[nosep]
            \item Let $F = f + g$ where $f$ is $L$ Lipschitz smooth and, $g$ is closed convex and proper.
            \item $\forall y \in \RR^n, \exists B \ge 0\; \exists \bar y$ such that $\mathcal E(\bar y, y, \mu, B) \ge 0$. 
            \item For all $z, y \in \RR^n$, there exists $B > \mu$ such that $\mathcal E(z, y, \mu, B) + \frac{\mu}{2}\Vert z - y\Vert^2 \ge 0$.   
        \end{enumerate}
    \end{assumption}
    \begin{remark}
        Note that:
        \begin{enumerate}
            \item If the function is convex, all conditions are satisfies for $\mu = 0$, and for all $\bar y \in \RR^n$. 
            \item If $\mu \ge 0$ satisfies item (ii), (iii), then it's also satisfies for all $\tilde \mu$ such that $0 \le \tilde \mu \le \mu$. For the best convergence rate, the largest such $\mu$ is of our interest. 
        \end{enumerate}
        
        
    \end{remark}
    % LEMMA ========================================================================================================
    \begin{lemma}[equivalent representations of the iterates part I]\label{lemma:st-iterates-alt-form-part1}
        Suppose that the sequences of $\alpha_k, q_k, y_k, v_k, x_k, B_k$ satisfy the similar triangle form, then for all $k \ge 0$ the iterates $v_k$ admits the following equivalent representations: 
        \begin{align*}
            v_k
            &= v_{k - 1} + \alpha_k^{-1}q_k(y_k - v_{k - 1}) - \alpha_{k}^{-1}B_k^{-1}\mathcal G_{B_k}(y_k). 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Consider all $k \ge 1$. 
        The relations are direct, immediately from the update rule in Definition \ref{def:st-apg} of $y_k$ we have
        \begin{enumerate}[nosep]
            \item[(a)] $(\alpha_k - 1)x_{k - 1} = (\alpha_k - q_k)v_{k - 1} - (1 - q_k)y_k$. 
            \item[(b)] $x_k = y_k - B_k^{-1}\mathcal G_{B_k}(y_k)$. 
        \end{enumerate}
        Using the the above and the update rule for $v_k$ in Definition \ref{def:st-apg}. 
        \begin{align*}
            v_k &= x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1})
            \\
            &= (1 - \alpha_k^{-1})x_{k - 1} + \alpha_k^{-1}x_k
            \\
            &= \alpha_k^{-1}(\alpha_k - 1)x_{k - 1} + \alpha_k^{-1}x_k
            \\
            &\underset{\text{(a)}}{=} \alpha_k^{-1}(\alpha_k - q_k)v_{k - 1} - \alpha_{k}^{-1}(1 - q_k)y_k 
            + \alpha_k^{-1}x_k
            \\
            &\underset{\text{(b)}}{=} (1 - \alpha_k^{-1}q_k) v_{k - 1} - (\alpha_k^{-1} - \alpha_k^{-1}q_k)y_k
            + \alpha_k^{-1}(y_k - B_k^{-1}\mathcal G_{B_k}(y_k)). 
            \\
            &= 
            (1 - \alpha_k^{-1}q_k) v_{k - 1} + \alpha_k^{-1}q_ky_k
            - \alpha_k^{-1}B_k^{-1}\mathcal G_{B_k}(y_k)
            \\
            &= v_{k - 1} + \alpha_k^{-1}q_k(y_k - v_{k - 1}) - \alpha_{k}^{-1}B_k^{-1}\mathcal G_{B_k}(y_k). 
        \end{align*}
    \end{proof}
    \begin{lemma}[equivalent representations of the iterates part II]\;\label{lemma:st-iterates-alt-form-part2}\\
        Suppose the sequences of $\alpha_k, q_k, y_k, v_k, x_k, B_k$ satisfy the similar triangle form, then for all $k \ge 0$ the iterates $v_k$ admits the following equivalent representations: 
        \begin{align*}
            y_k &= 
            x_{k -1} + (1 - q_k)^{-1}(\alpha_{k - 1}^{-1} - 1)(\alpha_k - q_k)(x_{k - 1} - x_{k - 2}). 
        \end{align*}
    \end{lemma}
    \begin{proof}
        For all $k\ge 1$, from the update rules in Definition \ref{def:st-apg}: 
        \begin{align*}
            (1 - q_k)^{-1}y_k &= 
            (\alpha_k - q_k) v_{k - 1} 
            + (1 - \alpha_k)x_{k - 1}
            \\
            &= 
            (\alpha_k - q_k)\left(
                x_{k - 2} + \alpha_{k - 1}^{-1}(x_{k - 1} - x_{k - 2})
            \right) 
            + (1 - \alpha_k)x_{k - 1}
            \\
            &= 
            (\alpha_k - q_k)x_{k - 2} 
            + \alpha_{k - 1}^{-1}(x_{k - 1} - x_{k - 2}) + (1 - \alpha_k)x_{k - 1}
            \\
            &= (\alpha_k - q_k)(1 - \alpha_{k - 1}^{-1})x_{k - 2}
            + \left(
                \frac{\alpha_k - q_k}{\alpha_{k - 1}} + 1 - \alpha_k
            \right)x_{k - 1}. 
        \end{align*}
        Multiply $(1 - q_k)$ on both sides yield: 
        \begin{align*}
            y_k &= 
            \frac{(\alpha_k - q_k)(1 - \alpha_{k - 1}^{-1})}{1 - q_k}x_{k - 2} 
            + \left(
                \frac{\alpha_k - q_k}{\alpha_{k - 1}(1 - q_k)} + \frac{1 - \alpha_k}{1 - q_k}
            \right)x_{k - 1}
            \\
            &= 
            \frac{(\alpha_k - q_k)(1 - \alpha_{k - 1}^{-1})}{1 - q_k}x_{k - 2} 
            + \left(
                \frac{
                    (\alpha_{k - 1}^{-1} -1)(\alpha_k - q_k)
                    + \alpha_k - q_k + 1 - \alpha_k
                }{1 - q_k}
            \right)x_{k - 1}
            \\
            &= \frac{(\alpha_k - q_k)(1 - \alpha_{k - 1}^{-1})}{1 - q_k}x_{k - 2} 
            + \left(
                \frac{
                    (\alpha_{k - 1}^{-1} -1)(\alpha_k - q_k)g 
                }{1 - q_k} + 1
            \right)x_{k - 1}
            \\
            &= x_{k -1} + (1 - q_k)^{-1}\left(\alpha_{k - 1}^{-1} - 1\right)(\alpha_k - q_k)(x_{k - 1} - x_{k - 2}). 
        \end{align*}
    \end{proof}
    
    \subsection{Preparations for the convergence rate proof}
        The following lemma summarize important results that give a swift exposition for the proofs show up at the end for the convergence rate. 
        % LEMMA ========================================================================================================
        \begin{lemma}[convergence preparations part I]\label{lemma:cnvg-prep-part1}
            Let $(F, f, g,  \mathcal E, \mu, L)$ satisfies Assumption \ref{ass:for-cnvg}. 
            Suppose that 
            \begin{enumerate}[nosep]
                \item The iterates $(y_k, v_k, x_k)_{k \ge 0}$ satisfies Definition \ref{def:st-apg} where $T_B$ defined using $F = f + g$. 
                \item The sequences $(\alpha_k)_{k\ge 0}, (\rho_{k})_{k \ge 0}, (q_k)_{k \ge 0}$ satisfy Definition \ref{def:rlx-momentum-seq}. 
                \item We choose the parameters $q_k$ has $q_k = \mu/B_k$, with $B_k > \mu$, for all $k \ge 0$. 
            \end{enumerate}
            Then, for all $\bar x \in \RR^n$, $k \ge 0$: 
            \begin{align*}
                & \frac{B_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                -\frac{B_k(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \\
                &=
                \frac{\alpha_k \mu}{2}
                \Vert \bar x - v_{k - 1}\Vert^2
                + \frac{q_k\mu}{2}\Vert y_k - v_{k - 1} \Vert
                + \frac{q_k}{2\mu}\Vert \mathcal G_{B_k}(y_k)\Vert^2
                \\ &\quad 
                - \langle
                    q_k(y_k - v_{k - 1}) + \alpha_k(v_{k - 1} - \bar x), \mathcal G_{B_k}(y_k)
                \rangle
                    + \alpha_k \mu
                    \left\langle 
                        v_{k - 1} - \bar x, 
                        y_k - v_{k - 1}
                    \right\rangle. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Consider any $\bar x \in \RR^n$. 
            \begin{align*}
                & \frac{B_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                \\
                &\underset{(1)}{=} \frac{B_k\alpha_k^2}{2}\left\Vert
                    \bar x - 
                    \left(
                        v_{k - 1} 
                        + \alpha_k^{-1}q_k(y_k - v_{k - 1})
                        - \alpha_k^{-1} B_k^{-1} \mathcal G_{B_k}(y_k)
                    \right)
                \right\Vert^2
                \\
                &= \frac{B_k\alpha_k^2}{2}\left\Vert
                    (\bar x - v_{k - 1})
                    - \alpha_k^{-1}\left(
                        q_k(y_k - v_{k - 1})
                        -  B_k^{-1} \mathcal G_{B_k}(y_k)
                    \right)
                \right\Vert^2
                \\
                &= \frac{B_k\alpha_k^2}{2}
                \Vert \bar x - v_{k - 1}\Vert^2
                + \frac{B_k}{2}\left\Vert 
                    q_k(y_k - v_{k - 1})
                    -  B_k^{-1} \mathcal G_{B_k}(y_k)
                \right\Vert^2
                    \\ &\quad 
                    - \alpha_k B_k
                    \left\langle 
                        \bar x - v_{k - 1}, 
                        q_k(y_k - v_{k - 1}) - B_k^{-1}\mathcal G_{B_k}(y_k)
                    \right\rangle
                \\
                &= 
                \frac{B_k\alpha_k^2}{2}
                \Vert \bar x - v_{k - 1}\Vert^2
                + \frac{B_kq_k^2}{2}\Vert y_k - v_{k - 1} \Vert
                + \frac{1}{2B_k}\Vert \mathcal G_{B_k}(y_k)\Vert^2
                - q_k\langle 
                    y_k - v_{k - 1}, \mathcal G_{B_k}(y_k)
                \rangle
                    \\ &\quad 
                    + B_k\alpha_k
                    \left\langle 
                        v_{k - 1} - \bar x, 
                        q_k(y_k - v_{k - 1}) - B_k^{-1}\mathcal G_{B_k}(y_k)
                    \right\rangle
                \\
                &= 
                \frac{B_k\alpha_k^2}{2}
                \Vert \bar x - v_{k - 1}\Vert^2
                + \frac{B_kq_k^2}{2}\Vert y_k - v_{k - 1} \Vert
                + \frac{1}{2B_k}\Vert \mathcal G_{B_k}(y_k)\Vert^2
                - q_k\langle 
                    y_k - v_{k - 1}, \mathcal G_{B_k}(y_k)
                \rangle
                    \\ &\quad 
                    + 
                    B_k\alpha_k\left\langle 
                        v_{k - 1} - \bar x, 
                        q_k(y_k - v_{k - 1}) 
                    \right\rangle
                    - \alpha_k \left\langle 
                        v_{k - 1} - \bar x,
                        \mathcal G_{B_k}(y_k)
                    \right\rangle
                \\
                &= 
                \frac{\alpha_k^2B_k}{2}
                \Vert \bar x - v_{k - 1}\Vert^2
                + \frac{q_k^2B_k}{2}\Vert y_k - v_{k - 1} \Vert
                + \frac{1}{2B_k}\Vert \mathcal G_{B_k}(y_k)\Vert^2
                    \\ &\quad 
                    - \langle
                        q_k(y_k - v_{k - 1}) + \alpha_k(v_{k - 1} - \bar x), \mathcal G_{B_k}(y_k)
                    \rangle
                    \\ &\quad 
                    + \alpha_k q_k B_k
                    \left\langle 
                        v_{k - 1} - \bar x, 
                        y_k - v_{k - 1}
                    \right\rangle. 
            \end{align*}
            At (1) we used Lemma \ref{lemma:st-iterates-alt-form-part1}. 
            Subtracting $-\frac{B_k(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2}{2}\Vert \bar x - v_{k - 1}\Vert^2$ from both sides, the coefficient for $\Vert \bar x - v_{k - 1}\Vert^2$ comes out to be: 
            \begin{align*}
                & \frac{\alpha_k^2B_k}{2}
                - \frac{B_k(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2}{2} 
                = \frac{B_k}{2}(
                    \alpha_k^2 + (1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2
                )
                \underset{(1)}{=} \frac{B_k\alpha_kq_k}{2}. 
            \end{align*}
            At (1), we used the relation $(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2 = \alpha_k(\alpha_k - q_k)$ as in Definition \ref{def:rlx-momentum-seq}, for all $k \ge 1$. 
            Therefore, we have the equality: 
            \begin{align*}
                &
                \frac{B_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                -\frac{B_k(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \\
                &= 
                \frac{\alpha_k q_k B_k}{2}\Vert \bar x - v_{k - 1}\Vert^2
                + \frac{q_k^2B_k}{2}\Vert y_k - v_{k - 1} \Vert
                + \frac{1}{2B_k}\Vert \mathcal G_{B_k}(y_k)\Vert^2
                    \\ &\quad 
                    - \langle
                        q_k(y_k - v_{k - 1}) + \alpha_k(v_{k - 1} - \bar x), \mathcal G_{B_k}(y_k)
                    \rangle
                    \\ &\quad 
                    + \alpha_k q_k B_k
                    \left\langle 
                        v_{k - 1} - \bar x, 
                        y_k - v_{k - 1}
                    \right\rangle. 
                \\
                &\underset{(1)}{=} 
                \frac{\alpha_k \mu}{2}
                \Vert \bar x - v_{k - 1}\Vert^2
                + \frac{q_k\mu}{2}\Vert y_k - v_{k - 1} \Vert
                + \frac{q_k}{2\mu}\Vert \mathcal G_{B_k}(y_k)\Vert^2
                    \\ &\quad 
                - \langle
                        q_k(y_k - v_{k - 1}) + \alpha_k(v_{k - 1} - \bar x), \mathcal G_{B_k}(y_k)
                    \rangle
                    + \alpha_k \mu
                    \left\langle 
                        v_{k - 1} - \bar x, 
                        y_k - v_{k - 1}
                    \right\rangle. 
            \end{align*}
            At (1), we used the relation that $B_k = \mu/q_k$, for all $k \ge 0$. 
        \end{proof}
        % LEMMA ========================================================================================================
        \begin{lemma}[convergence preparations part II]\label{lemma:cnvg-prep-part2}
            The iterates $(y_k, x_k, v_k)_{k \ge 0}$ satisfies Definition \ref{def:st-apg} then, for all $k \ge 0, \bar x \in \RR^n$ the following identities: 
            \begin{align*}
                \alpha_k(v_{k - 1} - \bar x) 
                + q_k(y_k - v_{k - 1})
                + x_{k - 1} - y_k
                &= 
                \alpha_k(x_{k - 1} - \bar x). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            We first establish two intermediate results. 
            From Definition \ref{def:st-apg}, it has for all $k \ge 0$: 
            \begin{align*}
                y_k &= 
                \left(\frac{\alpha_k - q_k}{1 - q_k}\right)v_{k - 1} 
                + \left(\frac{1 - \alpha_k}{1 - q_k}\right) x_{k - 1}
                \\
                &= 
                \left(
                    1 - \frac{1 - \alpha_k}{1 - q_k}
                \right)v_{k - 1} 
                + \left(\frac{1 - \alpha_k}{1 - q_k}\right) x_{k - 1}
                \\
                \iff 
                y_k - v_{k - 1}
                &= \left(
                    \frac{1 - \alpha_k}{1 - q_k}
                \right)(x_{k - 1} - v_{k - 1}). 
            \end{align*}
            Similarly: 
            \begin{align*}
                y_k &= 
                \left(\frac{\alpha_k - q_k}{1 - q_k}\right)v_{k - 1} 
                + \left(\frac{1 - \alpha_k}{1 - q_k}\right) x_{k - 1}
                \\
                &= 
                \left(\frac{\alpha_k - q_k}{1 - q_k}\right)v_{k - 1} 
                + \left(1 - \frac{\alpha_k - q_k}{1 - q_k}\right) x_{k - 1}
                \\
                \iff 
                y_k - x_{k - 1} &= 
                \left(
                    \frac{\alpha_k - q_k}{1 - q_k}
                \right)(v_{k - 1} - x_{k - 1}). 
            \end{align*}
            Now, we use the above two results and it derives 
            \begin{align*}
                & \alpha_k(v_{k - 1} - \bar x) 
                + q_k(y_k - v_{k - 1})
                + x_{k - 1} - y_k
                \\
                &=
                \alpha_k(v_{k - 1} - \bar x)
                + q_k\left(
                    \frac{1 - \alpha_k}{1 - q_k}
                \right)(x_{k - 1} - v_{k - 1})
                - \left(
                    \frac{\alpha_k - q_k}{1 - q_k}    
                \right)(v_{k - 1} - x_{k - 1}). 
                \\
                &= 
                \alpha_k(v_{k - 1} - \bar x)
                + (1 - q_k)^{-1}\left(
                    q_k - q_k\alpha_k +(\alpha_k - q_k)
                \right)
                (x_{k - 1} - v_{k - 1})
                \\
                &= 
                \alpha_k(v_{k - 1} - \bar x)
                + \alpha_k(x_{k - 1} - v_{k - 1})
                \\
                &= \alpha_k(x_{k - 1} - \bar x). 
            \end{align*}

        \end{proof}
        % LEMMA ========================================================================================================
        \begin{lemma}[convergence preparations part III]\;\label{lemma:cnvg-prep-part3}\\
            Suppose that all of the following are satisfied
            \begin{enumerate}[nosep]
                \item $(F, f, g, \mathcal E, \mu, L)$ satisfies Assumption \ref{ass:for-cnvg}. 
                \item The sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}, (q_k)_{k \ge 0}$ satisfies Definition \ref{def:rlx-momentum-seq}. 
                \item We choose $(q_k)_{k \ge 0}$ is given by $q_k = \mu/B_k$ for all $k \ge 0$. 
                \item The sequence $(y_k, v_k, x_k)_{k \ge 0}$ satisfies Definition \ref{def:st-apg}. 
            \end{enumerate}
            Then, $\forall k \ge 1$, there exists $\bar x_k \in \RR^n$, such that: 
            \begin{align*}
                & F(x_k) - F(\bar x_k)
                + \frac{B_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                \\
                &\le 
                (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x_k))
                + \frac{B_k(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2}{2}\Vert \bar x_k - v_{k - 1}\Vert^2. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Recall Definition \ref{def:pg-gap}, consider: 
            \begin{align*}
                &\mathcal E(x_{k - 1}, y_k, \mu, B_k) 
                \\
                &=
                F(x_{k - 1}) - F(x_k)
                - \langle 
                    B_k(y_k - z_k), 
                    x_{k - 1} - y_k
                \rangle
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                - \frac{B_k}{2}\Vert y_k - x_k\Vert^2
                \\
                &= 
                F(x_{k - 1}) - F(x_k)
                - \langle 
                    \mathcal G_{B_k}(y_k), 
                    x_{k - 1} - y_k
                \rangle
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                - \frac{q_k}{2\mu}\Vert \mathcal G_{B_k}(y_k)\Vert^2. 
            \end{align*}
            On the second equality above, we used $B^{-1}_k(y_k - x_k)= \mathcal G_{B_k}(y_k)$, and $B_k = \mu/q_k$.
            For all $k \ge 0$, we define $\Xi_k$ and simplify using the above result: 
            \begin{align*}
                \Xi_k &\hspace{-0.3em}\defeq 
                \mathcal E(x_{k - 1}, y_k, \mu, B_k)
                + F(x_k) - F(\bar x_k)
                - (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x_k))
                \\
                &= 
                \mathcal E(x_{k - 1}, y_k, \mu, B_k)
                + F(x_k) - \alpha_kF(\bar x_k) - (1 - \alpha_k)F(x_{k - 1})
                \\
                &=
                \alpha_kF(x_{k - 1}) - \alpha_k F(\bar x_k)
                - \langle \mathcal G_{B_k}(y_k), x_{k - 1} - y_k\rangle
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2 
                - \frac{q_k}{2\mu}\Vert \mathcal G_{B_k}(y_k)\Vert^2. 
            \end{align*}
            Now consider the new term $\Xi_k'$ which we defined and simplify below: 
            {\allowdisplaybreaks
            \begin{align*}
                \Xi_k'
                &\hspace{-0.3em}:=
                \Xi_k + \frac{B_k\alpha_k^2}{2}\Vert \bar x_k - v_k\Vert^2 
                - \frac{B_k(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2}{2}\Vert \bar x_k - v_{k - 1}\Vert^2
                \\
                &\underset{(1)}{=} 
                \alpha_kF(x_{k - 1}) - \alpha_k F(\bar x_k)
                - \langle \mathcal G_{B_k}(y_k), x_{k - 1} - y_k\rangle
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2 
                - \frac{q_k}{2\mu}\Vert \mathcal G_{B_k}(y_k)\Vert^2
                    \\&\quad 
                    + \frac{\alpha_k \mu}{2}
                    \Vert \bar x_k - v_{k - 1}\Vert^2
                    + \frac{q_k\mu}{2}\Vert y_k - v_{k - 1} \Vert^2
                    + \frac{q_k}{2\mu}\Vert \mathcal G_{B_k}(y_k)\Vert^2
                    \\ &\quad 
                    - \langle
                        q_k(y_k - v_{k - 1}) + \alpha_k(v_{k - 1} - \bar x_k), \mathcal G_{B_k}(y_k)
                    \rangle
                    + \alpha_k \mu
                    \left\langle 
                        v_{k - 1} - \bar x_k, 
                        y_k - v_{k - 1}
                    \right\rangle
                \\
                &= 
                \alpha_kF(x_{k - 1}) - \alpha_k F(\bar x_k)
                - \langle \mathcal G_{B_k}(y_k), x_{k - 1} - y_k\rangle
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2 
                    \\&\quad 
                    + \frac{\alpha_k \mu}{2}
                    \Vert \bar x_k - v_{k - 1}\Vert^2
                    + \frac{q_k\mu}{2}\Vert y_k - v_{k - 1} \Vert^2
                    \\ &\quad 
                    - \langle
                        q_k(y_k - v_{k - 1}) + \alpha_k(v_{k - 1} - \bar x_k), \mathcal G_{B_k}(y_k)
                    \rangle
                    + \alpha_k \mu
                    \left\langle 
                        v_{k - 1} - \bar x_k, 
                        y_k - v_{k - 1}
                    \right\rangle
                \\
                &= 
                \alpha_kF(x_{k - 1}) - \alpha_k F(\bar x_k)
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2 
                + \frac{\alpha_k \mu}{2}
                \Vert \bar x_k - v_{k - 1}\Vert^2
                + \frac{q_k\mu}{2}\Vert y_k - v_{k - 1} \Vert^2
                    \\ &\quad 
                    - \langle
                        x_{k - 1} - y_k + q_k(y_k - v_{k - 1}) + \alpha_k(v_{k - 1} - \bar x_k), 
                        \mathcal G_{B_k}(y_k)
                    \rangle
                    \\&\quad 
                    + \alpha_k \mu
                    \left\langle 
                        v_{k - 1} - \bar x_k, 
                        q_k(y_k - v_{k - 1}) 
                    \right\rangle
                \\
                &\underset{(2)}{=} 
                \alpha_kF(x_{k - 1}) - \alpha_k F(\bar x_k)
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2 
                    \\&\quad 
                    + \frac{\alpha_k \mu}{2}
                    \Vert \bar x_k - v_{k - 1}\Vert^2
                    + \frac{q_k\mu}{2}\Vert y_k - v_{k - 1} \Vert^2
                    \\ &\quad 
                    - \alpha_k\langle
                        x_{k - 1} - \bar x_k, 
                        \mathcal G_{B_k}(y_k)
                    \rangle
                    + \alpha_k \mu
                    \left\langle 
                        v_{k - 1} - \bar x_k, 
                        q_k(y_k - v_{k - 1}) 
                    \right\rangle
                \\
                &=
                \alpha_kF(x_{k - 1}) - \alpha_k F(\bar x_k)
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2 
                    \\&\quad 
                    + \frac{\alpha_k \mu}{2}
                    \Vert \bar x_k - v_{k - 1}\Vert^2
                    + \frac{\alpha_k\mu}{2}\Vert y_k - v_{k - 1}\Vert^2
                    + \alpha_k \mu
                    \left\langle 
                        v_{k - 1} - \bar x_k, 
                        q_k(y_k - v_{k - 1}) 
                    \right\rangle
                    \\ &\quad 
                    - \alpha_k\langle
                        x_{k - 1} - \bar x_k, 
                        \mathcal G_{B_k}(y_k)
                    \rangle
                    + \frac{q_k\mu - \mu\alpha_k}{2}\Vert y_k - v_{k - 1} \Vert^2
                \\
                &= 
                \alpha_kF(x_{k - 1}) - \alpha_k F(\bar x_k)
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2 
                    \\&\quad 
                    \frac{\alpha_k\mu}{2}\Vert y_k - \bar x_k\Vert^2
                    - \alpha_k\langle
                        x_{k - 1} - \bar x_k, 
                        \mathcal G_{B_k}(y_k)
                    \rangle
                    + \frac{q_k\mu - \mu\alpha_k}{2}\Vert y_k - v_{k - 1} \Vert^2
                \\
                &\underset{(3)}{\le} 
                \alpha_kF(x_{k - 1}) - \alpha_k F(\bar x_k)
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2 
                    \\&\quad 
                    \frac{\alpha_k\mu}{2}\Vert y_k - \bar x_k\Vert^2
                    - \alpha_k\langle
                        x_{k - 1} - \bar x_k, 
                        \mathcal G_{B_k}(y_k)
                    \rangle. 
                \\
                &= \alpha_k\left(
                    F(x_{k  - 1}) - F(\bar x_k) 
                    - \langle x_{k - 1} - \bar x_k, \mathcal G_{B_k}(y_k)\rangle
                    +  \frac{\mu}{2}\Vert y_k - \bar x_k\Vert^2
                \right)
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2. 
            \end{align*}
            }
            At (1), we used Lemma \ref{lemma:cnvg-prep-part3}, and substituted $\Xi_k$. 
            At (2), we used Lemma \ref{lemma:cnvg-prep-part2} to simplify the inner product. 
            At (3), we used the $\alpha_k > q_k$ as in Definition \ref{def:rlx-momentum-seq}, hence it makes the coefficient $q_k \mu - \mu\alpha_k \le 0$, which gives us the inequality. 
            Now, subtracting $\mathcal E(x_{k - 1}, y_k, \mu, B_k)$ from both sides of the inequality will yield: 
            {\allowdisplaybreaks\small
            \begin{align*}
                & \Xi_k' - \mathcal E(x_{k - 1}, y_k, \mu, B_k)
                \\
                &\le 
                \alpha_k\left(
                    F(x_{k  - 1}) - F(\bar x_k) 
                    - \langle x_{k - 1} - \bar x_k, \mathcal G_{B_k}(y_k)\rangle
                    +  \frac{\mu}{2}\Vert y_k - \bar x_k\Vert^2
                    - \mathcal E(x_{k - 1}, y_k, \mu, B_k)
                \right)
                    \\ &\quad 
                    - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                    - (1 - \alpha_k)\mathcal E(x_{k - 1}, y_k, \mu, B_k)
                \\
                &\underset{(1)}{=} 
                \alpha_k\left(
                    F(x_k) - F(\bar x_k) 
                    - \langle \mathcal G_{B_k}(y_k), x_{k - 1} - \bar x_k - (x_{k - 1} - y_k)\rangle
                \right)
                    \\&\quad 
                    + \alpha_k\left(
                        \frac{\mu}{2}\Vert y_k - \bar x_k\Vert^2
                        + \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                        + \frac{B_k}{2} \Vert y_k - x_k\Vert^2
                    \right)
                    \\ &\quad 
                    - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                    - (1 - \alpha_k)\mathcal E(x_{k - 1}, y_k, \mu, B_k)
                \\
                &= \alpha_k\left(
                    F(x_k) - F(\bar x_k) 
                    - \langle \mathcal G_{B_k}(y_k), y_k - \bar x_k\rangle
                    + \frac{\mu}{2}\Vert y_k - \bar x_k\Vert^2
                    + \frac{B_k}{2} \Vert y_k - x_k\Vert^2
                    + \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                \right)
                    \\ &\quad 
                    - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                    - (1 - \alpha_k)\mathcal E(x_{k - 1}, y_k, \mu, B_k)
                \\
                &=
                \alpha_k\left(
                    - \mathcal E(\bar x_k, y_k, \mu, B_k)
                    + \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                \right)
                    \\ &\quad 
                    - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                    - (1 - \alpha_k)\mathcal E(x_{k - 1}, y_k, \mu, B_k)
                \\
                &\underset{(2)}{\le} 
                \frac{\alpha_k \mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                - \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                - (1 - \alpha_k)\mathcal E(x_{k - 1}, y_k, \mu, B_k)
                \\
                &= -(1 - \alpha_k)\left(
                    \mathcal E(x_{k - 1}, y_k, \mu, B_k)
                    + \frac{\mu}{2}\Vert x_{k - 1} - y_k\Vert^2
                \right) 
                \\
                & \underset{(3)}{\le} 0. 
            \end{align*}
            }
            At (1), we substituted $\mathcal E(x_{k - 1}, y_k, \mu, B_k)$. 
            At (2), we used the $\mathcal E(\bar x_k, y_k, \mu, B_k)\le 0$ by chosing $\bar x_k = \bar y$ in Assumption \ref{ass:for-cnvg} (iii) to make the inequality. 
            At (3), we used Assumption \ref{ass:for-cnvg} (iv). 
            At this point, we had proved what we wanted becaues using the definitions of $\Xi_k, \Xi_k'$ it has:
            \begin{align*}
                & \Xi_k' - \mathcal E(x_{k - 1}, y_k, \mu, B_k) 
                \\
                &= 
                \Xi_k 
                + \frac{B_k\alpha_k^2}{2}\Vert \bar x_k - v_k\Vert^2 
                - \frac{B_k(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2}{2}\Vert \bar x_k - v_{k - 1}\Vert^2
                - \mathcal E(x_{k - 1}, y_k, \mu, B_k) 
                \\
                &= \mathcal E(x_{k - 1}, y_k, \mu, B_k)
                + F(x_k) - F(\bar x_k)
                - (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x_k))
                    \\ &\quad 
                    + \frac{B_k\alpha_k^2}{2}\Vert \bar x_k - v_k\Vert^2 
                    - \frac{B_k(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2}{2}\Vert \bar x_k - v_{k - 1}\Vert^2
                    - \mathcal E(x_{k - 1}, y_k, \mu, B_k)
                \\
                &=  
                F(x_k) - F(\bar x_k)
                - (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x_k))
                    \\ &\quad 
                    + \frac{B_k\alpha_k^2}{2}\Vert \bar x_k - v_k\Vert^2 
                    - \frac{B_k(1 - \alpha_k)\rho_{k - 1}\alpha_{k - 1}^2}{2}\Vert \bar x_k - v_{k - 1}\Vert^2
                \\
                &\le 0. 
            \end{align*}
        \end{proof}
    \subsection{Proving the convergence rate}
        To finally find the convergence rate, we will strengthen Assumption \ref{ass:for-cnvg}. 
        Our convergence rate is expressed for sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k\ge0}$ as long as they satisfies Definition \ref{def:rlx-momentum-seq}. 
        This means that any sequence with $\alpha_k \in (q_k, 1)$ would work. 
        \par
        The following assumptions describes the behaviors of an algorithm satisfying Definition \ref{def:st-apg}, its parameters, and the properties of the objective function. 
        \begin{assumption}[Assumptions for linear convergence rate]\;\label{ass:lin-cnvg}\\
            Let $(F, f, g, \mathcal E, \mu, L)$ satisfies Assumption \ref{ass:for-cnvg}. 
            In addition, we strength the prior assumptions:
            \begin{enumerate}[nosep]
                \item Define the set of minimizers $X^+ = \argmin_{z\in \RR^n}\{ f(z) + g(z)\}$, it has $X^+ \neq \emptyset$. 
                \item $\exists \mu > 0$ such that $\forall\; y \in \RR^n$, it has $\mathcal E \left(\Pi_{X^+}y, y, \mu, B\right) \ge 0$. 
            \end{enumerate}
            Now, suppose an algorithm which optimizes a $F = f + g$ satisfying all pervious assumptions, and it generates iterates $(y_k, x_k, v_k)_{k\ge 0}$ that satisfies Definition \ref{def:pg-gap}. 
            In addition, we assume that the parameter sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}, (q_k)_{k \ge 0}, (B_k)_{k \ge 0}$ satisfy the following: 
            \begin{enumerate}[nosep]\setcounter{enumi}{3}
                \item The sequences $\alpha_k, \rho_k, q_k$ are given by Definition \ref{def:rlx-momentum-seq}. 
                \item The sequence $q_k = \mu/B_k$, with $B_k > \mu$. 
                \item For all $k \ge 0$, $\Pi_{X^+}y_k$ is a unique. 
            \end{enumerate}
        \end{assumption}
        \begin{assumption}[Assumption for sublinear convergence rate]
            
        \end{assumption}

        % THEOREM ======================================================================================================
        \begin{theorem}[linear convergence with generic sequence]\;\label{thm:cnvg-generic-seq}\\
            Let $(F, f, g, \mathcal E, \mu, L)$, and sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}, (q_k)_{k \ge 0}, (B_k)_{k \ge 0}$ 
            satisfy Assumption \ref{ass:lin-cnvg}. 
            Denote $\beta_k = \prod_{i = 1}^k(1 - \alpha_i) \max \left(\frac{B_i\rho_{i - 1}}{B_{i - 1}}, 1\right)$, $\beta_0 = 1$. 
            Then, there exists a unique $\bar x \in \RR^n$ such that for all $k \ge 1$, $\bar x = \Pi_{X^+}y_k$, and it satisfies
            \begin{align*}
                F(x_k) - F(\bar x) + \frac{B_k}{\alpha_k^2}\Vert \bar x - v_k\Vert^2 
                \le 
                \beta_k\left(
                    F(x_0) - F(\bar x) + \frac{\alpha_0B_0}{2}\Vert \bar x - v_0\Vert^2
                \right). 
            \end{align*}
            If in addition, we assume $x_{-1} = v_{-1}, \alpha_0 = 1$, then the above inequality simplifies: 
            \begin{align*}
                & F(x_k) - F(\bar x) + \frac{B_k}{\alpha_k^2}\Vert \bar x - v_k\Vert^2 \le 
                \frac{\beta_kB_0}{2}\Vert \bar x - x_{-1}\Vert^2.
            \end{align*}
        \end{theorem}
        \begin{proof}
            Set $\bar x_k = \bar x$ in Lemma \ref{lemma:cnvg-prep-part3} then for all $k\ge 1$ it has 
            \begin{align*}
                & F(x_k) - F(\bar x) + \frac{B_k\alpha_k^2}{2}\Vert \bar x- v_k\Vert^2 
                \\
                &\le \left(
                    1 - \alpha_k
                \right)\left(
                    F(x_{k- 1}) - F(\bar x) + \frac{B_k\alpha_{k -1}^2 \rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right)
                \\
                &\le \left(
                    1 - \alpha_k
                \right)\left(
                    F(x_{k- 1}) - F(\bar x) + 
                    \frac{B_k\rho_{k - 1}}{B_{k - 1}}\frac{B_{k - 1}\alpha_{k -1}^2}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right)
                \\
                &= \left(
                    1 - \alpha_k
                \right)\max\left(
                    \frac{B_k\rho_{k - 1}}{B_{k - 1}}, 1
                \right)
                \left(
                    F(x_{k- 1}) - F(\bar x) + 
                    \frac{B_{k - 1}\alpha_{k -1}^2 }{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right). 
            \end{align*}
            From the above, an recurrence relation is formed for $k \ge 1$, unrolloing the recurrence relation it has 
            \begin{align*}
                & F(x_k) - F(\bar x) + \frac{B_k\alpha_k^2}{2}\Vert \bar x- v_k\Vert^2 
                \\
                &\le 
                \left(
                    1 - \alpha_k
                \right)\max\left(
                    \frac{B_k\rho_{k - 1}}{B_{k - 1}}, 1
                \right)
                \left(
                    F(x_{k- 1}) - F(\bar x) + 
                    \frac{B_{k - 1}\alpha_{k -1}^2 }{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right). 
            \end{align*}
            When $x_{-1} = v_{-1}$, from Definition \ref{def:st-apg}, when $k = 0$ it has $y_0 = v_{-1} = x_{-1}$, so $x_0 = T_{B_0}(y_0)$. 
            Because $\alpha_0 = 1$, it also has $v_0 = x_0$. 
            Choose $z = \bar x, y = x_{-1}$ we have from Assumption \ref{ass:for-cnvg} (iii) that
            \begin{align*}
                0 &\le \mathcal E(\bar x, x_{-1}, \mu, B_0) - \frac{\mu}{2}\Vert \bar x - x_{-1}\Vert^2
                \\
                &=
                F(\bar x) - F(x_{-1}) 
                - B_0\left\langle x_{-1} - T_{B_0}(x_{-1}), \bar x - x_{-1} \right\rangle
                - \frac{B_0}{2}\left\Vert x_{-1} - T_{B_0}x_{-1}\right\Vert^2
                \\
                &= 
                F(\bar x) - F(x_{-1}) 
                - \frac{B_0}{2}\Vert \bar x - T_{B_0}x_{-1}\Vert^2 
                + \frac{B_0}{2}\Vert \bar x - x_{-1}\Vert^2. 
                \\
                &= F(\bar x) - F(x_{-1}) 
                - \frac{B_0}{2}\Vert \bar x - v_0\Vert^2 
                + \frac{B_0}{2}\Vert \bar x - x_{-1}\Vert^2. 
            \end{align*}
            Substitute the above into the RHS of the inequality of previous results to complete the proof. 
        \end{proof}
        \par
        The above theorem shows that the convergence rate is exclusively depeneded on the momentum sequence. 
        The theorem below will definitively close the case to show that there exists choices for the sequence such that a linear convergence exists, meaning that $\beta_k$ will decrease at a linear rate. 
        \begin{lemma}[beta sequence bounds]\;\label{lemma:beta-seq}\\
            Let sequence $(\alpha_k)_{k \ge 0}, (\rho_{k})_{k \ge 0}, (q_k)_{k \ge 0}$ satisfies Definition \ref{def:rlx-momentum-seq}.
            Let sequences $(B_k)_{k \ge 1}$ be given by Definition \ref{def:st-apg}. 
            Define $\beta_k := \prod_{i = 1}^k(1 - \alpha_i) \max \left(\frac{B_i\rho_{i - 1}}{B_{i - 1}}, 1\right)$.
            Assume in addition that for all $k \ge 0$, $B_k = B_{k - 1} = B$. 
            Then, linear convergence of $\beta_k$ is possible under the following scenarios: 
            \begin{enumerate}
                \item If for all $k \ge 1$, $\sqrt{q} \le \alpha_k \le \alpha_{k - 1}$ so it's non-increasing, then $\beta_k = \Pi_{i = 1}^k(1 - q/\alpha_{i - 1})$.
                Since $\alpha_k \in (0, 1)$ and it's monotone, it has upper bound $\beta_k \le (1 - q)^k$. 
                \item If for all $k \ge 1$, $\alpha_k = \alpha_{k - 1}$, so there exists $\alpha = \alpha_k$, making the sequence a constant, then $\beta_k = \max(1 - q/\alpha, 1 - \alpha)^k$. And, it's lowest when $\alpha_k = \sqrt{q}$. 
            \end{enumerate}
        \end{lemma}
        \begin{proof}
            Since $B_k$ is a constant, and by Definition \ref{def:st-apg} it has $B_k = \frac{\mu}{q_k}$ it would make $q_k$ to be a constant for all $k \ge 0$, which we denote $q := q_k$. 
            \par
            \textbf{Proof of (i)}. 
            For all $k \ge 1$ it has 
            \begin{align*}
                & (1 - \alpha_k)\max\left(
                    \frac{B_k\rho_{k - 1}}{B_{k - 1}}, 1
                \right)
                \\
                &= \max(\rho_{k - 1}(1 - \alpha_k), 1 - \alpha_k) 
                \\
                &\underset{\text{(1)}}{=} \max\left(
                    \frac{\alpha_k(\alpha_k - q)}{\alpha_{k - 1}^2}, 1 - \alpha_k
                \right)
                \\
                &= \max\left(
                    \frac{\alpha_k^2}{\alpha_{k - 1}^2} - \frac{q}{\alpha_{k - 1}}, 1 - \alpha_k
                \right)
                \\
                &\underset{\text{(2)}}{\le} 
                \max\left(
                    1 - \frac{q}{\alpha_{k - 1}}, 1 - \alpha_k
                \right). 
            \end{align*}
            Then, observe that
            \begin{align*}
                1 - \frac{q}{\alpha_{k - 1}} &> 1 - \frac{q}{\sqrt{q}}
                \\
                &= 1 - \sqrt{q} \ge 1 - \alpha_k. 
            \end{align*}
            Combining the previous two results it has 
            \begin{align*}
                (1 - \alpha_k)\max\left(
                    \frac{B_k\rho_{k - 1}}{B_{k - 1}}, 1
                \right) \le 
                \left(1 - \frac{q}{\alpha_{k - 1}}\right).
            \end{align*}
            \par 
            \textbf{Proof of (ii)}. 
            
        \end{proof}

\section{Linear convergence beyond the strongly convex case}
    In this sections, we exam Assumption \ref{ass:lin-cnvg} and, propose examples for it. 
    In Necoara et al.'s setting, they have the following Assumptoins for their objective function. 
    \begin{assumption}[The settings for Necoara's et al]\;\label{ass:necoara}\\
        The assumption is about $(f, X, X^+, f^+)$ where 
        \begin{enumerate}[nosep]
            \item $X \subseteq \RR^n$ is a closed convex set. 
            \item $f: X\rightarrow \RR$ has $L$ Lipschitz continous gradient, and it's convex. 
            \item $X^+ = \argmin_{x \in X} f(x)\neq \emptyset$, and we denote $f^+$ to be the minimum value. 
        \end{enumerate}
    \end{assumption}
    The following definition on Quasi Strongly Convex function (Q-SCNVX) is taken from \cite[Definition 1]{necoara_linear_2019}. 
    By... the above definition satisfies Assumption \ref{ass:for-cnvg} 
    \begin{definition}[Q-SCNVX]\label{def:Q-SCNVX}
        Let $(f, X, X^+, f^+)$ be given by Assumption \ref{ass:necoara}. 
        We define $f$ to be Quasi Strong Convex on $X$ if there exists $\kappa_f  > 0$ such that $\forall x \in \RR^n$ , with $\bar x = \Pi_{X^+}x$ it has:
        \begin{align*}
            f^+ - f(x) - \langle \nabla f(x), \bar x - x\rangle - \frac{\kappa_f}{2}\Vert \bar x - x\Vert^2 
            &\ge 0. 
        \end{align*}
    \end{definition}
    \begin{proposition}[Q-SCNVX is an example of our assumptions]\label{prop:qscnvx-ass-ok}
        Let $(f, X, X^+, f^+)$ satisfies Assumption \ref{ass:necoara}.
        Let $\mathcal E$ be given by Definition \ref{def:pg-gap} with $g = \delta_X$. 
        Then for all $x \in \RR^n$, let $\bar x = \Pi_{X^+}x, x^+ = T_B(x)$, there exists $B \ge 0$, such that 
        \begin{align*}
            0 &\le 
            \mathcal E(\bar x, x, B, \kappa_f)
            = 
            f(\bar x) - f(x^+) 
            - \frac{B}{2}\Vert x - x^+\Vert^2 
            - B\langle \bar x - x, x - x^+\rangle
            - \frac{\kappa_f}{2}\Vert \bar x - x\Vert^2. 
        \end{align*}
        Therefore, it satisfies Assumtpion \ref{ass:for-cnvg}. 
    \end{proposition}
    \begin{proof}
        Let $h = z \mapsto \delta_X(z) + \langle \nabla f(x), z\rangle + \frac{B}{2}\Vert z - x\Vert^2$.
        So it has $T_B(x) = \argmin_{z\in \RR^n} h(z)$. 
        Furthermore, $h(z)$ is a $B$ strongly convex function by convexity of $X$ and, the fact that other parts are just the sum of a linear and quadratic function. 
        Denote $x^+ = T_B(x)$, since $x^+$ is a minimizer therefore it has $\forall x \in X\; h(z) - h(x^+) \ge \frac{B}{2}\Vert z - x^+\Vert^2$. 
        That was the quadratic growth condition of $h$. 
        Next, denote $\bar x  = \Pi_{X^+}x$, let $z = \bar x$ then the condition becomes: 
        \begin{align*}
            & \frac{B}{2}\Vert \bar x - x^+\Vert^2 
            \\
            &\le 
            \langle \nabla f(x), \bar x\rangle + \frac{B}{2}\Vert \bar x - x\Vert^2 - 
            \langle \nabla f(x), x^+\rangle + \frac{B}{2}\Vert x^+ - x\Vert^2
            \\
            &=
            \langle \nabla f(x), \bar x - x\rangle - \langle \nabla f(x), x^+ - x\rangle
            + \frac{B}{2}\Vert \bar x - x\Vert^2 
            - \frac{B}{2}\Vert x^+ - x\Vert^2
            \\
            &= - D_f(\bar x, x) - f(x) + f(\bar x) - f(x^+) + f(x) + D_f(x^+, x)
            + \frac{B}{2}\Vert \bar x - x\Vert^2 
            - \frac{B}{2}\Vert x^+ - x\Vert^2
            \\
            &= 
            - D_f(\bar x, x) + f(\bar x) - f(x^+) 
            + \frac{B}{2}\Vert \bar x - x\Vert^2 
            + D_f(x^+, x)
            - \frac{B}{2}\Vert x^+ - x\Vert^2
            \\
            &\underset{\text{(1)}}{\le} 
            - \frac{\kappa_f}{2}\Vert \bar x - x\Vert^2
            + f(\bar x) - f(x^+)
            + \frac{B}{2}\Vert \bar x - x\Vert^2 
            + 0
        \end{align*}
        At (1), we used the fact that $f$ is assumed satisfy \ref{def:Q-SCNVX} because $\bar x = \Pi_{X^+}x$, in addition the inequality $D_f(x^+, x) \le \frac{B}{2}\Vert x^+ - x\Vert^2$ is true because of Lipschitz gradient Assumption in \ref{ass:necoara} hence for all $B\ge L$, it's true for all $x$. 
        Rearranging it has 
        \begin{align*}
            0 &\le
            f(\bar x) - f(x^+) 
            + \frac{B}{2}\Vert \bar x - x\Vert^2 
            - \frac{B}{2}\Vert \bar x - x^+\Vert^2
            - \frac{\kappa_f}{2}\Vert \bar x - x\Vert^2
            \\
            &= 
            f(\bar x) - f(x^+) 
            - \frac{B}{2}\Vert x - x^+\Vert^2 
            - B\langle \bar x - x, x - x^+\rangle
            - \frac{\kappa_f}{2}\Vert \bar x - x\Vert^2
            \\
            &= \mathcal E(\bar x, x, B, \kappa_f). 
        \end{align*}
        The above results shows that Assumption \ref{ass:for-cnvg} has been satisfied, with $\bar y = \Pi_{X^+}y$. 
    \end{proof}

    \subsection{SCNVX affine composite over simple cone}


\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}
