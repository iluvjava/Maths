
@article{necoara_linear_2019,
	title = {Linear convergence of first order methods for non-strongly convex optimization},
	volume = {175},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-018-1232-1},
	doi = {10.1007/s10107-018-1232-1},
	abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
	language = {en},
	number = {1},
	urldate = {2023-10-11},
	journal = {Mathematical Programming},
	author = {Necoara, I. and Nesterov, Yu. and Glineur, F.},
	month = may,
	year = {2019},
	pages = {69--107},
	file = {Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:/Volumes/T6/Zotero Library/storage/7X79PGLC/Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:application/pdf},
}

@misc{drusvyatskiy_error_2016,
	title = {Error bounds, quadratic growth, and linear convergence of proximal methods},
	url = {http://arxiv.org/abs/1602.06661},
	doi = {10.48550/arXiv.1602.06661},
	abstract = {The proximal gradient algorithm for minimizing the sum of a smooth and a nonsmooth convex function often converges linearly even without strong convexity. One common reason is that a multiple of the step length at each iteration may linearly bound the "error" -- the distance to the solution set. We explain the observed linear convergence intuitively by proving the equivalence of such an error bound to a natural quadratic growth condition. Our approach generalizes to linear convergence analysis for proximal methods (of Gauss-Newton type) for minimizing compositions of nonsmooth functions with smooth mappings. We observe incidentally that short step-lengths in the algorithm indicate near-stationarity, suggesting a reliable termination criterion.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Drusvyatskiy, Dmitriy and Lewis, Adrian S.},
	month = jun,
	year = {2016},
	note = {arXiv:1602.06661 [math]},
	keywords = {90C25, 90C31, 90C55, 65K10, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Volumes/T6/Zotero Library/storage/MLYSVWJ6/Drusvyatskiy and Lewis - 2016 - Error bounds, quadratic growth, and linear convergence of proximal methods.pdf:application/pdf;arXiv.org Snapshot:/Volumes/T6/Zotero Library/storage/GK4FBD5X/1602.html:text/html;Snapshot:/Volumes/T6/Zotero Library/storage/5BFJ49ND/1602.html:text/html},
}

@article{aragon_artacho_characterization_2024,
	title = {Characterization of metric regularity of subdifferentials},
	url = {https://www.researchgate.net/publication/39522449_Characterization_of_Metric_Regularity_of_Subdifferentials},
	abstract = {PDF {\textbar} We study regularity properties of the subdifferential of proper lower semicontinuous convex functions in Hilbert spaces. More precisely, we... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-03-13},
	journal = {Journal of Convex Analysis},
	author = {Aragon Artacho, Francisco and Geoffroy, Michel},
	month = nov,
	year = {2024},
	file = {Characterization of metric regularity of subdifferentials:/Volumes/T6/Zotero Library/storage/2IFNRMAE/2024 - Characterization of metric regularity of subdifferentials.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/EMIW9EDD/39522449_Characterization_of_Metric_Regularity_of_Subdifferentials.html:text/html},
}

@misc{liao_error_2024,
	title = {Error bounds, {PL} condition, and quadratic growth for weakly convex functions, and linear convergences of proximal point methods},
	url = {http://arxiv.org/abs/2312.16775},
	doi = {10.48550/arXiv.2312.16775},
	abstract = {Many practical optimization problems lack strong convexity. Fortunately, recent studies have revealed that first-order algorithms also enjoy linear convergences under various weaker regularity conditions. While the relationship among different conditions for convex and smooth functions is well-understood, it is not the case for the nonsmooth setting. In this paper, we go beyond convexity and smoothness, and clarify the connections among common regularity conditions in the class of weakly convex functions, including \${\textbackslash}textit\{strong convexity\}\$, \${\textbackslash}textit\{restricted secant inequality\}\$, \${\textbackslash}textit\{subdifferential error bound\}\$, \${\textbackslash}textit\{Polyak-\{{\textbackslash}L\}ojasiewicz inequality\}\$, and \${\textbackslash}textit\{quadratic growth\}\$. In addition, using these regularity conditions, we present a simple and modular proof for the linear convergence of the proximal point method (PPM) for convex and weakly convex optimization problems. The linear convergence also holds when the subproblems of PPM are solved inexactly with a proper control of inexactness.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Liao, Feng-Yi and Ding, Lijun and Zheng, Yang},
	month = aug,
	year = {2024},
	note = {arXiv:2312.16775 [math]},
	keywords = {Computer Science - Systems and Control, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
	file = {Error bounds, PL condition, and quadratic growth for weakly convex functions, and linear convergence:/Volumes/T6/Zotero Library/storage/HZZANLQ7/Liao et al. - 2024 - Error bounds, PL condition, and quadratic growth for weakly convex functions, and linear convergence.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/GB5SR4CW/2312.html:text/html},
}
