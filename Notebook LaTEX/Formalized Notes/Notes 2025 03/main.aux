\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{tdo}{\contentsline {todo}{\fcolorbox {black}{yellow}{\textcolor {yellow}{o}}\ }{1}{section*.1}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{31105109}{38966904}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}The Basics of Optimization Theories}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{def:bregman-div}{{1.0.1}{1}{The Basics of Optimization Theories}{theorem.1.0.1}{}}
\newlabel{ass:smooth-add-nonsmooth}{{1.0.2}{1}{The Basics of Optimization Theories}{theorem.1.0.2}{}}
\newlabel{thm:pg-ineq-swcnvx-generic}{{1.0.4}{1}{The Basics of Optimization Theories}{theorem.1.0.4}{}}
\newlabel{thm:cnvx-pg-ineq}{{1.0.5}{2}{The Basics of Optimization Theories}{theorem.1.0.5}{}}
\citation{necoara_linear_2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Linear Convergence of First Order Method}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Necoara's et al.'s Paper}{3}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Settings}{3}{subsection.2.1.1}\protected@file@percent }
\newlabel{ass:necoara-2019-settings}{{2.1.1}{3}{The Settings}{theorem.2.1.1}{}}
\newlabel{problem:necoara-2019}{{2.1.1}{3}{The Settings}{equation.2.1.1}{}}
\newlabel{ineq:pg-opt-cond}{{2.1.2}{3}{The Settings}{equation.2.1.2}{}}
\newlabel{def:necoara-scnvx}{{2.1.2}{4}{The Settings}{theorem.2.1.2}{}}
\newlabel{def:necoara-weaker-scnvx}{{2.1.3}{4}{The Settings}{theorem.2.1.3}{}}
\newlabel{def:neocara-qscnvx}{{(i)}{4}{The Settings}{Item.1}{}}
\newlabel{def:necoara-qup}{{(ii)}{4}{The Settings}{Item.2}{}}
\newlabel{def:necoara-qgg}{{(iii)}{4}{The Settings}{Item.3}{}}
\newlabel{def:necoara-qfg}{{(iv)}{4}{The Settings}{Item.4}{}}
\newlabel{def:necoara-peb}{{(v)}{4}{The Settings}{Item.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Weaker conditions of strong convexity}{4}{subsection.2.1.2}\protected@file@percent }
\newlabel{thm:qscnvx-means-qua}{{2.1.5}{4}{Weaker conditions of strong convexity}{theorem.2.1.5}{}}
\newlabel{ineq:thm:qscnvx-means-qua-proof-item1}{{2.1.3}{5}{Weaker conditions of strong convexity}{equation.2.1.3}{}}
\citation{necoara_linear_2019}
\citation{necoara_linear_2019}
\newlabel{thm:qgg-implies-qua}{{2.1.7}{6}{Weaker conditions of strong convexity}{theorem.2.1.7}{}}
\newlabel{thm:qscnvx-implies-qgg}{{2.1.9}{6}{Weaker conditions of strong convexity}{theorem.2.1.9}{}}
\newlabel{thm:qfg-suff}{{2.1.11}{7}{Weaker conditions of strong convexity}{theorem.2.1.11}{}}
\newlabel{ineq:proj-grad}{{2.1.8}{7}{Weaker conditions of strong convexity}{equation.2.1.8}{}}
\newlabel{ineq:proj-grad2}{{2.1.9}{7}{Weaker conditions of strong convexity}{equation.2.1.9}{}}
\newlabel{lemma:grad-map-qfg}{{2.1.13}{8}{Weaker conditions of strong convexity}{theorem.2.1.13}{}}
\newlabel{thm:qfg-peb-equiv}{{2.1.14}{8}{Weaker conditions of strong convexity}{theorem.2.1.14}{}}
\newlabel{ineq:thm:qfg-peb-equiv-proof-item1}{{2.1.10}{8}{Weaker conditions of strong convexity}{equation.2.1.10}{}}
\newlabel{thm:q-cnvx-hierarchy}{{2.1.15}{9}{Weaker conditions of strong convexity}{theorem.2.1.15}{}}
\citation{necoara_linear_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Hoffman error bound and Q-SCNVX}{10}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Feasible descent and accelerated feasible descent}{10}{subsection.2.1.4}\protected@file@percent }
\newlabel{def:projg-alg}{{2.1.17}{10}{Feasible descent and accelerated feasible descent}{theorem.2.1.17}{}}
\newlabel{ineq:projg-variational-ineq}{{2.1.11}{10}{Feasible descent and accelerated feasible descent}{equation.2.1.11}{}}
\citation{necoara_linear_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Application, KKT of linear programming}{11}{subsection.2.1.5}\protected@file@percent }
\newlabel{problem:lp-cannon-form}{{2.1.12}{12}{Application, KKT of linear programming}{equation.2.1.12}{}}
\newlabel{problem:lp-kkt-min}{{2.1.13}{13}{Application, KKT of linear programming}{equation.2.1.13}{}}
\citation{beck_fast_2009-1}
\citation{nesterov_lectures_2018}
\citation{calatroni_backtracking_2019}
\citation{necoara_linear_2019}
\citation{alamo_restart_2019}
\citation{aujol_parameter-free_2024}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Advanced Enhancement Techniques in Accelerated Proximal Gradient}{15}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Preliminaries}{15}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}smooth plus nonsmooth weakly convex}{16}{subsection.3.1.1}\protected@file@percent }
\newlabel{def:wcnvx-fxn}{{3.1.1}{16}{smooth plus nonsmooth weakly convex}{theorem.3.1.1}{}}
\newlabel{ass:sum-of-wcnvx}{{3.1.3}{16}{smooth plus nonsmooth weakly convex}{theorem.3.1.3}{}}
\newlabel{def:gm-for-ch2}{{3.1.5}{16}{smooth plus nonsmooth weakly convex}{theorem.3.1.5}{}}
\newlabel{lemma:mono-wcnvx-descent}{{3.1.6}{16}{smooth plus nonsmooth weakly convex}{theorem.3.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}smooth plus nonsmooth convex}{17}{subsection.3.1.2}\protected@file@percent }
\newlabel{ass:standard-fista}{{3.1.7}{17}{smooth plus nonsmooth convex}{theorem.3.1.7}{}}
\newlabel{lemma:fitsa-pg-ineq}{{3.1.8}{17}{smooth plus nonsmooth convex}{theorem.3.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}FISTA made simple}{17}{section.3.2}\protected@file@percent }
\newlabel{def:generic-mapg-ls}{{3.2.1}{18}{FISTA made simple}{theorem.3.2.1}{}}
\newlabel{def:alpha-beta-rho-seq}{{3.2.2}{18}{FISTA made simple}{theorem.3.2.2}{}}
\newlabel{lemma:apg-iterates}{{3.2.3}{18}{FISTA made simple}{theorem.3.2.3}{}}
\newlabel{thm:gmapg-ls-convergence}{{3.2.4}{19}{FISTA made simple}{theorem.3.2.4}{}}
\newlabel{thm:gmapg-generic-gm-cnvg}{{3.2.6}{21}{FISTA made simple}{theorem.3.2.6}{}}
\newlabel{ineq:gmapg-generic-gm-cnvg-prt1}{{3.2.1}{21}{FISTA made simple}{equation.3.2.1}{}}
\newlabel{ineq:gmapg-generic-gm-cnvg-prt2}{{3.2.2}{21}{FISTA made simple}{equation.3.2.2}{}}
\citation{alamo_restart_2019}
\newlabel{lemma:gmapg-seq-bnd}{{3.2.8}{23}{FISTA made simple}{theorem.3.2.8}{}}
\citation{guler_new_1992}
\newlabel{thm:gmapg-specialized-cnvg}{{3.2.10}{25}{FISTA made simple}{theorem.3.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Algorithmic description of GMAPG}{26}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Line search routines}{27}{subsection.3.3.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Armijo Line Search}}{27}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:armijo-ls}{{1}{27}{Armijo Line Search}{algorithm.1}{}}
\citation{calatroni_backtracking_2019}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Chambolle's Backtracking}}{28}{algorithm.2}\protected@file@percent }
\newlabel{alg:chambolle-btls}{{2}{28}{Chambolle's Backtracking}{algorithm.2}{}}
\citation{nesterov_lectures_2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Monotone routines}{29}{subsection.3.3.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Beck's monotone routine}}{29}{algorithm.3}\protected@file@percent }
\newlabel{alg:beck-mono}{{3}{29}{Beck's monotone routine}{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Nesterov's monotone routine}}{29}{algorithm.4}\protected@file@percent }
\newlabel{alg:nes-mono}{{4}{29}{Nesterov's monotone routine}{algorithm.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}GMAPG main algorithm}{30}{subsection.3.3.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces GMAPG with Chambolle's backtracking}}{30}{algorithm.5}\protected@file@percent }
\newlabel{alg:gmapg}{{5}{30}{GMAPG with Chambolle's backtracking}{algorithm.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Examples of GMAPG in the literature}{31}{section.3.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces MFISTA with Armijo Line Search}}{31}{algorithm.6}\protected@file@percent }
\newlabel{alg:mfista-armijo}{{6}{31}{MFISTA with Armijo Line Search}{algorithm.6}{}}
\newlabel{eqn:emp:result-item-1}{{3.4.1}{32}{Examples of GMAPG in the literature}{equation.3.4.1}{}}
\newlabel{eqn:emp:result-item-2}{{3.4.2}{32}{Examples of GMAPG in the literature}{equation.3.4.2}{}}
\newlabel{alg:nesterov-mono-generic-ls}{{\caption@xref {alg:nesterov-mono-generic-ls}{ on input line 1973}}{32}{Examples of GMAPG in the literature}{theorem.3.4.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Nesterov's monotone scheme with generic line search}}{32}{algorithm.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Practical enhancement from the Nesterov's Monotone Variant}{32}{section.3.5}\protected@file@percent }
\newlabel{def:nes-monotone-scheme}{{3.5.1}{32}{Practical enhancement from the Nesterov's Monotone Variant}{theorem.3.5.1}{}}
\newlabel{thm:nes-mono-wcnvx-convergence}{{3.5.2}{33}{Practical enhancement from the Nesterov's Monotone Variant}{theorem.3.5.2}{}}
\citation{alamo_restart_2019}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Restarting with function values for linear convergence}{34}{section.3.6}\protected@file@percent }
\newlabel{ass:q-growth-ch2}{{3.6.1}{34}{Restarting with function values for linear convergence}{theorem.3.6.1}{}}
\newlabel{prop:bnded-lip-ls}{{3.6.2}{35}{Restarting with function values for linear convergence}{theorem.3.6.2}{}}
\newlabel{ineq:rgmapg-exit-cond}{{3.6.1}{35}{Restarting with function values for linear convergence}{equation.3.6.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Linear convergence restarted GMAPG}}{36}{algorithm.8}\protected@file@percent }
\newlabel{alg:linear-rgmapg}{{8}{36}{Linear convergence restarted GMAPG}{algorithm.8}{}}
\newlabel{obs:rgmapg}{{3.6.3}{36}{Restarting with function values for linear convergence}{theorem.3.6.3}{}}
\newlabel{lemma:prog-ratio}{{3.6.4}{37}{Restarting with function values for linear convergence}{theorem.3.6.4}{}}
\newlabel{lemma:rgmapg-inner-bnds}{{3.6.5}{39}{Restarting with function values for linear convergence}{theorem.3.6.5}{}}
\newlabel{lemma:rgmapg-outer-itr-bnd}{{3.6.6}{40}{Restarting with function values for linear convergence}{theorem.3.6.6}{}}
\newlabel{thm:rgmapg-cnvg-complexity}{{3.6.8}{43}{Restarting with function values for linear convergence}{theorem.3.6.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Applying them to large scale LP}{44}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Hoffman Bounds and infeasibility detection}{44}{section.3.8}\protected@file@percent }
\bibstyle{siam}
\bibdata{references/refs.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Enhanced Primal Dual Methods for LP}{45}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{alamo_restart_2019}{1}
\bibcite{aujol_parameter-free_2024}{2}
\bibcite{beck_fast_2009-1}{3}
\bibcite{calatroni_backtracking_2019}{4}
\bibcite{guler_new_1992}{5}
\bibcite{necoara_linear_2019}{6}
\bibcite{nesterov_lectures_2018}{7}
\gdef \@abspage@last{48}
