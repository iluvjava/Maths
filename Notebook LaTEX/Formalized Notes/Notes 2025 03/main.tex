\documentclass[12pt]{report}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}
\title{{\fontfamily{ptm}\selectfont Reading Notes}}

\author{
    Alto
    % \thanks{
    %     Subject type, Some Department of Some University, Location of the University,
    %     Country. E-mail: \texttt{author.name@university.edu}.
    % }
}

\date{Last Compiled: \today}

\maketitle

\begin{abstract} 
    Reports on papers read. 
    This is a LaTEX file for my own notes taking. 
    It may accelerate the process of writing my thesis for my PhD degree. 
    \todoinline{This paper is currently in draft mode. Check source to change options. }
\end{abstract}
\chapter{The Basics of Optimization Theories}
    Notations in this chapter are not shared, and they are for this chapter only. 
    % ===================================================================================
    % BREGMAN DIV DEFINITION 
    % ===================================================================================
    \begin{definition}[Bregman Divergence]\label{def:bregman-div}
        Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
        Define Bregman Divergence: 
        \begin{align*}
            D_f: \RR^n \times \dom \nabla f \rightarrow \overline \RR:= 
            (x, y) \mapsto f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
        \end{align*}
    \end{definition}
    \begin{assumption}[smooth plus nonsmooth]\label{ass:smooth-add-nonsmooth}
        Let $F = f+ g$ where $f:\RR^n \rightarrow \overline \RR$ is differentiable and there exists $q\in \RR$ such that $g - q/2\Vert \cdot\Vert^2$ is convex.
    \end{assumption}
    \begin{definition}[proximal gradient operator]
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth}. 
        Define the proximal gradient operator by: 
        \begin{align*}
            T_{\beta^{-1}, f, g}(x) &= \hprox_{\beta^{-1}g} \left(
                x - \beta^{-1} \nabla f(x)
            \right)
            \\
            &= \argmin_{z}\left\lbrace
                g(z) + f(x) + \langle \nabla f(x), z - x\rangle
                + \frac{L}{2}\Vert x - z\Vert^2
            \right\rbrace. 
        \end{align*}
    \end{definition}
    % =================================================
    % WEAKLY CONVEX GENERIC PROXIMAL GRADIENT INEQUALITY
    % ==================================================
    \begin{theorem}[weakly convex generic proximal gradient inequality]\label{thm:pg-ineq-wcnvx-generic}\;\\
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} with $\beta > 0$ and $q \in \RR$. 
        Then for all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$, it has: 
        \begin{align*}
            \frac{q}{2}\Vert z - x^+\Vert^2 
            &\le 
            F(z) - F(\bar x) - \langle \beta(x - \bar x), z - \bar x\rangle 
            + D_f(x, \bar x ) - D_f(z, x).  
        \end{align*}
    \end{theorem}
    \begin{proof}
        Nonsmooth analysis calculus rules has 
        \begin{align*}
            \bar x &\in \argmin{z} \left\lbrace
                g(z) + \langle \nabla f(x), z\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
            \right\rbrace
            \\
            \implies
            \mathbf 0 
            &\in \partial g(x^+) + \nabla f(x) + \beta(x^+ - x)
            \\
            \iff 
            \partial g(x^+) &\ni
            - \nabla f(x) - \beta(x^+ - x). 
        \end{align*}
        The subgradient inequality for weak convexity has 
        \begin{align*}
            \frac{q}{2}\Vert z - \bar x\Vert^2 
            &\le 
            g(z) - g(\bar x) + \langle \nabla f(x) + \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) + \langle \nabla f(x), z - \bar x\rangle + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= g(z) - g(\bar x) + \langle \nabla f(x), z - x\rangle
            + \langle \nabla f(x), x - \bar x\rangle
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) 
            + (-D_f(z, x) + f(z) - f(x))
            \\
            & \quad 
            + (D_f(\bar x, x) - f(\bar x) + f(x))
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= F(z) - F(\bar x) - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle. 
        \end{align*}
    \end{proof}
    \begin{theorem}[convex proximal gradient inequality]\label{theorem:pg-ineq}
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} such that $q = \mu_g \ge 0$, $\beta \ge L_f$. 
        In addition, suppose that $f:\RR^n\rightarrow \RR$ has $L_f$ Lipschitz continuous gradient, and it's $\mu_f \ge 0$ strongly convex. 
        For all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has 
        \begin{align*}
            0 &\le 
            F(z) - F(\bar x) + 
            \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2
            - \frac{\beta + \mu_g}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{theorem}
    \begin{proof}
        The Bregman Divergence of $f$ has inequality 
        \begin{align*}
            \left(\forall x \in \RR^n, y \in \RR^n\right)\; 
            \frac{\mu_f}{2}\Vert x - y\Vert^2 \le D_f(x, y) \le \frac{L_f}{2}\Vert x - y\Vert^2. 
        \end{align*}
        Specializing Theorem \ref{thm:pg-ineq-wcnvx-generic}, let $x \in \RR^n$ and define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has $\forall z \in \RR^n:$
        \begin{align*}
            \frac{\mu_g}{2}\Vert z - \bar x \Vert^2 
            &\le 
            F(z) - F(\bar x) 
            - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \frac{L_f}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x + x - \bar x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \left(
                \frac{L_f}{2} - \beta
            \right)\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}
            \left(
                \Vert x - \bar x\Vert^2
                + 2\langle x - \bar x, z - x\rangle
            \right)
            \\
            &= 
            F(z) - F(\bar x) 
            + \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{proof}
    
\chapter{Linear Convergence of First Order Method}
    In this chapter, we are specifically interested in characterizing linear convergence of well known first order optimization algorithms. 
    In this section, $D_f$ will denote the Bregman Divergence as defined in Definition \ref{def:bregman-div}. 
    \section{Necoara's et al's Paper}
        \subsection{The Settings}
            The assumption follows give the same setting as Necoara et al. \cite{necoara_linear_2019}. 
            % =======================
            % NECOARA's ASSUMPTIONS 
            % =======================
            \begin{assumption}\label{ass:necoara-2019-settings}
                Consider optimization problem: 
                \begin{align}
                    -\infty < f^+ = \min_{x \in X} f(x) . 
                \end{align}\label{problem:necoara-2019}
                $X\subseteq \RR^n$ is a closed convex set. 
                Assume projection onto $X$, denoted by $\Pi_X$ is easy. 
                Denote $X^+ = \argmin_{x \in X}f(x) \neq \emptyset$, assume it's a closed set. 
                Assume $f$ has $L_f$ Lipschitz continuous gradient, i.e: for all $x, y\in X$: 
                \begin{align*}
                    \Vert \nabla f(x) - \nabla f(y)\Vert \le L_f\Vert x - y\Vert. 
                \end{align*}
            \end{assumption}
            Some immediate consequences of Assumption \ref{ass:necoara-2019-settings} now follows. 
            The variational inequality characterizing optimal solution has: 
            \begin{align*}
                x^+ \in X^+ \implies 
                (\forall x \in X)\; \langle \nabla f(x^+), x - x^+\rangle \ge 0. 
            \end{align*}
            The converse is true if $f$ is convex. 
            The gradient mapping in this case is: 
            \begin{align*}
                \mathcal G_{L_f}x = L_f(x - \Pi_{X}x). 
            \end{align*}
            % ==============================
            % STRONG CONVEXITY DEFINITION
            % ==============================
            \begin{definition}[strong convexity]\label{def:necoara-scnvx}
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then $f \in \mathbb S(L_f, \kappa_f, X)$ is strongly convex iff 
                \begin{align*}
                    (\forall x, y\in X)\; 
                    \kappa_f \Vert x - y\Vert^2 \le 
                    D_f(x, y) \le L_f \Vert x - y\Vert^2. 
                \end{align*}
            \end{definition}
            Then it's not hard to imagine the following natural relaxation of the above conditions. 
            %===================================================================
            % DEFINITION OF WEAKER STRONG CONVEXITY 
            % ==================================================================
            \begin{definition}[relaxations of strong convexity]\;\\
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}.
                \label{def:necoara-weaker-scnvx}
                Let $L_f \ge \kappa_f \ge 0$ such that for all $x \in X$, $\bar x = \Pi_{X^+} x$. 
                We define the following: 
                \begin{enumerate}
                    \item\label{def:neocara-qscnvx} Quasi-strong convexity (Q-SCNVX): $0 \le D_f(\bar x, x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb S'(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qup} Quadratic under approximation (QUA): $0 \le D_f(x, \bar x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb U(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qgg} Quadratic Gradient Growth (QGG): $0\le D_f(x, \bar x) + D_f(\bar x, x) - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb G(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qfg} Quadratic Function Growth (QFG): $0 \le f(x) - f^* - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb F(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-peb} Proximal Error Bound (PEB): $\Vert \mathcal G_{L_f}x\Vert \ge \kappa_f\Vert x - \bar x\Vert$. 
                    Denoted by $\mathbb E(L_f, \kappa_f, X)$. 
                \end{enumerate}
            \end{definition}
            \begin{remark}
                The error bound condition in Necoara et al. is sometimes referred to as the "Proximal Error Bound". 
            \end{remark}

        \subsection{Weaker conditions of strong convexity}
            In Necoara's et al, major results assume convexity of $f$. 
            % ============================================================================
            % THEOREM | Q-SCNVX IMPLIES QUA 
            % ============================================================================
            \begin{theorem}[Q-SCNVX implies QUA]\label{thm:qscnvx-means-qua}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings} and assume $f$ is convex: 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                We proof by induction. 
                Convexity of $f$ makes $X^+$ convex and $\Pi_{X^+}X$ unique for all $x \in X$. 
                Make inductive hypothesis that there exists $\kappa^{(k)} \ge 0$ such that 
                \begin{align*}
                    (\forall x \in X)\quad
                    f(x) \ge f^+ + \langle \nabla f(\Pi_{X^+}x), x - \Pi_{X^+}x\rangle 
                    + \kappa^{(k)}/2\Vert x - \Pi_{X^+}x \Vert^2. 
                \end{align*}
                The base case is true by convexity of $f$ with $\kappa_f^{(0)} = 0$. 
                Choose any $x \in X$ define $\bar x = \Pi_{X^+}x$. 
                Consider $x_\tau = \bar x + \tau(x - \bar x)$ for $\tau \in [0, 1]$. 
                Calculus rule has 
                \begin{align*}
                    f(x) &= 
                    f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau
                    \\
                    &= 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), \tau(x - \bar x)\rangle d\tau
                    \\
                    &= 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), x_\tau - \bar x\rangle d\tau.
                \end{align*}
                $f$ is Q-SCNVX so
                \begin{align*}
                    f^+ - f(x_\tau) &\ge \langle \nabla f(x_\tau), \Pi_{X^+}x_\tau - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2
                    \\
                    &= 
                    \langle \nabla f(x_\tau), \bar x - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \bar x\Vert^2
                    \\
                    \iff 
                    \langle \nabla f(x_\tau), x_\tau - \bar x\rangle
                    &\ge f(x_\tau) - f^+ + \kappa_f/2\Vert x_\tau -\bar x\Vert^2. 
                \end{align*}
                We used $\Pi_{X^+}x_\tau = \bar x$ by convexity of $f$. 
                Therefore:
                {\footnotesize
                \begin{align*}
                    f(x) &\ge 
                    f(\bar x) + 
                    \int_0^1 \tau^{-1} \left(
                        f(x_\tau) - f^+ + \frac{\kappa_f}{2}\Vert x_\tau - \bar x\Vert^2
                    \right) d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            f(x_\tau) - f^+ 
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &\ge 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\Pi_{X^+}x_\tau), x_\tau - \Pi_{X^+}x_\tau
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \Pi_{X^+}x_\tau\Vert^2
                    d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\bar x), x_\tau - \bar x
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \bar x\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \int_0^1 
                        \langle 
                            \nabla f(\bar x), x - \bar x
                        \rangle
                        + \frac{\tau\kappa_f^{(k)}}{2} \Vert x - \bar x\Vert^2
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \langle 
                        \nabla f(\bar x), x - \bar x
                    \rangle
                    +
                    \frac{\kappa^{(k)}_f + \kappa_f}{4}
                    \Vert x - \bar x\Vert^2. 
                \end{align*}
                }
                This is the new inductive hypothesis, and it has $\kappa_f^{(k + 1)} = (\kappa_f^{(k)} + \kappa_f)/2$. 
                The induction admits recurrence: 
                \begin{align*}
                    \kappa_f^{(n)} = (1/2^n)(\kappa_f^{(0)} + (2^n - 1)\kappa_f). 
                \end{align*}
                Inductive hypothesis is true for $\kappa_f^{(0)} = 0$ and $f$ being convex is sufficient. 
                It has $\lim_{n\rightarrow \infty} \kappa_f^{(n)} = \kappa_f$. 
            \end{proof}
            \begin{remark}
                This is Theorem 1 in the paper. 
                Convexity assumption of $f$ makes $X^+$ convex, so the projection is unique, and it has $\Pi_{X^+}x_\tau = \bar x$ for all $\tau \in [0, 1]$. 
                In addition, the inductive hypothesis has $\kappa_f^{(n)} \ge 0$, which is not sufficient for convexity, but necessary. 
                The projection property remains true for nonconvex $X^+$, however the base case require rethinking. 
            \end{remark}
            % ================================================================================
            % THEOREM | QGG IMPLIES QUA 
            % ================================================================================
            \begin{theorem}[QGG implies QUA]
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}, under convexity it has 
                \begin{align*}
                    \mathbb G(L_f, \kappa_f, X)\subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                For all $x \in X$, define $\bar x = \Pi_{X^+}x$, $x_\tau = \bar x + \tau(x - \bar x)\; \forall \tau \in [0, 1]$. 
                Observe that $\frac{d}{d\tau}x_\tau = x - \bar x$ and $\Pi_{X^+}x_\tau = \bar x\; \forall \tau \in [0, 1]$. 
                Using calculus and Theorem \ref{def:necoara-weaker-scnvx} \ref{def:necoara-qgg}: 
                \begin{align*}
                    f(x) &= f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau  
                    \\
                    &= f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \langle \nabla f(x_\tau) - \nabla f(\bar x), x - \bar x\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), \tau(x - \bar x)\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), x_\tau - \bar x\rangle d \tau
                    \\
                    &\ge
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\kappa_f\Vert \tau(x - \bar x)\Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau\kappa_f\Vert x - \bar x \Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \frac{\kappa}{2}\Vert x - \bar x\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                This is Theorem 3 in Neocara et al. \cite{necoara_linear_2019}. 
                There is no immediate use of convexity besides that the projection $\bar x = \Pi_{X^+}x$ is a singleton.
            \end{remark}
            % ===============================================================================
            % THEOREM | QFC IMPLIES QGG  
            % ===============================================================================
            \begin{theorem}[Q-SCNVX implies QGG]
                Under Assumption \ref{ass:necoara-2019-settings} and convexity of $f$, it has 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb G(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                If $f \in \mathbb S'(L_f, \kappa_f, X)$ then Theorem \ref{thm:qscnvx-means-qua} has $f \in \mathbb U(L_f, \kappa_f, X)$. 
                Then, add \ref{def:necoara-qup}, \ref{def:neocara-qscnvx} in Definition \ref{def:necoara-weaker-scnvx} yield the results. 
            \end{proof}
            \begin{remark}
                This is Theorem 2 in the Necoara et al. \cite{necoara_linear_2019}, right after it claims $\mathbb U(L_f, \kappa_f, X)\subseteq \mathbb G(L_f, \kappa_f/2, X)$ under convexity. 
            \end{remark}
            % =============================================================================
            % THEOREM | SUFFICIENCY OF QFG 
            % =============================================================================
            \begin{theorem}[sufficiency of QFG]\label{thm:qfg-suff}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                For all $0 < \beta < 1$, $x \in X$, let $x^+ = \Pi_{X}(x - L^{-1}_f \nabla f(x))$. 
                If 
                \begin{align*}
                    \Vert x^+ - \Pi_{X^+}x^+\Vert \le \beta \Vert x - \Pi_{X^+}x \Vert, 
                \end{align*}
                then $f$ satisfies the QFG condition with $\kappa_f = L_f(1 - \beta)^2$. 
            \end{theorem}
            \begin{proof}
                The proof is direct. 
                \begin{align}
                    \Vert x - \Pi_{X^+}x\Vert 
                    &\le \Vert x - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \Vert x^+ - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \beta \Vert x - \Pi_{X^+}x\Vert
                    \\
                    \iff 
                    0 &\le \Vert x - x^+\Vert - (1 - \beta) \Vert x - \Pi_{X^+}x\Vert. 
                \end{align}
                $x^+$ has descent lemma hence we have 
                \begin{align*}
                    f^+ - f(X) \le f(x^+) - f(x) 
                    \le - \frac{L_f}{2}\Vert x^+ - x\Vert^2 
                    \le - \frac{L_f}{2}(1 - \beta)^2 \Vert x - \Pi_{X^+}\Vert^2. 
                \end{align*}
                Hence, it gives the quadratic growth condition. 
            \end{proof}
            \begin{remark}
                It's unclear where convexity is used. 
                However, it' still assumed in Necoara et al. paper. 
            \end{remark}
            Before we start, we will specialize Theorem \ref{theorem:pg-ineq} because it will be used in later proofs. 
            In Assumption \ref{ass:necoara-2019-settings}, it can be seemed as taking $F = f + g$ in Assumption \ref{ass:smooth-add-nonsmooth} with $g = \delta_{X}$. 
            This makes $\mu_g = 0$ and assuming $f$ is convex we have $\mu_f = 0$. 
            Let $\beta = L_f$, and $x^+ = \Pi_{X}(x - L_f^{-1}\nabla f(x))$, it has for all $z \in X$: 
            \begin{align}\label{ineq:proj-grad}
                \begin{split}
                    0 &\le 
                    f(z) - f(x^+) + \frac{L_f}{2}\Vert z - x\Vert^2
                    - \frac{L_f}{2}\Vert z - x^+\Vert^2
                    \\
                    &= 
                    f(z) - f(x^+) + L_f\langle z - x^+, x^+ - x\rangle
                    + \frac{L_f}{2}\Vert x - x^+\Vert^2. 
                \end{split}
            \end{align}
            Take note that when $z = x$ it has 
            \begin{align}\label{ineq:proj-grad2}
                0 &\le f(x) - f(x^+) - \frac{L_f}{2}\Vert x - x^+\Vert^2. 
            \end{align}
            \par
            The following theorems are about the relation between PEB and QFG.
            % ==========================================
            % THEOREM | EQUIVALENCE BETWEEN QFG AND PEB 
            % ==========================================
            \begin{theorem}[equivalence between QFG and PEB]
                If $f$ is convex and satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then we have: 
                \begin{align*}
                    \mathbb E(L_f, \kappa_f, X) &\subseteq \mathbb F(L_f, \kappa^2/L_f, X), 
                    \\
                    \mathbb F(L_f, \kappa_f) 
                    &\subseteq 
                    \mathbb E\left(
                        L_f,
                        \frac{\kappa_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}, 
                        X
                    \right). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                \textbf{Firstly, we show that PEB implies QFG.}
                For any $x \in X$, define the gradient projection steps by $x^+ = \Pi_{X}(x - L^{-1}_f\nabla f(x))$. 
                Denote $\bar x^+_\Pi = \Pi_{X^+}x^+$. 
                Using \eqref{ineq:proj-grad} with $z = \bar x^+_\Pi$ it yields: 
                {\small
                \begin{align*}
                    0 &\ge 
                    f(x^+) - f(x^+_\Pi) - L_f\langle x_\Pi^+ - x^+, x^+ - x\rangle
                    - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert^2
                    \\
                    \quad&\ge 
                    \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \Vert L_f(x - x^+)\Vert\Vert x^+_\Pi - x^+\Vert
                    - \frac{1}{2L_f}\Vert L_f(x - x^+)\Vert^2   &\text{QFG, Cauchy}
                    \\
                    &= \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert^2
                        + L_f\Vert L_f(x - x^+)\Vert\Vert x_\Pi^+ - x^+\Vert
                    \right)
                    \\
                    &= 
                    \frac{\kappa_f + L_f}{2}\Vert x^+ - x^+_\Pi\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert + L_f\Vert x - x_\Pi^+\Vert
                    \right)^2
                \end{align*}
                }
                From the last line, by positivity of norm it has
                \begin{align*}
                    \Vert L_f(x - x^+)\Vert + L_f\Vert x^+ - x_\Pi^+\Vert
                    &\ge \sqrt{L_f(\kappa_f + L_f)}\Vert x^+ - x^+_\Pi\Vert
                    \\
                    \iff 
                    \Vert L_f(x - x^+)\Vert
                    &\ge 
                    \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\Vert x^+ - x^+_\Pi\Vert. 
                \end{align*}
                Using property of projection onto $X$ we have 
                \begin{align*}
                    \Vert x - \bar x\Vert &\le \Vert x - x_\Pi^+\Vert
                    \le \Vert x - x^+\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \\
                    &= \frac{1}{L_f}\Vert L_f(x - x^+)\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \\
                    \iff 
                    \Vert x - \bar x\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert
                    &\le 
                    \Vert x^+ - x^+_\Pi\Vert. 
                \end{align*}
                Combining the two above inequalities gives 
                {\small
                \begin{align*}
                    0 &\le 
                    \Vert L_f(x - x^+)\Vert
                    -
                    \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\left(
                        \Vert x - \bar x\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert
                    \right)
                    \\
                    &=
                    - \left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \left(
                        L^{-1}_f\left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right) + 1
                    \right)\Vert L_f(x - x^+)\Vert
                    \\
                    &= 
                    -\left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \sqrt{L_f(\kappa_f + L_f)}
                    \Vert L_f(x - x^+)\Vert
                    \\
                    \iff&
                    \frac{\sqrt{L_f(\kappa_f + L_f)} - L_f}{\sqrt{L_f(\kappa_f + L_f)}}
                    \Vert x - \bar x\Vert 
                    \le
                    \Vert \mathcal G_{L_f}x\Vert. 
                \end{align*}
                }
                Skipping algebra, the fraction simplifies to 
                \begin{align*}
                    \frac{\kappa_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}. 
                \end{align*}
                This gives PEB condition. 
                \textbf{We now show QFG implies PEB}. 
                From the error bound condition using $\kappa_f$ it has
                \begin{align*}
                    \kappa_f^2\Vert x - \bar x\Vert^2
                    \le \Vert \mathcal G_{L_f}(x)\Vert^2
                    \underset{\eqref{ineq:proj-grad2}}{\le }
                    2L_f(f(x) - f(x^+)) \le 2L_f(f(x) - f^+). 
                \end{align*}
            \end{proof}
            \par
            The following theorem summarizes the hierarchy of the conditions listed in Definition \ref{def:necoara-weaker-scnvx}. 
            \begin{theorem}[Hierarchy of weaker S-CNVX conditions]\label{thm:weaker-scnvx-hierarchy}
                Let $f$ satisfy Assumption \ref{ass:necoara-2019-settings}, assuming convexity then the following relations are true: 
                \begin{align*}
                    \mathbb S(\kappa_f, L_f, X) 
                    \subseteq \mathbb S'(\kappa_f, L_f, X)
                    \subseteq \mathbb G(\kappa_f, L_f, X) 
                    \subseteq \mathbb U(\kappa_f, L_f, X) 
                    \subseteq \mathbb F(\kappa_f, L_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                
            \end{proof}
            \begin{remark}
                It's Theorem 4 in Necoara et al. \cite{necoara_linear_2019}.
            \end{remark}
        \subsection{Hoffman error bound and Q-SCNVX}

        \subsection{Feasible descent and accelerated feasible descent}

        \subsection{Application, KKT of linear programming}

        


% ==============================================================================

\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}