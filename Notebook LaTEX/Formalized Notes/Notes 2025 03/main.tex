\documentclass[12pt]{report}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}
\title{{\fontfamily{ptm}\selectfont Reading Notes}}

\author{
    Alto
    % \thanks{
    %     Subject type, Some Department of Some University, Location of the University,
    %     Country. E-mail: \texttt{author.name@university.edu}.
    % }
}

\date{Last Compiled: \today}

\maketitle

\begin{abstract} 
    Reports on papers read. 
    This is a LaTEX file for my own notes taking. 
    It may accelerate the process of writing my thesis for my PhD degree. 
    \todoinline{This paper is currently in draft mode. Check source to change options. }
\end{abstract}
\chapter{The Basics of Optimization Theories}
    Notations in this chapter are not shared, and they are for this chapter only. 
    % ==========================================================================
    % BREGMAN DIV DEFINITION 
    % ==========================================================================
    \begin{definition}[Bregman Divergence]\label{def:bregman-div}
        Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
        Define Bregman Divergence: 
        \begin{align*}
            D_f: \RR^n \times \dom \nabla f \rightarrow \overline \RR:= 
            (x, y) \mapsto f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
        \end{align*}
    \end{definition}
    \begin{assumption}[smooth plus nonsmooth]\label{ass:smooth-add-nonsmooth}
        Let $F = f+ g$ where $f:\RR^n \rightarrow \overline \RR$ is differentiable and there exists $q\in \RR$ such that $g - \mu/2\Vert \cdot\Vert^2$ is convex.
    \end{assumption}
    \begin{definition}[proximal gradient operator]
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth}. 
        Let $\beta > 0$, we define the proximal gradient operator for all $x \in \RR^n$: 
        \begin{align*}
            T_{\beta^{-1}, f, g}(x) &:= \hprox_{\beta^{-1}g} \left(
                x - \beta^{-1} \nabla f(x)
            \right)
            \\
            &= \argmin_{z}\left\lbrace
                g(z) + f(x) + \langle \nabla f(x), z - x\rangle
                + \frac{\beta}{2}\Vert x - z\Vert^2
            \right\rbrace. 
        \end{align*}
    \end{definition}
    % ==========================================================================
    % WEAKLY CONVEX GENERIC PROXIMAL GRADIENT INEQUALITY
    % ==========================================================================
    \begin{theorem}[strongly/weakly convex generic proximal gradient inequality]\;\label{thm:pg-ineq-swcnvx-generic}\\
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} with $\beta > 0$ and $\mu \in \RR$. 
        Then for all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$, it has: 
        \begin{align*}
            \frac{\mu}{2}\Vert z - \bar x\Vert^2 
            &\le 
            F(z) - F(\bar x) - \langle \beta(x - \bar x), z - \bar x\rangle 
            + D_f(x, \bar x ) - D_f(z, x).  
        \end{align*}
    \end{theorem}
    \begin{proof}
        Nonsmooth analysis calculus rules has 
        \begin{align*}
            \bar x &\in \argmin{z} \left\lbrace
                g(z) + \langle \nabla f(x), z\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
            \right\rbrace
            \\
            \implies
            \mathbf 0 
            &\in \partial g(\bar x) + \nabla f(x) + \beta(\bar x - x)
            \\
            \iff 
            \partial g(x^+) &\ni
            - \nabla f(x) - \beta(\bar x - x). 
        \end{align*}
        The subgradient inequality for weak convexity has 
        \begin{align*}
            \frac{\mu}{2}\Vert z - \bar x\Vert^2 
            &\le 
            g(z) - g(\bar x) + \langle \nabla f(x) + \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) + \langle \nabla f(x), z - \bar x\rangle + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= g(z) - g(\bar x) + \langle \nabla f(x), z - x\rangle
            + \langle \nabla f(x), x - \bar x\rangle
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) 
            + (-D_f(z, x) + f(z) - f(x))
            \\
            & \quad 
            + (D_f(\bar x, x) - f(\bar x) + f(x))
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= F(z) - F(\bar x) - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle. 
        \end{align*}
    \end{proof}
    \begin{theorem}[convex proximal gradient inequality]\label{thm:cnvx-pg-ineq}
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} such that $\mu = \mu_g \ge 0$, $\beta \ge L_f$. 
        In addition, suppose that $f:\RR^n\rightarrow \RR$ has $L_f$ Lipschitz continuous gradient, and it's $\mu_f \ge 0$ strongly convex. 
        For all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has 
        \begin{align*}
            0 &\le 
            F(z) - F(\bar x) + 
            \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2
            - \frac{\beta + \mu_g}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{theorem}
    \begin{proof}
        The Bregman Divergence of $f$ has inequality 
        \begin{align*}
            \left(\forall x \in \RR^n, y \in \RR^n\right)\; 
            \frac{\mu_f}{2}\Vert x - y\Vert^2 \le D_f(x, y) \le \frac{L_f}{2}\Vert x - y\Vert^2. 
        \end{align*}
        Specializing Theorem \ref{thm:pg-ineq-swcnvx-generic}, let $x \in \RR^n$ and define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has $\forall z \in \RR^n:$
        \begin{align*}
            \frac{\mu_g}{2}\Vert z - \bar x \Vert^2 
            &\le 
            F(z) - F(\bar x) 
            - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \frac{L_f}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x + x - \bar x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \left(
                \frac{L_f}{2} - \beta
            \right)\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}
            \left(
                \Vert x - \bar x\Vert^2
                + 2\langle x - \bar x, z - x\rangle
            \right)
            \\
            &= 
            F(z) - F(\bar x) 
            + \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{proof}
    
\chapter{Linear Convergence of First Order Method}
    In this chapter, we are specifically interested in characterizing linear convergence of well known first order optimization algorithms. 
    In this section, $D_f$ will denote the Bregman Divergence as defined in Definition \ref{def:bregman-div}. 
    \section{Necoara's et al.'s Paper}
        \subsection{The Settings}
            The assumption follows give the same setting as Necoara et al. \cite{necoara_linear_2019}. 
            % ==================================================================
            % NECOARA's ASSUMPTIONS 
            % ==================================================================
            \begin{assumption}\label{ass:necoara-2019-settings}
                Consider optimization problem: 
                \begin{align}
                    -\infty < f^+ = \min_{x \in X} f(x) . 
                \end{align}\label{problem:necoara-2019}
                $X\subseteq \RR^n$ is a closed convex set. 
                Assume projection onto $X$, denoted by $\Pi_X$ is easy. 
                Denote $X^+ = \argmin_{x \in X}f(x) \neq \emptyset$, assume it's a closed set. 
                Assume $f$ has $L_f$ Lipschitz continuous gradient, i.e: for all $x, y\in X$: 
                \begin{align*}
                    \Vert \nabla f(x) - \nabla f(y)\Vert \le L_f\Vert x - y\Vert. 
                \end{align*}
            \end{assumption}
            Some immediate consequences of Assumption \ref{ass:necoara-2019-settings} now follows. 
            The variational inequality characterizing optimal solution has: 
            \begin{align}\label{ineq:pg-opt-cond}
                x^+ \in X^+ \implies 
                (\forall x \in X)\; \langle \nabla f(x^+), x - x^+\rangle \ge 0. 
            \end{align}
            The converse is true if $f$ is convex. 
            The gradient mapping in this case is: 
            \begin{align*}
                \mathcal G_{L_f}x = L_f(x - \Pi_{X}x). 
            \end{align*}
            % ==================================================================
            % STRONG CONVEXITY DEFINITION
            % ==================================================================
            \begin{definition}[strong convexity]\label{def:necoara-scnvx}
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then $f \in \mathbb S(L_f, \kappa_f, X)$ is strongly convex iff 
                \begin{align*}
                    (\forall x, y\in X)\; 
                    \kappa_f \Vert x - y\Vert^2 \le 
                    D_f(x, y) \le L_f \Vert x - y\Vert^2. 
                \end{align*}
            \end{definition}
            Then it's not hard to imagine the following natural relaxation of the above conditions. 
            %===================================================================
            % DEFINITION OF WEAKER STRONG CONVEXITY 
            % ==================================================================
            \begin{definition}[relaxations of strong convexity]\;\\
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}.
                \label{def:necoara-weaker-scnvx}
                Let $L_f \ge \kappa_f \ge 0$ such that for all $x \in X$, $\bar x = \Pi_{X^+} x$. 
                We define the following: 
                \begin{enumerate}
                    \item\label{def:neocara-qscnvx} Quasi-strong convexity (Q-SCNVX): $0 \le D_f(\bar x, x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb S'(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qup} Quadratic under approximation (QUA): $0 \le D_f(x, \bar x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb U(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qgg} Quadratic Gradient Growth (QGG): $0\le D_f(x, \bar x) + D_f(\bar x, x) - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb G(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qfg} Quadratic Function Growth (QFG): $0 \le f(x) - f^* - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb F(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-peb} Proximal Error Bound (PEB): $\Vert \mathcal G_{L_f}x\Vert \ge \kappa_f\Vert x - \bar x\Vert$. 
                    Denoted by $\mathbb E(L_f, \kappa_f, X)$. 
                \end{enumerate}
            \end{definition}
            \begin{remark}
                The error bound condition in Necoara et al. is sometimes referred to as the "Proximal Error Bound". 
            \end{remark}

        \subsection{Weaker conditions of strong convexity}
            In Necoara's et al., major results assume convexity of $f$. 
            % ==================================================================
            % THEOREM | Q-SCNVX IMPLIES QUA 
            % ==================================================================
            \begin{theorem}[Q-SCNVX implies QUA]\label{thm:qscnvx-means-qua}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings} and assume $f$ is convex: 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                We prove by induction. 
                Convexity of $f$ makes $X^+$ convex, so $\Pi_{X^+}x$ is unique for all $x \in \RR^n$. 
                Make inductive hypothesis that there exists $\kappa^{(k)} \ge 0$ such that 
                \begin{align*}
                    (\forall x \in X)\quad
                    f(x) \ge f^+ + \langle \nabla f(\Pi_{X^+}x), x - \Pi_{X^+}x\rangle 
                    + \kappa^{(k)}_f/2\Vert x - \Pi_{X^+}x \Vert^2. 
                \end{align*}
                The base case is true by convexity of $f$ with $\kappa_f^{(0)} = 0$. 
                Choose any $x \in X$ define $\bar x = \Pi_{X^+}x$. 
                Consider $x_\tau = \bar x + \tau(x - \bar x)$ for $\tau \in [0, 1]$. 
                $f$ is Q-SCNVX so
                \begin{align}\label{ineq:thm:qscnvx-means-qua-proof-item1}
                    f^+ - f(x_\tau) &\ge \langle \nabla f(x_\tau), \Pi_{X^+}x_\tau - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2 
                    \notag\\
                    &= 
                    \langle \nabla f(x_\tau), \bar x - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \bar x\Vert^2
                    \notag\\
                    \iff 
                    \langle \nabla f(x_\tau), x_\tau - \bar x\rangle
                    &\ge f(x_\tau) - f^+ + \kappa_f/2\Vert x_\tau -\bar x\Vert^2. 
                \end{align}
                In the inductive proof that comes, we will use the following intermediate results. 
                They are labeled for ease of refernecing. 
                \begin{enumerate}
                    \item The inequality \eqref{ineq:thm:qscnvx-means-qua-proof-item1}. 
                    \item By the property of projection, it has $\Pi_{X^+} x_\tau = \bar x$. 
                    \item The inductive hypothesis with $k \ge 0$. 
                    \item $\bar x = \Pi_{X^+}x$, $X^+$ is the set of minimizer of the of $f$ over $X$, hence $f(\bar x) = f^+$, the minimum. 
                \end{enumerate}
                Using calculus rules, we start with: 
                {\footnotesize
                \begin{align*}
                    f(x) &= 
                    f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau
                    = 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), \tau(x - \bar x)\rangle d\tau
                    \\
                    &= 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), x_\tau - \bar x\rangle d\tau.
                    \\
                    &\underset{\text{(i)}}{\ge }
                    f(\bar x) + 
                    \int_0^1 \tau^{-1} \left(
                        f(x_\tau) - f^+ + \frac{\kappa_f}{2}\Vert x_\tau - \bar x\Vert^2
                    \right) d\tau
                    = 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            f(x_\tau) - f^+ 
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &\underset{\text{(iii)}}{\ge }
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\Pi_{X^+}x_\tau), x_\tau - \Pi_{X^+}x_\tau
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \Pi_{X^+}x_\tau\Vert^2
                    d\tau
                    \\
                    &\underset{\text{(ii)}}{=} 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\bar x), x_\tau - \bar x
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \bar x\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \int_0^1 
                        \langle 
                            \nabla f(\bar x), x - \bar x
                        \rangle
                        + \frac{\tau\kappa_f^{(k)}}{2} \Vert x - \bar x\Vert^2
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &\underset{\text{(iv)}}{=} 
                    f^+ + 
                    \langle 
                        \nabla f(\bar x), x - \bar x
                    \rangle
                    +
                    \frac{\kappa^{(k)}_f + \kappa_f}{4}
                    \Vert x - \bar x\Vert^2. 
                \end{align*}
                }
                This is the new inductive hypothesis, and it has $\kappa_f^{(k + 1)} = (\kappa_f^{(k)} + \kappa_f)/2$. 
                The induction admits recurrence: 
                \begin{align*}
                    \kappa_f^{(n)} = (1/2^n)(\kappa_f^{(0)} + (2^n - 1)\kappa_f). 
                \end{align*}
                Inductive hypothesis is true for $\kappa_f^{(0)} = 0$ and $f$ being convex is sufficient. 
                It has $\lim_{n\rightarrow \infty} \kappa_f^{(n)} = \kappa_f$. 
            \end{proof}
            \begin{remark}
                This is Theorem 1 in the paper. 
                Convexity assumption of $f$ makes $X^+$ convex, so the projection is unique, and it has $\Pi_{X^+}x_\tau = \bar x$ for all $\tau \in [0, 1]$. 
                In addition, the inductive hypothesis has $\kappa_f^{(n)} \ge 0$, which is not sufficient for convexity, but necessary. 
                The projection property remains true for nonconvex $X^+$, however the base case require rethinking. 
            \end{remark}
            % ================================================================================
            % THEOREM | QGG IMPLIES QUA 
            % ================================================================================
            \begin{theorem}[QGG implies QUA]\label{thm:qgg-implies-qua}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}, under convexity it has 
                \begin{align*}
                    \mathbb G(L_f, \kappa_f, X)\subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                For all $x \in X$, define $\bar x = \Pi_{X^+}x$, $x_\tau = \bar x + \tau(x - \bar x)\; \forall \tau \in [0, 1]$. 
                Observe that $\frac{d}{d\tau}x_\tau = x - \bar x$ and $\Pi_{X^+}x_\tau = \bar x\; \forall \tau \in [0, 1]$. 
                Using calculus, Definition \ref{def:necoara-weaker-scnvx} \ref{def:necoara-qgg}: 
                \begin{align*}
                    f(x) &= f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau  
                    \\
                    &= f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \langle \nabla f(x_\tau) - \nabla f(\bar x), x - \bar x\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), \tau(x - \bar x)\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), x_\tau - \bar x\rangle d \tau
                    \\
                    &\ge
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\kappa_f\Vert \tau(x - \bar x)\Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau\kappa_f\Vert x - \bar x \Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \frac{\kappa}{2}\Vert x - \bar x\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                This is Theorem 3 in Neocara et al. \cite{necoara_linear_2019}. 
                There is no immediate use of convexity besides that the projection $\bar x = \Pi_{X^+}x$ is a singleton.
            \end{remark}
            % ==================================================================
            % THEOREM | QFC IMPLIES QGG  
            % ==================================================================
            \begin{theorem}[Q-SCNVX implies QGG]\label{thm:qscnvx-implies-qgg}
                Under Assumption \ref{ass:necoara-2019-settings} and convexity of $f$, it has 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb G(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                If $f \in \mathbb S'(L_f, \kappa_f, X)$ then Theorem \ref{thm:qscnvx-means-qua} has $f \in \mathbb U(L_f, \kappa_f, X)$. 
                Then, add \ref{def:necoara-qup}, \ref{def:neocara-qscnvx} in Definition \ref{def:necoara-weaker-scnvx} yield the results. 
            \end{proof}
            \begin{remark}
                This is Theorem 2 in the Necoara et al. \cite{necoara_linear_2019}, right after it claims $\mathbb U(L_f, \kappa_f, X)\subseteq \mathbb G(L_f, \kappa_f/2, X)$ under convexity. 
            \end{remark}
            % ==================================================================
            % THEOREM | SUFFICIENCY OF QFG 
            % ==================================================================
            \begin{theorem}[sufficiency of QFG]\label{thm:qfg-suff}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                For all $0 < \beta < 1$, $x \in X$, let $x^+ = \Pi_{X}(x - L^{-1}_f \nabla f(x))$. 
                If 
                \begin{align*}
                    \Vert x^+ - \Pi_{X^+}x^+\Vert \le \beta \Vert x - \Pi_{X^+}x \Vert, 
                \end{align*}
                then $f$ satisfies the QFG condition with $\kappa_f = L_f(1 - \beta)^2$. 
            \end{theorem}
            \begin{proof}
                The proof is direct. 
                \begin{align}
                    \Vert x - \Pi_{X^+}x\Vert 
                    &\le \Vert x - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \Vert x^+ - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \beta \Vert x - \Pi_{X^+}x\Vert
                    \\
                    \iff 
                    0 &\le \Vert x - x^+\Vert - (1 - \beta) \Vert x - \Pi_{X^+}x\Vert. 
                \end{align}
                $x^+$ has descent lemma hence we have 
                \begin{align*}
                    f^+ - f(X) \le f(x^+) - f(x) 
                    \le - \frac{L_f}{2}\Vert x^+ - x\Vert^2 
                    \le - \frac{L_f}{2}(1 - \beta)^2 \Vert x - \Pi_{X^+}\Vert^2. 
                \end{align*}
                Hence, it gives the quadratic growth condition. 
            \end{proof}
            \begin{remark}
                It's unclear where convexity is used. 
                However, it' still assumed in Necoara et al. paper. 
            \end{remark}
            Before we start, we will specialize Theorem \ref{thm:cnvx-pg-ineq} because it will be used in later proofs. 
            In Assumption \ref{ass:necoara-2019-settings}, it can be seemed as taking $F = f + g$ in Assumption \ref{ass:smooth-add-nonsmooth} with $g = \delta_{X}$. 
            This makes $\mu_g = 0$ and assuming $f$ is convex we have $\mu_f = 0$. 
            Let $\beta = L_f$, and $x^+ = \Pi_{X}(x - L_f^{-1}\nabla f(x))$, it has for all $z \in X$: 
            \begin{align}\label{ineq:proj-grad}
                \begin{split}
                    0 &\le 
                    f(z) - f(x^+) + \frac{L_f}{2}\Vert z - x\Vert^2
                    - \frac{L_f}{2}\Vert z - x^+\Vert^2
                    \\
                    &= 
                    f(z) - f(x^+) + L_f\langle z - x^+, x^+ - x\rangle
                    + \frac{L_f}{2}\Vert x - x^+\Vert^2. 
                \end{split}
            \end{align}
            Take note that when $z = x$ it has 
            \begin{align}\label{ineq:proj-grad2}
                0 &\le f(x) - f(x^+) - \frac{L_f}{2}\Vert x - x^+\Vert^2. 
            \end{align}
            \par
            The following theorems are about the relation between PEB and QFG.
            % ==================================================================
            % LEMMA | QFG AND GRADIENT MAPPING
            % ==================================================================
            \begin{lemma}[gradient mapping and quadratic function growth]\;\label{lemma:grad-map-qfg}\\
                Let $f$ satisfies Assumtion \ref{ass:necoara-2019-settings}. 
                Suppose that $f \in \mathbb F(L_f, \mu_f, X)$ so it satisfies the quadratic function growth condition. 
                For all $x \in \RR^n$, define $x^+ = \Pi_X(x - L^{-1}_f\nabla f(x))$, 
                definte projections onto the set of minimizers $x^+_\Pi = \Pi_{X^+} x^+, X_\Pi = \Pi_{X^+}x$, then
                \begin{align*}
                    \left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x^+ - x_\Pi^+\Vert
                    &\le \Vert L_f(x - x^+)\Vert. 
                \end{align*}
            \end{lemma}
            \begin{proof}
                Using convexity, consider \eqref{ineq:proj-grad} with $z = x^+_\Pi$ it yields: 
                {\small
                \begin{align*}
                    0 &\ge 
                    f(x^+) - f(x^+_\Pi) - L_f\langle x_\Pi^+ - x^+, x^+ - x\rangle
                    - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert^2
                    \\
                    &\ge
                    \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \Vert L_f(x - x^+)\Vert\Vert x^+_\Pi - x^+\Vert
                    - \frac{1}{2L_f}\Vert L_f(x - x^+)\Vert^2 
                    \\
                    &= \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert^2
                        + L_f\Vert L_f(x - x^+)\Vert\Vert x_\Pi^+ - x^+\Vert
                    \right)
                    \\
                    &= 
                    \frac{\kappa_f + L_f}{2}\Vert x^+ - x^+_\Pi\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert + L_f\Vert x - x_\Pi^+\Vert
                    \right)^2.
                \end{align*}
                }
                From the last line, it's can be equivalently expressed as:
                \begin{align*}
                    0 &\le
                    \Vert L_f(x - x^+)\Vert + L_f\Vert x^+ - x_\Pi^+\Vert
                    - \sqrt{L_f(\kappa_f + L_f)}\Vert x^+ - x^+_\Pi\Vert
                    \\
                    &=
                    \Vert L_f(x - x^+)\Vert
                    - \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\Vert x^+ - x^+_\Pi\Vert.
                \end{align*}
            \end{proof}
            % ==================================================================
            % THEOREM | EQUIVALENCE BETWEEN QFG AND PEB 
            % ==================================================================
            \begin{theorem}[equivalence between QFG and PEB]\label{thm:qfg-peb-equiv}
                If $f$ is convex and satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then we have: 
                \begin{align*}
                    \mathbb E(L_f, \kappa_f, X) &\subseteq \mathbb F(L_f, \kappa^2_f/L_f, X), 
                    \\
                    \mathbb F(L_f, \kappa_f) 
                    &\subseteq 
                    \mathbb E\left(
                        L_f,
                        \frac{\kappa_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}, 
                        X
                    \right). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                For any $x \in X$, define the gradient projection steps by $x^+ = \Pi_{X}(x - L^{-1}_f\nabla f(x))$. 
                Denote $x^+_\Pi = \Pi_{X^+}x^+$. 
                Let $x_\Pi = \Pi_{X^+}x$, using the property of projection onto $X$ we have 
                \begin{align}\label{ineq:thm:qfg-peb-equiv-proof-item1}
                    \Vert x - x_\Pi\Vert &\le \Vert x - x_\Pi^+\Vert
                    \le \Vert x - x^+\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \notag\\
                    &= \frac{1}{L_f}\Vert L_f(x - x^+)\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \notag\\
                    \iff 
                    \Vert x^+ - x^+_\Pi\Vert &\ge
                    \Vert x - x_\Pi\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert. 
                \end{align}
                Before we start, we list intermediate results and conditions which are going to be used in the proof that follows for the ease of referencing. 
                \begin{enumerate}
                    \item The inequality \eqref{ineq:thm:qfg-peb-equiv-proof-item1}. It uses the property of projection onto a set hence convexity of $X^+$ is not needed. 
                \end{enumerate}
                Starting with Lemma \ref{lemma:grad-map-qfg} because $f$ satisfies quadratic growth and it is assumed convex, then it has: 
                {\small
                \begin{align*}
                    0 &\le 
                    \Vert L_f(x - x^+)\Vert
                    - \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\Vert x^+ - x^+_\Pi\Vert
                    \\
                    &\underset{\text{(i)}}{\le}
                    \Vert L_f(x - x^+)\Vert
                    -
                    \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\left(
                        \Vert x - \bar x\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert
                    \right)
                    \\
                    &=
                    - \left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \left(
                        L^{-1}_f\left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right) + 1
                    \right)\Vert L_f(x - x^+)\Vert
                    \\
                    &= 
                    -\left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \sqrt{L_f(\kappa_f + L_f)}
                    \Vert L_f(x - x^+)\Vert
                    \\
                    \iff&
                    \frac{\sqrt{L_f(\kappa_f + L_f)} - L_f}{\sqrt{L_f(\kappa_f + L_f)}}
                    \Vert x - \bar x\Vert 
                    \le
                    \Vert \mathcal G_{L_f}x\Vert. 
                \end{align*}
                }
                Skipping some algebra, the fraction simplifies to 
                \begin{align*}
                    \frac{\kappa_f/L_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}. 
                \end{align*}
                This gives PEB condition. 
                \textbf{We now show PEB implies QFG}. 
                From the error bound condition using $\kappa_f$ it has
                \begin{align*}
                    \kappa_f^2\Vert x - \bar x\Vert^2
                    \le \Vert \mathcal G_{L_f}(x)\Vert^2
                    \underset{\eqref{ineq:proj-grad2}}{\le }
                    2L_f(f(x) - f(x^+)) \le 2L_f(f(x) - f^+). 
                \end{align*}
            \end{proof}
            \par
            The following theorem summarizes the hierarchy of the conditions listed in Definition \ref{def:necoara-weaker-scnvx}. 
            \begin{theorem}[Hierarchy of weaker S-CNVX conditions]\label{thm:q-cnvx-hierarchy}
                Let $f$ satisfy Assumption \ref{ass:necoara-2019-settings}, assuming convexity then the following relations are true: 
                \begin{align*}
                    \mathbb S(\kappa_f, L_f, X) 
                    \subseteq \mathbb S'(\kappa_f, L_f, X)
                    \subseteq \mathbb G(\kappa_f, L_f, X) 
                    \subseteq \mathbb U(\kappa_f, L_f, X) 
                    \subseteq \mathbb F(\kappa_f, L_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                $\mathbb S' \subseteq \mathbb G$ is proved in Theorem \ref{thm:qscnvx-implies-qgg} and $\mathbb G \subseteq \mathbb U$ is proved in \ref{thm:qgg-implies-qua}. 
                $\mathbb S\subseteq \mathbb S'$ is obvious and it remains to show $\mathbb U \subseteq \mathbb F$. 
                Let $f\in \mathbb U(\kappa_f, L_f, X)$, it has for all $x \in X$: 
                \begin{align*}
                    0 &\le f(x) - f^+ - \langle \nabla f(\bar x), x - \bar x\rangle - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2
                    \\
                    &\hspace{-0.5em}\underset{\eqref{ineq:pg-opt-cond}}{\le} 
                    f(x) - f^+ - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                It's Theorem 4 in Necoara et al. \cite{necoara_linear_2019}.
            \end{remark}

        \subsection{Feasible descent and accelerated feasible descent}
            This section summarizes results from Necoara et al. on the method of feasible descent, fast feasible descent, and fast feasible descent with restart. 
            \begin{definition}[projected gradient algorithm]\label{def:projg-alg}\;\\
                The projected gradient algorithm generates a sequence of iterates $(x_k)_{k \ge 0}$ such that they satisfy for all $k \ge 0$
                \begin{align*}
                    x_{k + 1} &= \Pi_X(x_k - \alpha_k \nabla f(x_k)), 
                \end{align*}
                Where $\alpha_k \ge L_f^{-1}$ for all $k \ge 1$. 
            \end{definition}
            Under Assumption \ref{ass:necoara-2019-settings}, convexity of $X$ means obtuse angle theorem from projection, and it specializes to 
            \begin{align}\label{ineq:projg-variational-ineq}
                (\forall x \in X)\; \langle x_{k + 1} - (x_k + \alpha_k \nabla f(x_k)), x_{k + 1} - x\rangle \le 0. 
            \end{align}

            \begin{theorem}{feasible descent linear convergence under Q-SCNVX}
                Under Assumption \ref{ass:necoara-2019-settings}, assume that $f$ is Q-CNVX with $\mu_f, L_f$, then the sequence that satisfies Definition \ref{def:projg-alg} has a linear convergence rate. 
                Let $\bar x_k = \Pi_{X^+}x_k, \bar x_0 = \Pi_{X^+} x_0$. 
                For all $k \ge 1$, the iterates satisfy
                \begin{align*}
                    \Vert x_k - \bar x_k\Vert^2 &\le \left(
                        \frac{1 - \kappa_f/L_f}{1 + \kappa_f/L_f}
                    \right)^k \Vert x_0 - \bar x_0\Vert^2. 
                \end{align*}
            \end{theorem}
            \begin{proof}
                Our proof makes use of the following properties which we label it in advance for swift exposition: 
                \begin{enumerate}
                    \item Inequality \eqref{ineq:projg-variational-ineq}, from the projected gradient and convexity of $X$. 
                    \item $f \in \mathbb S'$ which is the hypothesis that $f$ is Q-CNVX. 
                    \item $\alpha_k \le L_f^{-1}$, the stepsize is sufficient to apply descent lemma globally. 
                    \item $f \in \mathbb Q$ satisfying Q-Growth, a consequence of Q-CNVX by Theorem \ref{thm:q-cnvx-hierarchy}. 
                \end{enumerate}
                With $\overline{(\cdot)} = \Pi_{X^+}(\cdot)$ to denote the projection of a vector to the set of minimizers. 
                The sequence of inequalities and equalities proves the theorem. 
                {\allowdisplaybreaks
                \begin{align*}
                    \Vert x_{k + 1} - \bar x_k\Vert^2
                    &= 
                    \Vert x_{k + 1} - x_k + x_k - \bar x_k\Vert^2 
                    = \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2 + 2\langle x_{k + 1} - x_k, x_k - \bar x_k\rangle
                    \\
                    &= (- \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2)
                    + 2\Vert x_{k + 1} - x_k\Vert^2 + 2\langle x_{k + 1} - x_k, x_k - \bar x_k\rangle
                    \\
                    &= - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    + 2 \langle x_{k + 1} - x_{k}, x_{k + 1} - \bar x_k\rangle
                    \\
                    &= 
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    \\  & \quad \;
                        + 2 \langle x_{k + 1} - x_{k} + \alpha_k \nabla f(x_k), x_{k + 1} - \bar x_k\rangle
                        - 2\alpha_k \langle \nabla f(x_k), x_{k + 1} - \bar x_k\rangle
                    \\
                    &\underset{\text{(i)}}{\le}
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    - 2\alpha_k \langle \nabla f(x_k), x_{k + 1} - \bar x_k\rangle
                    \\
                    &= 
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    + 2\alpha_k \langle \nabla f(x_k), \bar x_k - x_k\rangle
                    + 2\alpha_k \langle \nabla f(x_k), x_k - x_{k + 1}\rangle
                    \\
                    &\underset{\text{(ii)}}{\le}
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    \\ &\quad 
                        + 2\alpha_k \left(
                            f^+ - f(x_k) - \frac{\kappa_f}{2}\Vert x_k - \bar x_k\Vert^2
                        \right)
                        + 2\alpha_k \langle \nabla f(x_k), x_k - x_{k + 1}\rangle
                    \\
                    &= (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2
                    \\&\quad 
                        + 2\alpha_k(f^+ - f(x_k)) - 2\alpha_k 
                        \left(
                            \langle \nabla f(x_k), x_{k + 1} - x_k\rangle + \frac{1}{2\alpha_k}\Vert x_{k + 1} - x_k\Vert^2
                        \right)
                    \\
                    &= 
                    (1 - \alpha_k\kappa_f)\Vert x_k - \bar x_k\Vert^2 + 2 \alpha_k f^+
                    \\&\quad 
                        - 2 \alpha_k\left(
                            f(x_k) + \langle \nabla f(x_k), x_{k + 1} - x_k\rangle 
                            + \frac{1}{2\alpha_k}\Vert x_{k + 1} - x_k\Vert^2
                        \right)
                    \\
                    &\underset{\text{(iii)}}{\le} 
                    (1 - \alpha_k\kappa_f)\Vert x_k - \bar x_k\Vert^2 + 2 \alpha_k f^+
                    \\ &\quad 
                        - 2 \alpha_k\left(
                            f(x_k) + \langle \nabla f(x_k), x_{k + 1} - x_k\rangle 
                            + \frac{L_f}{2}\Vert x_{k + 1} - x_k\Vert^2
                        \right)
                    \\
                    &\le 
                    (1 - \alpha_k\kappa_f)\Vert x_k - \bar x_k\Vert^2 + 2 \alpha_k f^+
                    - 2\alpha_kf(x_{k + 1})
                    \\
                    &\underset{\text{(iv)}}{\le} 
                    (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2 - \alpha_k \kappa_k \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2. 
                \end{align*}
                }
                Therefore, it has 
                \begin{align*}
                    0 &\le \Vert x_{k + 1} - \bar x_k\Vert^2 - \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2
                    \\
                    &\le 
                    (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2 
                    - \alpha_k \kappa_k \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2
                    - \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2
                    \\
                    &= (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2 
                    - (1 + \alpha_k \kappa_k)\Vert x_{k + 1} - \bar x_{k + 1}\Vert^2. 
                \end{align*}
                Unrolling recursively, then use (iii), the claim is proved. 
            \end{proof}

\chapter{Application, Linear Feasibility Problems}
    This chapter extends ideas by the end of Necoara et al.'s paper \cite{necoara_linear_2019}.
    \section{Reducing LP to linear feasibility problems}
        We first introduce the linear feasibility problem as our primary problem. 
        \begin{definition}[a linear feasibility problem]
            Let $A\in \RR^{m\times n}$, $b \in \RR^m$
            Let $\mathcal K$ be a simple cone. 
            The linear feasibility problem is defined as the following optimization problem: 
            \begin{align*}
                \min_{x \in \mathcal K} \Vert Ax - b\Vert^2.
            \end{align*}
        \end{definition}
        The KKT of a linear programming problem is an instance of a linear feasibility problem. 
        \par
        Let $X_1, X_2, Y$ be Euclidean spaces. 
        Define linear mapping $E:X_1 \times X_2 \rightarrow Y := (x_1, x_2)\mapsto E_1 x_1 + E_2 x_2$ where $E_1, E_2$ each are mappings of $X_1 \rightarrow Y, X_2 \rightarrow Y$. 
        Denote the adjoint of linear mapping by $(\cdot)^*$. 
        Let $c = (c_1, c_2) \in X_1 \times X_2$, $b \in Y$. 
        Suppose that $\mathcal K \subseteq X_1$ is a simple cone and $K^*$ is its dual cone. 
        We consider the following linear programming problem 
        \begin{align}\label{problem:lp-cannon-form}
            \inf_{x \in X_1\times X_2}\left\lbrace
                \langle - c, x\rangle
                \left| \;
                    Ex = b, x \in \mathcal K \times X_2
                \right.
            \right\rbrace. 
        \end{align}
        Define linear mapping $g, F$ and indicator function $h$ by the following: 
        \begin{align*}
            g:X_1\times X_2 \rightarrow \RR 
                &:= x \mapsto \langle - c, x\rangle, 
            \\
            F: X_1\times X_2 \rightarrow Y \times X_1 
                &:= (x_1, x_2) \mapsto (E_1x_1 + E_2 x_2, x_1),
            \\
            h: Y \times X_1 \rightarrow \overline \RR &:= 
                (y, z) \mapsto \delta_{\{\mathbf 0\}}(y - b) + \delta_{\mathcal K}(z). 
        \end{align*}
        It's not hard to identify that problem in \eqref{problem:lp-cannon-form} has representations 
        \begin{align*}
            \inf_{x \in X_1\times X_2}
            \left\lbrace
                g(x) + h(Fx)
            \right\rbrace. 
        \end{align*}
        The dual problem of the above is given by
        \begin{align*}
            -\inf_{u \in Y\times X_1}
            \left\lbrace
                h^\star(u) + g^\star(-F^* u)
            \right\rbrace. 
        \end{align*}
        Where $h^\star, g^\star$ are the conjugate of $h, g$ and $F^*: Y\times X_1 \rightarrow X_1 \times X_2 = (y, z)\mapsto (E_1^*y + z, E_2^*y)$ is the adjoint operator of $F$. 
        Note that $g^\star(x) = \delta_{\mathbf 0}(x + c)$ and $h^\star((y, z)) = \langle b, y\rangle + \delta_{\mathcal K^*}(z)$. 
        This gives the following dual problem 
        \begin{align*}
            - \inf_{(y, z) \in Y \times \mathcal K^*} \left\lbrace
                \langle b, y\rangle 
                \left | \;
                    E_1^*y + z = c_1, 
                    E^*_2y = c_2
                \right.
            \right\rbrace. 
        \end{align*}
        The KKT conditions give the following linear feasibility problem 
        \begin{align*}
            E_1 x_1 + E_2 x_2 &= b, \\
            E_1^* y + z &= c_1, \\
            E_2^* y &= c_2, \\
            \langle b, y\rangle &= \langle c_1, x_1\rangle + \langle c_2, x_2\rangle, \\
            (x_1, x_2) &\in \mathcal K \times X_2, \\
            (y, z) &\in Y \times \mathcal K^*.
        \end{align*}
        Assuming $X_1 = \RR^{n_1}, X_2 = \RR^{n_2}, Y = \RR^m$. 
        Define 
        \begin{align*}
            \mathbf K &:= \mathcal K \times \RR^{n_2} \times \RR^m \times \mathcal K^*, 
            \\
            A &:= \begin{bmatrix}
                E_1 & E_2 & \mathbf 0 & \mathbf 0
                \\
                \mathbf 0 &\mathbf 0  & E_1^T & I_{n_1}
                \\
                \mathbf 0 &\mathbf 0  & E_2^T & \mathbf 0
                \\
                c_1^T & c_2^T & - b^T & 0
            \end{bmatrix}, 
            v := 
            \begin{bmatrix}
                x_1\\ x_2 \\ y \\ z \\
            \end{bmatrix} \in \mathbf K, 
            d := 
            \begin{bmatrix}
                b \\ c_1 \\ c_2 \\ 0
            \end{bmatrix}. 
        \end{align*}
        The KKT conditions is a convex feasibility problem which can be formulated by best approximation problem: 
        \begin{align}\label{problem:lp-kkt-min}
            \min_{v \in \mathbf K} 
            \frac{1}{2}\Vert Ax - d \Vert^2. 
        \end{align}
        It is minimizing a quadratic problem on a simple cone. 
        Solving \eqref{problem:lp-cannon-form} can be approached by optimizing \eqref{problem:lp-kkt-min}. 
        It's necessary to investigate the matrices $A, A^T$ which are essential to solving it numerically. 
        The properties of $A^TA$ will determine the convergence rate of algorithms. 
        The matrix is a block matrix and possibly sparse in practice. 
        Let $v = (x_1, x_2, y, z)$, it admits implicit representation: 
        \begin{align*}
            Av = (E_1x_1 + E_2 x_2,\; E_1^Ty + z,\; E_2^Ty,\; c^T_1x_1 + c_2^Tx_2 - b^Ty). 
        \end{align*}
        It involves 
        \begin{enumerate}
            \item Two multiplications of $E$: $x_1, x_2$ on the right and $y$ on the right,  
            \item inner product using $x_1, x_2$ and $y$. 
        \end{enumerate}
        Let $\bar v = (\bar y, \bar x_1, \bar x_2, \xi) \in \RR^m\times \RR^{n_1} \times \RR^{n_2}\times \RR$ then the right multiplication of has: 
        \begin{align*}
            \bar v^TA  &= (
                E_1^T\bar y + \xi c_1^T,\; E_2^T\bar y + \xi c_2^T,\; 
                \bar x_1^TE_1^T + \bar x_2^TE_2^T - \xi b^T,\; \bar x_1^T
            )
            \\
            &= 
            (
                E_1^T\bar y + \xi c_1, \;
                E_2^T \bar y + \xi c_2, \;
                E_1\bar x_1 + E_2\bar x_2 - \xi b, \;
                \bar x_1
            )^T. 
        \end{align*}
        \begin{enumerate}
            \item Two multiplications of $E$: $\bar y$ on the left and for $\bar x_1, \bar x_2$ on the right, 
            \item one vector addition with $c = (c_1, c_2)$ and $b$. 
        \end{enumerate}
        Therefore, computing $A^TAv$ has four vector multiplications using $E$. 
        In practice, a sparse matrix $E$ from the model can speed up computations. 
        \par
        Another key operation would be $A^TAv$. 
        Let $\bar v = Av$, then 
        \begin{align*}
            A^TAv &= 
            \begin{bmatrix}
                E^T_1(E_1x_1 + E_2x_2) + (c_1^Tx_1 + c_2^Tx_2 - b^Ty)c_1
                \\
                E^T_2(E_1x_1 + E_2x_2) + (c_1^Tx_1 + c_2^Tx_2 - b^Ty)c_2
                \\
                E_1(E_1^Ty + z) + E_2E_2^Ty - (c_1^Tx_1 + c_2^Tx_2 - b^Ty)b
                \\
                E_1^Ty + z
            \end{bmatrix}
            \\
            &= 
            \begin{bmatrix}
                (E_1^TE_1 + c_1^T)x_1 + (E_1^TE_2 + c_2^T)x_2 - (c_1b^T)y
                \\
                (E_2^TE_1 + c_1^T)x_1 + (E_2^TE_2 + c_2^T)x_2 - (c_2b^T)y
                \\
                -(bc_1^T)x_1 - (bc_2^T)x_2 + (E_2E_2^T + E_1E_1^T + bb^T)y
                + E_1z
                \\
                E_1^Ty + z
            \end{bmatrix}
            \\
            &= 
            \begin{bmatrix}
                E_1^TE_1 + c_1^T & E_1^TE_2 + c_2^T & -c_1b^T & \\
                E_2^TE_1 + c_1^T & E_2^TE_2 + c_2^T & -c_2b^T & \\
                -bc_1^T& -bc_2^T & E_2E_2^T + E_1E_1^T + bb^T & E_1 \\
                & & E_1^T & I_{n_1}\\
            \end{bmatrix}
            \begin{bmatrix}
                x_1 \\ x_2 \\ y \\ z
            \end{bmatrix}. 
        \end{align*}
        In practice, implicitly representing the process of $A^TAv$ is better in computing software. 
        Here we write it out to view, for theoretical interests. 
        \subsection{Investigating the matrix}
            
        \subsection{Speedy evaluations}
            Let $f(v) = (1/2)\Vert Av - d\Vert^2$ to be the objective function of optimization problem \eqref{problem:lp-kkt-min}. 
            Its gradient, objective value, and Bregman Divergence have: 
            \begin{align*}
                \nabla f(v) &= A^TAv - A^Td, 
                \\
                f(v) &= 
                \frac{1}{2}\langle v, \nabla f(v) - A^Td\rangle + \frac{1}{2}\Vert d\Vert^2, 
                \\
                D_f(u, v) &= (1/2)\langle u - v, A^TA (u - v)\rangle
                \\
                &= (1/2)\langle \nabla f(u) - \nabla f(v), u - v\rangle. 
            \end{align*}
            The value $\nabla f(v), f(v)$ when evaluated together, require minimal additional computations. 
            This fact is favorable for implementations in practice. 
            Furthermore, the difference of the function value between 2 points $v, u$ admits an interesting relation via the Bregman Divergence. 
            Observe that $\forall u, v \in \RR^n$ it has 
            \begin{align*}
                f(u) - f(v) &= \langle \nabla f(v), u - v \rangle + D_f(u, v)
                \\
                &= \langle \nabla f(v), u - v \rangle + (1/2)\langle \nabla f(u) - \nabla f(v), u - v\rangle
                \\
                &= (1/2)\langle \nabla f(u) + \nabla f(v), u - v\rangle. 
            \end{align*}
            For this problem, the computation overhead for $f(u) - f(v), D_f(u, v)$ is very little if $\nabla f(u), \nabla f(v)$ is known. 
    \section{Hoffman error bound and Q-SCNVX}

\chapter{Advanced Enhancement Techniques in Accelerated Proximal Gradient}
    We review advanced enhancement techniques in Accelerated Proximal Gradient method. 
    The review will be based on several papers. 
    \par
    There are several notable enhancements of the FISTA for function that are not strongly convex. 
    Monotone variants of FISTA proposed by Beck \cite{beck_fast_2009-1} and Nesterov \cite[2.2.32]{nesterov_lectures_2018} imposes monotonicity in function value at the iterates.  
    Backtracking strategies from Chambolle \cite{calatroni_backtracking_2019} shows that the underestimating Lipschitz constant using a backtracking technique to choose a next iterate improves the average runtime of the algorithm in practice. 
    They showed that the convergence rate is bounded by the estimates of the Lipschitz constant. 
    Restart is a technique discussed in Necoara et al. \cite{necoara_linear_2019}.
    They showed that there exists an optimal restarting interval to achieve fast linear convergence rate for all functions with quadratic growth condition. 
    The method of restarting was improved to be parameter free while still retaining a fast linear convergence rate, see Alamo et al. \cite{alamo_restart_2019} and Aujol et al. \cite{aujol_parameter-free_2024}. 
    \par
    In this chapter, we will go through the details of these enhancements of FISTA and discuss why they are important in theories, and in practice. 
    
    \section{Preliminaries}
        This section introduces the full scope of the theories in our analysis. 
        Firstly, recall the definition of Bregman divergence $D_f(x, y)$ from Definition \ref{def:bregman-div} for a differentiable function $f:\RR^n \rightarrow \overline \RR$. 
        \subsection{smooth plus nonsmooth weakly convex}
            \begin{definition}[weakly convex function]\;\label{def:wcnvx-fxn}\\
                Let $F: \RR^n \rightarrow\overline \RR$ be an l.s.c proper function. 
                We define $F$ to be $q$ weakly convex if there exists $q \ge 0$ such that the function $F + q/2\Vert \cdot\Vert^2$ is a convex function and $q$ is the infimum of all such possible parameters. 
            \end{definition}
            \begin{remark}
                If $q = 0$, $F$ is convex.
                If $F$ is weakly convex, then $F + q/2\Vert \cdot\Vert^2$ is convex and, it has $\dom F$ convex, and locally Lipschitz continuous on $\reli \dom F$. 
            \end{remark}
            \begin{assumption}[sum of weakly convex smooth and nonsmooth]\;\label{ass:sum-of-wcnvx}\\
                Let $F: \RR^n \rightarrow \overline\RR:= f + g$ such that $f, g$ satisfy 
                \begin{enumerate}
                    \item $f$ is $L$ Lipschitz smooth and $q_f$ weakly convex. 
                    \item $g$ is $q_g$ weakly convex. 
                \end{enumerate}
            \end{assumption}
            \begin{remark}
                If a function is $L$ smooth, it's $L$ weakly convex also. 
                Here we defined $q_f$ because the actual weakly convex constant may be much smaller than $L$, and it is true in the case when $g$ is in fact convex.  
            \end{remark}
            \begin{definition}[gradient mapping]\label{def:gm-for-ch2}
                Suppose $F = f + g$ satisfies Assumption \ref{ass:sum-of-wcnvx}, define the gradient mapping for all $x \in \RR^n$
                \begin{align*}
                    \mathcal G_{\beta^{-1}, f, g}(x) = \beta(x - T_{\beta^{-1}, f, g}(x)). 
                \end{align*}
                If $f, g$ are clear in the context then we omit subscript and present $\mathcal G_\beta$. 
            \end{definition}
            \begin{lemma}[weakly convex monotone descent]\;\label{lemma:mono-wcnvx-descent}\\
                Let $F = f + g$ satisfies Assumption \ref{ass:sum-of-wcnvx}. 
                Let $\bar x = T_{\beta^{-1}, f, g}(x)$. 
                Then, for all $x \in \RR^n$, it has the following inequality: 
                $$
                \begin{aligned}
                    0 \le F(x) - F(\bar x) - (\beta - q_g/2 - L/2)\Vert x - \bar x\Vert^2. 
                \end{aligned}
                $$
                And descent is possible when $\beta \ge (L + q_g)/2$ and, it yields the descent lemma: 
                \begin{align*}
                    F(\bar x) - F(x) &\le - 1/\beta \Vert \mathcal G_{1/\beta}(x)\Vert^2. 
                \end{align*}
            \end{lemma}
            \begin{proof}
                Use Theorem \ref{thm:pg-ineq-swcnvx-generic}, set $z = x$, after some algebra it yields: 
                \begin{align*}
                    0 \le F(x) - F(\bar x) - \left(
                        \beta - \frac{q_g + L}{2}
                    \right)\Vert x - \bar x\Vert^2. 
                \end{align*}
                Using the definition of gradient mapping previously, it has for all $\beta > 0$: 
                \begin{align*}
                    0 &\le F(x) - F(\bar x) - \left(
                        \beta - \frac{q_g + L}{2}
                    \right)\Vert \beta^{-1}\mathcal G_{1/\beta}(x) \Vert^2
                    \\
                    &\le F(x) - F(\bar x) - \left(
                        \beta^{-1} - \frac{q_g + L}{2\beta^2}
                    \right)\Vert\mathcal G_{1/\beta}(x) \Vert^2
                \end{align*}
                % Optimizing $x\mapsto x - x^2(q_g + L)/2$ yields $x = (q_g + L)^{-1}$ so $\beta = q_g - L$ gives the most amount of descent. 
                Consider any $\beta \ge (q_g + L)$: 
                \begin{align*}
                    0 &\le F(x) - F(\bar x) - \left(
                        \beta^{-1} - \frac{q_g + L}{2\beta^2}
                    \right)\Vert\mathcal G_{1/\beta}(x) \Vert^2
                    \\
                    &\le F(x) - F(\bar x) + \left(
                        \beta^{-1}/2 - \beta^{-1}
                    \right)\Vert\mathcal G_{1/\beta}(x) \Vert^2
                    \\
                    &= 
                    F(x) - F(\bar x) - \frac{1}{2\beta}\Vert\mathcal G_{1/\beta}(x) \Vert^2. 
                \end{align*}
            \end{proof}
        \subsection{smooth plus nonsmooth convex}
            The following assumption is strictly stronger than \ref{ass:sum-of-wcnvx}. 
            \begin{assumption}[convex smooth and nonsmooth]\label{ass:standard-fista}
                Let $F = f + g$ where $f:\RR^n\rightarrow \overline \RR$ is $L$ Lipschitz smooth, $g$ is convex. 
                Suppose that $\argmin_{x \in \RR^n} F(x)\neq \emptyset$.
            \end{assumption}
            \begin{lemma}[proximal gradient inequality]\label{lemma:fitsa-pg-ineq}
                If $F = f + g$ satisfies Assumption \ref{ass:standard-fista}, then for all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{L^{-1}, f, g}(x)$ it has 
                \begin{align*}
                    0 &\le F(z) - F(\bar x) + \frac{L}{2}\Vert z - x\Vert^2 - \frac{L}{2}\Vert z - \bar x\Vert^2. 
                \end{align*}
            \end{lemma}
            \begin{proof}
                Use Theorem \ref{thm:cnvx-pg-ineq} with $\mu_f = \mu_g = 0$. 
            \end{proof}
            
    \section{FISTA made simple}
        We make the proofs for FISTA  with common enhancement technique available in simple proofs. 
        We showcase the theories using a generic similar triangle representations of the algorithm which tremendously simplifies the arguments. 
        The following definition captures several monotone variants of FISTA with line search,  or backtracking strategies.
        \par
        In this section, we give convergence results for a unified formulation of monotone accelerated proximal gradient methods with line search or backtracking enhancements. 
        Definition \ref{def:generic-mapg-ls} gives a unified view of several combined heuristics. 
        Theorem \ref{thm:gmapg-ls-convergence} gives a generic description for the convergence rate. 
        Lemma \ref{lemma:gmapg-seq-bnd} specializes the sequence, attains the lowest upper bound. 
        Theorem \ref{thm:gmapg-specialized-cnvg} gives a concrete $\mathcal O(1/k^2)$ upper bound for the optimality gap and normed gradient mapping on the last iteration. 
        \par
        ``MAPG'' stands for monotone accelerated gradient. 
        We refer to ``Generic Monotone Accelerated Proximal Gradient with line search" as ``GMAPG''. 
        \begin{definition}[GMAPG]\label{def:generic-mapg-ls}\;\\ 
            Initialize any $x_0, v_0 \in \RR^n$. 
            Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_k \in (0, 1) \;\forall k \ge 0$ and $\alpha_0 \in (0, 1]$. 
            \begin{tcolorbox}
                The algorithm makes sequences $(x_k, v_k, y_k)_{k \ge 1}$, such that for all $k = 1, 2, \ldots$ they satisfy: 
                \begin{align*}
                    & y_k = \alpha_k v_{k - 1} + (1 - \alpha_k) x_{k - 1}, \\
                    & \tilde x_k = T_{L_k^{-1}}(y_{k}), \\ 
                    & v_k = x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1}), \\
                    & D_{f}(\tilde x_k, y_k) \le \frac{L_k}{2}\Vert \tilde x_k - y_k\Vert^2, \\
                    & \text{Choose any } x_k: F(x_k) \le \min(F(\tilde x_k), F(x_{k - 1})). 
                \end{align*}    
            \end{tcolorbox}
        \end{definition}
        The following definition characterizes the sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$, $(L_k)_{k \ge 0}$ and defines $(\beta_k)_{k \ge 0}$ for the proofs for the convergence rate. 
        \begin{definition}[alpha momentum sequence]\label{def:alpha-beta-rho-seq}
            Let $(\alpha_k)_{k \ge 0}$ be a sequence in $\RR$ such that $\alpha_k \in (0, 1)$ for all $k \in \N$. 
            Take $(L_k)_{k \ge 0}$ from Definition \ref{def:generic-mapg-ls} so it has $L_k > 0$ for all $k \in \N \cup \{0\}$. 
            The sequence $(\rho_k)_{k \ge 0}$ is defined as 
            \begin{align*}
               \rho_{k} &= (1 - \alpha_{k + 1})^{-1}a_{k + 1}^2\alpha_k^{-2}. 
            \end{align*}
            Define sequence $(\beta_k)_{k \ge 0}$ such that $\beta_0 = 1$ and for all $k \ge 1$: 
            \begin{align*}
                \beta_k := \prod_{i = 0}^{k - 1} (1 - \alpha_{i + 1})\max\left(1, \rho_i L_{i + 1}L_i^{-1}\right). 
            \end{align*}
        \end{definition}
        \begin{lemma}[acceerated proximal gradient iterates relation]\;\label{lemma:apg-iterates}\;\\
            The iterates $(x_k, v_k, y_k)_{k \ge 1}$ generated by Definition \ref{def:generic-mapg-ls}. 
            Let $z_k = \alpha_k x^+ + (1 - \alpha_k)x_{k - 1}$. 
            Then it has for all $k \ge 1$ that: 
            \begin{align*}
                z_k - \tilde x_k &= \alpha_k(x^+ - v_k)
                \\
                x_k - y_k &= \alpha_k(x^+ - v_{k - 1}). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            It's direct from the algorithm. 
            \begin{align*}
                z_k - \tilde x_k &= (\alpha_k x^+ + (1 - \alpha_k)x_{k - 1}) - \tilde x_k
                \\
                &= \alpha_k (x^+ + \alpha_k^{-1}(1 - \alpha_k)x_{k - 1} - \alpha_k^{-1}\tilde x_k)
                \\
                &= \alpha_k(x^+ + \alpha_k^{-1}x_{k - 1} - x_{k - 1} - \alpha_k^{-1}\tilde x_k)
                \\
                &= \alpha_k(x^+ + \alpha_k^{-1}(x_{k - 1} - \tilde x_k) - x_{k - 1})
                \\
                &= \alpha_k(x^+ - v_{k}), 
                \\
                z_k - y_k &= 
                (\alpha_k x^+ + (1 - \alpha_k)x_{k - 1}) - \left(
                    \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                \right)
                \\
                &= \alpha_k(x^+ + \alpha_k^{-1}(1 - \alpha_k)x_{k - 1} - v_{k - 1} - \alpha_k^{-1}(1 - \alpha_k)x_{k - 1})
                \\
                &= \alpha_k(x^+ - v_{k - 1}). 
            \end{align*}
        \end{proof}
        \begin{theorem}[generic GMAPG convergence]\; \label{thm:gmapg-ls-convergence}\;\\
            Let $F = f + g$ satisfy Assumptions \ref{ass:standard-fista}. 
            % Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_k \in (0, 1)$ for all $k \ge 1$ and $\alpha_0 \in (0, 1]$. 
            % Let $\rho_k = (1 - \alpha_{k + 1})^{-1}\alpha_{k + 1}^2 \alpha_k^{-2}$ for all $k \ge 0$. 
            Take the sequence $(\alpha_k)_{k \ge 0}, (\beta_k)_{k \ge 0}$ and $(\rho_k)_{k \ge 0}$ from Definition \ref{def:alpha-beta-rho-seq}. 
            Then, for all $x^+ \in \RR^n, k \ge 1$, the convergence rate of GMAPG (Definition \ref{def:generic-mapg-ls}) is given by: 
            \begin{align*}
                F(x_k) - F(x^+) + \frac{L_k\alpha_k}{2}\Vert x^+ - v_k\Vert^2
                \le 
                \beta_k
                \left(
                    F(x_0) - F(x^+) + \frac{L_0\alpha_0}{2} \Vert x^+ - v_0\Vert^2
                \right). 
            \end{align*}
            If in addition, the algorithm is initialized using line search so that \mbox{$D_f(x_0, x_{-1}) \le L_0/2 \Vert x_0 - x_{-1}\Vert^2$}, $\alpha_0 = 1, x_0 = v_0 = T_{L_{0}}x_{-1} \in \dom F$ and, $x^+$ is a minimizer of $F$.
            Then, the convergence rate simplifies: 
            \begin{align*}
                F(x_k) - F(x^+) + \frac{L_k\alpha_k}{2}\Vert x^+ - v_k\Vert^2
                & \le 
                \frac{\beta_kL_0}{2}\Vert x^+ - x_{-1}\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Define $z_k = \alpha_k x^+ + (1 - \alpha_k)x_{k - 1}$ for all $k \ge 1$. 
            In the proof follows, the follow facts will be used. 
            We list them in advance, and they will be labeled during the proof. 
            \begin{enumerate}
                \item Lemma \ref{lemma:apg-iterates}. 
                \item The sequence $(\alpha_k)_{k \ge 1}$ has for all $k \ge 1$, $1 - \alpha_k = \alpha_k^2\alpha_{k - 1}^2\rho_{k - 1}$, $\alpha_k \in (0, 1)$ from Definition \ref{def:alpha-beta-rho-seq}. 
                \item $F$ is convex and hence $F(z_k) \le \alpha_k F(x^+) + (1 - \alpha_k)F(x_{k - 1})$ from Assumption \ref{ass:standard-fista}. 
                \item $F(x_k) \le F(\tilde x_k)$ which is true by definition of GMAPG (Definition \ref{def:generic-mapg-ls}). 
            \end{enumerate}
            Now, using Theorem \ref{thm:cnvx-pg-ineq}, it has for all $k \in \N$: 
            {\allowdisplaybreaks\small
            \begin{align*}
                0 &\le 
                F(z_k) 
                - F(\tilde x_k) - \frac{L_k}{2}\Vert z_k - \tilde x_k\Vert^2 + 
                \frac{L_k}{2}\Vert z_k - y_k\Vert^2
                \\
                &\underset{\text{(i)}}{=}
                F(\alpha_k x^+ + (1 - \alpha_k)x_{k - 1}) - F(\tilde x_k)
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k \Vert^2 
                + \frac{L_k\alpha_k^2}{2}\Vert(x^+ - v_{k - 1})\Vert^2
                \\
                &\hspace{-0.3em}\underset{\text{(iii)}}{\le} 
                \alpha_k F(x^+) + (1 - \alpha_k) F(x_{k - 1}) - F(\tilde x_k)
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k \Vert^2 
                + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1} \Vert^2
                \\
                &= 
                (\alpha_k - 1)F(x^+) + (1 - \alpha_k) F(x_{k - 1}) + F(x^+) - F(\tilde x_k)
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k \Vert^2 
                + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1}\Vert^2
                \\
                &= 
                (1 - \alpha_k)(F(x_{k - 1}) - F(x^+)) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1}\Vert^2
                - \left(
                    F(\tilde x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \right)
                \\
                &\hspace{-0.3em}\underset{\text{(iv)}}{\le} 
                (1 - \alpha_k)(F(x_{k - 1}) - F(x^+)) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1}\Vert^2
                - \left(
                    F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \right)
                \\
                &= 
                (1 - \alpha_k)(F(x_{k - 1}) - F(x^+)) + 
                \left(
                    \frac{\alpha_k^2}{\alpha_{k - 1}^2\rho_{k - 1}}
                \right)
                \frac{L_{k - 1}\alpha_{k - 1}^2(\rho_{k - 1}L_kL_{k - 1}^{-1})}{2}\Vert x^+ - v_{k-1}\Vert^2 \\
                    &\quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right)
                \\
                &= 
                \left(
                    1 - \alpha_k
                \right)\left(
                    F(x_{k - 1}) - F(x^+) + \frac{L_{k - 1}\alpha_{k - 1}^2(\rho_{k - 1}L_kL_{k - 1}^{-1})}{2}
                    \Vert x^+ - v_{k - 1}\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right)
                \\
                &\le 
                \left(
                    1 - \alpha_k
                \right)\left(
                    F(x_{k - 1}) - F(x^+) + \frac{L_{k - 1}\alpha_{k - 1}^2\max(1, \rho_{k - 1}L_kL_{k - 1}^{-1})}{2}
                    \Vert x^+ - v_{k - 1}\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right)
                \\
                &\le 
                \left(
                    1 - \alpha_k
                \right)\max(1, \rho_{k - 1}L_kL_{k - 1}^{-1})
                \left(
                    F(x_{k - 1}) - F(x^+) + \frac{L_{k - 1}\alpha_{k - 1}^2}{2}
                    \Vert x^+ - v_{k - 1}\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right). 
            \end{align*}
            }
            Unroll recursively for $k, k-1, \ldots, 0$, it implies: 
            \begin{align*}
                0
                &\le 
                \left(
                    \prod^{k - 1}_{i = 0} (1 - \alpha_{i + 1})\max(1, \rho_{i}L_{i + 1}L^{-1}_i)
                \right)\left(
                    F(x_0) - F(x^+) + \frac{L_0 \alpha_0}{2}\Vert x^+ - v_0\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right). 
            \end{align*}
            If in addition, we assume that $x^+$ is a minimizer of $F$, and $\alpha_0 = 1, x_0 = v_0 = T_{L_0}x_{-1}$. 
            Using Theorem \ref{thm:cnvx-pg-ineq} it gives: 
            \begin{align*}
                0 &\le 
                F(x^+) - F(T_{L_{-1}}x_{-1}) - \frac{L_0}{2}\Vert x^+ - T_{L_0}x_{-1}\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2
                \\
                &= 
                F(x^+) - F(x_0) - \frac{L_0}{2}\Vert x^+ - v_0\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2. 
            \end{align*}
            Substituting it back to the previous inequality it yields the desired results. 
        \end{proof}
        \begin{remark}
            The sequence has explicit update formula: 
            \begin{align*}
                \alpha_k = 
                \frac{1}{2}
                \left(
                    \alpha_{k - 1}\sqrt{\alpha_{k -1}^2 + 4 \rho_{k - 1}} - \alpha^2_{k - 1}
                \right)
            \end{align*}
        \end{remark}
        \begin{theorem}[generic GMAPG gradient mapping convergence]\;\label{thm:gmapg-generic-gm-cnvg}\\
            Suppose that $F = f + g$ satisfies Assumption \ref{ass:standard-fista}. 
            % Let the sequences $(x_k, y_k, v_k)$ satisfy GMAPG (Definition \ref{def:generic-mapg-ls}).
            Let the sequences $(x_k, y_k, v_k)$ satisfy GMAPG (Definition \ref{def:generic-mapg-ls}), and take the momentum sequences $(\alpha_k)_{k \ge 0}, (\beta_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ from Definition \ref{def:alpha-beta-rho-seq}. 
            If in addition, 
            \begin{enumerate}
                \item The sequence $(\alpha_k)_{k \ge 0}$ has $\alpha_0 = 1$ and, GMAPG is initialized with $L_0 \ge L$ or, equivalently a successful line search satisfying \mbox{$D_f(x_{0}, x_{-1}) \le L_0/2\Vert x_{0} - x_{-1}\Vert^2$};
                \item $v_0=x_0 = T_{1/L_0, f, g}(x_{-1})$ for any $x_{-1} \in\RR^n$ and there exists $x^+$ which is a minimizer of $F$. 
            \end{enumerate}
            Then, we have the convergence of gradient mapping, it satisfies for all $k \ge 1$ the inequality:
            \begin{align}\label{ineq:gmapg-generic-gm-cnvg-prt1}
                \Vert \mathcal G_{1/L_k} (y_k)\Vert &\le 
                \sqrt{\beta_k}L_k L_0 \left(
                    1 - 
                    \min(\rho_{k - 1}, L_k^{-1} L_{k - 1})^{1/2}
                \right)\Vert x^+ - v_0\Vert. 
            \end{align}
            It has also:
            \begin{align}\label{ineq:gmapg-generic-gm-cnvg-prt2}
                \frac{1}{2L_0}\Vert \mathcal G_{1/L_0}(x_{-1}) \Vert^2
                \le F(x_{-1}) - F(x_0). 
            \end{align}
        \end{theorem}
        \begin{proof}
            \eqref{ineq:gmapg-generic-gm-cnvg-prt2} is direct because $x_0 = T_{1/L_0, f, g}(x_{-1})$ and \mbox{$D_f(x_{0}, x_{-1}) \le L_0/2\Vert x_{0} - x_{-1}\Vert^2$} is assumed in the statement hypothesis, using \ref{lemma:fitsa-pg-ineq} with $x = x_{-1}, z = x_{-1}$, using  gradient mapping as defined in Definition \ref{def:gm-for-ch2} it has 
            \begin{align*}
                0 &\le F(x_{-1}) - F(x_0) + 0 - \frac{L_0}{2} \Vert x_{-1} - x_0\Vert^2
                \\
                &= (F(x_{-1}) - F(\bar x)) - \frac{L_0}{2}\Vert L_0^{-1}\mathcal G_{1/L_0}(x_{-1})\Vert^2. 
            \end{align*}
            We label the following results prior to their proofs for a sleeker exposition for the proof of \eqref{eqn:emp:result-item-2}. 
            \begin{enumerate}
                \item[(a)] From Definition \ref{def:generic-mapg-ls}, the gradient mapping satisfies for all $k \ge 1$ that $\Vert \mathcal G_{1/L_k} (y_k)\Vert = L_k\alpha_k \Vert v_k - v_{k - 1}\Vert$.
                \item[(b)] We have $(a_k)_{k \ge 1}$ satisfying $\forall k \ge 1$ that $(1 - \alpha_k)\rho_{k - 1} = \alpha_k^2/\alpha_{k - 1}^2$ from the statement hypothesis. We assumed $\alpha_0 = 0, \beta_0 = 1$, $x^+$ is a minimizer of $F$. Then using these it has for all $k \ge 0$ it has $\frac{\alpha_k}{\sqrt{\beta_k L_0}}\Vert x^+ - v_k\Vert \le \Vert x^+ - v_0\Vert$. 
                \item[(c)] The sequence $(\alpha_k)_{k \ge 0}$ has $(1 - \alpha_k)\rho_{k - 1} = \alpha_k^2/\alpha_{k - 1}^2$ from the statement hypothesis so $\alpha_k/\alpha_{k - 1} = \sqrt{\rho_{k - 1}(1 - \alpha_k)}$ for all $k \ge 1$. 
                \item[(d)] The definition of $(\beta_k)_{k \ge 0}$ from Definition \ref{def:alpha-beta-rho-seq}. 
            \end{enumerate}
            Using the above intermediate results, the convergence in \eqref{ineq:gmapg-generic-gm-cnvg-prt1} can be derived. 
            From (a) it has for all $k \ge 0$: 
            \begin{align*}
                \Vert \mathcal G_{1/L_k} (y_k)\Vert 
                &= L_k\alpha_k \Vert v_k - v_{k - 1}\Vert
                \\
                &\le 
                L_k\alpha_k(\Vert v_k - x^+\Vert + \Vert v_{k - 1} - x^+\Vert)
                \\
                &\underset{\text{(b)}}{\le} 
                L_k \alpha_k \left(
                    \frac{\sqrt{\beta_kL_0}}{\alpha_k}\Vert x^+ - v_0\Vert
                    +
                    \frac{\sqrt{\beta_{k - 1}L_0}}{\alpha_{k - 1}}\Vert x^+ - v_0\Vert
                \right) 
                \\
                &= L_k\sqrt{L_0} \left(
                    \sqrt{\beta_k}
                    +
                    \frac{\alpha_k\sqrt{\beta_{k - 1}}}{\alpha_{k - 1}}
                \right)\Vert x^+ - v_0\Vert
                \\
                &= \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \frac{\alpha_k}{\alpha_{k - 1}}\sqrt{\frac{\beta_{k - 1}}{\beta_k}}
                \right)\Vert x^+ - v_0\Vert
                \\
                &\underset{\text{(d)}}{=} \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \frac{\alpha_k}{\alpha_{k - 1}}
                    \left((1 - \alpha_k)\max(1, \rho_{k - 1}L_k L_{k - 1}^{-1})\right)^{-1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &\underset{\text{(c)}}{=} 
                \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    ((1 - \alpha_k)\rho_{k - 1})^{1/2}
                    \left((1 - \alpha_k)\max(1, \rho_{k - 1}L_k L_{k - 1}^{-1})\right)^{-1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &= 
                \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \left(\rho_{k - 1}^{-1}\max(1, \rho_{k - 1}L_k L_{k - 1}^{-1})\right)^{-1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &=
                \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \max(\rho_{k - 1}^{-1}, L_k L_{k - 1}^{-1})^{-1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &= 
                \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \min(\rho_{k - 1}, L_k^{-1} L_{k - 1})^{1/2}
                \right)\Vert x^+ - v_0\Vert. 
            \end{align*}
            Now, \textbf{let's proof intermediate results (a)}. 
            From the definition of GMAPG it has 
            \begin{align*}
                y_k &= \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                \iff 
                v_{k - 1} = \alpha_k^{-1}(y_k - (1 - \alpha_k)x_{k - 1}). 
            \end{align*}
            Using the above, and definition of GMAPG, it yields 
            \begin{align*}    
                v_k - v_{k - 1} &= 
                (x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})) - \alpha_k^{-1}(y_k - (1 - \alpha_k)x_{k - 1})
                \\
                &= 
                x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})
                - \alpha_k^{-1}y_k + (\alpha_k^{-1} - 1)x_{k - 1}
                \\
                &= \alpha_k^{-1}(\tilde x_k - x_{k - 1}) - \alpha_k^{-1}y_k + \alpha_k^{-1} x_{k - 1}
                \\
                &= \alpha_k^{-1}\tilde x_k - \alpha_k^{-1} y_k 
                = \alpha^{-1}_k(\tilde x_k - y_k) = \alpha_k^{-1}(T_{L_k} y_k - y_k)
                \\
                &= -\alpha_k^{-1}L_k^{-1}(\mathcal G_{1/L_k}(y_k)). 
            \end{align*}
            \textbf{We now prove result (b)}. 
            The base case $k = 1$ is verified by the assumption that $x_0 = v_0 = T_{L_0} x_{-1}$. 
            Apply Lemma \ref{lemma:fitsa-pg-ineq} with $z =x^+$ as a minimizer it yields: 
            \begin{align*}
                0 &\le 
                F(x^+) - F(T_{L_{-1}}x_{-1}) - \frac{L_0}{2}\Vert x^+ - T_{L_0}x_{-1}\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2
                \\
                &= 
                F(x^+) - F(x_0) - \frac{L_0}{2}\Vert x^+ - v_0\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2
                \\
                &\le 
                - \frac{L_0}{2}\Vert x^+ - v_0\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2
                \\
                \implies 
                0&\le \frac{L_0}{2}\left(
                    \Vert x^+ - x_{-1}\Vert- \Vert x^+ - v_0\Vert 
                \right). 
            \end{align*}
            Because $\beta_0 = \alpha_0 = 1$, the base case holds. 
            For all $k \ge 1$, we consider the convergence claim and use the assumption that $x^+$ is a minimizer of $F$ so, it has from Theorem \ref{thm:gmapg-ls-convergence} that 
            \begin{align*}
                0 &\le \frac{L_0\beta_k }{2}\Vert x^+ - x_{-1}\Vert^2 
                - F(x_k) + F(x^+) - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \\
                &\le 
                \frac{L_0\beta_k }{2}\Vert x^+ - x_{-1}\Vert^2 
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \\
                &= \frac{\alpha_k^2L_k}{2}\left(
                    \frac{\beta_k}{\alpha_k^2L_0}
                    \Vert x^+ - x_{-1}\Vert^2 
                    - \Vert x^+ - v_k\Vert^2
                \right)
                \\
                \iff 
                0 &\le 
                \Vert x^+ - x_{-1}\Vert - \frac{\alpha_k}{\sqrt{\beta_k L_0}}\Vert x^+ - v_k\Vert. 
            \end{align*}
        \end{proof}
        \begin{remark}
            The above theorem is improved from Alamo et al. \cite{alamo_restart_2019}. 
        \end{remark}
        \begin{lemma}[specialized GMAPG momentum sequence]\;\label{lemma:gmapg-seq-bnd}\\
            Take sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}, (\beta_k)_{k \ge 0}$ as in Definition \ref{def:alpha-beta-rho-seq}. 
            In addition, assume that $\alpha_0 = 1$. 
            If, we set $\rho_{k - 1} = L_{k}^{-1}L_{k - 1}$ such that $L_k >0$ for all $k \ge 1$, then for all $k \ge 1$, the sequence $(\beta_k)_{k \ge 0}$ has the inequality: 
            \begin{align*}
                \beta_k \le \left(
                    1 + \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                \right)^{-2}. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            We state the following intermediate results needed to construct the proof. 
            They will be proved at the end. 
            \begin{enumerate}
                \item[(a)] $(\beta_k)_{k \ge 0}$ is monotone decreasing, and it's strictly larger than zero. 
                \item[(b)] Because $\rho_{k}L_{k + 1}L_k^{-1} = 1$ for all $k \ge 0$, the definition of $(\beta_k)_{k\ge 0}$ simplifies and $\beta_k = (\alpha_k^2/\alpha_0^2)(L_k/L_0)$. As a consequence it also gives for all $k \ge 1$ that: 
                \begin{align*}
                    \alpha_k^2 &= \alpha_0^2\beta_kL_0L_k^{-1},
                    \\
                    \alpha_k &= 1 - \beta_k / \beta_{k - 1}. 
                \end{align*}
            \end{enumerate}
            Starting with results (b), and combine the two equality it gives for all $k \ge 1$ the equality 
            \begin{align*}
                0 &=
                (1 - \beta_k/\beta_{k - 1})^2 - \alpha_0^2L_0L_k^{-1}\beta_k 
                \\
                \iff 
                0 &= 
                (1 - \beta_k/\beta_{k - 1}) - \alpha_0\sqrt{L_0L_k^{-1}\beta_k}
                \\
                \iff 
                0 &= 
                (\beta_k^{-1} - \beta_{k - 1}^{-1}) - \alpha_0 \sqrt{L_0 L_k^{-1}\beta_k^{-1}}
                \\
                &= \left(
                    \sqrt{\beta_k^{-1}} + \sqrt{\beta_{k - 1}^{-1}}
                \right)\left(
                    \sqrt{\beta_k^{-1}} - \sqrt{\beta_{k - 1}^{-1}}
                \right)
                - \alpha_0 \sqrt{L_0L_k^{-1}\beta_k^{-1}}
                \\
                &\underset{\text{(a)}}{\le} 
                2\sqrt{\beta_k^{-1}}\left(
                    \sqrt{\beta_k^{-1}} - \sqrt{\beta_{k - 1}^{-1}}
                \right) - \alpha_0 \sqrt{L_0L_k^{-1}\beta_k^{-1}}
                \\
                \iff
                0 &\le 
                2\left(
                    \sqrt{\beta_k^{-1}} - \sqrt{\beta_{k - 1}^{-1}} 
                \right) - \alpha_0 \sqrt{L_0L_k^{-1}}. 
            \end{align*}
            Since this is true for all $k \ge 1$, taking a telescoping sum of the above series gives
            \begin{align*}
                0 &\le 
                \left(
                    \sum_{i = 1}^{k} \sqrt{\beta_i^{-1}} - \sqrt{\beta_{i - 1}^{-1}}
                \right)
                - \sum_{i = 1}^{k} \frac{\alpha_0}{2} \sqrt{L_0L_k^{-1}}
                \\
                &= 
                \sqrt{\beta_k^{-1}} - \sqrt{\beta_0^{-1}} 
                - \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{ L_k^{-1}}
                \\
                &= 
                \sqrt{\beta_k^{-1}} - 1
                - \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{ L_k^{-1}}. 
            \end{align*}
            Therefore, transforming the inequality it has: 
            \begin{align*}
                \beta_k &\le 
                \left(
                    1 + \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{ L_k^{-1}}
                \right)^{-2}. 
            \end{align*}
            \textbf{Let's now justify (a)}.
            When $\rho_{i} = L_{i + 1}L_i^{-1}$, the big product simplifies and, it has: 
            \begin{align*}
                \beta_k &= \prod_{i = 0}^{k - 1}(1 - \alpha_{i + 1}) 
                = (1 - \alpha_k)\beta_{k - 1}. 
            \end{align*}
            Since $\alpha_k \in (0, 1)$, $\beta_k$ is monotonically decreasing. 
            \textbf{To see (b)}, it has from the above which also justifies $1 - \alpha_k = \beta_k / \beta_{k - 1}$. 
            Recall that sequence $(\alpha_k)_{k \ge 0}$ has $\forall k \ge 1$ that $\alpha_{k - 1}^2\rho_{k - 1}(1 - \alpha_k) = \alpha_k^2$, using it we can simplify the product for $\beta_k$, it follows that 
            \begin{align*}
                \beta_k &= \prod_{i = 0}^{k - 1}\left(
                    1 - \alpha_{i + 1}
                \right)
                = 
                \prod_{i = 1}^{k}\alpha_i^2\alpha_{i - 1}^{-2}\rho_{i - 1}^{-1}
                = 
                \prod_{i = 1}^{k}\alpha_i^2\alpha_{i - 1}^{-2} L_i^{-1}L_{i - 1}
                \\
                &= \left(
                    \frac{\alpha_k^2\alpha_{k - 1}^2\ldots \alpha_1^2}
                    {\alpha_{k -1}^2\alpha_{k - 2}^2\ldots \alpha_0^2}
                \right)\left(
                    \frac{L_kL_{k - 1}\ldots L_1}{L_{k - 1}L_{k - 1}\ldots L_0}
                \right)
                = \frac{\alpha_k^2}{\alpha_0^2}\frac{L_k}{L_0}. 
            \end{align*}
            Rearranging it gives: $\alpha_0^2L_0 \beta_kL_k^{-1} = \alpha_k^2$.
        \end{proof}
        \begin{remark}
            The technique of the proof we used here is very similar to Güler \cite[Lemma 2.2]{guler_new_1992}.
            A simpler upper bound is more practical. 
            For all $k \ge 1$ let $\widehat L_k = \max_{i = 0, 1, \ldots, k} L_i$ then 
            \begin{align*}
                \beta_k 
                &\le \left(
                    1 + 
                    \frac{\alpha_0 \sqrt{L_0}}{2}\sum_{i = 1}^{k}\sqrt{L^{-1}_k}
                \right)^{-2}
                \le \left(
                    1 + \frac{1}{2}\alpha_0 \sqrt{L_0}k\sqrt{\widehat L^{-1}_k}
                \right)^{-2}
                \\
                &= \left(
                    1 + \frac{k\alpha_0\sqrt{L_0 \widehat L^{-1}_k}}{2}
                \right)^{-2} = L^{-1}_0\widehat L\left(
                    \sqrt{L_0^{-1}\widehat L_k} + \frac{k\alpha_0}{2}
                \right)^{-2}
                \\
                &\le 
                L^{-1}_0\widehat L_k\left(
                    1 + \frac{k\alpha_0}{2}
                \right)^{-2} 
                = \frac{4\widehat L_k}{L_0(2 + k \alpha_0)^2}. 
            \end{align*}
            This simplifies the convergence claim back in Theorem \ref{thm:gmapg-ls-convergence}. 
            The above inequality would work the same if we set: 
            \begin{align*}
                \widehat L_k &= \max\left(
                    L_0, \left(
                        \frac{1}{k} \sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                    \right)^{-2}
                \right). 
            \end{align*}
        \end{remark}
        \begin{theorem}[specialized GMAPG convergence rate]\label{thm:gmapg-specialized-cnvg}
            Suppose that $F = f + g$ satisfy Assumption \ref{ass:standard-fista}. 
            Let the sequences $(x_k, v_k, v_k)_{k \ge 0}$ and $(L_k)_{k \ge 0}$ satisfy GMAPG in Definition \ref{def:generic-mapg-ls} and, assume that the GMAPG is initialized by $x_0 = v_0 = T_{1/L_0}(x_{-1})$ and, assume $\rho_{k - 1} = L_{k}^{-1}L_{k}$, $\alpha_0 = 1$ so the sequence $(\alpha_k)_{k \ge 0}$ satisfies for all $k\ge 1$: $\alpha_{k - 1}^2L_k^{-1}L_{k - 1}(1 - \alpha_k) = \alpha_k^2$. 
            Let $x^+$ be a minimizer of $F$, define 
            \begin{align*}
                \widehat L_k &:= \max\left(
                    L_0, \left(
                        \frac{1}{k} \sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                    \right)^{-2}
                \right). 
            \end{align*}
            Then, we have convergence claim: 
            \begin{enumerate}
                \item \begin{align*}
                    F(x_k) - F(x^+) + \frac{L_k\alpha_k}{2}\Vert x^+ - v_k\Vert^2 \le 
                    \frac{2\widehat L_k}{(2 + k)^2}\Vert x^+ - x_{-1}\Vert^2.
                \end{align*}
                \item 
                \begin{align*}
                    \Vert \mathcal G_{1/L_k}(y_k)\Vert \le 
                    \frac{2\widehat L_k L_k}{2 + k}
                    \left(
                        1 - L_k^{-1/2}L_{k - 1}^{1/2}
                    \right)
                    \Vert x^+ - v_0\Vert. 
                \end{align*}
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            To see (i), use Lemma \ref{lemma:gmapg-seq-bnd} and its remark to bound $(\beta_k)_{k \ge 1}$ and then, apply Theorem \ref{thm:gmapg-ls-convergence} because the assumptions of $x^+, (\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ suit. 
            To see (ii), the convergence claim from \ref{thm:gmapg-generic-gm-cnvg} simplifies with $\widehat L_k \ge L_0$ and, it has 
            \begin{align}
                \Vert \mathcal G_{1/L_k}(y_k) \Vert
                &\le 
                \left(
                    \frac{2\sqrt{\widehat L_kL_0}L_k}{2 + k}
                \right)\left(
                    1 + \min(\rho_{k - 1}, L_k^{-1}L_{k - 1})^{1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &= 
                \left(
                    \frac{2\sqrt{\widehat L_kL_0}L_k}{2 + k}
                \right)\left(
                    1 + L_k^{-1/2}L_{k - 1}^{1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &\le 
                \left(
                    \frac{2\widehat L_k L_k}{2 + k}
                \right)\left(
                    1 + L_k^{-1/2}L_{k - 1}^{1/2}
                \right)\Vert x^+ - v_0\Vert. 
            \end{align}
        \end{proof}
    
    \section{Algorithmic description of GMAPG}
        There are several components to the GMAPG algorithm. 
        This section will introduce various type of implementations that can be fitted into GMAPG in Definition \ref{def:generic-mapg-ls}. 
        \subsection{Line search routines}
            % ARMIJO LINE SEARCH -----------------------------------------------
            \begin{algorithm}[H]
                {\small
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function ArmijoLS: }
                        \begin{tabular}{|ll}
                            $f : \RR^n \rightarrow \RR$ & Convex Lipschitz smooth\\ 
                            $g: \RR^n \rightarrow \overline \RR$  &  Convex\\ 
                            $x \in \RR^n$ & Vector\\
                            $v \in \RR^n$ & Vector\\
                            $L \in \RR$ & $L > 0$\\
                            $\alpha \in \RR$  & $\alpha \in (0, 1]$\\
                            $\cdots$ & Ignore extra inputs
                        \end{tabular}\vspace{0.5em}
                    }
                    \STATE{$\alpha^+ := (1/2)\left(\alpha\sqrt{\alpha^2 + 1} - \alpha^2\right)$.}
                    \STATE{$y^+ := \alpha^+ v + (1 - \alpha^+)x$.}
                    \STATE{$L^+ := L$. }
                    \FOR{$i = 1, 2,\ldots, 53$}
                        \STATE{$L^+ := 2L^+$. }
                        \STATE{$x^+ := T_{1/L^+, f, g}(y^+)$. }
                        \IF{$D_f(x^+, y^+) \le (L^+/2)\Vert x^+ - y^+\Vert^2$}
                            \STATE{\textbf{break}}
                        \ENDIF
                        \STATE{$L^+ := 2^{i}L$}
                    \ENDFOR
                    \STATE{\textbf{Return:} $x^+, y^+, \alpha^+, L^+$}
                    \caption{Armijo Line Search}\label{alg:armijo-ls}
                \end{algorithmic}
                }
            \end{algorithm}
            \par
            Algorithm \ref{alg:armijo-ls} performs a step of Armijo line search and a step of accelerated proximal gradient. 
            The function can be used for each iteration in the inner loop of the algorithm. 
            Here are the explanations for all its input parameters: 
            \begin{enumerate}
                \item $f, g$ are functions satisfying Assumption \ref{ass:standard-fista}. 
                \item $x, v$ are the $x_k, v_k$ iterates in Definition \ref{def:generic-mapg-ls}. 
                \item $\alpha$ are the current $\alpha_k$ in Definition \ref{def:generic-mapg-ls}. 
                \item $L$ is the estimate of the Lipschitz constant of $f$ passed in by the inner loop. 
            \end{enumerate}
            Iterates $x^+, y^+$ and parameters $\alpha^+, L^+$ are returned to the callers at the end. 
            % CHAMBOLLE'S BACKTRACKING LINE SEARCH -----------------------------
            \begin{algorithm}[H]
                {\small
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function ChamBT Inputs: }
                        \begin{tabular}{|ll}
                            $f:\RR^n \rightarrow \RR$ & Convex Lipschitz smooth\\ 
                            $g:\RR^n \rightarrow \overline\RR$ & Convex\\ 
                            $x\in \RR^n$ & Vector\\
                            $v\in \RR^n$ & Vector\\
                            $L\in \RR$ & Number, $L > 0$ \\
                            $\alpha \in \RR$ & Vector\\
                            $L_{\min} \in \RR$ & Number, $L_{\min} > 0$\\
                            $\rho \in \RR$ & Number, $\rho \in (0, 1)$\\
                        \end{tabular}\vspace{0.5em}
                    }
                    \STATE{$L^+ := \max(L_{\min}, \rho L)$.}
                    \FOR{$i = 1, 2, \ldots, 53$}
                        \STATE{$\alpha^+ := (1/2)\left(\alpha\sqrt{\alpha^2 + L/L^+} - \alpha^2\right)$. }
                        \STATE{$y^+ := \alpha^+ v + (1 - \alpha^+) x$. }
                        \STATE{$x^+ := T_{1/L^+, f, g}(y^+)$.}
                        \IF{$2D_f(x^+, y^+) \le \Vert x^+ - y^+\Vert^2$}
                            \STATE{\textbf{break}}
                        \ENDIF
                        \STATE{$L^+ := 2^iL^+$.}
                    \ENDFOR
                    \STATE{\textbf{Return: }$x^+, \alpha^+, L^+$}
                    \caption{Chambolle's Backtracking}\label{alg:chambolle-btls}
                \end{algorithmic}
                }
            \end{algorithm}
            \par
            Algorithm \ref{alg:chambolle-btls} attempts to decrease the Lipschitz estimate $L_k$ for $f$ in an iteration of the inner loop. 
            The above implementations were adapted and simplified from Chambolle et al. \cite{calatroni_backtracking_2019}. 
            It takes in additional parameters $L_{\min}, \rho$ compared to Algorithm \ref{alg:armijo-ls}. 
            Here are their explanations: 
            \begin{enumerate}
                \item $L_{\min}$ determines a lower bound of Lipschitz estimates. It's the lowest value of an estimate $L_k$ allowed. It increases stability of the algorithm by preventing unnecessary triggering a line search routine to recovers from an underestimated $L_k$ that doesn't satisfy the Lipschitz smoothness condition for $f$ at the current iterate. 
                \item $\rho \in (0, 1)$ is the decay ratio. It's use to shrink the current estimate of $L$ and produce $L^+$ at the start of the forloop before verifying the smoothness condition. 
            \end{enumerate}

        \subsection{Monotone routines}
            % BECK'S MONO ------------------------------------------------------
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function BeckMono Inputs: }
                        \begin{tabular}{|ll}
                            $f: \RR^n \rightarrow \RR$ & Convex Smooth\\
                            $g: \RR^n \rightarrow \overline \RR$ & Convex\\
                            $\tilde x \in \RR^n$ & Vector\\
                            $x \in \RR^n$ & Vector\\
                            $\rho$ & Number $\rho \in (0, 1)$ Number\\
                            $G \in \RR$ & Number 
                        \end{tabular}\vspace{0.5em}
                    }
                    \STATE{\noindent
                        $x^+ = \argmin \{(f + g)(z): z \in \{\tilde x, x\}\}$. 
                    }
                    \STATE{\textbf{Return: } $x^+, \eta, G$}
                    \caption{Beck's monotone routine}\label{alg:beck-mono}
                \end{algorithmic}
            \end{algorithm}
            \par
            Algorithm \ref{alg:beck-mono} is a subroutine for asserting monotone condition on function value. 
            The parameter $G$ has no actual usage besides making it compatible with Algorithm \ref{alg:nes-mono} in the context of Algorithm \ref{alg:gmapg}. 
            The input $\tilde x$ is the candidate iterate produced by FISTA without monotone constraints and $x$ is the previous iterates $x_{k - 1}$ in the inner loop. 
            % NES'S MONO -------------------------------------------------------
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function NesMono Inputs: }
                        \begin{tabular}{|ll}
                            $f:\RR^n\rightarrow \RR$ & Lipschitz Smooth \\ 
                            $g: \RR^n \rightarrow \overline \RR$ & Weakly Convex \\ 
                            $\tilde x \in \RR^n$ & Vector \\ 
                            $x \in \RR^n$ & Vector \\
                            $\eta \in \RR$ & Number $\eta > 0$\\
                            $G \in \RR$  & Number
                        \end{tabular}\vspace{0.5em}
                    }
                    \STATE{\noindent
                        $\hat y := \argmin\{(f + g)(z), x \in \{\tilde x, x\}\}$. 
                    }
                    \STATE{\noindent
                        $x^+ := T_{1/\eta}(\hat y)$. 
                    }
                    \FOR {i = 1, 2, \ldots, 53}
                        \IF{$(f + g)(x^+) - (f + g)(\hat y) \le -1/(2\eta)\Vert \mathcal G_{1/\eta}(\hat y)\Vert^2$}
                            \STATE{\textbf{Break}}
                        \ENDIF
                        \STATE{$\eta := 2\eta$. }
                            \STATE{$x^+ := T_{1/\eta}(\hat y)$.}
                    \ENDFOR
                    \STATE{$G := \eta (x^+ - \hat y).$}
                    \STATE{\textbf{return: } $x^+, \eta, G$ }
                    \caption{Nesterov's monotone routine}\label{alg:nes-mono}
                \end{algorithmic}
            \end{algorithm}
            \par
            The above Algorithm \ref{alg:nes-mono} implements and adapts Nesterov's monotone scheme from Nesterov \cite[2.2.32]{nesterov_lectures_2018} for GMAPG. 
            In addition to Algorithm \ref{alg:beck-mono}, $\eta$ is a new input parameter and $G$ has a significance role. 
            $\eta$ is a stepsize parameter for weakly convex objective $F = f + g$ satisfying Assumption \ref{ass:sum-of-wcnvx}. 
            $G$ is the norm of the gradient mapping updated at at $\hat y$ which will be returned to the inner loop to verify exit conditions. 
        
        \subsection{GMAPG main algorithm}
            % GMAPG ------------------------------------------------------------
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function GMAPG Inputs: }
                        \begin{tabular}{|ll}
                            $f:\RR^n \rightarrow \RR$ & Lipschitz Smooth \\
                            $g: \RR^n \rightarrow \overline\RR$ & Weakly Convex \\ 
                            $x_{-1}$ & Vector\\
                            $L\in \RR$ & $L > 0$\\ 
                            $r $ & $r \in (0, 1)$ \\ 
                            $\rho \in \RR$ & $\rho \in (0, 1)$ \\
                            $N_{\min}\in \N$ & $N \ge 1$ \\
                            $N \in \N$ & $N \ge N_{\min}$ \\
                            $\epsilon \in \RR$ & Number\\
                            $\mathbf L$ & Algorithm \ref{alg:armijo-ls} or \ref{alg:chambolle-btls}\\
                            $\mathbf M$ & Algorithm \ref{alg:beck-mono} or \ref{alg:nes-mono}\\
                            $\mathbf E_\chi$ & Exit Condition 
                        \end{tabular}
                    }
                    \STATE{$\alpha_0 := 1$.}
                    \STATE{$x_0, y_0,\alpha_1, L_0 := \textbf{ArmijoLS}(f, g, x_{-1}, x_{-1}, L, \alpha_0)$.}
                    \STATE{$\eta_0 := L_0; v_0 := x_0; G_0 = \Vert \sqrt{L_0}(x_0 - y_0)\Vert.$}
                    \IF{$G_0 \le \epsilon$}
                        \STATE{\textbf{Return: }$x_k, 0, L_0, G_0$}
                    \ENDIF
                    \FOR{$k := 1, 2,\ldots, N$}
                        \STATE{\noindent
                            $\tilde x_{k}, y_k, \alpha_{k + 1}, L_{k} := \mathbf{L}(f, g, v_{k - 1}, x_{k - 1}, L_{k - 1}, \alpha_{k}, rL_{k - 1}, \rho)$.
                        }
                        \STATE{$\overline L := \max(L_k, L_{k - 1})$. }
                        \STATE{\noindent
                            $\rho := \rho^{1/2} \textbf{ if } L_{k} > L_{k - 1} \textbf{ else } \rho$.
                        }
                        \STATE{$G_k:= \Vert \sqrt L_k(\tilde x_k - y_k)\Vert $}
                        \STATE{\noindent
                            $x_k, \eta_{k + 1}, G_k^+ := \mathbf M(f, g, \tilde x_k, x_{k - 1}, \eta_k, G_k)$.
                        }
                        \IF{$G_k^+ < \epsilon \textbf{ or } (\mathbf E_\chi  \textbf{ and } k > N_{\min})$}
                            \STATE{\textbf{break}}
                        \ENDIF
                    \ENDFOR
                    \STATE{\textbf{Return: }$x_k, k, \overline L, G_k^+ $}
                \caption{GMAPG with Chambolle's backtracking}\label{alg:gmapg}
                \end{algorithmic}
            \end{algorithm}
            The above Algorithm \ref{alg:gmapg} is an implementation of GMAPG in Definition \ref{alg:gmapg}. 
            The first iterates $x_0$ is produced by a step of proximal gradient descent through Algorithm \ref{alg:armijo-ls} so, it has $x_0 = v_0 = T_{1/L_0}(x_{-1})$, and consequently all results from Theorem \ref{thm:gmapg-specialized-cnvg} apply. 
            Parameters $r, \rho$ are chosen in the discretion of the practitioners. 
            For example, we chose $r= 0.4, \rho = 2^{1/1024}$. 
    
    \section{Examples of GMAPG in the literature}
        \begin{example}[MFISTA with Armijo line search]\;\\\vspace{-1em}
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                    \STATE{\textbf{Input:} $x_{-1} \in \RR^n, L_0 \in \RR^n, f:\RR^n \rightarrow \RR, g: \mathbb \RR^n \rightarrow \overline \RR$} 
                    \STATE{
                        $x_0 := y_0, t_0 := 1$.
                    }
                    \FOR{$k = 0, 1, 2, \ldots$}
                        \STATE{$\tilde x_{k + 1} := T_{L_k^{-1}}(y_k)$.}
                        \IF{$D_f(\tilde x_{k + 1}, y_k) > L_k/2\Vert \tilde x_{k + 1} - y_k\Vert^2$}
                            \STATE{\noindent
                                $L_k := \argmin_{i = 1,2, \ldots} \left\{ i: 
                                    D_f(T_{2^{-i}L_k^{-1}}(y_k), y_k) 
                                    \le 2^{i-1}L_k\left\Vert 
                                        T_{2^{-i}L^{-1}}y_k - y_k
                                    \right\Vert^2
                                \right\}$.
                            }
                            \STATE{$\tilde x_{k + 1} := T_{L_k^{-1}}y_k$.}
                        \ENDIF
                        \STATE{\noindent
                            Choose $x_{k + 1} \in \{\tilde x_{k + 1}, x_k\}$ such that $F(x_{k + 1}) \le \min(F(x_k), F(\tilde x_{k + 1}))$. 
                        }
                        \STATE{\noindent
                            $t_{k + 1} := (1/2)\left(1 + \sqrt{1 + 4t_k^2}\right)$. 
                        }
                        \STATE{\noindent
                            $y_{k + 1} := x_{k + 1} + t_kt_{k + 1}^{-1}(\tilde x_{k + 1} - x_{k + 1})+ (t_k - 1)t_{k + 1}^{-1}(x_{k + 1} - x_k)$. 
                        }
                    \ENDFOR
                \end{algorithmic}
                \caption{MFISTA with Armijo Line Search}
                \label{alg:mfista-armijo}
            \end{algorithm}
            We now demonstrate that Algorithm \ref{alg:mfista-armijo} is a special case of Definition \ref{def:generic-mapg-ls}.
            Let's consider $y_{k + 1}$ produced the GMAPG. 
            If $x_k = x_{k - 1}$ then replacing all instance of $x_k$ by $x_{k - 1}$ it has: 
            \begin{align*}
                y_{k + 1} &= \alpha_{k + 1}(v_k) + (1 - \alpha_{k + 1})x_{k - 1}
                \\
                &= \alpha_{k + 1}(x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})) + (1 - \alpha_{k + 1})x_{k - 1}
                \\
                &= \alpha_{k + 1} x_{k - 1} + \alpha_{k + 1}\alpha_k^{-1}(\tilde x_k - x_{k - 1}) + (1 - \alpha_{k + 1}) x_{k - 1}
                \\
                &= x_{k - 1} + \alpha_{k + 1}\alpha_k^{-1}(\tilde x_k - x_{k - 1})
                % \\
                % &= \tilde x_k + (\alpha_{k + 1} \alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1}). 
            \end{align*}
            Similarly when $x_k = \tilde x_k$ it produces: 
            \begin{align*}
                y_{k + 1} &= 
                \alpha_{k + 1}v_k + (1 - \alpha_{k + 1})\tilde x_k
                \\
                &= 
                \alpha_{k + 1}(x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})) + (1 - \alpha_{k + 1})x_k
                \\
                &= 
                \alpha_{k + 1}\left(
                    (1 - \alpha_{k}^{-1})x_{k - 1} + (\alpha_k^{-1} - 1)\tilde x_k + \tilde x_k
                \right) + 
                (1 - \alpha_{k + 1})\tilde x_k
                \\
                &= 
                \alpha_{k + 1}\left(
                    (\alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1}) + \tilde x_k
                \right) + 
                (1 - \alpha_{k + 1})\tilde x_k. 
                \\
                &= \tilde x_k + \alpha_{k + 1}(\alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1}). 
            \end{align*}
            Let's denote $y'_{k}, x'_{k}, \tilde x_k'$ as the $y_k, x_k, \tilde x_k$ produced by Algorithm \ref{alg:mfista-armijo}.
            Observe that if $x_0'$ is not the minimizer then it has $\tilde x_1' = T_{L_0^{-1}}(y_0') = T_{L_0^{-1}}(x_0')$. 
            Then $F(\tilde x_1') < F(x_0')$ is true. 
            So $x_1' = \tilde x_1 = T_{L_0^{-1}}(x_0')$. 
            Since $t_0 = 1$, it has $y_1' =\tilde x_1' + (t_0 - 1)t_1^{-1}(\tilde x_1' - x_0')= \tilde x_1'$. 
            \par
            Summarize the above results compactly, it has for all $k \ge 0$
            \begin{align}\label{eqn:emp:result-item-1}
                y_{k + 1} = \begin{cases}
                    x_{k - 1} + \alpha_{k + 1}\alpha_k^{-1}(\tilde x_k - x_{k - 1})
                    & \text{if } x_k = x_{k - 1} \wedge k \ge 1,
                    \\
                    \tilde x_k + \alpha_{k + 1}(\alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1})
                    & \text{if } x_k = \tilde x_k \wedge k \ge 1,
                    \\
                    \alpha_1v_0 + (1 - \alpha_1)x_0 & \text{if } k = 0. 
                \end{cases}
            \end{align}
            Then it has for all $k \ge 0$: 
            \begin{align}\label{eqn:emp:result-item-2}
                y'_{k + 1} &= 
                \begin{cases}
                    x'_k + t_kt_{k + 1}^{-1}(\tilde x_{k + 1} - x_k) 
                    & \text{if } x_{k + 1}' = x_k' \wedge k \ge 1,
                    \\
                    x_{k + 1}' + (t_k - 1)t_{k + 1}^{-1}(\tilde x_{k + 1}' - x_k')  
                    & \text{if } x_{k + 1}' = \tilde x_{k + 1}'\wedge k \ge 1, 
                    \\
                    \tilde x_1'
                    & 
                    \text{if } k = 0. 
                \end{cases}
            \end{align}
            Let $x_{-1} \in \RR^n$. 
            If we choose $v_0 = x_0 = T_{L_0^{-1}} x_{-1}$, then $y_1 = \alpha_1 x_0 + (1 - \alpha_1)x_0 = x_0 = T_{L_0^{-1}}(x_{-1})$.
            Next, we make $\alpha_k^{-1} = t_k$, then \eqref{eqn:emp:result-item-1}, \eqref{eqn:emp:result-item-2} are equivalent. 
        \end{example}
        \begin{example}[Nesterov's monotone scheme with generic line search]\;\\
            The following is (2.2.32) in Nesterov's book, phrased in our GMAPG framework. 
            \begin{algorithm}\label{alg:nesterov-mono-generic-ls}
            \begin{algorithmic}[1]
            \STATE{\textbf{Input: } }
            \end{algorithmic}\caption{Nesterov's monotone scheme with generic line search}
            \end{algorithm}
        \end{example}

    \section{Practical enhancement from the Nesterov's Monotone Variant}
        The following definition is a set of necessary conditions GMAPG with Nesterov's monotone subroutine. 
        Our goal in this section is to show that Nesterov's implementations of monotone accelerated gradient method has convergence results under Assumption \ref{ass:sum-of-wcnvx}.
        \begin{definition}[nonconvex Nesterov's monotone scheme]\;\label{def:nes-monotone-scheme}\\
            Suppose $F = f + g$ satisfies Assumption \ref{ass:sum-of-wcnvx}. 
            Let $L_0 \ge L$. 
            Let $(\alpha_k)_{k \ge 0}$ with $\alpha_0 = 1$ and, it satisfies for all $k \ge 1$: $L_{k}^{-1}L_{k - 1}\alpha_{k - 1}^2(1 - \alpha_k) = \alpha_k^2$. 
            Initialize the algorithm with $\hat y_0 =v_0=x_0 = T_{1/L_0}(x_{-1})$, $\eta_0 = L_0$, for some $x_{-1} \in \RR^n$ and $L_0$ such that $F(x_0) \le F(x_{-1})$. 
            The algorithm is defined by sequences $(y_k, v_k, x_k)_{k \ge 1}$ and $(\tilde x_k, \hat y_k)_{k \ge 1}$ such that they all satisfy: 
            $$
            \begin{aligned}
                &y_k = \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1},
                \\
                &\tilde x_k = T_{1/L_k}(y_k), \text{ with line search or backtracking. }
                \\
                &v_k = x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1}),
                \\
                &\hat y_k = \argmin{}\left\lbrace
                    F(y): y \in \{x_{k - 1}, \tilde x_k\}
                \right\rbrace,
                \\
                & \eta_{k}\text{ s.t: } F(x_k) - F(\hat y_k) \le - 1/(2\eta_k)\Vert \mathcal G_{1/\eta_k}(\hat y_k) \Vert^2, \eta_{k} \ge \eta_{k - 1}, 
                \\
                &x_k = T_{1/\eta_k}(\hat y_k) . 
            \end{aligned}
            $$
        \end{definition}
        The following theorem states the fact that the algorithm should eventually terminate if the objective function is bounded below. 
        \begin{theorem}[convergence of Nesterov's monotone scheme nonconvex]\;\label{thm:nes-mono-wcnvx-convergence}\\
            Suppose that the sequences $(y_{k + 1}, v_k, x_k)_{k \ge 0}$ and $(\hat y_k, \tilde x_k)_{k \ge 0}$, $(\alpha_k)_{k \ge 0}$ satisfy Definition \ref{def:nes-monotone-scheme}. 
            Assume that $F$ is bounded below with $F^+ := \inf_{x}F(x)$. 
            Then for all $N \ge 1$ it has
            \begin{align*}
                \min_{1 \le k \le N}\Vert \mathcal G_{1/\eta_k}(\hat y_k) \Vert^2 
                &\le\frac{2\overline \eta_N}{N}(F(x_{-1}) - F^+). 
            \end{align*}
            Here, $\overline \eta_k = \max_{i = 0, \ldots, k}\eta_i$. 
            If the monotone routine in Algorithm \ref{alg:nes-mono} is used, then it's bounded above by $2(q_g + L)$. 
        \end{theorem}
        \begin{proof}
            $\overline \eta_k = \max_{i = 0, \ldots, k}\eta_i$
            Using Lemma \ref{lemma:mono-wcnvx-descent} it has from the descent condition of monotone routine that for all $k \ge 1$,  
            \begin{align*}
                0 &\le F(\hat y_k) - F\left(T_{1/\eta_k}\hat y_k\right) 
                - \frac{1}{2\eta_k}\Vert \mathcal G_{1/\eta_k}(\hat y_k)\Vert^2
                \\
                &= \min(F(x_{k - 1}), F(\tilde x_k)) - F(x_k) 
                - \frac{1}{2\eta_k}\Vert \mathcal G_{1/\eta_k}(\hat y_k)\Vert^2
                \\
                &\le 
                F(x_{k - 1}) - F(x_k) - \frac{1}{2\eta_k} \Vert \mathcal G_{1/\eta_k}(\hat y_k)\Vert^2
                \\
                &\le 
                F(x_{k - 1}) - F(x_k) - \frac{1}{2\overline\eta_k} \Vert \mathcal G_{1/\eta_k}(\hat y_k)\Vert^2. 
            \end{align*}
            Telescoping it has: 
            \begin{align*}
                0 &\le \left(
                    \sum_{i = 1}^{N} F(x_{i - 1}) - F(x_i)
                \right) 
                - \frac{1}{2\overline\eta_N}\sum_{i = 1}^{N} \Vert \mathcal G_{1/\eta_i}(\hat y_k)\Vert^2
                \\
                &= 
                F(x_{0}) - F(x_N) 
                - \frac{1}{2\overline\eta_N}\sum_{i = 1}^{N} \Vert \mathcal G_{1/\eta_i}(\hat y_k)\Vert^2
                \\
                &\le 
                F(x_{0}) - F(x_N) 
                - \frac{N }{2\overline\eta_N}\left(
                    \min_{1 \le i \le N} \Vert \mathcal G_{1/\eta_i}(\hat y_i)\Vert^2
                \right)
                \\
                &\le F(x_{0}) - F^+ 
                - \frac{N}{2\overline\eta_N}\left(
                    \min_{1 \le i \le N} \Vert \mathcal G_{1/\eta_i}(\hat y_i)\Vert^2
                \right)
                \\
                &\le F(x_{-1}) - F^+ 
                - \frac{N}{2\overline\eta_N}\left(
                    \min_{1 \le i \le N} \Vert \mathcal G_{1/\eta_i}(\hat y_i)\Vert^2
                \right).
            \end{align*}
            Finally, we show $\overline \eta_k \le 2(q_g - L)$. 
            If there exists $k$ such that $\eta_k \ge q_g - L$ in the algorithm, then by Lemma \ref{lemma:mono-wcnvx-descent} the condition $F(x_k) - F(\hat y_k) \le -1/(2\eta_k)\Vert \mathcal G_{1/\eta_k}(\hat y_k)\Vert$ for all possible $\hat y_k \in \RR^n$, therefore Algorithm \ref{alg:nes-mono} won't increase the value of $\eta_k$ in the future iteration. 
            It has for all $i \ge k$, $\eta_i = \eta_k$. 
            Suppose that some $\eta_i > 2(q_g + L), i \ge k$ then it means there exists $\eta_k > q_g + L$, this contradicts what we had right before, hence impossible and $\eta_i \le 2(q_g + L)$ is an upper bound. 
        \end{proof}
        \begin{remark}
            The convergence claim still works for restarts. 
        \end{remark}
        \par
        A stronger result on the convergence rate of $\Vert \mathcal G_{1/\eta_k}(y_k)\Vert$ can be obtained if, we assume that the function $F=f + g$ satisfies Assumption \ref{ass:standard-fista}. 
    
    \section{Restarting with function values for linear convergence}
        In this section, we show that restarting GMAPG with a well suited conditions yields fast linear global convergence. 
        We adapt and improve prior theories from Alamo \cite{alamo_restart_2019} for our GMAPG method. 
        The following definition, gives the quadratic growth property of $f$ which allows for a fast linear convergence rate using adaptive restarts. 
        \begin{assumption}[quadratic growth condition]\label{ass:q-growth-ch2}
            Let $F = f + g$ satisfies Assumption \ref{ass:standard-fista} so that minimizers exists and, it's bounded below. 
            Denote $F^+ = \inf_{x} F(x)$. 
            Denote $X^+ = \argmin_{x} F(x)$ and for all $x \in \RR^n, \bar x \in \Pi_{X^+}x$ there exists $\mu > 0$ such that 
            \begin{align*}
                F(x) - F^+ &\ge \frac{\mu}{2}\Vert x - \bar x\Vert^2. 
            \end{align*}
        \end{assumption}
        The following proposition about line search will furnish convergence results from previous section. 
        \begin{proposition}[Lipschitz line search estimates are bounded]\label{prop:bnded-lip-ls}
            Suppose that $F = f + g$ satisfies Assumption \ref{ass:standard-fista}. 
            Choose such $F$ for Algorithm \ref{alg:gmapg} so, it generates the sequence $(L_k)_{k\ge 0}$. 
            Then, the sequence $(\hat L_k)_{k \ge 1}$ from Theorem \ref{thm:gmapg-specialized-cnvg} is bounded above and, it has
            \begin{align*}
                \widehat L_k &:= \max\left(
                    L_0, \left(
                        \frac{1}{k} \sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                    \right)^{-2}
                \right)\le  \overline L \le \max\left(L_0,  2L\right). 
            \end{align*}
        \end{proposition}
        \begin{proof}
            A line search is triggered in Algorithm \ref{alg:armijo-ls}, \ref{alg:chambolle-btls} if and only if $L_{k + 1} = 2L_{k}$ for some $k \ge 0$ and, there exists some $x\in \RR^n, y \in \RR^n$ such that $D_f(x, y) > L_k\Vert x - y \Vert^2$. 
            For all $k = 0, 1, \ldots $, if $L_k \ge L$ then, it has $D_f(x, y) \le L_k/2\Vert x - y\Vert^2$ for all $x, y \in \RR^n$. 
            Hence, $L_{k + 1} \le L_k$ because a line sarch is never triggered. 
            \par
            For contradiction let's assume that there exists $k \ge 1$ such that a line search is triggered for $L_k$ and, $L_{k + 1} = 2L_{k} > 2L$, then $L_k > L$, but we just showed that this implies $L_{k + 1} \le L_k$, which is a contradiction. 
            Therefore, if $L_{k + 1} = 2L_k$ then it must be that $L_k < L$ so, the highest value it can achieve is either $L_0$, or $2L$. 
        \end{proof}
        \par
        Let's introduce our first set of restart conditions which denote it by $\mathbf E_{\chi}^{a}$. 
        $\mathbf E_{\chi}^{a}$ only uses function values on iterates available to the inner loop to determine the exit conditions, and it achieves global fast linear convergence. 
        The inner loop algorithm is described in Algorithm \ref{alg:gmapg}, let $k$ denote the inner loop counter and define $m = \lfloor k/2 \rfloor + 1$ the conditions are: 
        \begin{align}\label{ineq:rgmapg-exit-cond}
            \mathbf E_{\chi}^a \iff 
            f(x_m) - f(x_k) \le \exp(-1)(f(x_{-1}) - f(x_m)). 
        \end{align}
        \begin{algorithm}[H]
            \begin{algorithmic}[1]
            \STATE{\textbf{Input: }\begin{tabular}{ll}
                $f:\RR^n \rightarrow \RR$ & Lipschitz Smooth \\
                $g: \RR^n \rightarrow \overline\RR$ & Weakly Convex \\ 
                $x_{-1}$ & Vector\\
                $M \in \N$ & Integer\\
                $\epsilon \in \RR$ & Number\\
                $\mathbf L$ & Algorithm \ref{alg:armijo-ls} or \ref{alg:chambolle-btls}\\
                $\mathbf M$ & Algorithm \ref{alg:beck-mono} or \ref{alg:nes-mono}\\
                $L:=1$ & $L > 0$\\ 
                $r:=0.5$ & $r \in (0, 1)$ \\ 
                $\rho := 2^{1/1024}$ & $\rho \in (0, 1)$ 
            \end{tabular}}
            \STATE{$n_0 := 0; z_0 = x_{-1}$.}
            \STATE{$z_1, p_0, \overline L_1, G^{(0)}:= \textbf{GMAPG}(f, g, x_{-1}, L, r_j, \rho, N_{\min}=n_0, N=M, \epsilon, \textbf{L}, \textbf{M}, \textbf{E}_{\chi}^a).$}
            \STATE{$n_1:=p_0$.}
            \STATE{$M := M - n_1$.}
            \FOR{$j = 1,2, \ldots, M$}
                \IF{$M \le 0 \textbf{ or } G^{(j-1)} \le \epsilon$}
                    \STATE{\textbf{break}}                
                \ENDIF
                \STATE{$z_{j + 1}, p_j, \overline L_{j + 1}, G^{(j)} := \textbf{GMAPG}(f, g, z_j, \overline L_j, r, \rho, N_{\min}=n_j, N=M, \epsilon, \textbf{L}, \textbf{M}, \textbf{E}_{\chi}^a).$}
                \STATE{$M := M - p_j$.}
                \STATE{$\overline L_{j + 1} = \max(\overline L_{j}, \overline L_{j + 1})$.}
                \IF{$f(z_{j}) - f(z_{j + 1}) > \exp(-1)(f(z_{j - 1}) - f(z_{j}))$}
                    \STATE{$n_{j + 1} := 2p_j$.}
                \ELSE
                \STATE{$n_{j + 1} := p_j$.}
                \ENDIF
            \ENDFOR
            \end{algorithmic}\caption{Linear convergence restarted GMAPG}\label{alg:linear-rgmapg}
        \end{algorithm}
        \par
        Algorithm \ref{alg:linear-rgmapg} implements a restarted GMAPG with condition $\textbf{E}_{\chi}^a$ and, it has a fast linear convergence rate. 
        The following observation on it is crucial to take notice for the convergence rate. 
        \begin{observation}\label{obs:rgmapg}
            If the outer loop runs for $j = 1, 2, \ldots, J$ iterations with $M \ge J$ so the algorithm terminated due $G_k^+ \le \epsilon$. 
            Then for all $J\ge j\ge 1$ it has $n_{j}\le n_{j + 1}$ hence $(n_j)_{j \ge 0}^J$ is monotone increasing
            For all $J -1 \ge j \ge 1 $ it has $p_{j-1}\le p_{j}$ so $(p_j)_{j \ge 1}^{J}$ is monotone excluding the last iterates $p_J$. 
        \end{observation}
        \par
        For all $1 \le j \le J$, $n_j$ is used as the lower bound $N_{\min}$ for the GMAPG inner loop described in Algorithm \ref{alg:gmapg} at iteration $j$ and, $p_j$ is the actual iteration underwent by the inner loop therefore it has $p_j \ge n_{j}$. 
        $n_{j + 1}$ is the minimum iteration for the next execution of GMAGP, and it has $n_{j + 1} = p_{j}$  if the ``if statement" isn't triggered otherwise $n_{j + 1} = 2p_{j}$. 
        For $j < J$ so it excludes the last iteration, it has either $p_{j + 1} \ge n_{j + 1} = 2p_j \ge 2n_{j} \ge n_j$ or $p_{j + 1} \ge n_{j + 1} = p_j \ge n_j$. 
        When $j = J$ it's not necessarily true that $p_J \ge n_J$ because on the last iteration, it can exit the for loop through conditions $G_k^+ \le \epsilon$ in line 14 of Algorithm \ref{alg:gmapg}, but $n_J \ge p_{J - 1} \ge n_{J - 1}$ still. 
        \par
        We now introduce the following notations for the convergence proof. 
        Since the restarted GMAPG consist of executions of an outer loop in Algorithm \ref{alg:linear-rgmapg} in variable $j$ and, an inner loop in Algorithm \ref{alg:gmapg} in variable $k$, we denote $x_{k}^{(j)}$ for the iterates $x_k$ in the inner loop during the $j$ iteration of the outer loop together. 
        We make the choice to have $x_{-1}^{(j + 1)} = z_{j + 1} = x_{p_j}^{(j)}$ for consistencies across theorems from previous sections. 
        For example in line 10 of Algorithm \ref{alg:linear-rgmapg} at the $j$ iteration, the inner loop returns $x_{p_j}$ as the last iterate. 
        So $x_{p_j}$ is assigned to $z_{j + 1}$ by the outer loop, and it has $x_{p_j} = x_{p_j}^{(j)} = z_{j + 1}$. 
        It continues and $z_{j + 1}$ will be the initial iterate pass into the inner loop for the $j + 1$ iteration, and hence $z_{j + 1} =x_{-1}^{(j + 1)}$. 
        \par
        Let $e$ denotes the base of natural log. 
        The following lemma combines quadratic growth assumption of the objective to assert lower bound on the number of iteration required to achieve a certain optimality on the objective function $F$. 
        \begin{lemma}[maximum iteration needed for an optimality gap ratio]\;\label{lemma:prog-ratio}\\
            Suppose that $F = f + g$ satisfies Assumption \ref{ass:standard-fista} so $F^+ := \inf_xF(x)$, $X^+$ is the set of minimizers, and $\mu > 0$ is the quadratic growth constant. 
            Let the sequence $(x_k)_{k \ge -1}$ be generated by GMAPG (Definition \ref{alg:gmapg}). 
            Then it has
            \begin{align*}
                \forall k \ge \left\lfloor \frac{2\sqrt{1 + e}}{\sqrt{\mu\widehat L_k^{-1}}}\right\rfloor:\; 
                F(x_k) - F^+  &\le e^{-1} (F(x_{-1}) - F(x_k)). 
            \end{align*} 
            Where $\widehat L_k$ is defined by:
            \begin{align*}
                \widehat L_k &:= \max\left(
                    L_0, \left(
                        \frac{1}{k} \sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                    \right)^{-2}
                \right). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            For all $k \ge 0$, denote minimizer $x^+_k = \Pi_{X^+}x_k$ so, $F(x^+_k) = F^+$. 
            From Theorem \ref{thm:gmapg-specialized-cnvg} it has 
            \begin{align*}
                F(x_k) - F^+ \le \frac{2 \widehat L_k}{(2 + k)^2}\Vert x_k - x_{-1}^+\Vert^2 \le \frac{4 \widehat L_k}{\mu(2 + k)^2}(F(x_{-1}) - F^+). 
            \end{align*}
            The second inequality comes from Assumption \ref{ass:q-growth-ch2} directly. 
            Suppose that $k \ge 2\sqrt{1 + e}\left(\mu\widehat L_k^{-1}\right)^{-1/2}$. 
            Denote $\gamma_k = 4\widehat L_k \mu^{-1}(2 + k)^{-2}$.
            We make the following assumption first, and it will be proved later:
            \begin{enumerate}
                \item [(a)] It has $\gamma_k \in (0, 1)$. 
            \end{enumerate}
            Using the above we have inequality: 
            \begin{align*}
                0 &\le \gamma_k (F(x_{-1}) - F^+) - (F(x_k) - F^+) 
                \\
                &= \gamma_k (F(x_{-1}) - F(x_k)) - (1 - \gamma_k)(F(x_{k}) - F^+). 
                \\
                \underset{\text{(a)}}{\iff} F(x_k) - F^+
                &\le \gamma_k(1 - \gamma_k)^{-1}(F(x_{-1}) - F(x_k)). 
            \end{align*}
            Continuing it has 
            \begin{align*}
                F(x_k) - F^+ &\le \gamma_k (1 -\gamma_k)^{-1}(F(x_{-1}) - F^+)
                \\
                &= \frac{4\widehat L_k}{\mu(2 + k)^{2}}\left(
                    1 - \frac{4\widehat L_k}{\mu(2 + k)^2}
                \right)^{-1}(F(x_{-1}) - F^+)
                \\
                &= 4\widehat L_k(\mu(2 + k)^2 - 4 \widehat L_k)(F(x_{-1}) - F^+)
                \\
                &\le 4\widehat L_k\left(
                    \mu\left(
                        2 + \left\lfloor \frac{2\sqrt{1 + e}}{\sqrt{\mu/\widehat L_k}}\right\rfloor
                    \right)^2 - 4 \widehat L_k
                \right)^{-1}(F(x_{-1}) - F^+)
                \\
                &\le 4\widehat L_k\left(
                    \mu\left(
                        \frac{2\sqrt{1 + e}}{\sqrt{\mu/\widehat L_k}} 
                    \right)^2 - 4 \widehat L_k
                \right)^{-1}(F(x_{-1}) - F^+)
                \\
                &\le 4\widehat L_k\left(
                    \mu\left(
                        \frac{4(1 + e)\widehat L_k}{\mu}
                    \right) - 4 \widehat L_k
                \right)^{-1}(F(x_{-1}) - F^+)
                \\
                &= 4\widehat L_k\left(
                    4\widehat L_k(1 + e)
                    - 4 \widehat L_k
                \right)^{-1}(F(x_{-1}) - F^+)
                =e^{-1} (F(x_{-1}) - F^+). 
            \end{align*}
            \textbf{The proof for (a)} now follows. 
            From the assumption on $k$ it has: 
            {\allowdisplaybreaks
            \begin{align*}
                0 &\ge \left\lfloor 
                    \frac{2\sqrt{1 + e}}{\sqrt{\mu/\widehat L_k}}
                \right\rfloor - k
                > \frac{2\sqrt{1 + e}}{\sqrt{\mu/\widehat L_k}} - 1- k
                > 
                \frac{2}{\sqrt{\mu/\widehat L_k}} - 1- k
                \\
                &> \frac{2}{\sqrt{\mu/\widehat L_k}} - (2 + k)
                =(2 + k)\left(
                    \frac{2}{(k + 2)\sqrt{\mu/\widehat L_k}} - 1
                \right)
                \\
                &= (2 + k)(\sqrt{\gamma_k} - 1). 
            \end{align*}
            }
        \end{proof}
        \par
        Continuing with the quadratic growth assumption, the following lemma states the fact that $p_j$ in Algorithm \ref{alg:linear-rgmapg} is bounded above and there exists a $j \ge 1$ such that $n_{j + 1 + k} = p_{j + k}$ for all $k \ge 0$ until it terminates. 
        \begin{lemma}[inner iteration is bounded above]\label{lemma:rgmapg-inner-bnds}
            Suppose that $F = f + g$ satisfies Assumption \ref{ass:q-growth-ch2}, denote $F^+ := \inf_xF(x)$ and, $X^+$ as the set of minimizers. 
            Consider any $j \ge 1$ iteration experienced by the outer loop. 
            Let $\left(x_{k}^{(j)}\right)_{k\ge 0}^{p_{j}}$ be the sequence generated by Algorithm \ref{alg:linear-rgmapg} in the $j$ th iteration of outer loop. 
            Define $\bar p := \frac{4\sqrt{2L(1 + e)}}{\sqrt{\mu}}$, then $p_{j} \le \bar p, n_j \le \bar p$. 
        \end{lemma}
        \begin{proof}
            The end result is constructed upon the following intermediate results that are proved at the end: 
            \begin{enumerate}
                \item [(i)] If $k \ge \bar p$, then exit condition $\mathbf E_{\chi}^a$ is true hence $p_j \le \max(\bar p, n_j)$ for all $j \ge 0$. 
                \item [(ii)] If $p_{j - 1} \le p_j\le \bar p$  then $n_{j + 1} \le \bar p$. 
            \end{enumerate}
            Take note that $n_0 = 0, n_1 = p_0$ hence (i) gives $p_0 \le \bar p$, and $p_1 \le \max(\bar p, n_1) = \max(\bar p, p_0) \le \bar p$. 
            We now have the base case: $p_0 \le p_1 = n_0 \le \bar p$. 
            Inductively assume $p_{j - 1} \le p_j \le \bar p$ then: 
            \begin{align*}
                p_{j + 1} \underset{\text{(i)}}{\le} \max(\bar p, n_{j + 1}) \underset{\text{(ii)}}{\le} \bar p. 
            \end{align*}
            Therefore, for all $j \ge 0$, $p_j \le \bar p$, and $n_{j + 1} \le \bar p$. 
            \par
            \textbf{Proof of (i)}. 
            Recall exit condition in \eqref{ineq:rgmapg-exit-cond} has $m = \lfloor k/2\rfloor + 1$. 
            Starting with the statement hypothesis it has $k \ge \bar p$ therefore: 
            \begin{align*}
                0 &\le k/2 - \bar p/2 \le \lfloor k/2\rfloor + 1 - \bar p/2
                = m - \bar p /2 \le m - \lfloor \bar p/2\rfloor
                \\
                &= m - \left\lfloor 
                    \frac{2\sqrt{2L(1 + e)}}{\sqrt{\mu}}
                \right\rfloor \le 
                m - \left\lfloor 
                    \frac{2\sqrt{\widehat L_k(1 + e)}}{\sqrt{\mu}}
                \right\rfloor. 
            \end{align*}
            On the last inequality we used Proposition \ref{prop:bnded-lip-ls} which has $\widehat L_k \le 2 L$. 
            Observe that the inequality allow us to apply Lemma \ref{lemma:prog-ratio} with $m = k$ which yields: 
            \begin{align*}
                e^{-1} \ge 
                \frac{
                    F\left(x_{m}^{(j)}\right) - F^+
                }{
                    F\left(x_{-1}^{(j)}\right) 
                    - F\left(x_{m}^{(j)}\right)
                } 
                \ge 
                \frac{
                    F\left(x_{m}^{(j)}\right)
                    - F\left(x_k^{(j)}\right)
                }{
                    F\left(x_{-1}^{(j)}\right) 
                    - F\left(x_{m}^{(j)}\right)
                } 
                \implies \mathbf E_{\chi}^a. 
            \end{align*}
            Observe that line 14 of Algorithm \ref{alg:gmapg} exits as soon as possible if $\mathbf E_\chi^a$ is true and $k > N_{\min} = n_j$ and, $G_k \le \epsilon$ will only cause it to exit earlier therefore it has $p_j \le \max(\bar p, n_j)$. 
            \par
            \textbf{Proof of (ii)}. 
            Inductively assume that $p_{j - 1} \le p_j \le \bar p$. 
            If $p_{j - 1} \le \bar p /2$ then Line 14, 16 in Algorithm \ref{alg:linear-rgmapg} implies that $n_{j} \le \max(p_{j - 1}, 2p_{j - 1}) \le \bar p$. 
            Otherwise, $p_{j - 1} > \bar p/2$, and using Proposition \ref{prop:bnded-lip-ls} it means 
            \begin{align*}
                p_{j - 1} \ge \bar p /2 = \frac{2\sqrt{2L(1 + e)}}{\sqrt{\mu}} 
                \ge 
                \left \lfloor \frac{2\sqrt{\widehat L_k(1 + e)}}{\sqrt{\mu}} \right\rfloor. 
            \end{align*}
            The above inequality allows us to use Lemma \ref{lemma:prog-ratio} which yields 
            \begin{align*}
                e^{-1} \ge 
                \frac{
                    F\left(x_{p_{j - 1}}^{(j - 1)}\right) - F^+
                }
                {
                    F\left(x_{-1}^{(j - 1)}\right) - F\left(z_{p_{j - 1}}^{(j - 1)}\right)
                }
                \ge 
                \frac{F(z_j) - F(z_{j + 1})}
                {F(z_{j - 1}) - F(z_{j})} \implies n_{j + 1} = p_{j + 1}.
            \end{align*}
            Therefore, there is no doubling at line 14 of Algorithm \ref{alg:linear-rgmapg}, hence $n_{j + 1} \le \bar p$ still. 
        \end{proof}
        \par
        We just show that the sequence $n_j$ is bounded above, and it must have a limit because it's also monotone increasing, which implies that at some point, the doubling of $n_{j + 1} = 2p_j$ must stop for the outer loop. 
        The following lemma shows that when that happens, the algorithm will always terminate after a finite number of iterations of the outer loop. 
        \begin{lemma}[bounds on outer iteration counts]\label{lemma:rgmapg-outer-itr-bnd}
            Let $F = f + g$ satisfies Assumption \ref{ass:standard-fista}. 
            Suppose we apply Algorithm \ref{alg:linear-rgmapg} on $F = f + g$. 
            Define $T_\epsilon = \lceil\ln(2\epsilon^{-2}(F(z_0) - F^+))\rceil$
            Assume that after iteration $j$, no doubling occurred in the if statement, i.e: $n_{t + 1} = p_t$ for $t \ge j$, then it must terminate before, or at iteration $j + T_\epsilon$. 
        \end{lemma}
        \begin{proof}
            Suppose that since the $j\ge 2$ th iteration, there is no period doubling for $T_\epsilon$ number of iterations in the outer loop of Algorithm \ref{alg:linear-rgmapg}, i.e: $n_{t + 1} = p_t$
            for $j \le t \le j + T_\epsilon - 1$, and denote $s = j + T_\epsilon -1$ for better notations, giving us for all $j \le t \le s$, $n_{t + 1} = p_t$. 
            \par
            Our goal now is to show that at iteration $j = s$ of Algorithm \ref{alg:linear-rgmapg} it must have $G_0 \le \epsilon$ at line 5 of Algorithm \ref{alg:gmapg}. 
            \par
            Consider the start of the $s$ th iteration of the outer loop, denote $L_0^{(s)}, G_0^{(s)}$ as the $L_0$ in line 4 in Algorithm \ref{alg:gmapg}, it would give the following inequalities
            {\allowdisplaybreaks
            \begin{align*}
                \frac{1}{2}\left(
                    G_0^{(s)}
                \right)^2
                &\underset{\text{(a)}}{=} 
                \frac{1}{2}\left\Vert
                    \sqrt{L_0^{(s)}}\left(z_s - T_{1/L_0^{(s)}}(z_s)\right)
                \right\Vert^2
                \\
                &\underset{\text{(b)}}{=} \frac{1}{2L_0^{(s)}}\left\Vert
                        \mathcal G_{1/L_0^{(s)}}(z_s)
                \right\Vert^2
                \\
                &\underset{\text{(c)}}{\le} F(z_s) - F\left(x_0^{(s)}\right) 
                \\
                &\underset{\text{(d)}}{\le} 
                F(z_s) - F\left(x_{p_s}^{(s)}\right)
                \\
                &= F(z_s) - F(z_{s + 1})
                \\
                &\underset{\text{(e)}}{\le} \exp(-T_\epsilon)\left(F(z_{s - T_\epsilon}) - F(z_{s - T_\epsilon + 1})\right)
                \\
                &= \exp(-T_\epsilon)(F(z_{j - 1}) - F(z_j)) 
                \\
                &\underset{\text{(f)}}{\le} \exp(-T_\epsilon)(F(z_{0}) - F(z_j))
                \\
                &\underset{\text{(g)}}{\le} \left(
                    \frac{2(F(z_0) - F^+)}{\epsilon^2}
                \right)^{-1}(F(z_0) - F^+) 
                \\
                &= \epsilon^2/2.
            \end{align*}
            }
            \begin{enumerate}
                \item [(a)] At the $s$ iteration of the loop in Algorithm \ref{alg:linear-rgmapg}, $z_s$ is passed into Algorithm \ref{alg:gmapg} with $x_{-1} = z_s$. Therefore, at line 3 in Algorithm \ref{alg:gmapg} it calls Algorithm \ref{alg:armijo-ls} with $x_{-1} = z_s, \alpha_0 = 1, L^+$ which means $y^+ = x_{-1} = z_s$ at line 5 and $x^+ = T_{1/L^+}(z_s)$. Coming back to line 3 in Algorithm \ref{alg:gmapg}, it assigns $y_0 = y^+ = z_s, x_0 = x^+ = T_{1/L^+}(z_s)$ and $L_0 = L^+$. Assuming the line search went successful, it will have $D_f(z_s, x_0) \le L^{(s)}_0/2 \Vert z_s - x_0\Vert^2$. 
                \item [(b)] We used definition of gradient mapping in Definition \ref{def:gm-for-ch2}. 
                \item [(c)] By the assumption that the line search in Algorithm \ref{alg:armijo-ls} is successful at $z_s$ back in item (a), here we can use \eqref{eqn:emp:result-item-2} in Theorem \ref{thm:gmapg-generic-gm-cnvg}. 
                \item [(d)] GMAPG is monotone in function value for the use of $\mathbf M$ that is either Algorithm \ref{alg:beck-mono} or \ref{alg:nes-mono}, so it has $F\left(x_{p_s}^{(s)}\right) \le F\left(x_0^{(s)}\right)$. 
                \item [(e)] Here we used the assumption that no doubling occurs so $n_{t + 1} = p_t$ for all $j \le t \le s$ meaning that line 13 in Algorithm \ref{alg:linear-rgmapg} has $F(z_t) - F(z_{t + 1}) \ge e^{-1}(F(z_{t - 1}) - F(z_t))$. For that, we recursively unrolled it for $T_\epsilon$ many iterations starting with $t = s = j + T_\epsilon - 1$ ending with $t = j$. 
                \item [(f)] We used the monotone property of subroutine $\mathbf M$ in GMAPG again so $F(z_0) \ge F(z_{j - 1})$. 
                \item [(g)] We substituted $T_\epsilon = \lceil\ln(2\epsilon^{-2}(F(z_0) - F^+))\rceil$ and, removing $\lceil\cdot\rceil$ to make for the $\le$ inequality. We also replaced $F(z_j)$ by $F^+$ the minimum which is always smaller. 
            \end{enumerate}
            Therefore, it has $G_0^{(s)} \le \epsilon$, hence it must have terminated at, or before iteration $s$. 
        \end{proof}
        \begin{theorem}[bounds on the total iterations]\label{thm:rgmapg-total-itr-bnds}
            Let $F = f + g$ satisfies Assumption \ref{ass:q-growth-ch2}. 
            Suppose that we applied Algorithm \ref{alg:linear-rgmapg} to it. 
            For any $\epsilon > 0$, assuming that it terminated at $j = J < M$ iteration.
            Then the total number of iterations has an upper bound:
            \begin{align*}
                \sum_{i = 0}^{J - 1}p_i &\le 
                \frac{8\sqrt{2L(1 + e)}}{\sqrt{\mu}} \left\lceil 
                \ln \left(
                    \frac{2(F(z_0) - F^+)}{\epsilon^2}
                \right) 
                \right\rceil. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Firstly, Algorithm \ref{alg:linear-rgmapg} must terminate within finite many iterations under Assumption \ref{ass:q-growth-ch2} for all total budget $M \in \N$. 
            This is because Observation \ref{obs:rgmapg} shows that $n_j$ is a non-decreasing sequence, Lemma \ref{lemma:rgmapg-inner-bnds} shows that $n_j \le \bar p$ under this quadratic growth assumption, therefore $n_j$ must converge which implies that doubling of $n_{j + 1} = 2p_{j}$ must stop. 
            Finally, since the doubling must stop at some $j$, Lemma \ref{lemma:rgmapg-outer-itr-bnd} applies hence, it terminates at most $j + T_\epsilon$ iteration. 
            \par
            Using it let's assume that it executed for $j = 1, 2, \cdots, J$ and $M$ is large enough to achieve optimality $G^+ \le \epsilon$ right at the start of iteration $J$.
            This is one of the way the algorithm can terminate, making it a sufficient condition for deriving the upper bound on the total number of iterations. 
            Representing it by the maximum period between doubling: $J = m + nT_\epsilon$ with $0 \le m < T_\epsilon$. 
            The following intermediate results are important to the proof. 
            \begin{enumerate}
                \item [(a)] $n_{J - lT_\epsilon} \le n_{J - lT_\epsilon}\le (1/2)^ln_J$ for all $l = 1, \ldots, n$. From Lemma \ref{lemma:rgmapg-outer-itr-bnd} we know that doubling must have occured within a period of $T_\epsilon$ iterations at least once.
                \item [(b)] $n_{j} \le n_{j + 1}$ for all $0 \le j \le J - 1$. The sequence is monotone from Observation \ref{obs:rgmapg}. 
                \item [(c)] $n_j \le \bar p$ with $\bar p = 4\sqrt{2L(1 + e)}/\sqrt{\mu}$, proved in Lemma \ref{lemma:rgmapg-inner-bnds}. 
            \end{enumerate}
            The upper bound on the total number of iterations of GMAPG over $J$ iteration of outer loop is given by: 
            \begin{align*}
                \sum_{i = 0}^{J} n_i &= \sum_{i = 0}^{m + nT_\epsilon} n_i
                \\
                &= \sum_{i = 0}^{m}n_i + \sum_{l = 0}^{n - 1}\sum_{i = 1}^{T_\epsilon} n_{m + i + lT_\epsilon}
                \\
                &\hspace{-0.5em}
                \underset{\substack{(b), \\ m \le T_\epsilon.}}\le T_\epsilon n_m + \sum_{l = 0}^{n - 1} T_\epsilon n_{m + (l + 1)T_\epsilon}
                \\
                &= T_\epsilon \sum_{l = 0}^{n} n_{m + lT_\epsilon} = T_\epsilon \sum_{l = 0}^{n} n_{J - lT_\epsilon}
                \\
                &\underset{\text{(a)}}{\le} T_\epsilon \sum_{l = 0}^{n} (1/2)^l n_J \le T_\epsilon \sum_{l = 0}^{\infty} (1/2)^l n_J
                \\
                &\le 2T_\epsilon n_J \le 2T_\epsilon \bar p.
            \end{align*}
            The total number of iterations is bounded by: 
            \begin{align*}
                \sum_{i = 0}^{J - 1}p_i \le \sum_{i = 0}^{J} n_i &\le 
                \frac{8\sqrt{2L(1 + e)}}{\sqrt{\mu}} \left\lceil 
                \ln \left(
                    \frac{2(F(z_0) - F^+)}{\epsilon^2}
                \right) 
                \right\rceil. 
            \end{align*}
            Note that $n_0 = 0$. 
        \end{proof}
        \par
        The final results from above theorem provide the convergence rate of iterates and the complexity of restart GMAPG under Assumption \ref{ass:q-growth-ch2}. 
        We denote $\kappa := L/\mu$. 
        \begin{theorem}[restarted GMAPG convergence and complexity]\;\label{thm:rgmapg-cnvg-complexity}\\
            Let $F = f + g$ satisfy Assumption \ref{ass:q-growth-ch2}. 
            Let $J$ be the total number of iteration performed to achieve accuracy in the outer loop.
            Let $K := \sum_{i = 0}^{J - 1}p_i$ be the total iterations. 
            Then, the maximum $K$ needed to achieve optimality $\Vert z_J - z_J^+\Vert \le \delta$ on the iterates is bounded by $\mathcal O\left(\frac{1}{\sqrt{\kappa}}\ln\left(\frac{1}{\kappa\delta}\right)\right)$. 
        \end{theorem}
        \begin{proof}
            Suppose a total of $K := \sum_{i = 0}^{J - 1} p_i$ iteration were performed and, at line 10 of Algorithm \ref{alg:linear-rgmapg} at iteration $j = J$ it achieved $G_0 \le \epsilon$ at line 5 of Algorithm \ref{alg:gmapg}. 
            That will cause the inner GMAPG to return $G_0$ so, at line 10 it has $G^{(J)} < \epsilon$ in Algorithm \ref{alg:linear-rgmapg}. 
            This is one of the ways the algorithm can terminate hence, it suffices for deriving an upper bound. 
            \par
            Now we show the convergence rate of $G^{(J)}$, let $k > 0$ and let $\epsilon = \sqrt{2}(F(z_0) - F^+)^{1/2}\exp(-k + 1)$ then: 
            \begin{align*}
                \ln \left(
                    \frac{2(F(z_0) - F^+)}{\epsilon^2}
                \right)
                &= 2(k - 1). 
            \end{align*}
            Then it has from Theorem \ref{thm:rgmapg-total-itr-bnds} that:
            \begin{align*}
                0 &\le 
                \frac{8\sqrt{2L(1 + e)}}{\sqrt{\mu}}\left\lceil 
                    \ln\left(
                        \frac{2(F(z_0) - F^+)}{\epsilon^2}
                    \right)
                \right\rceil - K
                =
                \frac{8\sqrt{2L(1 + e)}}{\sqrt{\mu}}\left\lceil 
                    2(k - 1)
                \right\rceil - K 
                \\
                &\le 
                \frac{8\sqrt{2L(1 + e)}}{\sqrt{\mu}} (2(k - 1) + 1) - K
                \\
                \implies
                0 &\le k - 1 + 1/2 - \frac{K\sqrt{\mu}}{16\sqrt{2L(1 + e)}}
                \\
                &\le k - \frac{K\sqrt{\mu}}{16\sqrt{2L(1 + e)}} .
            \end{align*}
            This gives us 
            \begin{align*}
                G^{(J)} &\le \epsilon = \sqrt{2}(F(z_0) - F^+)^{1/2}\exp(-k + 1)
                \\
                &=
                e\sqrt{2}(F(z_0) - F^+)^{1/2}\exp(-k)
                \\
                &\le
                e\sqrt{2}(F(z_0) - F^+)^{1/2}\exp\left(
                    - \frac{K\sqrt{\mu}}{16\sqrt{2L(1 + e)}}
                \right)
                \\
                &= e\sqrt{2}(F(z_0) - F^+)^{1/2}\exp\left(
                    - \frac{K\sqrt{\kappa}}{16\sqrt{2 + 2e}}
                \right). 
            \end{align*}
            The above inequality shows a linear convergence rate of the quantity $G_0^{(J)}$ with respect to the total number of iterations required for GMAPG. 
            Denote $z_J^+ = \Pi_{X^+}z_J$ then it has 
            {\allowdisplaybreaks
            \begin{align*}
                G^{(J)} 
                &\underset{\text{(a)}}{=}\left\Vert \sqrt{L_0^{(J)}} \left(z_J - x_0^{(J)}\right) \right\Vert 
                = \frac{1}{\sqrt{L_0^{(J)}}} \left\Vert 
                    L_0^{(J)}\left(
                        z_J - x_0^{(J)}
                    \right)
                \right\Vert
                \underset{\text{(a)}}{=}\frac{1}{\sqrt{L_0^{(J)}}} \left\Vert 
                    \mathcal G_{1/L_0^{(J)}}(z_J)
                \right\Vert
                \\
                &\underset{\text{(b)}}{\ge} \frac{1}{\sqrt{2L}}\left\Vert\mathcal G_{1/L_0^{(J)}}(z_J)\right\Vert
                \\
                &\underset{\text{(c)}}{\ge} \frac{\sqrt{L(\mu + L)} - L}{\sqrt{L(\mu + L)}} \frac{1}{\sqrt{2L}}\left\Vert
                    z_J - z_J^+
                \right\Vert
                \\
                &= \frac{1}{\sqrt{2L}}\left(
                    1 - \frac{1}{\sqrt{1 + \mu/L}}
                \right)\left\Vert
                    z_J - z_J^+
                \right\Vert
                \\
                &\underset{\text{(d)}}{\ge} \frac{1}{2\sqrt{L}}\left(
                    \frac{\mu}{2L}
                \right)\left\Vert
                    z_J - z_J^+
                \right\Vert 
                = \frac{\kappa}{4\sqrt{L}} \left\Vert
                    z_J - z_J^+
                \right\Vert. 
            \end{align*}
            }
            \begin{enumerate}
                \item [(a)] For the first equality, we assumed that $G^{(J)} < \epsilon$ in the outer loop and, it's returned by Algorithm \ref{alg:gmapg} at line 5. Therefore, it has $G_0 = G^{(J)} = \sqrt{L^{(J)}_0} \Vert z^{(J)} - x_0^{(J)}\Vert$ because it calls Algorithm \ref{alg:armijo-ls} (Armijo line search) with $\alpha_0 = 0$. The second equality comes by using Definition \ref{def:gm-for-ch2}. 
                \item [(b)] Proposition \ref{prop:bnded-lip-ls} has $L_0^{(J)} \le 2L$. 
                \item [(c)] Using Theorem \ref{thm:qfg-peb-equiv} from previous chapter and take note that Assumption \ref{ass:q-growth-ch2} is Definition \ref{def:necoara-weaker-scnvx}\ref{def:necoara-qfg}. 
                \item [(d)] The function $1/\sqrt{1 - x/L}$ is convex hence a Taylor expansion has $1/\sqrt{1 - x/L} \le 1 - x/(2L) $, substituting it with $x = \mu$ makes for a lower bound. 
            \end{enumerate}
            It then yields the convergence rate of 
            \begin{align*}
                \left\Vert z_J - z^+_J\right\Vert 
                &\le \frac{4eL\sqrt{2}(F(z_0) - F^+)^{1/2}}{\kappa}
                \exp\left(
                    - \frac{K\sqrt{\kappa}}{16\sqrt{2 + 2e}}
                \right) 
                \le 
                \mathcal O\left(
                    \kappa^{-1}\exp\left(-\sqrt{\kappa}K\right)
                \right). 
            \end{align*}
            To achieve accuracy $\Vert z_J - z^+_J\Vert \le \delta$, it would require $\mathcal O\left(\frac{1}{\sqrt{\kappa}}\ln\left(\frac{1}{\kappa\delta}\right)\right)$ many total iterations of GMAGP. 
        \end{proof}
        
    
    \section{Applying them to large scale LP}
    
    \section{Hoffman Bounds and infeasibility detection}
    
\chapter{Enhanced Primal Dual Methods for LP}
    


% ==============================================================================

\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}