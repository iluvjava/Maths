\documentclass[12pt]{report}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}
\title{{\fontfamily{ptm}\selectfont Reading Notes}}

\author{
    Alto
    % \thanks{
    %     Subject type, Some Department of Some University, Location of the University,
    %     Country. E-mail: \texttt{author.name@university.edu}.
    % }
}

\date{Last Compiled: \today}

\maketitle

\begin{abstract} 
    Reports on papers read. 
    This is a LaTEX file for my own notes taking. 
    It may accelerate the process of writing my thesis for my PhD degree. 
    \todoinline{This paper is currently in draft mode. Check source to change options. }
\end{abstract}
\chapter{The Basics of Optimization Theories}
    Notations in this chapter are not shared, and they are for this chapter only. 
    % ===================================================================================
    % BREGMAN DIV DEFINITION 
    % ===================================================================================
    \begin{definition}[Bregman Divergence]\label{def:bregman-div}
        Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
        Define Bregman Divergence: 
        \begin{align*}
            D_f: \RR^n \times \dom \nabla f \rightarrow \overline \RR:= 
            (x, y) \mapsto f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
        \end{align*}
    \end{definition}
    \begin{assumption}[smooth plus nonsmooth]\label{ass:smooth-add-nonsmooth}
        Let $F = f+ g$ where $f:\RR^n \rightarrow \overline \RR$ is differentiable and there exists $q\in \RR$ such that $g - q/2\Vert \cdot\Vert^2$ is convex.
    \end{assumption}
    \begin{definition}[proximal gradient operator]
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth}. 
        Define the proximal gradient operator by: 
        \begin{align*}
            T_{\beta^{-1}, f, g}(x) &= \hprox_{\beta^{-1}g} \left(
                x - \beta^{-1} \nabla f(x)
            \right)
            \\
            &= \argmin_{z}\left\lbrace
                g(z) + f(x) + \langle \nabla f(x), z - x\rangle
                + \frac{\beta}{2}\Vert x - z\Vert^2
            \right\rbrace. 
        \end{align*}
    \end{definition}
    % ==========================================================================
    % WEAKLY CONVEX GENERIC PROXIMAL GRADIENT INEQUALITY
    % ==========================================================================
    \begin{theorem}[weakly convex generic proximal gradient inequality]\label{thm:pg-ineq-wcnvx-generic}\;\\
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} with $\beta > 0$ and $q \in \RR$. 
        Then for all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$, it has: 
        \begin{align*}
            \frac{q}{2}\Vert z - \bar x\Vert^2 
            &\le 
            F(z) - F(\bar x) - \langle \beta(x - \bar x), z - \bar x\rangle 
            + D_f(x, \bar x ) - D_f(z, x).  
        \end{align*}
    \end{theorem}
    \begin{proof}
        Nonsmooth analysis calculus rules has 
        \begin{align*}
            \bar x &\in \argmin{z} \left\lbrace
                g(z) + \langle \nabla f(x), z\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
            \right\rbrace
            \\
            \implies
            \mathbf 0 
            &\in \partial g(x^+) + \nabla f(x) + \beta(x^+ - x)
            \\
            \iff 
            \partial g(x^+) &\ni
            - \nabla f(x) - \beta(x^+ - x). 
        \end{align*}
        The subgradient inequality for weak convexity has 
        \begin{align*}
            \frac{q}{2}\Vert z - \bar x\Vert^2 
            &\le 
            g(z) - g(\bar x) + \langle \nabla f(x) + \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) + \langle \nabla f(x), z - \bar x\rangle + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= g(z) - g(\bar x) + \langle \nabla f(x), z - x\rangle
            + \langle \nabla f(x), x - \bar x\rangle
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) 
            + (-D_f(z, x) + f(z) - f(x))
            \\
            & \quad 
            + (D_f(\bar x, x) - f(\bar x) + f(x))
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= F(z) - F(\bar x) - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle. 
        \end{align*}
    \end{proof}
    \begin{theorem}[convex proximal gradient inequality]\label{thm:cnvx-pg-ineq}
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} such that $q = \mu_g \ge 0$, $\beta \ge L_f$. 
        In addition, suppose that $f:\RR^n\rightarrow \RR$ has $L_f$ Lipschitz continuous gradient, and it's $\mu_f \ge 0$ strongly convex. 
        For all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has 
        \begin{align*}
            0 &\le 
            F(z) - F(\bar x) + 
            \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2
            - \frac{\beta + \mu_g}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{theorem}
    \begin{proof}
        The Bregman Divergence of $f$ has inequality 
        \begin{align*}
            \left(\forall x \in \RR^n, y \in \RR^n\right)\; 
            \frac{\mu_f}{2}\Vert x - y\Vert^2 \le D_f(x, y) \le \frac{L_f}{2}\Vert x - y\Vert^2. 
        \end{align*}
        Specializing Theorem \ref{thm:pg-ineq-wcnvx-generic}, let $x \in \RR^n$ and define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has $\forall z \in \RR^n:$
        \begin{align*}
            \frac{\mu_g}{2}\Vert z - \bar x \Vert^2 
            &\le 
            F(z) - F(\bar x) 
            - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \frac{L_f}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x + x - \bar x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \left(
                \frac{L_f}{2} - \beta
            \right)\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}
            \left(
                \Vert x - \bar x\Vert^2
                + 2\langle x - \bar x, z - x\rangle
            \right)
            \\
            &= 
            F(z) - F(\bar x) 
            + \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{proof}
    
\chapter{Linear Convergence of First Order Method}
    In this chapter, we are specifically interested in characterizing linear convergence of well known first order optimization algorithms. 
    In this section, $D_f$ will denote the Bregman Divergence as defined in Definition \ref{def:bregman-div}. 
    \section{Necoara's et al's Paper}
        \subsection{The Settings}
            The assumption follows give the same setting as Necoara et al. \cite{necoara_linear_2019}. 
            % ==================================================================
            % NECOARA's ASSUMPTIONS 
            % ==================================================================
            \begin{assumption}\label{ass:necoara-2019-settings}
                Consider optimization problem: 
                \begin{align}
                    -\infty < f^+ = \min_{x \in X} f(x) . 
                \end{align}\label{problem:necoara-2019}
                $X\subseteq \RR^n$ is a closed convex set. 
                Assume projection onto $X$, denoted by $\Pi_X$ is easy. 
                Denote $X^+ = \argmin_{x \in X}f(x) \neq \emptyset$, assume it's a closed set. 
                Assume $f$ has $L_f$ Lipschitz continuous gradient, i.e: for all $x, y\in X$: 
                \begin{align*}
                    \Vert \nabla f(x) - \nabla f(y)\Vert \le L_f\Vert x - y\Vert. 
                \end{align*}
            \end{assumption}
            Some immediate consequences of Assumption \ref{ass:necoara-2019-settings} now follows. 
            The variational inequality characterizing optimal solution has: 
            \begin{align}\label{ineq:pg-opt-cond}
                x^+ \in X^+ \implies 
                (\forall x \in X)\; \langle \nabla f(x^+), x - x^+\rangle \ge 0. 
            \end{align}
            The converse is true if $f$ is convex. 
            The gradient mapping in this case is: 
            \begin{align*}
                \mathcal G_{L_f}x = L_f(x - \Pi_{X}x). 
            \end{align*}
            % ==================================================================
            % STRONG CONVEXITY DEFINITION
            % ==================================================================
            \begin{definition}[strong convexity]\label{def:necoara-scnvx}
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then $f \in \mathbb S(L_f, \kappa_f, X)$ is strongly convex iff 
                \begin{align*}
                    (\forall x, y\in X)\; 
                    \kappa_f \Vert x - y\Vert^2 \le 
                    D_f(x, y) \le L_f \Vert x - y\Vert^2. 
                \end{align*}
            \end{definition}
            Then it's not hard to imagine the following natural relaxation of the above conditions. 
            %===================================================================
            % DEFINITION OF WEAKER STRONG CONVEXITY 
            % ==================================================================
            \begin{definition}[relaxations of strong convexity]\;\\
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}.
                \label{def:necoara-weaker-scnvx}
                Let $L_f \ge \kappa_f \ge 0$ such that for all $x \in X$, $\bar x = \Pi_{X^+} x$. 
                We define the following: 
                \begin{enumerate}
                    \item\label{def:neocara-qscnvx} Quasi-strong convexity (Q-SCNVX): $0 \le D_f(\bar x, x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb S'(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qup} Quadratic under approximation (QUA): $0 \le D_f(x, \bar x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb U(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qgg} Quadratic Gradient Growth (QGG): $0\le D_f(x, \bar x) + D_f(\bar x, x) - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb G(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qfg} Quadratic Function Growth (QFG): $0 \le f(x) - f^* - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb F(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-peb} Proximal Error Bound (PEB): $\Vert \mathcal G_{L_f}x\Vert \ge \kappa_f\Vert x - \bar x\Vert$. 
                    Denoted by $\mathbb E(L_f, \kappa_f, X)$. 
                \end{enumerate}
            \end{definition}
            \begin{remark}
                The error bound condition in Necoara et al. is sometimes referred to as the "Proximal Error Bound". 
            \end{remark}

        \subsection{Weaker conditions of strong convexity}
            In Necoara's et al., major results assume convexity of $f$. 
            % ==================================================================
            % THEOREM | Q-SCNVX IMPLIES QUA 
            % ==================================================================
            \begin{theorem}[Q-SCNVX implies QUA]\label{thm:qscnvx-means-qua}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings} and assume $f$ is convex: 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                We prove by induction. 
                Convexity of $f$ makes $X^+$ convex, so $\Pi_{X^+}x$ is unique for all $x \in \RR^n$. 
                Make inductive hypothesis that there exists $\kappa^{(k)} \ge 0$ such that 
                \begin{align*}
                    (\forall x \in X)\quad
                    f(x) \ge f^+ + \langle \nabla f(\Pi_{X^+}x), x - \Pi_{X^+}x\rangle 
                    + \kappa^{(k)}_f/2\Vert x - \Pi_{X^+}x \Vert^2. 
                \end{align*}
                The base case is true by convexity of $f$ with $\kappa_f^{(0)} = 0$. 
                Choose any $x \in X$ define $\bar x = \Pi_{X^+}x$. 
                Consider $x_\tau = \bar x + \tau(x - \bar x)$ for $\tau \in [0, 1]$. 
                $f$ is Q-SCNVX so
                \begin{align}\label{ineq:thm:qscnvx-means-qua-proof-item1}
                    f^+ - f(x_\tau) &\ge \langle \nabla f(x_\tau), \Pi_{X^+}x_\tau - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2 
                    \notag\\
                    &= 
                    \langle \nabla f(x_\tau), \bar x - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \bar x\Vert^2
                    \notag\\
                    \iff 
                    \langle \nabla f(x_\tau), x_\tau - \bar x\rangle
                    &\ge f(x_\tau) - f^+ + \kappa_f/2\Vert x_\tau -\bar x\Vert^2. 
                \end{align}
                In the inductive proof that comes, we will use the following intermediate results. 
                They are labeled for ease of refernecing. 
                \begin{enumerate}
                    \item The inequality \eqref{ineq:thm:qscnvx-means-qua-proof-item1}. 
                    \item By the property of projection, it has $\Pi_{X^+} x_\tau = \bar x$. 
                    \item The inductive hypothesis with $k \ge 0$. 
                    \item $\bar x = \Pi_{X^+}x$, $X^+$ is the set of minimizer of the of $f$ over $X$, hence $f(\bar x) = f^+$, the minimum. 
                \end{enumerate}
                Using calculus rules, we start with: 
                {\footnotesize
                \begin{align*}
                    f(x) &= 
                    f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau
                    = 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), \tau(x - \bar x)\rangle d\tau
                    \\
                    &= 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), x_\tau - \bar x\rangle d\tau.
                    \\
                    &\underset{\text{(i)}}{\ge }
                    f(\bar x) + 
                    \int_0^1 \tau^{-1} \left(
                        f(x_\tau) - f^+ + \frac{\kappa_f}{2}\Vert x_\tau - \bar x\Vert^2
                    \right) d\tau
                    = 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            f(x_\tau) - f^+ 
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &\underset{\text{(iii)}}{\ge }
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\Pi_{X^+}x_\tau), x_\tau - \Pi_{X^+}x_\tau
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \Pi_{X^+}x_\tau\Vert^2
                    d\tau
                    \\
                    &\underset{\text{(ii)}}{=} 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\bar x), x_\tau - \bar x
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \bar x\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \int_0^1 
                        \langle 
                            \nabla f(\bar x), x - \bar x
                        \rangle
                        + \frac{\tau\kappa_f^{(k)}}{2} \Vert x - \bar x\Vert^2
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &\underset{\text{(iv)}}{=} 
                    f^+ + 
                    \langle 
                        \nabla f(\bar x), x - \bar x
                    \rangle
                    +
                    \frac{\kappa^{(k)}_f + \kappa_f}{4}
                    \Vert x - \bar x\Vert^2. 
                \end{align*}
                }
                This is the new inductive hypothesis, and it has $\kappa_f^{(k + 1)} = (\kappa_f^{(k)} + \kappa_f)/2$. 
                The induction admits recurrence: 
                \begin{align*}
                    \kappa_f^{(n)} = (1/2^n)(\kappa_f^{(0)} + (2^n - 1)\kappa_f). 
                \end{align*}
                Inductive hypothesis is true for $\kappa_f^{(0)} = 0$ and $f$ being convex is sufficient. 
                It has $\lim_{n\rightarrow \infty} \kappa_f^{(n)} = \kappa_f$. 
            \end{proof}
            \begin{remark}
                This is Theorem 1 in the paper. 
                Convexity assumption of $f$ makes $X^+$ convex, so the projection is unique, and it has $\Pi_{X^+}x_\tau = \bar x$ for all $\tau \in [0, 1]$. 
                In addition, the inductive hypothesis has $\kappa_f^{(n)} \ge 0$, which is not sufficient for convexity, but necessary. 
                The projection property remains true for nonconvex $X^+$, however the base case require rethinking. 
            \end{remark}
            % ================================================================================
            % THEOREM | QGG IMPLIES QUA 
            % ================================================================================
            \begin{theorem}[QGG implies QUA]\label{thm:qgg-implies-qua}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}, under convexity it has 
                \begin{align*}
                    \mathbb G(L_f, \kappa_f, X)\subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                For all $x \in X$, define $\bar x = \Pi_{X^+}x$, $x_\tau = \bar x + \tau(x - \bar x)\; \forall \tau \in [0, 1]$. 
                Observe that $\frac{d}{d\tau}x_\tau = x - \bar x$ and $\Pi_{X^+}x_\tau = \bar x\; \forall \tau \in [0, 1]$. 
                Using calculus, Definition \ref{def:necoara-weaker-scnvx} \ref{def:necoara-qgg}: 
                \begin{align*}
                    f(x) &= f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau  
                    \\
                    &= f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \langle \nabla f(x_\tau) - \nabla f(\bar x), x - \bar x\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), \tau(x - \bar x)\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), x_\tau - \bar x\rangle d \tau
                    \\
                    &\ge
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\kappa_f\Vert \tau(x - \bar x)\Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau\kappa_f\Vert x - \bar x \Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \frac{\kappa}{2}\Vert x - \bar x\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                This is Theorem 3 in Neocara et al. \cite{necoara_linear_2019}. 
                There is no immediate use of convexity besides that the projection $\bar x = \Pi_{X^+}x$ is a singleton.
            \end{remark}
            % ==================================================================
            % THEOREM | QFC IMPLIES QGG  
            % ==================================================================
            \begin{theorem}[Q-SCNVX implies QGG]\label{thm:qscnvx-implies-qgg}
                Under Assumption \ref{ass:necoara-2019-settings} and convexity of $f$, it has 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb G(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                If $f \in \mathbb S'(L_f, \kappa_f, X)$ then Theorem \ref{thm:qscnvx-means-qua} has $f \in \mathbb U(L_f, \kappa_f, X)$. 
                Then, add \ref{def:necoara-qup}, \ref{def:neocara-qscnvx} in Definition \ref{def:necoara-weaker-scnvx} yield the results. 
            \end{proof}
            \begin{remark}
                This is Theorem 2 in the Necoara et al. \cite{necoara_linear_2019}, right after it claims $\mathbb U(L_f, \kappa_f, X)\subseteq \mathbb G(L_f, \kappa_f/2, X)$ under convexity. 
            \end{remark}
            % ==================================================================
            % THEOREM | SUFFICIENCY OF QFG 
            % ==================================================================
            \begin{theorem}[sufficiency of QFG]\label{thm:qfg-suff}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                For all $0 < \beta < 1$, $x \in X$, let $x^+ = \Pi_{X}(x - L^{-1}_f \nabla f(x))$. 
                If 
                \begin{align*}
                    \Vert x^+ - \Pi_{X^+}x^+\Vert \le \beta \Vert x - \Pi_{X^+}x \Vert, 
                \end{align*}
                then $f$ satisfies the QFG condition with $\kappa_f = L_f(1 - \beta)^2$. 
            \end{theorem}
            \begin{proof}
                The proof is direct. 
                \begin{align}
                    \Vert x - \Pi_{X^+}x\Vert 
                    &\le \Vert x - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \Vert x^+ - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \beta \Vert x - \Pi_{X^+}x\Vert
                    \\
                    \iff 
                    0 &\le \Vert x - x^+\Vert - (1 - \beta) \Vert x - \Pi_{X^+}x\Vert. 
                \end{align}
                $x^+$ has descent lemma hence we have 
                \begin{align*}
                    f^+ - f(X) \le f(x^+) - f(x) 
                    \le - \frac{L_f}{2}\Vert x^+ - x\Vert^2 
                    \le - \frac{L_f}{2}(1 - \beta)^2 \Vert x - \Pi_{X^+}\Vert^2. 
                \end{align*}
                Hence, it gives the quadratic growth condition. 
            \end{proof}
            \begin{remark}
                It's unclear where convexity is used. 
                However, it' still assumed in Necoara et al. paper. 
            \end{remark}
            Before we start, we will specialize Theorem \ref{thm:cnvx-pg-ineq} because it will be used in later proofs. 
            In Assumption \ref{ass:necoara-2019-settings}, it can be seemed as taking $F = f + g$ in Assumption \ref{ass:smooth-add-nonsmooth} with $g = \delta_{X}$. 
            This makes $\mu_g = 0$ and assuming $f$ is convex we have $\mu_f = 0$. 
            Let $\beta = L_f$, and $x^+ = \Pi_{X}(x - L_f^{-1}\nabla f(x))$, it has for all $z \in X$: 
            \begin{align}\label{ineq:proj-grad}
                \begin{split}
                    0 &\le 
                    f(z) - f(x^+) + \frac{L_f}{2}\Vert z - x\Vert^2
                    - \frac{L_f}{2}\Vert z - x^+\Vert^2
                    \\
                    &= 
                    f(z) - f(x^+) + L_f\langle z - x^+, x^+ - x\rangle
                    + \frac{L_f}{2}\Vert x - x^+\Vert^2. 
                \end{split}
            \end{align}
            Take note that when $z = x$ it has 
            \begin{align}\label{ineq:proj-grad2}
                0 &\le f(x) - f(x^+) - \frac{L_f}{2}\Vert x - x^+\Vert^2. 
            \end{align}
            \par
            The following theorems are about the relation between PEB and QFG.
            % ==================================================================
            % LEMMA | QFG AND GRADIENT MAPPING
            % ==================================================================
            \begin{lemma}[gradient mapping and quadratic function growth]\;\label{lemma:grad-map-qfg}\\
                Let $f$ satisfies Assumtion \ref{ass:necoara-2019-settings}. 
                Suppose that $f \in \mathbb F(L_f, \mu_f, X)$ so it satisfies the quadratic function growth condition. 
                For all $x \in \RR^n$, define $x^+ = \Pi_X(x - L^{-1}_f\nabla f(x))$, 
                definte projections onto the set of minimizers $x^+_\Pi = \Pi_{X^+} x^+, X_\Pi = \Pi_{X^+}x$, then
                \begin{align*}
                    \left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x^+ - x_\Pi^+\Vert
                    &\le \Vert L_f(x - x^+)\Vert. 
                \end{align*}
            \end{lemma}
            \begin{proof}
                Using convexity, consider \eqref{ineq:proj-grad} with $z = x^+_\Pi$ it yields: 
                {\small
                \begin{align*}
                    0 &\ge 
                    f(x^+) - f(x^+_\Pi) - L_f\langle x_\Pi^+ - x^+, x^+ - x\rangle
                    - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert^2
                    \\
                    &\ge
                    \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \Vert L_f(x - x^+)\Vert\Vert x^+_\Pi - x^+\Vert
                    - \frac{1}{2L_f}\Vert L_f(x - x^+)\Vert^2 
                    \\
                    &= \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert^2
                        + L_f\Vert L_f(x - x^+)\Vert\Vert x_\Pi^+ - x^+\Vert
                    \right)
                    \\
                    &= 
                    \frac{\kappa_f + L_f}{2}\Vert x^+ - x^+_\Pi\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert + L_f\Vert x - x_\Pi^+\Vert
                    \right)^2.
                \end{align*}
                }
                From the last line, it's can be equivalently expressed as:
                \begin{align*}
                    0 &\le
                    \Vert L_f(x - x^+)\Vert + L_f\Vert x^+ - x_\Pi^+\Vert
                    - \sqrt{L_f(\kappa_f + L_f)}\Vert x^+ - x^+_\Pi\Vert
                    \\
                    &=
                    \Vert L_f(x - x^+)\Vert
                    - \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\Vert x^+ - x^+_\Pi\Vert.
                \end{align*}
            \end{proof}
            % ==================================================================
            % THEOREM | EQUIVALENCE BETWEEN QFG AND PEB 
            % ==================================================================
            \begin{theorem}[equivalence between QFG and PEB]\label{thm:qfg-peb-equiv}
                If $f$ is convex and satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then we have: 
                \begin{align*}
                    \mathbb E(L_f, \kappa_f, X) &\subseteq \mathbb F(L_f, \kappa^2_f/L_f, X), 
                    \\
                    \mathbb F(L_f, \kappa_f) 
                    &\subseteq 
                    \mathbb E\left(
                        L_f,
                        \frac{\kappa_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}, 
                        X
                    \right). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                For any $x \in X$, define the gradient projection steps by $x^+ = \Pi_{X}(x - L^{-1}_f\nabla f(x))$. 
                Denote $x^+_\Pi = \Pi_{X^+}x^+$. 
                Let $x_\Pi = \Pi_{X^+}x$, using the property of projection onto $X$ we have 
                \begin{align}\label{ineq:thm:qfg-peb-equiv-proof-item1}
                    \Vert x - x_\Pi\Vert &\le \Vert x - x_\Pi^+\Vert
                    \le \Vert x - x^+\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \notag\\
                    &= \frac{1}{L_f}\Vert L_f(x - x^+)\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \notag\\
                    \iff 
                    \Vert x^+ - x^+_\Pi\Vert &\ge
                    \Vert x - x_\Pi\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert. 
                \end{align}
                Before we start, we list intermediate results and conditions which are going to be used in the proof that follows for the ease of referencing. 
                \begin{enumerate}
                    \item The inequality \eqref{ineq:thm:qfg-peb-equiv-proof-item1}. It uses the property of projection onto a set hence convexity of $X^+$ is not needed. 
                \end{enumerate}
                Starting with Lemma \ref{lemma:grad-map-qfg} because $f$ satisfies quadratic growth and it is assumed convex, then it has: 
                {\small
                \begin{align*}
                    0 &\le 
                    \Vert L_f(x - x^+)\Vert
                    - \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\Vert x^+ - x^+_\Pi\Vert
                    \\
                    &\underset{\text{(i)}}{\le}
                    \Vert L_f(x - x^+)\Vert
                    -
                    \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\left(
                        \Vert x - \bar x\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert
                    \right)
                    \\
                    &=
                    - \left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \left(
                        L^{-1}_f\left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right) + 1
                    \right)\Vert L_f(x - x^+)\Vert
                    \\
                    &= 
                    -\left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \sqrt{L_f(\kappa_f + L_f)}
                    \Vert L_f(x - x^+)\Vert
                    \\
                    \iff&
                    \frac{\sqrt{L_f(\kappa_f + L_f)} - L_f}{\sqrt{L_f(\kappa_f + L_f)}}
                    \Vert x - \bar x\Vert 
                    \le
                    \Vert \mathcal G_{L_f}x\Vert. 
                \end{align*}
                }
                Skipping some algebra, the fraction simplifies to 
                \begin{align*}
                    \frac{\kappa_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}. 
                \end{align*}
                This gives PEB condition. 
                \textbf{We now show PEB implies QFG}. 
                From the error bound condition using $\kappa_f$ it has
                \begin{align*}
                    \kappa_f^2\Vert x - \bar x\Vert^2
                    \le \Vert \mathcal G_{L_f}(x)\Vert^2
                    \underset{\eqref{ineq:proj-grad2}}{\le }
                    2L_f(f(x) - f(x^+)) \le 2L_f(f(x) - f^+). 
                \end{align*}
            \end{proof}
            \par
            The following theorem summarizes the hierarchy of the conditions listed in Definition \ref{def:necoara-weaker-scnvx}. 
            \begin{theorem}[Hierarchy of weaker S-CNVX conditions]\label{thm:q-cnvx-hierarchy}
                Let $f$ satisfy Assumption \ref{ass:necoara-2019-settings}, assuming convexity then the following relations are true: 
                \begin{align*}
                    \mathbb S(\kappa_f, L_f, X) 
                    \subseteq \mathbb S'(\kappa_f, L_f, X)
                    \subseteq \mathbb G(\kappa_f, L_f, X) 
                    \subseteq \mathbb U(\kappa_f, L_f, X) 
                    \subseteq \mathbb F(\kappa_f, L_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                $\mathbb S' \subseteq \mathbb G$ is proved in Theorem \ref{thm:qscnvx-implies-qgg} and $\mathbb G \subseteq \mathbb U$ is proved in \ref{thm:qgg-implies-qua}. 
                $\mathbb S\subseteq \mathbb S'$ is obvious and it remains to show $\mathbb U \subseteq \mathbb F$. 
                Let $f\in \mathbb U(\kappa_f, L_f, X)$, it has for all $x \in X$: 
                \begin{align*}
                    0 &\le f(x) - f^+ - \langle \nabla f(\bar x), x - \bar x\rangle - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2
                    \\
                    &\hspace{-0.5em}\underset{\eqref{ineq:pg-opt-cond}}{\le} 
                    f(x) - f^+ - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                It's Theorem 4 in Necoara et al. \cite{necoara_linear_2019}.
            \end{remark}
        \subsection{Hoffman error bound and Q-SCNVX}

        \subsection{Feasible descent and accelerated feasible descent}
            This section summarizes results from Necoara et al. on the method of feasible descent, fast feasible descent, and fast feasible descent with restart. 
            \begin{definition}[projected gradient algorithm]\label{def:projg-alg}\;\\
                The projected gradient algorithm generates a sequence of iterates $(x_k)_{k \ge 0}$ such that they satisfy for all $k \ge 0$
                \begin{align*}
                    x_{k + 1} &= \Pi_X(x_k - \alpha_k \nabla f(x_k)), 
                \end{align*}
                Where $\alpha_k \ge L_f^{-1}$ for all $k \ge 1$. 
            \end{definition}
            Under Assumption \ref{ass:necoara-2019-settings}, convexity of $X$ means obtuse angle theorem from projection, and it specializes to 
            \begin{align}\label{ineq:projg-variational-ineq}
                (\forall x \in X)\; \langle x_{k + 1} - (x_k + \alpha_k \nabla f(x_k)), x_{k + 1} - x\rangle \le 0. 
            \end{align}

            \begin{theorem}{feasible descent linear convergence under Q-SCNVX}
                Under Assumption \ref{ass:necoara-2019-settings}, assume that $f$ is Q-CNVX with $\mu_f, L_f$, then the sequence that satisfies Definition \ref{def:projg-alg} has a linear convergence rate. 
                Let $\bar x_k = \Pi_{X^+}x_k, \bar x_0 = \Pi_{X^+} x_0$. 
                For all $k \ge 1$, the iterates satisfy
                \begin{align*}
                    \Vert x_k - \bar x_k\Vert^2 &\le \left(
                        \frac{1 - \kappa_f/L_f}{1 + \kappa_f/L_f}
                    \right)^k \Vert x_0 - \bar x_0\Vert^2. 
                \end{align*}
            \end{theorem}
            \begin{proof}
                Our proof makes use of the following properties which we label it in advance for swift exposition: 
                \begin{enumerate}
                    \item Inequality \eqref{ineq:projg-variational-ineq}, from the projected gradient and convexity of $X$. 
                    \item $f \in \mathbb S'$ which is the hypothesis that $f$ is Q-CNVX. 
                    \item $\alpha_k \le L_f^{-1}$, the stepsize is sufficient to apply descent lemma globally. 
                    \item $f \in \mathbb Q$ satisfying Q-Growth, a consequence of Q-CNVX by Theorem \ref{thm:q-cnvx-hierarchy}. 
                \end{enumerate}
                With $\overline{(\cdot)} = \Pi_{X^+}(\cdot)$ to denote the projection of a vector to the set of minimizers. 
                The sequence of inequalities and equalities proves the theorem. 
                {\allowdisplaybreaks
                \begin{align*}
                    \Vert x_{k + 1} - \bar x_k\Vert^2
                    &= 
                    \Vert x_{k + 1} - x_k + x_k - \bar x_k\Vert^2 
                    = \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2 + 2\langle x_{k + 1} - x_k, x_k - \bar x_k\rangle
                    \\
                    &= (- \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2)
                    + 2\Vert x_{k + 1} - x_k\Vert^2 + 2\langle x_{k + 1} - x_k, x_k - \bar x_k\rangle
                    \\
                    &= - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    + 2 \langle x_{k + 1} - x_{k}, x_{k + 1} - \bar x_k\rangle
                    \\
                    &= 
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    \\  & \quad \;
                        + 2 \langle x_{k + 1} - x_{k} + \alpha_k \nabla f(x_k), x_{k + 1} - \bar x_k\rangle
                        - 2\alpha_k \langle \nabla f(x_k), x_{k + 1} - \bar x_k\rangle
                    \\
                    &\underset{\text{(i)}}{\le}
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    - 2\alpha_k \langle \nabla f(x_k), x_{k + 1} - \bar x_k\rangle
                    \\
                    &= 
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    + 2\alpha_k \langle \nabla f(x_k), \bar x_k - x_k\rangle
                    + 2\alpha_k \langle \nabla f(x_k), x_k - x_{k + 1}\rangle
                    \\
                    &\underset{\text{(ii)}}{\le}
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    \\ &\quad 
                        + 2\alpha_k \left(
                            f^+ - f(x_k) - \frac{\kappa_f}{2}\Vert x_k - \bar x_k\Vert^2
                        \right)
                        + 2\alpha_k \langle \nabla f(x_k), x_k - x_{k + 1}\rangle
                    \\
                    &= (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2
                    \\&\quad 
                        + 2\alpha_k(f^+ - f(x_k)) - 2\alpha_k 
                        \left(
                            \langle \nabla f(x_k), x_{k + 1} - x_k\rangle + \frac{1}{2\alpha_k}\Vert x_{k + 1} - x_k\Vert^2
                        \right)
                    \\
                    &= 
                    (1 - \alpha_k\kappa_f)\Vert x_k - \bar x_k\Vert^2 + 2 \alpha_k f^+
                    \\&\quad 
                        - 2 \alpha_k\left(
                            f(x_k) + \langle \nabla f(x_k), x_{k + 1} - x_k\rangle 
                            + \frac{1}{2\alpha_k}\Vert x_{k + 1} - x_k\Vert^2
                        \right)
                    \\
                    &\underset{\text{(iii)}}{\le} 
                    (1 - \alpha_k\kappa_f)\Vert x_k - \bar x_k\Vert^2 + 2 \alpha_k f^+
                    \\ &\quad 
                        - 2 \alpha_k\left(
                            f(x_k) + \langle \nabla f(x_k), x_{k + 1} - x_k\rangle 
                            + \frac{L_f}{2}\Vert x_{k + 1} - x_k\Vert^2
                        \right)
                    \\
                    &\le 
                    (1 - \alpha_k\kappa_f)\Vert x_k - \bar x_k\Vert^2 + 2 \alpha_k f^+
                    - 2\alpha_kf(x_{k + 1})
                    \\
                    &\underset{\text{(iv)}}{\le} 
                    (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2 - \alpha_k \kappa_k \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2. 
                \end{align*}
                }
                Therefore, it has 
                \begin{align*}
                    0 &\le \Vert x_{k + 1} - \bar x_k\Vert^2 - \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2
                    \\
                    &\le 
                    (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2 
                    - \alpha_k \kappa_k \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2
                    - \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2
                    \\
                    &= (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2 
                    - (1 + \alpha_k \kappa_k)\Vert x_{k + 1} - \bar x_{k + 1}\Vert^2. 
                \end{align*}
                Unrolling recursively, then use (iii), the claim is proved. 
            \end{proof}

        \subsection{Application, KKT of linear programming}
            This section extends and ideas in the discussion section of Necoara et al. \cite{necoara_linear_2019}. 
            \par
            Let $X_1, X_2, Y$ be Hilbert spaces. 
            Define linear mapping $E:X_1 \times X_2 \rightarrow Y := (x_1, x_2)\mapsto E_1 x_1 + E_2 x_2$ where $E_1, E_2$ each are mappings of $X_1 \rightarrow Y, X_2 \rightarrow Y$. 
            Denote the adjoint of linear mapping by $(\cdot)^*$. 
            Let $c = (c_1, c_2) \in X_1 \times X_2$, $b \in Y$. 
            Suppose that $\mathcal K \subseteq X_1$ is a simple cone and $K^*$ is its dual cone. 
            We consider the following linear programming problem 
            \begin{align}\label{problem:lp-cannon-form}
                \inf_{x \in X_1\times X_2}\left\lbrace
                    \langle - c, x\rangle
                    \left| \;
                        Ex = b, x \in \mathcal K \times X_2
                    \right.
                \right\rbrace. 
            \end{align}
            Define linear mapping $g, F$ and indicator function $h$ by the following: 
            \begin{align*}
                g:X_1\times X_2 \rightarrow \RR 
                    &:= x \mapsto \langle - c, x\rangle, 
                \\
                F: X_1\times X_2 \rightarrow Y \times X_1 
                    &:= (x_1, x_2) \mapsto (E_1x_1 + E_2 x_2, x_1),
                \\
                h: Y \times X_1 \rightarrow \overline \RR &:= 
                    (y, z) \mapsto \delta_{\{\mathbf 0\}}(y - b) + \delta_{\mathcal K}(z). 
            \end{align*}
            It's not hard to identify that problem in \eqref{problem:lp-cannon-form} has representations 
            \begin{align*}
                \inf_{x \in X_1\times X_2}
                \left\lbrace
                    g(x) + h(Fx)
                \right\rbrace. 
            \end{align*}
            The dual problem of the above is given by
            \begin{align*}
                -\inf_{u \in Y\times X_1}
                \left\lbrace
                    h^\star(u) + g^\star(-F^* u)
                \right\rbrace. 
            \end{align*}
            Where $h^\star, g^\star$ are the conjugate of $h, g$ and $F^*: Y\times X_1 \rightarrow X_1 \times X_2 = (y, z)\mapsto (E_1^*y + z, E_2^*y)$ is the adjoint operator of $F$. 
            Note that $g^\star(x) = \delta_{\mathbf 0}(x + c)$ and $h^\star((y, z)) = \langle b, y\rangle + \delta_{\mathcal K^*}(z)$. 
            This gives the following dual problem 
            \begin{align*}
                - \inf_{(y, z) \in Y \times \mathcal K^*} \left\lbrace
                    \langle b, y\rangle 
                    \left | \;
                        E_1^*y + z = c_1, 
                        E^*_2y = c_2
                    \right.
                \right\rbrace. 
            \end{align*}
            The KKT conditions give the following convex feasibility problem 
            \begin{align*}
                E_1 x_1 + E_2 x_2 &= b, \\
                E_1^* y + z &= c_1, \\
                E_2^* y &= c_2, \\
                \langle b, y\rangle &= \langle c_1, x_1\rangle + \langle c_2, x_2\rangle, \\
                (x_1, x_2) &\in \mathcal K \times X_2, \\
                (y, z) &\in Y \times \mathcal K^*.
            \end{align*}
            Allow $X_1 = \RR^{n_1}, X_2 = \RR^{n_2}, Y = \RR^m$. 
            Define 
            \begin{align*}
                \mathbf K &:= \mathcal K \times \RR^{n_2} \times \RR^m \times \mathcal K^*, 
                \\
                A &:= \begin{bmatrix}
                    E_1 & E_2 & \mathbf 0 & \mathbf 0
                    \\
                    \mathbf 0 &\mathbf 0  & E_1^T & I_{n_1}
                    \\
                    \mathbf 0 &\mathbf 0  & E_2^T & \mathbf 0
                    \\
                    c_1^T & c_2^T & - b^T & 0
                \end{bmatrix}, 
                v := 
                \begin{bmatrix}
                    x_1\\ x_2 \\ y \\ z \\
                \end{bmatrix} \in \mathbf K, 
                d := 
                \begin{bmatrix}
                    b \\ c_1 \\ c_2 \\ 0
                \end{bmatrix}. 
            \end{align*}
            The KKT conditions is a convex feasibility problem which can be formulated by best approximation problem: 
            \begin{align}\label{problem:lp-kkt-min}
                \min_{v \in \mathbf K} 
                \frac{1}{2}\Vert Ax - d \Vert^2. 
            \end{align}
            It is minimizing a quadratic problem on a simple cone. 
            Solving \eqref{problem:lp-cannon-form} can be approached by optimizing \eqref{problem:lp-kkt-min}. 
            It's necessary to investigate the matrices $A, A^T$ which are essential to solving it numerically. 
            The properties of $A^TA$ will determine the convergence rate of algorithms. 
            The matrix is a block matrix and possibly sparse in practice. 
            Let $v = (x_1, x_2, y, z)$, it admits implicit representation: 
            \begin{align*}
                Av = (E_1x_1 + E_2 x_2,\; E_1^Ty + z,\; E_2^Ty,\; c^T_1x_1 + c_2^Tx_2 - b^Ty). 
            \end{align*}
            It involves 
            \begin{enumerate}
                \item Two multiplications of $E$: $x_1, x_2$ on the right and $y$ on the right,  
                \item inner product using $x_1, x_2$ and $y$. 
            \end{enumerate}
            
            Let $\bar v = (\bar y, \bar x_1, \bar x_2, \xi) \in \RR^m\times \RR^{n_1} \times \RR^{n_2}\times \RR$ then the right multiplication of has: 
            \begin{align*}
                \bar v^TA  &= (
                    E_1^T\bar y + \xi c_1^T,\; E_2^T\bar y + \xi c_2^T,\; 
                    \bar x_1^TE_1^T + \bar x_2^TE_2^T - \xi b^T,\; \bar x_1^T
                )
                \\
                &= 
                (
                    E_1^T\bar y + \xi c_1, \;
                    E_2^T \bar y + \xi c_2, \;
                    E_1\bar x_1 + E_2\bar x_2 - \xi b, \;
                    \bar x_1
                )^T. 
            \end{align*}
            \begin{enumerate}
                \item Two multiplications of $E$: $\bar y$ on the left and for $\bar x_1, \bar x_2$ on the right, 
                \item one vector addition with $c = (c_1, c_2)$ and $b$. 
            \end{enumerate}
            Therefore, computing $A^TAv$ has four vector multiplications using $E$. 
            In practice, a sparse matrix $E$ from the model can speed up computations. 
            \par
            Another key operation would be $A^TAv$. 
            Let $\bar v = Av$, then 
            \begin{align*}
                A^TAv &= 
                \begin{bmatrix}
                    E^T_1(E_1x_1 + E_2x_2) + (c_1^Tx_1 + c_2^Tx_2 - b^Ty)c_1
                    \\
                    E^T_2(E_1x_1 + E_2x_2) + (c_1^Tx_1 + c_2^Tx_2 - b^Ty)c_2
                    \\
                    E_1(E_1^Ty + z) + E_2E_2^Ty - (c_1^Tx_1 + c_2^Tx_2 - b^Ty)b
                    \\
                    E_1^Ty + z
                \end{bmatrix}
                \\
                &= 
                \begin{bmatrix}
                    (E_1^TE_1 + c_1^T)x_1 + (E_1^TE_2 + c_2^T)x_2 - (c_1b^T)y
                    \\
                    (E_2^TE_1 + c_1^T)x_1 + (E_2^TE_2 + c_2^T)x_2 - (c_2b^T)y
                    \\
                    -(bc_1^T)x_1 - (bc_2^T)x_2 + (E_2E_2^T + E_1E_1^T + bb^T)y
                    + (E_1E_1^T)z
                    \\
                    E_1^Ty + z
                \end{bmatrix}
                \\
                &= 
                \begin{bmatrix}
                    E_1^TE_1 + c_1^T & E_1^TE_2 + c_2^T & -c_1b^T & \\
                    E_2^TE_1 + c_1^T & E_2^TE_2 + c_2^T & -c_2b^T & \\
                    -bc_1^T& -bc_2^T & E_2E_2^T + E_1E_1^T + bb^T & E_1E_1^T \\
                    & & & E_1^Ty + z\\
                \end{bmatrix}
                \begin{bmatrix}
                    x_1 \\ x_2 \\ y \\ z
                \end{bmatrix}. 
            \end{align*}
            In practice, implicitly representing the process of $A^TAv$ is better in computing software. 
            Here we write it out to view, for theoretical interests. 
            \par
            Let $f(v) = (1/2)\Vert Av - d\Vert^2$ to be the objective function of optimization problem \eqref{problem:lp-kkt-min}. 
            Its gradient, objective value, and Bregman Divergence have: 
            \begin{align*}
                \nabla f(v) &= A^TAv - A^Td, 
                \\
                f(v) &= 
                \frac{1}{2}\langle v, \nabla f(v) - A^Td\rangle + \frac{1}{2}\Vert d\Vert^2, 
                \\
                D_f(u, v) &= (1/2)\langle u - v, A^TA (u - v)\rangle
                \\
                &= (1/2)\langle \nabla f(u) - \nabla f(v), u - v\rangle. 
            \end{align*}
            The value $\nabla f(v), f(v)$ when evaluated together, require minimal additional computations. 
            This fact is favorable for implementations in practice. 
            Furthermore, the difference of the function value between 2 points $v, u$ admits an interesting relation via the Bregman Divergence. 
            Observe that $\forall u, v \in \RR^n$ it has 
            \begin{align*}
                f(u) - f(v) &= \langle \nabla f(v), u - v \rangle + D_f(u, v)
                \\
                &= \langle \nabla f(v), u - v \rangle + (1/2)\langle \nabla f(u) - \nabla f(v), u - v\rangle
                \\
                &= (1/2)\langle \nabla f(u) + \nabla f(v), u - v\rangle. 
            \end{align*}
            For this problem, the computation overhead for $f(u) - f(v), D_f(u, v)$ is very little if $\nabla f(u), \nabla f(v)$ is known. 

\chapter{Advanced Enhancement Techniques in Accelerated Proximal Gradient}
    We review advanced enhancement techniques in Accelerated Proximal Gradient method. 
    The review will be based on several papers. 
    \par
    There are several notable enhancements of the FISTA for function that are not strongly convex. 
    Monotone variants of FISTA proposed by Beck \todo{[?]} and Nesterov \todo{[?]} imposes monotonicity in function value at the iterates.  
    Backtracking strategies from Chambolle \todo{[?]} shows that the underestimating Lipschitz constant using a backtracking technique to choose a next iterate improves the average runtime of the algorithm in practice. 
    They showed that the convergence rate is bounded by the estimates of the Lipschitz constant. 
    Restart is a technique pioneer early by ??? \todo{[?]}. 
    Necoara et al. \cite{necoara_linear_2019} showed that there exists an optimal restarting interval to achieve fast line convergence rate for all functions with quadratic growth condition. 
    \par
    In this chapter, we will go through the details of these enhancements of FISTA and discuss why they are important in theories, and in practice. 
    \section{FISTA made simple}
        Most literatures overcomplicate the proofs and the matters regarding FISTA algorithm and its convergence rate. 
        We showcase the theories using a generic similar triangle representations of the algorithm which tremendously simplifies the arguments. 
        \begin{assumption}[the standard FISTA setting]\label{ass:standard-fista}
            Let $F = f + g$ where $f:\RR^n\rightarrow \overline \RR$ is $L$ Lipschitz smooth, $g$ is convex. 
            Suppose that $\argmin_{x \in \RR^n} F(x)\neq \emptyset$. 
        \end{assumption}
        Specializing Theorem \ref{thm:cnvx-pg-ineq}, we have the following lemma: 
        \begin{lemma}[proximal gradient inequality]
            If $F = f + g$ satisfies Assumption \ref{ass:standard-fista}, then for all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{L^{-1}, f, g}(x)$ it has 
            \begin{align*}
                0 &\le F(z) - F(\bar x) + \frac{L}{2}\Vert z - x\Vert^2 - \frac{L}{2}\Vert^2 z - \bar x\Vert^2. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Use Theorem \ref{thm:cnvx-pg-ineq}. 
        \end{proof}
        \begin{definition}[gradient mapping]
            Suppose $F = f + g$ satisfies Assumption \ref{ass:standard-fista}, define the gradient mapping for all $x \in \RR^n$
            \begin{align*}
                \mathcal G_{\beta^{-1}, f, g}(x) = \beta(x - T_{\beta^{-1}, f, g}(x)). 
            \end{align*}
        \end{definition}
        If $f, g$ are clear in the context then we simply omit subscript and present $\mathcal G_\beta$. 
        The following definition can capture monotone variants of FISTA with line search, including backtracking strategies. 
        \par
        ``MAPG'' stands for monotone accelerated gradient. 
        We refer to ``Generic Monotone Accelerated Proximal Gradient with line search" as ``GMAPG LS''. 
        \begin{definition}[GMAPG LS]\label{def:generic-mapg-ls}\;\\ 
            Initialize any $x_0, v_0 \in \RR^n, \alpha_0 = 1, L_0 \in \RR$ such that
            \begin{align*}
                D_f\left(T_{L_0^{-1}} (y_0)\right) 
                &\le \frac{L_0}{2}\left\Vert T_{L_0^{-1}}(x_0) - y_0\right\Vert^2. 
            \end{align*}
            Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_k \in (0, 1) \;\forall k \ge 0$ and $\alpha_0 \in (0, 1]$. 
            \begin{tcolorbox}
                The algorithm makes sequences $(x_k, v_k, y_k)_{k \ge 1}$, such that for all $k = 1, 2, \ldots$ they satisfy: 
                \begin{align*}
                    & y_k = \alpha_k v_{k - 1} + (1 - \alpha_k) x_{k - 1}, \\
                    & \tilde x_k = T_{L_k^{-1}}(y_{k}), \\ 
                    & v_k = x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1}), \\
                    & D_{f}(\tilde x_k, y_k) \le \frac{L_k}{2}\Vert \tilde x_k - y_k\Vert^2, \\
                    & \text{Choose any } x_k: F(x_k) \le \min(F(\tilde x_k), F(x_{k - 1})). 
                \end{align*}    
            \end{tcolorbox}
        \end{definition}
        \begin{lemma}[acceerated proximal gradient iterates relation]\;\label{lemma:apg-iterates}\;\\
            The iterates $(x_k, v_k, y_k)_{k \ge 1}$ generated by Definition \ref{def:generic-mapg-ls}. 
            Let $z_k = \alpha_k x^+ + (1 - \alpha_k)x_{k - 1}$. 
            Then it has for all $k \ge 1$ that: 
            \begin{align*}
                z_k - \tilde x_k &= \alpha_k(x^+ - v_k)
                \\
                x_k - y_k &= \alpha_k(x^+ - v_{k - 1}). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            It's direct from the algorithm. 
            \begin{align*}
                z_k - \tilde x_k &= (\alpha_k x^+ + (1 - \alpha_k)x_{k - 1}) - \tilde x_k
                \\
                &= \alpha_k (x^+ + \alpha_k^{-1}(1 - \alpha_k)x_{k - 1} - \alpha_k^{-1}\tilde x_k)
                \\
                &= \alpha_k(x^+ + \alpha_k^{-1}x_{k - 1} - x_{k - 1} - \alpha_k^{-1}\tilde x_k)
                \\
                &= \alpha_k(x^+ + \alpha_k^{-1}(x_{k - 1} - \tilde x_k) - x_{k - 1})
                \\
                &= \alpha_k(x^+ - v_{k}), 
                \\
                z_k - y_k &= 
                (\alpha_k x^+ + (1 - \alpha_k)x_{k - 1}) - \left(
                    \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                \right)
                \\
                &= \alpha_k(x^+ + \alpha_k^{-1}(1 - \alpha_k)x_{k - 1} - v_{k - 1} - \alpha_k^{-1}(1 - \alpha_k)x_{k - 1})
                \\
                &= \alpha_k(x^+ - v_{k - 1}). 
            \end{align*}
        \end{proof}
        % ======================================================================
        % THEOREM | GENERIC GMAPG LS CONVERGENCE IN FXN VAL
        % ======================================================================
        \begin{theorem}[generic GMAPG LS convergence]\; \label{thm:gmapg-ls-convergence}\;\\
            Let $F = f + g$ satisfy Assumptions \ref{ass:standard-fista}. 
            Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_k \in (0, 1)$ for all $k \ge 1$ and $\alpha_0 \in (0, 1]$. 
            Let $\rho_k = (1 - \alpha_{k + 1})^{-1}\alpha_{k + 1}^2 \alpha_k^{-2}$ for all $k \ge 0$. 
            Then, for all $x^+ \in \RR^n, k \ge 1$, the convergence rate of GMAPG-LS (Definition \ref{def:generic-mapg-ls}) is given by: 
            \begin{align*}
                & \beta_k := \prod_{i = 0}^{k - 1} (1 - \alpha_{i + 1})
                \max\left(1, \rho_i L_{i + 1}L_i^{-1}\right), 
                \\
                & F(x_k) - F(x^+) + \frac{L_k\alpha_k}{2}\Vert x^+ - v_k\Vert^2
                \le 
                \beta_k
                \left(
                    F(x_0) - F(x^+) + \frac{L_0\alpha_0}{2} \Vert x^+ - v_k\Vert^2
                \right). 
            \end{align*}
            If in addition, the algorithm is initialized with $\alpha_0 = 1, x_0 = v_0 = T_{L_{0}}x_{-1} \in \text{dom}\; F$ and $x^+$ is a minimizer of $F$, then the convergence rate simplifies: 
            \begin{align*}
                F(x_k) - F(x^+) + \frac{L_k\alpha_k}{2}\Vert x^+ - v_k\Vert^2
                & \le 
                \left(
                    \prod_{i = 0}^{k - 1} (1 - \alpha_{i + 1})
                    \max\left(1, \rho_i L_{i + 1}L_i^{-1}\right)
                \right)
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Define $z_k = \alpha_k x^+ + (1 - \alpha_k)x_{k - 1}$ for all $k \ge 1$. 
            In the proof follows, the follow facts will be used. 
            We list them in advance, and they will be labeled during the proof. 
            \begin{enumerate}
                \item Lemma \ref{lemma:apg-iterates}. 
                \item The sequence $(\alpha_k)_{k \ge 1}$ has for all $k \ge 1$, $1 - \alpha_k = \alpha_k^2\alpha_{k - 1}^2\rho_{k - 1}$, $\alpha_k \in (0, 1)$. 
                \item $F$ is convex and hence $F(z_k) \le \alpha_k F(x^+) + (1 - \alpha_k)F(x_{k - 1})$. 
                \item $F(x_k) \le F(\tilde x_k)$ which is true by definition of GMAPG LS. 
            \end{enumerate}
            Now, using Theorem \ref{thm:cnvx-pg-ineq}, it has for all $k \in \N$: 
            {\allowdisplaybreaks\small
            \begin{align*}
                0 &\le 
                F(z_k) 
                - F(\tilde x_k) - \frac{L_k}{2}\Vert z_k - \tilde x_k\Vert^2 + 
                \frac{L_k}{2}\Vert z_k - y_k\Vert^2
                \\
                &\underset{\text{(i)}}{=}
                F(\alpha_k x^+ + (1 - \alpha_k)x_{k - 1}) - F(\tilde x_k)
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k \Vert^2 
                + \frac{L_k\alpha_k^2}{2}\Vert(x^+ - v_{k - 1})\Vert^2
                \\
                &\underset{\text{(iii)}}{\le} 
                \alpha_k F(x^+) + (1 - \alpha_k) F(x_{k - 1}) - F(\tilde x_k)
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k \Vert^2 
                + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1} \Vert^2
                \\
                &= 
                (\alpha_k - 1)F(x^+) + (1 - \alpha_k) F(x_{k - 1}) + F(x^+) - F(\tilde x_k)
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k \Vert^2 
                + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1}\Vert^2
                \\
                &= 
                (1 - \alpha_k)(F(x_{k - 1}) - F(x^+)) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1}\Vert^2
                - \left(
                    F(\tilde x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \right)
                \\
                &\underset{\text{(iv)}}{\le} 
                (1 - \alpha_k)(F(x_{k - 1}) - F(x^+)) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1}\Vert^2
                - \left(
                    F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \right)
                \\
                &= 
                (1 - \alpha_k)(F(x_{k - 1}) - F(x^+)) + 
                \left(
                    \frac{\alpha_k^2}{\alpha_{k - 1}^2\rho_{k - 1}}
                \right)
                \frac{L_{k - 1}\alpha_{k - 1}^2(\rho_{k - 1}L_kL_{k - 1}^{-1})}{2}\Vert x^+ - v_{k-1}\Vert^2 \\
                    &\quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right)
                \\
                &= 
                \left(
                    1 - \alpha_k
                \right)\left(
                    F(x_{k - 1}) - F(x^+) + \frac{L_{k - 1}\alpha_{k - 1}^2(\rho_{k - 1}L_kL_{k - 1}^{-1})}{2}
                    \Vert x^+ - v_{k - 1}\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right)
                \\
                &\le 
                \left(
                    1 - \alpha_k
                \right)\left(
                    F(x_{k - 1}) - F(x^+) + \frac{L_{k - 1}\alpha_{k - 1}^2\max(1, \rho_{k - 1}L_kL_{k - 1}^{-1})}{2}
                    \Vert x^+ - v_{k - 1}\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right)
                \\
                &\le 
                \left(
                    1 - \alpha_k
                \right)\max(1, \rho_{k - 1}L_kL_{k - 1}^{-1})
                \left(
                    F(x_{k - 1}) - F(x^+) + \frac{L_{k - 1}\alpha_{k - 1}^2}{2}
                    \Vert x^+ - v_{k - 1}\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right). 
            \end{align*}
            }
            Unroll recursively for $k, k-1, \ldots, 0$, it implies: 
            \begin{align*}
                0
                &\le 
                \left(
                    \prod^{k - 1}_{i = 0} (1 - \alpha_{i + 1})\max(1, \rho_{i}L_{i + 1}L^{-1}_i)
                \right)\left(
                    F(x_0) - F(x^+) + \frac{L_0 \alpha_0}{2}\Vert x^+ - v_0\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right). 
            \end{align*}
            If in addition, we assume that $x^+$ is a minimizer of $F$, and $\alpha_0 = 1, x_0 = v_0 = T_{L_0}x_{-1}$. 
            Using Theorem \ref{thm:cnvx-pg-ineq} it gives: 
            \begin{align*}
                0 &\le 
                F(x^+) - F(T_{L_{-1}}x_{-1}) - \frac{L_0}{2}\Vert x^+ - T_{L_0}x_{-1}\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2
                \\
                &= 
                F(x^+) - F(x_0) - \frac{L_0}{2}\Vert x^+ - v_0\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2. 
            \end{align*}
            Substituting it back to the previous inequality it yields the desired results. 
        \end{proof}
        % ======================================================================
        % THEOREM | GENERIC GMAPG LS GRADIENT MAPPING CONVERGENCE
        % ======================================================================
        \begin{theorem}[generic GMAPG LS gradient mapping convergence]
            
        \end{theorem}
        We introduce two examples variants of line search method used to enhance the accelerated proximal gradient methods in the literatures to demonstrate Definition \ref{def:generic-mapg-ls} and Theorem \ref{thm:gmapg-ls-convergence}.  
    \section{Algorithmic description of GMAPG}
        There are several components to the GMAPG algorithm. 
        \begin{algorithm}[H]
            \begin{algorithmic}[1]\label{alg:armijo-ls}
                \STATE{\textbf{Function ArmijoLS}
                    $(f, g, x, v, L, \alpha, \_, \_)$
                }
                \caption{Armijo Line Search}
            \end{algorithmic}
        \end{algorithm}
        \begin{algorithm}[H]\label{alg:chambolle-btls}
            \begin{algorithmic}[1]
                \STATE{\noindent
                    \textbf{Function ChambBT}($f, g, x, v, L, \alpha, L_{\min}, \rho$)
                }
                \STATE{$L^+ := \max(L_{\min}, \rho L)$.}
                \FOR{$i = 1, 2, \ldots, 53$}
                    \STATE{$\alpha^+ := (1/2)\left(\alpha\sqrt{\alpha^2 + L/L^+} - \alpha^2\right)$. }
                    \STATE{$y^+ := \alpha^+ v + (1 - \alpha^+) x$. }
                    \STATE{$x^+ := T_{1/L^+, f, g}(y^+)$.}
                    \IF{$2D_f(x^+, y^+) \le \Vert x^+ - y^+\Vert^2$}
                        \STATE{\textbf{break}}
                    \ENDIF
                    \STATE{$L^+ := 2^iL^+$.}
                \ENDFOR
                \STATE{\textbf{Return: }$y^+, x^+, \alpha^+, L^+$}
                \caption{Chambolle's Backtracking}
            \end{algorithmic}
        \end{algorithm}
        \begin{algorithm}[H]\label{alg:beck-mono}
            \begin{algorithmic}[1]
                \STATE{\noindent
                    \textbf{Function BeckMono}($f, g, x, v, L, L_{\min}, \rho, \alpha$)
                }
                \caption{Beck's monotone routine}
            \end{algorithmic}
        \end{algorithm}
        \begin{algorithm}[H]\label{alg:nes-mono}
            \begin{algorithmic}[1]
                \STATE{\noindent
                    \textbf{Function NesMono}($f, g, x, v, L, L_{\min}, \rho, \alpha$)
                }
                \caption{Nesterov's monotone routine}
            \end{algorithmic}
        \end{algorithm}
        \begin{algorithm}[H]\label{alg:gmapg}
            \begin{algorithmic}[1]
                \STATE{\noindent
                    \textbf{Function GMAPG} (
                        $f, g, x_{-1}, L_0, r_{\min}, \text{Exit condition: }\mathbb E_\chi$
                    )
                }
            \end{algorithmic}
            \caption{GMAPG with Chambolle's backtracking}
        \end{algorithm}
    % ==========================================================================
    % SECTION | EXAMPLES OF GMAPG IN THE LITERATURE
    % ==========================================================================
    \section{Examples of GMAPG in the literature}
        \begin{example}[MFISTA with Armijo line search]\;\\\vspace{-1em}
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                    \STATE{\textbf{Input:} $x_{-1} \in \RR^n, L_0 \in \RR^n, f:\RR^n \rightarrow \RR, g: \mathbb \RR^n \rightarrow \overline \RR$} 
                    \STATE{
                        $x_0 := y_0, t_0 := 1$.
                    }
                    \FOR{$k = 0, 1, 2, \ldots$}
                        \STATE{$\tilde x_{k + 1} := T_{L_k^{-1}}(y_k)$.}
                        \IF{$D_f(\tilde x_{k + 1}, y_k) > L_k/2\Vert \tilde x_{k + 1} - y_k\Vert^2$}
                            \STATE{\noindent
                                $L_k := \argmin_{i = 1,2, \ldots} \left\{ i: 
                                    D_f(T_{2^{-i}L_k^{-1}}(y_k), y_k) 
                                    \le 2^{i-1}L_k\left\Vert 
                                        T_{2^{-i}L^{-1}}y_k - y_k
                                    \right\Vert^2
                                \right\}$.
                            }
                            \STATE{$\tilde x_{k + 1} := T_{L_0^{-1}}y_k$.}
                        \ENDIF
                        \STATE{\noindent
                            Choose $x_{k + 1} \in \{\tilde x_{k + 1}, x_k\}$ such that $F(x_{k + 1}) \le \min(F(x_k), F(\tilde x_{k + 1}))$. 
                        }
                        \STATE{\noindent
                            $t_{k + 1} := (1/2)\left(1 + \sqrt{1 + 4t_k^2}\right)$. 
                        }
                        \STATE{\noindent
                            $y_{k + 1} := x_{k + 1} + t_kt_{k + 1}^{-1}(\tilde x_{k + 1} - x_{k + 1})+ (t_k - 1)t_{k + 1}^{-1}(x_{k + 1} - x_k)$. 
                        }
                    \ENDFOR
                \end{algorithmic}
                \caption{MFISTA with Armijo Line Search}
                \label{alg:mfista-armijo}
            \end{algorithm}
            We now demonstrate that Algorithm \ref{alg:mfista-armijo} is a special case of Definition \ref{def:generic-mapg-ls}.
            Let's consider $y_{k + 1}$ produced the GMAPG LS. 
            If $x_k = x_{k - 1}$ then replacing all instance of $x_k$ by $x_{k - 1}$ it has: 
            \begin{align*}
                y_{k + 1} &= \alpha_{k + 1}(v_k) + (1 - \alpha_{k + 1})x_{k - 1}
                \\
                &= \alpha_{k + 1}(x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})) + (1 - \alpha_{k + 1})x_{k - 1}
                \\
                &= \alpha_{k + 1} x_{k - 1} + \alpha_{k + 1}\alpha_k^{-1}(\tilde x_k - x_{k - 1}) + (1 - \alpha_{k + 1}) x_{k - 1}
                \\
                &= x_{k - 1} + \alpha_{k + 1}\alpha_k^{-1}(\tilde x_k - x_{k - 1})
                % \\
                % &= \tilde x_k + (\alpha_{k + 1} \alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1}). 
            \end{align*}
            Similarly when $x_k = \tilde x_k$ it produces: 
            \begin{align*}
                y_{k + 1} &= 
                \alpha_{k + 1}v_k + (1 - \alpha_{k + 1})\tilde x_k
                \\
                &= 
                \alpha_{k + 1}(x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})) + (1 - \alpha_{k + 1})x_k
                \\
                &= 
                \alpha_{k + 1}\left(
                    (1 - \alpha_{k}^{-1})x_{k - 1} + (\alpha_k^{-1} - 1)\tilde x_k + \tilde x_k
                \right) + 
                (1 - \alpha_{k + 1})\tilde x_k
                \\
                &= 
                \alpha_{k + 1}\left(
                    (\alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1}) + \tilde x_k
                \right) + 
                (1 - \alpha_{k + 1})\tilde x_k. 
                \\
                &= \tilde x_k + \alpha_{k + 1}(\alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1}). 
            \end{align*}
            Let's denote $y'_{k}, x'_{k}, \tilde x_k'$ as the $y_k, x_k, \tilde x_k$ produced by Algorithm \ref{alg:mfista-armijo}.
            Observe that if $x_0'$ is not the minimizer then it has $\tilde x_1' = T_{L_0^{-1}}(y_0') = T_{L_0^{-1}}(x_0')$. 
            Then $F(\tilde x_1') < F(x_0')$ is true. 
            So $x_1' = \tilde x_1 = T_{L_0^{-1}}(x_0')$. 
            Since $t_0 = 1$, it has $y_1' =\tilde x_1' + (t_0 - 1)t_1^{-1}(\tilde x_1' - x_0')= \tilde x_1'$. 
            \par
            Summarize the above results compactly, it has for all $k \ge 0$
            \begin{align}\label{eqn:emp:result-item-1}
                y_{k + 1} = \begin{cases}
                    x_{k - 1} + \alpha_{k + 1}\alpha_k^{-1}(\tilde x_k - x_{k - 1})
                    & \text{if } x_k = x_{k - 1} \wedge k \ge 1,
                    \\
                    \tilde x_k + \alpha_{k + 1}(\alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1})
                    & \text{if } x_k = \tilde x_k \wedge k \ge 1,
                    \\
                    \alpha_1v_0 + (1 - \alpha_1)x_0 & \text{if } k = 0. 
                \end{cases}
            \end{align}
            Then it has for all $k \ge 0$: 
            \begin{align}\label{eqn:emp:result-item-2}
                y'_{k + 1} &= 
                \begin{cases}
                    x'_k + t_kt_{k + 1}^{-1}(\tilde x_{k + 1} - x_k) 
                    & \text{if } x_{k + 1}' = x_k' \wedge k \ge 1,
                    \\
                    x_{k + 1}' + (t_k - 1)t_{k + 1}^{-1}(\tilde x_{k + 1}' - x_k')  
                    & \text{if } x_{k + 1}' = \tilde x_{k + 1}'\wedge k \ge 1, 
                    \\
                    \tilde x_1'
                    & 
                    \text{if } k = 0. 
                \end{cases}
            \end{align}
            Let $x_{-1} \in \RR^n$. 
            If we choose $v_0 = x_0 = T_{L_0^{-1}} x_{-1}$, then $y_1 = \alpha_1 x_0 + (1 - \alpha_1)x_0 = x_0 = T_{L_0^{-1}}(x_{-1})$.
            Next, we make $\alpha_k^{-1} = t_k$, then \eqref{eqn:emp:result-item-1}, \eqref{eqn:emp:result-item-2} are equivalent. 
        \end{example}
        % ======================================================================
        % EXAMPLE | NESTEROV'S MONOTONE SCHEME WITH GENERIC LINE SEARCH
        % ======================================================================
        \begin{example}[Nesterov's monotone scheme with generic line search]\;\\\vspace{-1em}
            \begin{algorithm}\label{alg:nesterov-mono-generic-ls}
            \begin{algorithmic}[1]
            \STATE{\textbf{Input: } }
            \end{algorithmic}\caption{Nesterov's monotone scheme with generic line search}
            \end{algorithm}
        \end{example}

    \section{Practical enhancement}
        In this section, we provide results for the faster convergence rate of examples listed from the previous section. 
    \section{Nonconvex convergence}
        
    \section{Restarting with gradient and function values}
    
        


\section{Primal Dual Lagrangian for LP}
    


% ==============================================================================

\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}