\documentclass[12pt]{report}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}
\title{{\fontfamily{ptm}\selectfont Reading Notes}}

\author{
    Alto
    % \thanks{
    %     Subject type, Some Department of Some University, Location of the University,
    %     Country. E-mail: \texttt{author.name@university.edu}.
    % }
}

\date{Last Compiled: \today}

\maketitle

\begin{abstract} 
    Reports on papers read. 
    This is a LaTEX file for my own notes taking. 
    It may accelerate the process of writing my thesis for my PhD degree. 
    \todoinline{This paper is currently in draft mode. Check source to change options. }
\end{abstract}
\chapter{The Basics of Optimization Theories}
    Notations in this chapter are not shared, and they are for this chapter only. 
    % ===================================================================================
    % BREGMAN DIV DEFINITION 
    % ===================================================================================
    \begin{definition}[Bregman Divergence]\label{def:bregman-div}
        Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
        Define Bregman Divergence: 
        \begin{align*}
            D_f: \RR^n \times \dom \nabla f \rightarrow \overline \RR:= 
            (x, y) \mapsto f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
        \end{align*}
    \end{definition}
    \begin{assumption}[smooth plus nonsmooth]\label{ass:smooth-add-nonsmooth}
        Let $F = f+ g$ where $f:\RR^n \rightarrow \overline \RR$ is differentiable and there exists $q\in \RR$ such that $g - q/2\Vert \cdot\Vert^2$ is convex.
    \end{assumption}
    \begin{definition}[proximal gradient operator]
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth}. 
        Define the proximal gradient operator by: 
        \begin{align*}
            T_{\beta^{-1}, f, g}(x) &= \hprox_{\beta^{-1}g} \left(
                x - \beta^{-1} \nabla f(x)
            \right)
            \\
            &= \argmin_{z}\left\lbrace
                g(z) + f(x) + \langle \nabla f(x), z - x\rangle
                + \frac{\beta}{2}\Vert x - z\Vert^2
            \right\rbrace. 
        \end{align*}
    \end{definition}
    % =================================================
    % WEAKLY CONVEX GENERIC PROXIMAL GRADIENT INEQUALITY
    % ==================================================
    \begin{theorem}[weakly convex generic proximal gradient inequality]\label{thm:pg-ineq-wcnvx-generic}\;\\
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} with $\beta > 0$ and $q \in \RR$. 
        Then for all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$, it has: 
        \begin{align*}
            \frac{q}{2}\Vert z - x^+\Vert^2 
            &\le 
            F(z) - F(\bar x) - \langle \beta(x - \bar x), z - \bar x\rangle 
            + D_f(x, \bar x ) - D_f(z, x).  
        \end{align*}
    \end{theorem}
    \begin{proof}
        Nonsmooth analysis calculus rules has 
        \begin{align*}
            \bar x &\in \argmin{z} \left\lbrace
                g(z) + \langle \nabla f(x), z\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
            \right\rbrace
            \\
            \implies
            \mathbf 0 
            &\in \partial g(x^+) + \nabla f(x) + \beta(x^+ - x)
            \\
            \iff 
            \partial g(x^+) &\ni
            - \nabla f(x) - \beta(x^+ - x). 
        \end{align*}
        The subgradient inequality for weak convexity has 
        \begin{align*}
            \frac{q}{2}\Vert z - \bar x\Vert^2 
            &\le 
            g(z) - g(\bar x) + \langle \nabla f(x) + \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) + \langle \nabla f(x), z - \bar x\rangle + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= g(z) - g(\bar x) + \langle \nabla f(x), z - x\rangle
            + \langle \nabla f(x), x - \bar x\rangle
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) 
            + (-D_f(z, x) + f(z) - f(x))
            \\
            & \quad 
            + (D_f(\bar x, x) - f(\bar x) + f(x))
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= F(z) - F(\bar x) - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle. 
        \end{align*}
    \end{proof}
    \begin{theorem}[convex proximal gradient inequality]\label{theorem:pg-ineq}
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} such that $q = \mu_g \ge 0$, $\beta \ge L_f$. 
        In addition, suppose that $f:\RR^n\rightarrow \RR$ has $L_f$ Lipschitz continuous gradient, and it's $\mu_f \ge 0$ strongly convex. 
        For all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has 
        \begin{align*}
            0 &\le 
            F(z) - F(\bar x) + 
            \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2
            - \frac{\beta + \mu_g}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{theorem}
    \begin{proof}
        The Bregman Divergence of $f$ has inequality 
        \begin{align*}
            \left(\forall x \in \RR^n, y \in \RR^n\right)\; 
            \frac{\mu_f}{2}\Vert x - y\Vert^2 \le D_f(x, y) \le \frac{L_f}{2}\Vert x - y\Vert^2. 
        \end{align*}
        Specializing Theorem \ref{thm:pg-ineq-wcnvx-generic}, let $x \in \RR^n$ and define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has $\forall z \in \RR^n:$
        \begin{align*}
            \frac{\mu_g}{2}\Vert z - \bar x \Vert^2 
            &\le 
            F(z) - F(\bar x) 
            - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \frac{L_f}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x + x - \bar x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \left(
                \frac{L_f}{2} - \beta
            \right)\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}
            \left(
                \Vert x - \bar x\Vert^2
                + 2\langle x - \bar x, z - x\rangle
            \right)
            \\
            &= 
            F(z) - F(\bar x) 
            + \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{proof}
    
\chapter{Linear Convergence of First Order Method}
    In this chapter, we are specifically interested in characterizing linear convergence of well known first order optimization algorithms. 
    In this section, $D_f$ will denote the Bregman Divergence as defined in Definition \ref{def:bregman-div}. 
    \section{Necoara's et al's Paper}
        \subsection{The Settings}
            The assumption follows give the same setting as Necoara et al. \cite{necoara_linear_2019}. 
            % =======================
            % NECOARA's ASSUMPTIONS 
            % =======================
            \begin{assumption}\label{ass:necoara-2019-settings}
                Consider optimization problem: 
                \begin{align}
                    -\infty < f^+ = \min_{x \in X} f(x) . 
                \end{align}\label{problem:necoara-2019}
                $X\subseteq \RR^n$ is a closed convex set. 
                Assume projection onto $X$, denoted by $\Pi_X$ is easy. 
                Denote $X^+ = \argmin_{x \in X}f(x) \neq \emptyset$, assume it's a closed set. 
                Assume $f$ has $L_f$ Lipschitz continuous gradient, i.e: for all $x, y\in X$: 
                \begin{align*}
                    \Vert \nabla f(x) - \nabla f(y)\Vert \le L_f\Vert x - y\Vert. 
                \end{align*}
            \end{assumption}
            Some immediate consequences of Assumption \ref{ass:necoara-2019-settings} now follows. 
            The variational inequality characterizing optimal solution has: 
            \begin{align}\label{ineq:pg-opt-cond}
                x^+ \in X^+ \implies 
                (\forall x \in X)\; \langle \nabla f(x^+), x - x^+\rangle \ge 0. 
            \end{align}
            The converse is true if $f$ is convex. 
            The gradient mapping in this case is: 
            \begin{align*}
                \mathcal G_{L_f}x = L_f(x - \Pi_{X}x). 
            \end{align*}
            % ==============================
            % STRONG CONVEXITY DEFINITION
            % ==============================
            \begin{definition}[strong convexity]\label{def:necoara-scnvx}
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then $f \in \mathbb S(L_f, \kappa_f, X)$ is strongly convex iff 
                \begin{align*}
                    (\forall x, y\in X)\; 
                    \kappa_f \Vert x - y\Vert^2 \le 
                    D_f(x, y) \le L_f \Vert x - y\Vert^2. 
                \end{align*}
            \end{definition}
            Then it's not hard to imagine the following natural relaxation of the above conditions. 
            %===================================================================
            % DEFINITION OF WEAKER STRONG CONVEXITY 
            % ==================================================================
            \begin{definition}[relaxations of strong convexity]\;\\
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}.
                \label{def:necoara-weaker-scnvx}
                Let $L_f \ge \kappa_f \ge 0$ such that for all $x \in X$, $\bar x = \Pi_{X^+} x$. 
                We define the following: 
                \begin{enumerate}
                    \item\label{def:neocara-qscnvx} Quasi-strong convexity (Q-SCNVX): $0 \le D_f(\bar x, x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb S'(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qup} Quadratic under approximation (QUA): $0 \le D_f(x, \bar x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb U(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qgg} Quadratic Gradient Growth (QGG): $0\le D_f(x, \bar x) + D_f(\bar x, x) - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb G(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qfg} Quadratic Function Growth (QFG): $0 \le f(x) - f^* - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb F(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-peb} Proximal Error Bound (PEB): $\Vert \mathcal G_{L_f}x\Vert \ge \kappa_f\Vert x - \bar x\Vert$. 
                    Denoted by $\mathbb E(L_f, \kappa_f, X)$. 
                \end{enumerate}
            \end{definition}
            \begin{remark}
                The error bound condition in Necoara et al. is sometimes referred to as the "Proximal Error Bound". 
            \end{remark}

        \subsection{Weaker conditions of strong convexity}
            In Necoara's et al, major results assume convexity of $f$. 
            % ============================================================================
            % THEOREM | Q-SCNVX IMPLIES QUA 
            % ============================================================================
            \begin{theorem}[Q-SCNVX implies QUA]\label{thm:qscnvx-means-qua}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings} and assume $f$ is convex: 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                We proof by induction. 
                Convexity of $f$ makes $X^+$ convex and $\Pi_{X^+}X$ unique for all $x \in X$. 
                Make inductive hypothesis that there exists $\kappa^{(k)} \ge 0$ such that 
                \begin{align*}
                    (\forall x \in X)\quad
                    f(x) \ge f^+ + \langle \nabla f(\Pi_{X^+}x), x - \Pi_{X^+}x\rangle 
                    + \kappa^{(k)}/2\Vert x - \Pi_{X^+}x \Vert^2. 
                \end{align*}
                The base case is true by convexity of $f$ with $\kappa_f^{(0)} = 0$. 
                Choose any $x \in X$ define $\bar x = \Pi_{X^+}x$. 
                Consider $x_\tau = \bar x + \tau(x - \bar x)$ for $\tau \in [0, 1]$. 
                Calculus rule has 
                \begin{align*}
                    f(x) &= 
                    f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau
                    \\
                    &= 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), \tau(x - \bar x)\rangle d\tau
                    \\
                    &= 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), x_\tau - \bar x\rangle d\tau.
                \end{align*}
                $f$ is Q-SCNVX so
                \begin{align*}
                    f^+ - f(x_\tau) &\ge \langle \nabla f(x_\tau), \Pi_{X^+}x_\tau - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2
                    \\
                    &= 
                    \langle \nabla f(x_\tau), \bar x - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \bar x\Vert^2
                    \\
                    \iff 
                    \langle \nabla f(x_\tau), x_\tau - \bar x\rangle
                    &\ge f(x_\tau) - f^+ + \kappa_f/2\Vert x_\tau -\bar x\Vert^2. 
                \end{align*}
                We used $\Pi_{X^+}x_\tau = \bar x$ by convexity of $f$. 
                Therefore:
                {\footnotesize
                \begin{align*}
                    f(x) &\ge 
                    f(\bar x) + 
                    \int_0^1 \tau^{-1} \left(
                        f(x_\tau) - f^+ + \frac{\kappa_f}{2}\Vert x_\tau - \bar x\Vert^2
                    \right) d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            f(x_\tau) - f^+ 
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &\ge 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\Pi_{X^+}x_\tau), x_\tau - \Pi_{X^+}x_\tau
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \Pi_{X^+}x_\tau\Vert^2
                    d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\bar x), x_\tau - \bar x
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \bar x\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \int_0^1 
                        \langle 
                            \nabla f(\bar x), x - \bar x
                        \rangle
                        + \frac{\tau\kappa_f^{(k)}}{2} \Vert x - \bar x\Vert^2
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \langle 
                        \nabla f(\bar x), x - \bar x
                    \rangle
                    +
                    \frac{\kappa^{(k)}_f + \kappa_f}{4}
                    \Vert x - \bar x\Vert^2. 
                \end{align*}
                }
                This is the new inductive hypothesis, and it has $\kappa_f^{(k + 1)} = (\kappa_f^{(k)} + \kappa_f)/2$. 
                The induction admits recurrence: 
                \begin{align*}
                    \kappa_f^{(n)} = (1/2^n)(\kappa_f^{(0)} + (2^n - 1)\kappa_f). 
                \end{align*}
                Inductive hypothesis is true for $\kappa_f^{(0)} = 0$ and $f$ being convex is sufficient. 
                It has $\lim_{n\rightarrow \infty} \kappa_f^{(n)} = \kappa_f$. 
            \end{proof}
            \begin{remark}
                This is Theorem 1 in the paper. 
                Convexity assumption of $f$ makes $X^+$ convex, so the projection is unique, and it has $\Pi_{X^+}x_\tau = \bar x$ for all $\tau \in [0, 1]$. 
                In addition, the inductive hypothesis has $\kappa_f^{(n)} \ge 0$, which is not sufficient for convexity, but necessary. 
                The projection property remains true for nonconvex $X^+$, however the base case require rethinking. 
            \end{remark}
            % ================================================================================
            % THEOREM | QGG IMPLIES QUA 
            % ================================================================================
            \begin{theorem}[QGG implies QUA]\label{thm:qgg-implies-qua}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}, under convexity it has 
                \begin{align*}
                    \mathbb G(L_f, \kappa_f, X)\subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                For all $x \in X$, define $\bar x = \Pi_{X^+}x$, $x_\tau = \bar x + \tau(x - \bar x)\; \forall \tau \in [0, 1]$. 
                Observe that $\frac{d}{d\tau}x_\tau = x - \bar x$ and $\Pi_{X^+}x_\tau = \bar x\; \forall \tau \in [0, 1]$. 
                Using calculus and Theorem \ref{def:necoara-weaker-scnvx} \ref{def:necoara-qgg}: 
                \begin{align*}
                    f(x) &= f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau  
                    \\
                    &= f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \langle \nabla f(x_\tau) - \nabla f(\bar x), x - \bar x\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), \tau(x - \bar x)\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), x_\tau - \bar x\rangle d \tau
                    \\
                    &\ge
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\kappa_f\Vert \tau(x - \bar x)\Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau\kappa_f\Vert x - \bar x \Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \frac{\kappa}{2}\Vert x - \bar x\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                This is Theorem 3 in Neocara et al. \cite{necoara_linear_2019}. 
                There is no immediate use of convexity besides that the projection $\bar x = \Pi_{X^+}x$ is a singleton.
            \end{remark}
            % ===============================================================================
            % THEOREM | QFC IMPLIES QGG  
            % ===============================================================================
            \begin{theorem}[Q-SCNVX implies QGG]\label{thm:qscnvx-implies-qgg}
                Under Assumption \ref{ass:necoara-2019-settings} and convexity of $f$, it has 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb G(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                If $f \in \mathbb S'(L_f, \kappa_f, X)$ then Theorem \ref{thm:qscnvx-means-qua} has $f \in \mathbb U(L_f, \kappa_f, X)$. 
                Then, add \ref{def:necoara-qup}, \ref{def:neocara-qscnvx} in Definition \ref{def:necoara-weaker-scnvx} yield the results. 
            \end{proof}
            \begin{remark}
                This is Theorem 2 in the Necoara et al. \cite{necoara_linear_2019}, right after it claims $\mathbb U(L_f, \kappa_f, X)\subseteq \mathbb G(L_f, \kappa_f/2, X)$ under convexity. 
            \end{remark}
            % =============================================================================
            % THEOREM | SUFFICIENCY OF QFG 
            % =============================================================================
            \begin{theorem}[sufficiency of QFG]\label{thm:qfg-suff}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                For all $0 < \beta < 1$, $x \in X$, let $x^+ = \Pi_{X}(x - L^{-1}_f \nabla f(x))$. 
                If 
                \begin{align*}
                    \Vert x^+ - \Pi_{X^+}x^+\Vert \le \beta \Vert x - \Pi_{X^+}x \Vert, 
                \end{align*}
                then $f$ satisfies the QFG condition with $\kappa_f = L_f(1 - \beta)^2$. 
            \end{theorem}
            \begin{proof}
                The proof is direct. 
                \begin{align}
                    \Vert x - \Pi_{X^+}x\Vert 
                    &\le \Vert x - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \Vert x^+ - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \beta \Vert x - \Pi_{X^+}x\Vert
                    \\
                    \iff 
                    0 &\le \Vert x - x^+\Vert - (1 - \beta) \Vert x - \Pi_{X^+}x\Vert. 
                \end{align}
                $x^+$ has descent lemma hence we have 
                \begin{align*}
                    f^+ - f(X) \le f(x^+) - f(x) 
                    \le - \frac{L_f}{2}\Vert x^+ - x\Vert^2 
                    \le - \frac{L_f}{2}(1 - \beta)^2 \Vert x - \Pi_{X^+}\Vert^2. 
                \end{align*}
                Hence, it gives the quadratic growth condition. 
            \end{proof}
            \begin{remark}
                It's unclear where convexity is used. 
                However, it' still assumed in Necoara et al. paper. 
            \end{remark}
            Before we start, we will specialize Theorem \ref{theorem:pg-ineq} because it will be used in later proofs. 
            In Assumption \ref{ass:necoara-2019-settings}, it can be seemed as taking $F = f + g$ in Assumption \ref{ass:smooth-add-nonsmooth} with $g = \delta_{X}$. 
            This makes $\mu_g = 0$ and assuming $f$ is convex we have $\mu_f = 0$. 
            Let $\beta = L_f$, and $x^+ = \Pi_{X}(x - L_f^{-1}\nabla f(x))$, it has for all $z \in X$: 
            \begin{align}\label{ineq:proj-grad}
                \begin{split}
                    0 &\le 
                    f(z) - f(x^+) + \frac{L_f}{2}\Vert z - x\Vert^2
                    - \frac{L_f}{2}\Vert z - x^+\Vert^2
                    \\
                    &= 
                    f(z) - f(x^+) + L_f\langle z - x^+, x^+ - x\rangle
                    + \frac{L_f}{2}\Vert x - x^+\Vert^2. 
                \end{split}
            \end{align}
            Take note that when $z = x$ it has 
            \begin{align}\label{ineq:proj-grad2}
                0 &\le f(x) - f(x^+) - \frac{L_f}{2}\Vert x - x^+\Vert^2. 
            \end{align}
            \par
            The following theorems are about the relation between PEB and QFG.
            % ==========================================
            % THEOREM | EQUIVALENCE BETWEEN QFG AND PEB 
            % ==========================================
            \begin{theorem}[equivalence between QFG and PEB]\label{thm:qfg-peb-equiv}
                If $f$ is convex and satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then we have: 
                \begin{align*}
                    \mathbb E(L_f, \kappa_f, X) &\subseteq \mathbb F(L_f, \kappa^2/L_f, X), 
                    \\
                    \mathbb F(L_f, \kappa_f) 
                    &\subseteq 
                    \mathbb E\left(
                        L_f,
                        \frac{\kappa_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}, 
                        X
                    \right). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                \textbf{Firstly, we show that PEB implies QFG.}
                For any $x \in X$, define the gradient projection steps by $x^+ = \Pi_{X}(x - L^{-1}_f\nabla f(x))$. 
                Denote $\bar x^+_\Pi = \Pi_{X^+}x^+$. 
                Using \eqref{ineq:proj-grad} with $z = \bar x^+_\Pi$ it yields: 
                {\small
                \begin{align*}
                    0 &\ge 
                    f(x^+) - f(x^+_\Pi) - L_f\langle x_\Pi^+ - x^+, x^+ - x\rangle
                    - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert^2
                    \\
                    \quad&\ge 
                    \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \Vert L_f(x - x^+)\Vert\Vert x^+_\Pi - x^+\Vert
                    - \frac{1}{2L_f}\Vert L_f(x - x^+)\Vert^2   &\text{QFG, Cauchy}
                    \\
                    &= \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert^2
                        + L_f\Vert L_f(x - x^+)\Vert\Vert x_\Pi^+ - x^+\Vert
                    \right)
                    \\
                    &= 
                    \frac{\kappa_f + L_f}{2}\Vert x^+ - x^+_\Pi\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert + L_f\Vert x - x_\Pi^+\Vert
                    \right)^2
                \end{align*}
                }
                From the last line, by positivity of norm it has
                \begin{align*}
                    \Vert L_f(x - x^+)\Vert + L_f\Vert x^+ - x_\Pi^+\Vert
                    &\ge \sqrt{L_f(\kappa_f + L_f)}\Vert x^+ - x^+_\Pi\Vert
                    \\
                    \iff 
                    \Vert L_f(x - x^+)\Vert
                    &\ge 
                    \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\Vert x^+ - x^+_\Pi\Vert. 
                \end{align*}
                Using property of projection onto $X$ we have 
                \begin{align*}
                    \Vert x - \bar x\Vert &\le \Vert x - x_\Pi^+\Vert
                    \le \Vert x - x^+\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \\
                    &= \frac{1}{L_f}\Vert L_f(x - x^+)\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \\
                    \iff 
                    \Vert x - \bar x\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert
                    &\le 
                    \Vert x^+ - x^+_\Pi\Vert. 
                \end{align*}
                Combining the two above inequalities gives 
                {\small
                \begin{align*}
                    0 &\le 
                    \Vert L_f(x - x^+)\Vert
                    -
                    \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\left(
                        \Vert x - \bar x\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert
                    \right)
                    \\
                    &=
                    - \left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \left(
                        L^{-1}_f\left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right) + 1
                    \right)\Vert L_f(x - x^+)\Vert
                    \\
                    &= 
                    -\left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \sqrt{L_f(\kappa_f + L_f)}
                    \Vert L_f(x - x^+)\Vert
                    \\
                    \iff&
                    \frac{\sqrt{L_f(\kappa_f + L_f)} - L_f}{\sqrt{L_f(\kappa_f + L_f)}}
                    \Vert x - \bar x\Vert 
                    \le
                    \Vert \mathcal G_{L_f}x\Vert. 
                \end{align*}
                }
                Skipping algebra, the fraction simplifies to 
                \begin{align*}
                    \frac{\kappa_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}. 
                \end{align*}
                This gives PEB condition. 
                \textbf{We now show QFG implies PEB}. 
                From the error bound condition using $\kappa_f$ it has
                \begin{align*}
                    \kappa_f^2\Vert x - \bar x\Vert^2
                    \le \Vert \mathcal G_{L_f}(x)\Vert^2
                    \underset{\eqref{ineq:proj-grad2}}{\le }
                    2L_f(f(x) - f(x^+)) \le 2L_f(f(x) - f^+). 
                \end{align*}
            \end{proof}
            \par
            The following theorem summarizes the hierarchy of the conditions listed in Definition \ref{def:necoara-weaker-scnvx}. 
            \begin{theorem}[Hierarchy of weaker S-CNVX conditions]\label{thm:weaker-scnvx-hierarchy}
                Let $f$ satisfy Assumption \ref{ass:necoara-2019-settings}, assuming convexity then the following relations are true: 
                \begin{align*}
                    \mathbb S(\kappa_f, L_f, X) 
                    \subseteq \mathbb S'(\kappa_f, L_f, X)
                    \subseteq \mathbb G(\kappa_f, L_f, X) 
                    \subseteq \mathbb U(\kappa_f, L_f, X) 
                    \subseteq \mathbb F(\kappa_f, L_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                $\mathbb S' \subseteq \mathbb G$ is proved in Theorem \ref{thm:qscnvx-implies-qgg} and $\mathbb G \subseteq \mathbb U$ is proved in \ref{thm:qgg-implies-qua}. 
                $\mathbb S\subseteq \mathbb S'$ is obvious and it remains to show $\mathbb U \subseteq \mathbb F$. 
                Let $f\in \mathbb U(\kappa_f, L_f, X)$, it has for all $x \in X$: 
                \begin{align*}
                    0 &\le f(x) - f^+ - \langle \nabla f(\bar x), x - \bar x\rangle - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2
                    \\
                    &\hspace{-0.5em}\underset{\eqref{ineq:pg-opt-cond}}{\le} 
                    f(x) - f^+ - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                It's Theorem 4 in Necoara et al. \cite{necoara_linear_2019}.
            \end{remark}
        \subsection{Hoffman error bound and Q-SCNVX}

        \subsection{Feasible descent and accelerated feasible descent}
            This section summarizes results from Necoara et al. on the method of feasible descent, fast feasible descent, and fast feasible descent with restart. 

        \subsection{Application, KKT of linear programming}
            This section extends and ideas in the discussion section of Necoara et al. \cite{necoara_linear_2019}. 
            \par
            Let $X_1, X_2, Y$ be Hilbert spaces. 
            Define linear mapping $E:X_1 \times X_2 \rightarrow Y := (x_1, x_2)\mapsto E_1 x_1 + E_2 x_2$ where $E_1, E_2$ each are mappings of $X_1 \rightarrow Y, X_2 \rightarrow Y$. 
            Denote the adjoint of linear mapping by $(\cdot)^*$. 
            Let $c = (c_1, c_2) \in X_1 \times X_2$, $b \in Y$. 
            Suppose that $\mathcal K \subseteq X_1$ is a simple cone and $K^*$ is its dual cone. 
            We consider the following linear programming problem 
            \begin{align}\label{problem:lp-cannon-form}
                \inf_{x \in X_1\times X_2}\left\lbrace
                    \langle - c, x\rangle
                    \left| \;
                        Ex = b, x \in \mathcal K \times X_2
                    \right.
                \right\rbrace. 
            \end{align}
            Define linear mapping $g, F$ and indicator function $h$ by the following: 
            \begin{align*}
                g:X_1\times X_2 \rightarrow \RR 
                    &:= x \mapsto \langle - c, x\rangle, 
                \\
                F: X_1\times X_2 \rightarrow Y \times X_1 
                    &:= (x_1, x_2) \mapsto (E_1x_1 + E_2 x_2, x_1),
                \\
                h: Y \times X_1 \rightarrow \overline \RR &:= 
                    (y, z) \mapsto \delta_{\{\mathbf 0\}}(y - b) + \delta_{\mathcal K^*}(z). 
            \end{align*}
            It's not hard to identify that problem in \eqref{problem:lp-cannon-form} has representations 
            \begin{align*}
                \inf_{x \in X_1\times X_2}
                \left\lbrace
                    g(x) + h(Fx)
                \right\rbrace. 
            \end{align*}
            The dual problem of the above is given by
            \begin{align*}
                -\inf_{u \in Y\times X_1}
                \left\lbrace
                    h^\star(u) + g^\star(-F^* u)
                \right\rbrace. 
            \end{align*}
            Where $h^\star, g^\star$ are the conjugate of $h, g$ and $F^*: Y\times X_1 \rightarrow X_1 \times X_2 = (y, z)\mapsto (E_1^*y + z, E_2^*y)$ is the adjoint operator of $F$. 
            Note that $g^\star(x) = \delta_{\mathbf 0}(x + c)$ and $h^\star((y, z)) = \langle b, y\rangle + \delta_{\mathcal K^*}(z)$. 
            This gives the following dual problem 
            \begin{align*}
                - \inf_{(y, z) \in Y \times \mathcal K^*} \left\lbrace
                    \langle b, y\rangle 
                    \left | \;
                        E_1^*y + z = c_1, 
                        E^*_2y = c_2
                    \right.
                \right\rbrace. 
            \end{align*}
            The KKT conditions give the following convex feasibility problem 
            \begin{align*}
                E_1 x_1 + E_2 x_2 &= b, \\
                E_1^* y + z &= c_1, \\
                E_2^* y &= c_2, \\
                \langle b, y\rangle &= \langle c_1, x_1\rangle + \langle c_2, x_2\rangle, \\
                (x_1, x_2) &\in \mathcal K \times X_2, \\
                (y, z) &\in Y \times \mathcal K^*.
            \end{align*}
            Allow $X_1 = \RR^{n_1}, X_2 = \RR^{n_2}, Y = \RR^m$. 
            Define 
            \begin{align*}
                \mathbf K &:= \mathcal K \times \RR^{n_2} \times \RR^m \times \mathcal K^*, 
                \\
                A &:= \begin{bmatrix}
                    E_1 & E_2 & \mathbf 0 & \mathbf 0
                    \\
                    \mathbf 0 &\mathbf 0  & E_1^T & I_{n_1}
                    \\
                    \mathbf 0 &\mathbf 0  & E_2^T & \mathbf 0
                    \\
                    c_1^T & c_2^T & - b^T & 0
                \end{bmatrix}, 
                v := 
                \begin{bmatrix}
                    x_1\\ x_2 \\ y \\ z \\
                \end{bmatrix} \in \mathbf K, 
                d := 
                \begin{bmatrix}
                    b \\ c_1 \\ c_2 \\ 0
                \end{bmatrix}. 
            \end{align*}
            The KKT conditions is a convex feasibility problem which can be formulated by best approximation problem: 
            \begin{align}\label{problem:lp-kkt-min}
                \min_{v \in \mathbf K} 
                \frac{1}{2}\Vert Ax - d \Vert^2. 
            \end{align}
            It is minimizing a quadratic problem on a simple cone. 
            Solving \eqref{problem:lp-cannon-form} can be approached by optimizing \eqref{problem:lp-kkt-min}. 
            It's necessary to investigate the matrices $A, A^T$ which are essential to solving it numerically. 
            The properties of $A^TA$ will determine the convergence rate of algorithms. 
            The matrix is a block matrix and possibly sparse in practice. 
            Let $v = (x_1, x_2, y, z)$, it admits implicit representation: 
            \begin{align*}
                Av = (E_1x_1 + E_2 x_2,\; E_1^Ty + z,\; E_2^Ty,\; c^T_1x_1 + c_2^Tx_2 - b^Ty). 
            \end{align*}
            It involves 
            \begin{enumerate}
                \item Two multiplications of $E$: $x_1, x_2$ on the right and $y$ on the right,  
                \item inner product using $x_1, x_2$ and $y$. 
            \end{enumerate}
            
            Let $\bar v = (\bar y, \bar x_1, \bar x_2, \xi) \in \RR^m\times \RR^{n_1} \times \RR^{n_2}\times \RR$ then the right multiplication of has: 
            \begin{align*}
                \bar v^TA  &= (
                    E_1^T\bar y + \xi c_1^T,\; E_2^T\bar y + \xi c_2^T,\; 
                    \bar x_1^TE_1^T + \bar x_2^TE_2^T - \xi b^T,\; \bar x_1^T
                )
                \\
                &= 
                (
                    E_1^T\bar y + \xi c_1, \;
                    E_2^T \bar y + \xi c_2, \;
                    E_1\bar x_1 + E_2\bar x_2 - \xi b, \;
                    \bar x_1
                )^T. 
            \end{align*}
            \begin{enumerate}
                \item Two multiplications of $E$: $\bar y$ on the left and for $\bar x_1, \bar x_2$ on the right, 
                \item one vector addition with $c = (c_1, c_2)$ and $b$. 
            \end{enumerate}
            Therefore, computing $A^TAv$ has four vector multiplications using $E$. 
            In practice, a sparse matrix $E$ from the model can speed up computations. 
            \par
            Another key operation would be $A^TAv$. 
            Let $\bar v = Av$, then 
            \begin{align*}
                A^TAv &= 
                \begin{bmatrix}
                    E^T_1(E_1x_1 + E_2x_2) + (c_1^Tx_1 + c_2^Tx_2 - b^Ty)c_1
                    \\
                    E^T_2(E_1x_1 + E_2x_2) + (c_1^Tx_1 + c_2^Tx_2 - b^Ty)c_2
                    \\
                    E_1(E_1^Ty + z) + E_2E_2^Ty - (c_1^Tx_1 + c_2^Tx_2 - b^Ty)b
                    \\
                    E_1^Ty + z
                \end{bmatrix}
                \\
                &= 
                \begin{bmatrix}
                    (E_1^TE_1 + c_1^T)x_1 + (E_1^TE_2 + c_2^T)x_2 - (c_1b^T)y
                    \\
                    (E_2^TE_1 + c_1^T)x_1 + (E_2^TE_2 + c_2^T)x_2 - (c_2b^T)y
                    \\
                    -(bc_1^T)x_1 - (bc_2^T)x_2 + (E_2E_2^T + E_1E_1^T + bb^T)y
                    + (E_1E_1^T)z
                    \\
                    E_1^Ty + z
                \end{bmatrix}. 
            \end{align*}
            In practice, implicitly representing the process of $A^TAv$ is better in computing software. 
            Here we write it out to view, for theoretical interests. 
            \par
            Let $f(v) = (1/2)\Vert Av - d\Vert^2$ to be the objective function of optimization problem \eqref{problem:lp-kkt-min}. 
            Its gradient, objective value are related and if they have: 
            \begin{align*}
                \nabla f(v) &= A^TAv - A^Td, 
                \\
                f(v) &= 
                \frac{1}{2}\langle x, \nabla f(v) - A^Td\rangle + \frac{1}{2}\Vert d\Vert^2. 
            \end{align*}
            The value $\nabla f(v), f(v)$ when evaluated together, require minimal additional computations. 
            This fact is favorable for implementations in practice. 

\chapter{Restarting the Accelerated Proximal Gradient}
    We review specifically on the idea of restarting Accelerated Proximal Gradient. 
    There are a lot of contents on it in the literatures hence it will be condensed. 


% ==============================================================================

\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}