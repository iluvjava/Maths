\documentclass[12pt]{report}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}
\title{{\fontfamily{ptm}\selectfont Reading Notes}}

\author{
    Alto
    % \thanks{
    %     Subject type, Some Department of Some University, Location of the University,
    %     Country. E-mail: \texttt{author.name@university.edu}.
    % }
}

\date{Last Compiled: \today}

\maketitle

\begin{abstract} 
    Reports on papers read. 
    This is a LaTEX file for my own notes taking. 
    It may accelerate the process of writing my thesis for my PhD degree. 
    \todoinline{This paper is currently in draft mode. Check source to change options. }
\end{abstract}
\chapter{The Basics of Optimization Theories}
    Notations in this chapter are not shared, and they are for this chapter only. 
    % ==========================================================================
    % BREGMAN DIV DEFINITION 
    % ==========================================================================
    \begin{definition}[Bregman Divergence]\label{def:bregman-div}
        Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
        Define Bregman Divergence: 
        \begin{align*}
            D_f: \RR^n \times \dom \nabla f \rightarrow \overline \RR:= 
            (x, y) \mapsto f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
        \end{align*}
    \end{definition}
    \begin{assumption}[smooth plus nonsmooth]\label{ass:smooth-add-nonsmooth}
        Let $F = f+ g$ where $f:\RR^n \rightarrow \overline \RR$ is differentiable and there exists $q\in \RR$ such that $g - \mu/2\Vert \cdot\Vert^2$ is convex.
    \end{assumption}
    \begin{definition}[proximal gradient operator]
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth}. 
        Let $\beta > 0$, we define the proximal gradient operator for all $x \in \RR^n$: 
        \begin{align*}
            T_{\beta^{-1}, f, g}(x) &:= \hprox_{\beta^{-1}g} \left(
                x - \beta^{-1} \nabla f(x)
            \right)
            \\
            &= \argmin_{z}\left\lbrace
                g(z) + f(x) + \langle \nabla f(x), z - x\rangle
                + \frac{\beta}{2}\Vert x - z\Vert^2
            \right\rbrace. 
        \end{align*}
    \end{definition}
    % ==========================================================================
    % WEAKLY CONVEX GENERIC PROXIMAL GRADIENT INEQUALITY
    % ==========================================================================
    \begin{theorem}[weakly convex generic proximal gradient inequality]\;\label{thm:pg-ineq-wcnvx-generic}\\
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} with $\beta > 0$ and $\mu \in \RR$. 
        Then for all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$, it has: 
        \begin{align*}
            \frac{\mu}{2}\Vert z - \bar x\Vert^2 
            &\le 
            F(z) - F(\bar x) - \langle \beta(x - \bar x), z - \bar x\rangle 
            + D_f(x, \bar x ) - D_f(z, x).  
        \end{align*}
    \end{theorem}
    \begin{proof}
        Nonsmooth analysis calculus rules has 
        \begin{align*}
            \bar x &\in \argmin{z} \left\lbrace
                g(z) + \langle \nabla f(x), z\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
            \right\rbrace
            \\
            \implies
            \mathbf 0 
            &\in \partial g(\bar x) + \nabla f(x) + \beta(\bar x - x)
            \\
            \iff 
            \partial g(x^+) &\ni
            - \nabla f(x) - \beta(\bar x - x). 
        \end{align*}
        The subgradient inequality for weak convexity has 
        \begin{align*}
            \frac{\mu}{2}\Vert z - \bar x\Vert^2 
            &\le 
            g(z) - g(\bar x) + \langle \nabla f(x) + \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) + \langle \nabla f(x), z - \bar x\rangle + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= g(z) - g(\bar x) + \langle \nabla f(x), z - x\rangle
            + \langle \nabla f(x), x - \bar x\rangle
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= 
            g(z) - g(\bar x) 
            + (-D_f(z, x) + f(z) - f(x))
            \\
            & \quad 
            + (D_f(\bar x, x) - f(\bar x) + f(x))
            + \langle \beta(\bar x - x), z - \bar x\rangle
            \\
            &= F(z) - F(\bar x) - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle. 
        \end{align*}
    \end{proof}
    \begin{theorem}[convex proximal gradient inequality]\label{thm:cnvx-pg-ineq}
        Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-add-nonsmooth} such that $\mu = \mu_g \ge 0$, $\beta \ge L_f$. 
        In addition, suppose that $f:\RR^n\rightarrow \RR$ has $L_f$ Lipschitz continuous gradient, and it's $\mu_f \ge 0$ strongly convex. 
        For all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has 
        \begin{align*}
            0 &\le 
            F(z) - F(\bar x) + 
            \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2
            - \frac{\beta + \mu_g}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{theorem}
    \begin{proof}
        The Bregman Divergence of $f$ has inequality 
        \begin{align*}
            \left(\forall x \in \RR^n, y \in \RR^n\right)\; 
            \frac{\mu_f}{2}\Vert x - y\Vert^2 \le D_f(x, y) \le \frac{L_f}{2}\Vert x - y\Vert^2. 
        \end{align*}
        Specializing Theorem \ref{thm:pg-ineq-wcnvx-generic}, let $x \in \RR^n$ and define $\bar x = T_{\beta^{-1}, f, g}(x)$ it has $\forall z \in \RR^n:$
        \begin{align*}
            \frac{\mu_g}{2}\Vert z - \bar x \Vert^2 
            &\le 
            F(z) - F(\bar x) 
            - D_f(z, x) + D_f(\bar x, x) 
            - \langle \beta(x - \bar x), z - \bar x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \frac{L_f}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x + x - \bar x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            + \left(
                \frac{L_f}{2} - \beta
            \right)\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &\le 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert x - \bar x\Vert^2
            - \langle \beta(x - \bar x), z - x\rangle
            \\
            &= 
            F(z) - F(\bar x) 
            - \frac{\mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}
            \left(
                \Vert x - \bar x\Vert^2
                + 2\langle x - \bar x, z - x\rangle
            \right)
            \\
            &= 
            F(z) - F(\bar x) 
            + \frac{\beta - \mu_f}{2}\Vert z - x\Vert^2 
            - \frac{\beta}{2}\Vert z - \bar x\Vert^2. 
        \end{align*}
    \end{proof}
    
\chapter{Linear Convergence of First Order Method}
    In this chapter, we are specifically interested in characterizing linear convergence of well known first order optimization algorithms. 
    In this section, $D_f$ will denote the Bregman Divergence as defined in Definition \ref{def:bregman-div}. 
    \section{Necoara's et al's Paper}
        \subsection{The Settings}
            The assumption follows give the same setting as Necoara et al. \cite{necoara_linear_2019}. 
            % ==================================================================
            % NECOARA's ASSUMPTIONS 
            % ==================================================================
            \begin{assumption}\label{ass:necoara-2019-settings}
                Consider optimization problem: 
                \begin{align}
                    -\infty < f^+ = \min_{x \in X} f(x) . 
                \end{align}\label{problem:necoara-2019}
                $X\subseteq \RR^n$ is a closed convex set. 
                Assume projection onto $X$, denoted by $\Pi_X$ is easy. 
                Denote $X^+ = \argmin_{x \in X}f(x) \neq \emptyset$, assume it's a closed set. 
                Assume $f$ has $L_f$ Lipschitz continuous gradient, i.e: for all $x, y\in X$: 
                \begin{align*}
                    \Vert \nabla f(x) - \nabla f(y)\Vert \le L_f\Vert x - y\Vert. 
                \end{align*}
            \end{assumption}
            Some immediate consequences of Assumption \ref{ass:necoara-2019-settings} now follows. 
            The variational inequality characterizing optimal solution has: 
            \begin{align}\label{ineq:pg-opt-cond}
                x^+ \in X^+ \implies 
                (\forall x \in X)\; \langle \nabla f(x^+), x - x^+\rangle \ge 0. 
            \end{align}
            The converse is true if $f$ is convex. 
            The gradient mapping in this case is: 
            \begin{align*}
                \mathcal G_{L_f}x = L_f(x - \Pi_{X}x). 
            \end{align*}
            % ==================================================================
            % STRONG CONVEXITY DEFINITION
            % ==================================================================
            \begin{definition}[strong convexity]\label{def:necoara-scnvx}
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then $f \in \mathbb S(L_f, \kappa_f, X)$ is strongly convex iff 
                \begin{align*}
                    (\forall x, y\in X)\; 
                    \kappa_f \Vert x - y\Vert^2 \le 
                    D_f(x, y) \le L_f \Vert x - y\Vert^2. 
                \end{align*}
            \end{definition}
            Then it's not hard to imagine the following natural relaxation of the above conditions. 
            %===================================================================
            % DEFINITION OF WEAKER STRONG CONVEXITY 
            % ==================================================================
            \begin{definition}[relaxations of strong convexity]\;\\
                Suppose $f$ satisfies Assumption \ref{ass:necoara-2019-settings}.
                \label{def:necoara-weaker-scnvx}
                Let $L_f \ge \kappa_f \ge 0$ such that for all $x \in X$, $\bar x = \Pi_{X^+} x$. 
                We define the following: 
                \begin{enumerate}
                    \item\label{def:neocara-qscnvx} Quasi-strong convexity (Q-SCNVX): $0 \le D_f(\bar x, x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb S'(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qup} Quadratic under approximation (QUA): $0 \le D_f(x, \bar x) - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb U(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qgg} Quadratic Gradient Growth (QGG): $0\le D_f(x, \bar x) + D_f(\bar x, x) - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb G(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-qfg} Quadratic Function Growth (QFG): $0 \le f(x) - f^* - \kappa_f/2\Vert x - \bar x\Vert^2$. 
                    Denoted by $\mathbb F(L_f, \kappa_f, X)$. 
                    \item\label{def:necoara-peb} Proximal Error Bound (PEB): $\Vert \mathcal G_{L_f}x\Vert \ge \kappa_f\Vert x - \bar x\Vert$. 
                    Denoted by $\mathbb E(L_f, \kappa_f, X)$. 
                \end{enumerate}
            \end{definition}
            \begin{remark}
                The error bound condition in Necoara et al. is sometimes referred to as the "Proximal Error Bound". 
            \end{remark}

        \subsection{Weaker conditions of strong convexity}
            In Necoara's et al., major results assume convexity of $f$. 
            % ==================================================================
            % THEOREM | Q-SCNVX IMPLIES QUA 
            % ==================================================================
            \begin{theorem}[Q-SCNVX implies QUA]\label{thm:qscnvx-means-qua}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings} and assume $f$ is convex: 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                We prove by induction. 
                Convexity of $f$ makes $X^+$ convex, so $\Pi_{X^+}x$ is unique for all $x \in \RR^n$. 
                Make inductive hypothesis that there exists $\kappa^{(k)} \ge 0$ such that 
                \begin{align*}
                    (\forall x \in X)\quad
                    f(x) \ge f^+ + \langle \nabla f(\Pi_{X^+}x), x - \Pi_{X^+}x\rangle 
                    + \kappa^{(k)}_f/2\Vert x - \Pi_{X^+}x \Vert^2. 
                \end{align*}
                The base case is true by convexity of $f$ with $\kappa_f^{(0)} = 0$. 
                Choose any $x \in X$ define $\bar x = \Pi_{X^+}x$. 
                Consider $x_\tau = \bar x + \tau(x - \bar x)$ for $\tau \in [0, 1]$. 
                $f$ is Q-SCNVX so
                \begin{align}\label{ineq:thm:qscnvx-means-qua-proof-item1}
                    f^+ - f(x_\tau) &\ge \langle \nabla f(x_\tau), \Pi_{X^+}x_\tau - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2 
                    \notag\\
                    &= 
                    \langle \nabla f(x_\tau), \bar x - x_\tau\rangle + 
                    \kappa_f/2 \Vert x_\tau - \bar x\Vert^2
                    \notag\\
                    \iff 
                    \langle \nabla f(x_\tau), x_\tau - \bar x\rangle
                    &\ge f(x_\tau) - f^+ + \kappa_f/2\Vert x_\tau -\bar x\Vert^2. 
                \end{align}
                In the inductive proof that comes, we will use the following intermediate results. 
                They are labeled for ease of refernecing. 
                \begin{enumerate}
                    \item The inequality \eqref{ineq:thm:qscnvx-means-qua-proof-item1}. 
                    \item By the property of projection, it has $\Pi_{X^+} x_\tau = \bar x$. 
                    \item The inductive hypothesis with $k \ge 0$. 
                    \item $\bar x = \Pi_{X^+}x$, $X^+$ is the set of minimizer of the of $f$ over $X$, hence $f(\bar x) = f^+$, the minimum. 
                \end{enumerate}
                Using calculus rules, we start with: 
                {\footnotesize
                \begin{align*}
                    f(x) &= 
                    f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau
                    = 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), \tau(x - \bar x)\rangle d\tau
                    \\
                    &= 
                    f(\bar x) + \int_0^1 \tau^{-1}\langle \nabla f(x_\tau), x_\tau - \bar x\rangle d\tau.
                    \\
                    &\underset{\text{(i)}}{\ge }
                    f(\bar x) + 
                    \int_0^1 \tau^{-1} \left(
                        f(x_\tau) - f^+ + \frac{\kappa_f}{2}\Vert x_\tau - \bar x\Vert^2
                    \right) d\tau
                    = 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            f(x_\tau) - f^+ 
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &\underset{\text{(iii)}}{\ge }
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\Pi_{X^+}x_\tau), x_\tau - \Pi_{X^+}x_\tau
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \Pi_{X^+}x_\tau\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \Pi_{X^+}x_\tau\Vert^2
                    d\tau
                    \\
                    &\underset{\text{(ii)}}{=} 
                    f(\bar x) + 
                    \int_0^1 
                    \tau^{-1} \left(
                            \langle 
                                \nabla f(\bar x), x_\tau - \bar x
                            \rangle
                            + \frac{\kappa_f^{(k)}}{2} \Vert x_\tau - \bar x\Vert^2
                        \right)
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &= 
                    f(\bar x) + 
                    \int_0^1 
                        \langle 
                            \nabla f(\bar x), x - \bar x
                        \rangle
                        + \frac{\tau\kappa_f^{(k)}}{2} \Vert x - \bar x\Vert^2
                        + \frac{\tau\kappa_f}{2}\Vert x - \bar x\Vert^2
                    d\tau
                    \\
                    &\underset{\text{(iv)}}{=} 
                    f^+ + 
                    \langle 
                        \nabla f(\bar x), x - \bar x
                    \rangle
                    +
                    \frac{\kappa^{(k)}_f + \kappa_f}{4}
                    \Vert x - \bar x\Vert^2. 
                \end{align*}
                }
                This is the new inductive hypothesis, and it has $\kappa_f^{(k + 1)} = (\kappa_f^{(k)} + \kappa_f)/2$. 
                The induction admits recurrence: 
                \begin{align*}
                    \kappa_f^{(n)} = (1/2^n)(\kappa_f^{(0)} + (2^n - 1)\kappa_f). 
                \end{align*}
                Inductive hypothesis is true for $\kappa_f^{(0)} = 0$ and $f$ being convex is sufficient. 
                It has $\lim_{n\rightarrow \infty} \kappa_f^{(n)} = \kappa_f$. 
            \end{proof}
            \begin{remark}
                This is Theorem 1 in the paper. 
                Convexity assumption of $f$ makes $X^+$ convex, so the projection is unique, and it has $\Pi_{X^+}x_\tau = \bar x$ for all $\tau \in [0, 1]$. 
                In addition, the inductive hypothesis has $\kappa_f^{(n)} \ge 0$, which is not sufficient for convexity, but necessary. 
                The projection property remains true for nonconvex $X^+$, however the base case require rethinking. 
            \end{remark}
            % ================================================================================
            % THEOREM | QGG IMPLIES QUA 
            % ================================================================================
            \begin{theorem}[QGG implies QUA]\label{thm:qgg-implies-qua}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}, under convexity it has 
                \begin{align*}
                    \mathbb G(L_f, \kappa_f, X)\subseteq \mathbb U(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                For all $x \in X$, define $\bar x = \Pi_{X^+}x$, $x_\tau = \bar x + \tau(x - \bar x)\; \forall \tau \in [0, 1]$. 
                Observe that $\frac{d}{d\tau}x_\tau = x - \bar x$ and $\Pi_{X^+}x_\tau = \bar x\; \forall \tau \in [0, 1]$. 
                Using calculus, Definition \ref{def:necoara-weaker-scnvx} \ref{def:necoara-qgg}: 
                \begin{align*}
                    f(x) &= f(\bar x) + \int_0^1 \langle \nabla f(x_\tau), x - \bar x\rangle d\tau  
                    \\
                    &= f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \langle \nabla f(x_\tau) - \nabla f(\bar x), x - \bar x\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), \tau(x - \bar x)\rangle d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\langle \nabla f(x_\tau) - \nabla f(\bar x), x_\tau - \bar x\rangle d \tau
                    \\
                    &\ge
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau^{-1}\kappa_f\Vert \tau(x - \bar x)\Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \int_0^1 \tau\kappa_f\Vert x - \bar x \Vert^2 d \tau
                    \\
                    &= 
                    f(\bar x) + \langle \nabla f(\bar x), x - \bar x\rangle + 
                    \frac{\kappa}{2}\Vert x - \bar x\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                This is Theorem 3 in Neocara et al. \cite{necoara_linear_2019}. 
                There is no immediate use of convexity besides that the projection $\bar x = \Pi_{X^+}x$ is a singleton.
            \end{remark}
            % ==================================================================
            % THEOREM | QFC IMPLIES QGG  
            % ==================================================================
            \begin{theorem}[Q-SCNVX implies QGG]\label{thm:qscnvx-implies-qgg}
                Under Assumption \ref{ass:necoara-2019-settings} and convexity of $f$, it has 
                \begin{align*}
                    \mathbb S'(L_f, \kappa_f, X) \subseteq \mathbb G(L_f, \kappa_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                If $f \in \mathbb S'(L_f, \kappa_f, X)$ then Theorem \ref{thm:qscnvx-means-qua} has $f \in \mathbb U(L_f, \kappa_f, X)$. 
                Then, add \ref{def:necoara-qup}, \ref{def:neocara-qscnvx} in Definition \ref{def:necoara-weaker-scnvx} yield the results. 
            \end{proof}
            \begin{remark}
                This is Theorem 2 in the Necoara et al. \cite{necoara_linear_2019}, right after it claims $\mathbb U(L_f, \kappa_f, X)\subseteq \mathbb G(L_f, \kappa_f/2, X)$ under convexity. 
            \end{remark}
            % ==================================================================
            % THEOREM | SUFFICIENCY OF QFG 
            % ==================================================================
            \begin{theorem}[sufficiency of QFG]\label{thm:qfg-suff}
                Let $f$ satisfies Assumption \ref{ass:necoara-2019-settings}. 
                For all $0 < \beta < 1$, $x \in X$, let $x^+ = \Pi_{X}(x - L^{-1}_f \nabla f(x))$. 
                If 
                \begin{align*}
                    \Vert x^+ - \Pi_{X^+}x^+\Vert \le \beta \Vert x - \Pi_{X^+}x \Vert, 
                \end{align*}
                then $f$ satisfies the QFG condition with $\kappa_f = L_f(1 - \beta)^2$. 
            \end{theorem}
            \begin{proof}
                The proof is direct. 
                \begin{align}
                    \Vert x - \Pi_{X^+}x\Vert 
                    &\le \Vert x - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \Vert x^+ - \Pi_{X^+}x^+\Vert
                    \\
                    &\le \Vert x - x^+\Vert + \beta \Vert x - \Pi_{X^+}x\Vert
                    \\
                    \iff 
                    0 &\le \Vert x - x^+\Vert - (1 - \beta) \Vert x - \Pi_{X^+}x\Vert. 
                \end{align}
                $x^+$ has descent lemma hence we have 
                \begin{align*}
                    f^+ - f(X) \le f(x^+) - f(x) 
                    \le - \frac{L_f}{2}\Vert x^+ - x\Vert^2 
                    \le - \frac{L_f}{2}(1 - \beta)^2 \Vert x - \Pi_{X^+}\Vert^2. 
                \end{align*}
                Hence, it gives the quadratic growth condition. 
            \end{proof}
            \begin{remark}
                It's unclear where convexity is used. 
                However, it' still assumed in Necoara et al. paper. 
            \end{remark}
            Before we start, we will specialize Theorem \ref{thm:cnvx-pg-ineq} because it will be used in later proofs. 
            In Assumption \ref{ass:necoara-2019-settings}, it can be seemed as taking $F = f + g$ in Assumption \ref{ass:smooth-add-nonsmooth} with $g = \delta_{X}$. 
            This makes $\mu_g = 0$ and assuming $f$ is convex we have $\mu_f = 0$. 
            Let $\beta = L_f$, and $x^+ = \Pi_{X}(x - L_f^{-1}\nabla f(x))$, it has for all $z \in X$: 
            \begin{align}\label{ineq:proj-grad}
                \begin{split}
                    0 &\le 
                    f(z) - f(x^+) + \frac{L_f}{2}\Vert z - x\Vert^2
                    - \frac{L_f}{2}\Vert z - x^+\Vert^2
                    \\
                    &= 
                    f(z) - f(x^+) + L_f\langle z - x^+, x^+ - x\rangle
                    + \frac{L_f}{2}\Vert x - x^+\Vert^2. 
                \end{split}
            \end{align}
            Take note that when $z = x$ it has 
            \begin{align}\label{ineq:proj-grad2}
                0 &\le f(x) - f(x^+) - \frac{L_f}{2}\Vert x - x^+\Vert^2. 
            \end{align}
            \par
            The following theorems are about the relation between PEB and QFG.
            % ==================================================================
            % LEMMA | QFG AND GRADIENT MAPPING
            % ==================================================================
            \begin{lemma}[gradient mapping and quadratic function growth]\;\label{lemma:grad-map-qfg}\\
                Let $f$ satisfies Assumtion \ref{ass:necoara-2019-settings}. 
                Suppose that $f \in \mathbb F(L_f, \mu_f, X)$ so it satisfies the quadratic function growth condition. 
                For all $x \in \RR^n$, define $x^+ = \Pi_X(x - L^{-1}_f\nabla f(x))$, 
                definte projections onto the set of minimizers $x^+_\Pi = \Pi_{X^+} x^+, X_\Pi = \Pi_{X^+}x$, then
                \begin{align*}
                    \left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x^+ - x_\Pi^+\Vert
                    &\le \Vert L_f(x - x^+)\Vert. 
                \end{align*}
            \end{lemma}
            \begin{proof}
                Using convexity, consider \eqref{ineq:proj-grad} with $z = x^+_\Pi$ it yields: 
                {\small
                \begin{align*}
                    0 &\ge 
                    f(x^+) - f(x^+_\Pi) - L_f\langle x_\Pi^+ - x^+, x^+ - x\rangle
                    - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert^2
                    \\
                    &\ge
                    \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \Vert L_f(x - x^+)\Vert\Vert x^+_\Pi - x^+\Vert
                    - \frac{1}{2L_f}\Vert L_f(x - x^+)\Vert^2 
                    \\
                    &= \frac{\kappa_f}{2}\Vert x^+ - x_\Pi^+\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert^2
                        + L_f\Vert L_f(x - x^+)\Vert\Vert x_\Pi^+ - x^+\Vert
                    \right)
                    \\
                    &= 
                    \frac{\kappa_f + L_f}{2}\Vert x^+ - x^+_\Pi\Vert^2
                    - \frac{1}{2L_f}\left(
                        \Vert L_f(x - x^+)\Vert + L_f\Vert x - x_\Pi^+\Vert
                    \right)^2.
                \end{align*}
                }
                From the last line, it's can be equivalently expressed as:
                \begin{align*}
                    0 &\le
                    \Vert L_f(x - x^+)\Vert + L_f\Vert x^+ - x_\Pi^+\Vert
                    - \sqrt{L_f(\kappa_f + L_f)}\Vert x^+ - x^+_\Pi\Vert
                    \\
                    &=
                    \Vert L_f(x - x^+)\Vert
                    - \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\Vert x^+ - x^+_\Pi\Vert.
                \end{align*}
            \end{proof}
            % ==================================================================
            % THEOREM | EQUIVALENCE BETWEEN QFG AND PEB 
            % ==================================================================
            \begin{theorem}[equivalence between QFG and PEB]\label{thm:qfg-peb-equiv}
                If $f$ is convex and satisfies Assumption \ref{ass:necoara-2019-settings}. 
                Then we have: 
                \begin{align*}
                    \mathbb E(L_f, \kappa_f, X) &\subseteq \mathbb F(L_f, \kappa^2_f/L_f, X), 
                    \\
                    \mathbb F(L_f, \kappa_f) 
                    &\subseteq 
                    \mathbb E\left(
                        L_f,
                        \frac{\kappa_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}, 
                        X
                    \right). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                For any $x \in X$, define the gradient projection steps by $x^+ = \Pi_{X}(x - L^{-1}_f\nabla f(x))$. 
                Denote $x^+_\Pi = \Pi_{X^+}x^+$. 
                Let $x_\Pi = \Pi_{X^+}x$, using the property of projection onto $X$ we have 
                \begin{align}\label{ineq:thm:qfg-peb-equiv-proof-item1}
                    \Vert x - x_\Pi\Vert &\le \Vert x - x_\Pi^+\Vert
                    \le \Vert x - x^+\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \notag\\
                    &= \frac{1}{L_f}\Vert L_f(x - x^+)\Vert + \Vert x^+ - x^+_\Pi\Vert
                    \notag\\
                    \iff 
                    \Vert x^+ - x^+_\Pi\Vert &\ge
                    \Vert x - x_\Pi\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert. 
                \end{align}
                Before we start, we list intermediate results and conditions which are going to be used in the proof that follows for the ease of referencing. 
                \begin{enumerate}
                    \item The inequality \eqref{ineq:thm:qfg-peb-equiv-proof-item1}. It uses the property of projection onto a set hence convexity of $X^+$ is not needed. 
                \end{enumerate}
                Starting with Lemma \ref{lemma:grad-map-qfg} because $f$ satisfies quadratic growth and it is assumed convex, then it has: 
                {\small
                \begin{align*}
                    0 &\le 
                    \Vert L_f(x - x^+)\Vert
                    - \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\Vert x^+ - x^+_\Pi\Vert
                    \\
                    &\underset{\text{(i)}}{\le}
                    \Vert L_f(x - x^+)\Vert
                    -
                    \left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right)\left(
                        \Vert x - \bar x\Vert - \frac{1}{L_f}\Vert L_f(x - x^+)\Vert
                    \right)
                    \\
                    &=
                    - \left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \left(
                        L^{-1}_f\left(\sqrt{L_f(\kappa_f + L_f)} - L_f\right) + 1
                    \right)\Vert L_f(x - x^+)\Vert
                    \\
                    &= 
                    -\left(
                        \sqrt{L_f(\kappa_f + L_f)} - L_f
                    \right)\Vert x - \bar x\Vert
                    +
                    \sqrt{L_f(\kappa_f + L_f)}
                    \Vert L_f(x - x^+)\Vert
                    \\
                    \iff&
                    \frac{\sqrt{L_f(\kappa_f + L_f)} - L_f}{\sqrt{L_f(\kappa_f + L_f)}}
                    \Vert x - \bar x\Vert 
                    \le
                    \Vert \mathcal G_{L_f}x\Vert. 
                \end{align*}
                }
                Skipping some algebra, the fraction simplifies to 
                \begin{align*}
                    \frac{\kappa_f}{\kappa_f/L_f + 1 + \sqrt{\kappa_k/L_f + 1}}. 
                \end{align*}
                This gives PEB condition. 
                \textbf{We now show PEB implies QFG}. 
                From the error bound condition using $\kappa_f$ it has
                \begin{align*}
                    \kappa_f^2\Vert x - \bar x\Vert^2
                    \le \Vert \mathcal G_{L_f}(x)\Vert^2
                    \underset{\eqref{ineq:proj-grad2}}{\le }
                    2L_f(f(x) - f(x^+)) \le 2L_f(f(x) - f^+). 
                \end{align*}
            \end{proof}
            \par
            The following theorem summarizes the hierarchy of the conditions listed in Definition \ref{def:necoara-weaker-scnvx}. 
            \begin{theorem}[Hierarchy of weaker S-CNVX conditions]\label{thm:q-cnvx-hierarchy}
                Let $f$ satisfy Assumption \ref{ass:necoara-2019-settings}, assuming convexity then the following relations are true: 
                \begin{align*}
                    \mathbb S(\kappa_f, L_f, X) 
                    \subseteq \mathbb S'(\kappa_f, L_f, X)
                    \subseteq \mathbb G(\kappa_f, L_f, X) 
                    \subseteq \mathbb U(\kappa_f, L_f, X) 
                    \subseteq \mathbb F(\kappa_f, L_f, X). 
                \end{align*}
            \end{theorem}
            \begin{proof}
                $\mathbb S' \subseteq \mathbb G$ is proved in Theorem \ref{thm:qscnvx-implies-qgg} and $\mathbb G \subseteq \mathbb U$ is proved in \ref{thm:qgg-implies-qua}. 
                $\mathbb S\subseteq \mathbb S'$ is obvious and it remains to show $\mathbb U \subseteq \mathbb F$. 
                Let $f\in \mathbb U(\kappa_f, L_f, X)$, it has for all $x \in X$: 
                \begin{align*}
                    0 &\le f(x) - f^+ - \langle \nabla f(\bar x), x - \bar x\rangle - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2
                    \\
                    &\hspace{-0.5em}\underset{\eqref{ineq:pg-opt-cond}}{\le} 
                    f(x) - f^+ - \frac{\kappa_f}{2}\Vert x - \bar x\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                It's Theorem 4 in Necoara et al. \cite{necoara_linear_2019}.
            \end{remark}
        \subsection{Hoffman error bound and Q-SCNVX}

        \subsection{Feasible descent and accelerated feasible descent}
            This section summarizes results from Necoara et al. on the method of feasible descent, fast feasible descent, and fast feasible descent with restart. 
            \begin{definition}[projected gradient algorithm]\label{def:projg-alg}\;\\
                The projected gradient algorithm generates a sequence of iterates $(x_k)_{k \ge 0}$ such that they satisfy for all $k \ge 0$
                \begin{align*}
                    x_{k + 1} &= \Pi_X(x_k - \alpha_k \nabla f(x_k)), 
                \end{align*}
                Where $\alpha_k \ge L_f^{-1}$ for all $k \ge 1$. 
            \end{definition}
            Under Assumption \ref{ass:necoara-2019-settings}, convexity of $X$ means obtuse angle theorem from projection, and it specializes to 
            \begin{align}\label{ineq:projg-variational-ineq}
                (\forall x \in X)\; \langle x_{k + 1} - (x_k + \alpha_k \nabla f(x_k)), x_{k + 1} - x\rangle \le 0. 
            \end{align}

            \begin{theorem}{feasible descent linear convergence under Q-SCNVX}
                Under Assumption \ref{ass:necoara-2019-settings}, assume that $f$ is Q-CNVX with $\mu_f, L_f$, then the sequence that satisfies Definition \ref{def:projg-alg} has a linear convergence rate. 
                Let $\bar x_k = \Pi_{X^+}x_k, \bar x_0 = \Pi_{X^+} x_0$. 
                For all $k \ge 1$, the iterates satisfy
                \begin{align*}
                    \Vert x_k - \bar x_k\Vert^2 &\le \left(
                        \frac{1 - \kappa_f/L_f}{1 + \kappa_f/L_f}
                    \right)^k \Vert x_0 - \bar x_0\Vert^2. 
                \end{align*}
            \end{theorem}
            \begin{proof}
                Our proof makes use of the following properties which we label it in advance for swift exposition: 
                \begin{enumerate}
                    \item Inequality \eqref{ineq:projg-variational-ineq}, from the projected gradient and convexity of $X$. 
                    \item $f \in \mathbb S'$ which is the hypothesis that $f$ is Q-CNVX. 
                    \item $\alpha_k \le L_f^{-1}$, the stepsize is sufficient to apply descent lemma globally. 
                    \item $f \in \mathbb Q$ satisfying Q-Growth, a consequence of Q-CNVX by Theorem \ref{thm:q-cnvx-hierarchy}. 
                \end{enumerate}
                With $\overline{(\cdot)} = \Pi_{X^+}(\cdot)$ to denote the projection of a vector to the set of minimizers. 
                The sequence of inequalities and equalities proves the theorem. 
                {\allowdisplaybreaks
                \begin{align*}
                    \Vert x_{k + 1} - \bar x_k\Vert^2
                    &= 
                    \Vert x_{k + 1} - x_k + x_k - \bar x_k\Vert^2 
                    = \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2 + 2\langle x_{k + 1} - x_k, x_k - \bar x_k\rangle
                    \\
                    &= (- \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2)
                    + 2\Vert x_{k + 1} - x_k\Vert^2 + 2\langle x_{k + 1} - x_k, x_k - \bar x_k\rangle
                    \\
                    &= - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    + 2 \langle x_{k + 1} - x_{k}, x_{k + 1} - \bar x_k\rangle
                    \\
                    &= 
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    \\  & \quad \;
                        + 2 \langle x_{k + 1} - x_{k} + \alpha_k \nabla f(x_k), x_{k + 1} - \bar x_k\rangle
                        - 2\alpha_k \langle \nabla f(x_k), x_{k + 1} - \bar x_k\rangle
                    \\
                    &\underset{\text{(i)}}{\le}
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    - 2\alpha_k \langle \nabla f(x_k), x_{k + 1} - \bar x_k\rangle
                    \\
                    &= 
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    + 2\alpha_k \langle \nabla f(x_k), \bar x_k - x_k\rangle
                    + 2\alpha_k \langle \nabla f(x_k), x_k - x_{k + 1}\rangle
                    \\
                    &\underset{\text{(ii)}}{\le}
                    - \Vert x_{k + 1} - x_k\Vert^2 + \Vert x_k - \bar x_k\Vert^2
                    \\ &\quad 
                        + 2\alpha_k \left(
                            f^+ - f(x_k) - \frac{\kappa_f}{2}\Vert x_k - \bar x_k\Vert^2
                        \right)
                        + 2\alpha_k \langle \nabla f(x_k), x_k - x_{k + 1}\rangle
                    \\
                    &= (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2
                    \\&\quad 
                        + 2\alpha_k(f^+ - f(x_k)) - 2\alpha_k 
                        \left(
                            \langle \nabla f(x_k), x_{k + 1} - x_k\rangle + \frac{1}{2\alpha_k}\Vert x_{k + 1} - x_k\Vert^2
                        \right)
                    \\
                    &= 
                    (1 - \alpha_k\kappa_f)\Vert x_k - \bar x_k\Vert^2 + 2 \alpha_k f^+
                    \\&\quad 
                        - 2 \alpha_k\left(
                            f(x_k) + \langle \nabla f(x_k), x_{k + 1} - x_k\rangle 
                            + \frac{1}{2\alpha_k}\Vert x_{k + 1} - x_k\Vert^2
                        \right)
                    \\
                    &\underset{\text{(iii)}}{\le} 
                    (1 - \alpha_k\kappa_f)\Vert x_k - \bar x_k\Vert^2 + 2 \alpha_k f^+
                    \\ &\quad 
                        - 2 \alpha_k\left(
                            f(x_k) + \langle \nabla f(x_k), x_{k + 1} - x_k\rangle 
                            + \frac{L_f}{2}\Vert x_{k + 1} - x_k\Vert^2
                        \right)
                    \\
                    &\le 
                    (1 - \alpha_k\kappa_f)\Vert x_k - \bar x_k\Vert^2 + 2 \alpha_k f^+
                    - 2\alpha_kf(x_{k + 1})
                    \\
                    &\underset{\text{(iv)}}{\le} 
                    (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2 - \alpha_k \kappa_k \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2. 
                \end{align*}
                }
                Therefore, it has 
                \begin{align*}
                    0 &\le \Vert x_{k + 1} - \bar x_k\Vert^2 - \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2
                    \\
                    &\le 
                    (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2 
                    - \alpha_k \kappa_k \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2
                    - \Vert x_{k + 1} - \bar x_{k + 1}\Vert^2
                    \\
                    &= (1 - \alpha_k \kappa_f)\Vert x_k - \bar x_k\Vert^2 
                    - (1 + \alpha_k \kappa_k)\Vert x_{k + 1} - \bar x_{k + 1}\Vert^2. 
                \end{align*}
                Unrolling recursively, then use (iii), the claim is proved. 
            \end{proof}

        \subsection{Application, KKT of linear programming}
            This section extends and ideas in the discussion section of Necoara et al. \cite{necoara_linear_2019}. 
            \par
            Let $X_1, X_2, Y$ be Hilbert spaces. 
            Define linear mapping $E:X_1 \times X_2 \rightarrow Y := (x_1, x_2)\mapsto E_1 x_1 + E_2 x_2$ where $E_1, E_2$ each are mappings of $X_1 \rightarrow Y, X_2 \rightarrow Y$. 
            Denote the adjoint of linear mapping by $(\cdot)^*$. 
            Let $c = (c_1, c_2) \in X_1 \times X_2$, $b \in Y$. 
            Suppose that $\mathcal K \subseteq X_1$ is a simple cone and $K^*$ is its dual cone. 
            We consider the following linear programming problem 
            \begin{align}\label{problem:lp-cannon-form}
                \inf_{x \in X_1\times X_2}\left\lbrace
                    \langle - c, x\rangle
                    \left| \;
                        Ex = b, x \in \mathcal K \times X_2
                    \right.
                \right\rbrace. 
            \end{align}
            Define linear mapping $g, F$ and indicator function $h$ by the following: 
            \begin{align*}
                g:X_1\times X_2 \rightarrow \RR 
                    &:= x \mapsto \langle - c, x\rangle, 
                \\
                F: X_1\times X_2 \rightarrow Y \times X_1 
                    &:= (x_1, x_2) \mapsto (E_1x_1 + E_2 x_2, x_1),
                \\
                h: Y \times X_1 \rightarrow \overline \RR &:= 
                    (y, z) \mapsto \delta_{\{\mathbf 0\}}(y - b) + \delta_{\mathcal K}(z). 
            \end{align*}
            It's not hard to identify that problem in \eqref{problem:lp-cannon-form} has representations 
            \begin{align*}
                \inf_{x \in X_1\times X_2}
                \left\lbrace
                    g(x) + h(Fx)
                \right\rbrace. 
            \end{align*}
            The dual problem of the above is given by
            \begin{align*}
                -\inf_{u \in Y\times X_1}
                \left\lbrace
                    h^\star(u) + g^\star(-F^* u)
                \right\rbrace. 
            \end{align*}
            Where $h^\star, g^\star$ are the conjugate of $h, g$ and $F^*: Y\times X_1 \rightarrow X_1 \times X_2 = (y, z)\mapsto (E_1^*y + z, E_2^*y)$ is the adjoint operator of $F$. 
            Note that $g^\star(x) = \delta_{\mathbf 0}(x + c)$ and $h^\star((y, z)) = \langle b, y\rangle + \delta_{\mathcal K^*}(z)$. 
            This gives the following dual problem 
            \begin{align*}
                - \inf_{(y, z) \in Y \times \mathcal K^*} \left\lbrace
                    \langle b, y\rangle 
                    \left | \;
                        E_1^*y + z = c_1, 
                        E^*_2y = c_2
                    \right.
                \right\rbrace. 
            \end{align*}
            The KKT conditions give the following convex feasibility problem 
            \begin{align*}
                E_1 x_1 + E_2 x_2 &= b, \\
                E_1^* y + z &= c_1, \\
                E_2^* y &= c_2, \\
                \langle b, y\rangle &= \langle c_1, x_1\rangle + \langle c_2, x_2\rangle, \\
                (x_1, x_2) &\in \mathcal K \times X_2, \\
                (y, z) &\in Y \times \mathcal K^*.
            \end{align*}
            Allow $X_1 = \RR^{n_1}, X_2 = \RR^{n_2}, Y = \RR^m$. 
            Define 
            \begin{align*}
                \mathbf K &:= \mathcal K \times \RR^{n_2} \times \RR^m \times \mathcal K^*, 
                \\
                A &:= \begin{bmatrix}
                    E_1 & E_2 & \mathbf 0 & \mathbf 0
                    \\
                    \mathbf 0 &\mathbf 0  & E_1^T & I_{n_1}
                    \\
                    \mathbf 0 &\mathbf 0  & E_2^T & \mathbf 0
                    \\
                    c_1^T & c_2^T & - b^T & 0
                \end{bmatrix}, 
                v := 
                \begin{bmatrix}
                    x_1\\ x_2 \\ y \\ z \\
                \end{bmatrix} \in \mathbf K, 
                d := 
                \begin{bmatrix}
                    b \\ c_1 \\ c_2 \\ 0
                \end{bmatrix}. 
            \end{align*}
            The KKT conditions is a convex feasibility problem which can be formulated by best approximation problem: 
            \begin{align}\label{problem:lp-kkt-min}
                \min_{v \in \mathbf K} 
                \frac{1}{2}\Vert Ax - d \Vert^2. 
            \end{align}
            It is minimizing a quadratic problem on a simple cone. 
            Solving \eqref{problem:lp-cannon-form} can be approached by optimizing \eqref{problem:lp-kkt-min}. 
            It's necessary to investigate the matrices $A, A^T$ which are essential to solving it numerically. 
            The properties of $A^TA$ will determine the convergence rate of algorithms. 
            The matrix is a block matrix and possibly sparse in practice. 
            Let $v = (x_1, x_2, y, z)$, it admits implicit representation: 
            \begin{align*}
                Av = (E_1x_1 + E_2 x_2,\; E_1^Ty + z,\; E_2^Ty,\; c^T_1x_1 + c_2^Tx_2 - b^Ty). 
            \end{align*}
            It involves 
            \begin{enumerate}
                \item Two multiplications of $E$: $x_1, x_2$ on the right and $y$ on the right,  
                \item inner product using $x_1, x_2$ and $y$. 
            \end{enumerate}
            
            Let $\bar v = (\bar y, \bar x_1, \bar x_2, \xi) \in \RR^m\times \RR^{n_1} \times \RR^{n_2}\times \RR$ then the right multiplication of has: 
            \begin{align*}
                \bar v^TA  &= (
                    E_1^T\bar y + \xi c_1^T,\; E_2^T\bar y + \xi c_2^T,\; 
                    \bar x_1^TE_1^T + \bar x_2^TE_2^T - \xi b^T,\; \bar x_1^T
                )
                \\
                &= 
                (
                    E_1^T\bar y + \xi c_1, \;
                    E_2^T \bar y + \xi c_2, \;
                    E_1\bar x_1 + E_2\bar x_2 - \xi b, \;
                    \bar x_1
                )^T. 
            \end{align*}
            \begin{enumerate}
                \item Two multiplications of $E$: $\bar y$ on the left and for $\bar x_1, \bar x_2$ on the right, 
                \item one vector addition with $c = (c_1, c_2)$ and $b$. 
            \end{enumerate}
            Therefore, computing $A^TAv$ has four vector multiplications using $E$. 
            In practice, a sparse matrix $E$ from the model can speed up computations. 
            \par
            Another key operation would be $A^TAv$. 
            Let $\bar v = Av$, then 
            \begin{align*}
                A^TAv &= 
                \begin{bmatrix}
                    E^T_1(E_1x_1 + E_2x_2) + (c_1^Tx_1 + c_2^Tx_2 - b^Ty)c_1
                    \\
                    E^T_2(E_1x_1 + E_2x_2) + (c_1^Tx_1 + c_2^Tx_2 - b^Ty)c_2
                    \\
                    E_1(E_1^Ty + z) + E_2E_2^Ty - (c_1^Tx_1 + c_2^Tx_2 - b^Ty)b
                    \\
                    E_1^Ty + z
                \end{bmatrix}
                \\
                &= 
                \begin{bmatrix}
                    (E_1^TE_1 + c_1^T)x_1 + (E_1^TE_2 + c_2^T)x_2 - (c_1b^T)y
                    \\
                    (E_2^TE_1 + c_1^T)x_1 + (E_2^TE_2 + c_2^T)x_2 - (c_2b^T)y
                    \\
                    -(bc_1^T)x_1 - (bc_2^T)x_2 + (E_2E_2^T + E_1E_1^T + bb^T)y
                    + (E_1E_1^T)z
                    \\
                    E_1^Ty + z
                \end{bmatrix}
                \\
                &= 
                \begin{bmatrix}
                    E_1^TE_1 + c_1^T & E_1^TE_2 + c_2^T & -c_1b^T & \\
                    E_2^TE_1 + c_1^T & E_2^TE_2 + c_2^T & -c_2b^T & \\
                    -bc_1^T& -bc_2^T & E_2E_2^T + E_1E_1^T + bb^T & E_1E_1^T \\
                    & & & E_1^Ty + z\\
                \end{bmatrix}
                \begin{bmatrix}
                    x_1 \\ x_2 \\ y \\ z
                \end{bmatrix}. 
            \end{align*}
            In practice, implicitly representing the process of $A^TAv$ is better in computing software. 
            Here we write it out to view, for theoretical interests. 
            \par
            Let $f(v) = (1/2)\Vert Av - d\Vert^2$ to be the objective function of optimization problem \eqref{problem:lp-kkt-min}. 
            Its gradient, objective value, and Bregman Divergence have: 
            \begin{align*}
                \nabla f(v) &= A^TAv - A^Td, 
                \\
                f(v) &= 
                \frac{1}{2}\langle v, \nabla f(v) - A^Td\rangle + \frac{1}{2}\Vert d\Vert^2, 
                \\
                D_f(u, v) &= (1/2)\langle u - v, A^TA (u - v)\rangle
                \\
                &= (1/2)\langle \nabla f(u) - \nabla f(v), u - v\rangle. 
            \end{align*}
            The value $\nabla f(v), f(v)$ when evaluated together, require minimal additional computations. 
            This fact is favorable for implementations in practice. 
            Furthermore, the difference of the function value between 2 points $v, u$ admits an interesting relation via the Bregman Divergence. 
            Observe that $\forall u, v \in \RR^n$ it has 
            \begin{align*}
                f(u) - f(v) &= \langle \nabla f(v), u - v \rangle + D_f(u, v)
                \\
                &= \langle \nabla f(v), u - v \rangle + (1/2)\langle \nabla f(u) - \nabla f(v), u - v\rangle
                \\
                &= (1/2)\langle \nabla f(u) + \nabla f(v), u - v\rangle. 
            \end{align*}
            For this problem, the computation overhead for $f(u) - f(v), D_f(u, v)$ is very little if $\nabla f(u), \nabla f(v)$ is known. 

\chapter{Advanced Enhancement Techniques in Accelerated Proximal Gradient}
    We review advanced enhancement techniques in Accelerated Proximal Gradient method. 
    The review will be based on several papers. 
    \par
    There are several notable enhancements of the FISTA for function that are not strongly convex. 
    Monotone variants of FISTA proposed by Beck \todo{[?]} and Nesterov \todo{[?]} imposes monotonicity in function value at the iterates.  
    Backtracking strategies from Chambolle \todo{[?]} shows that the underestimating Lipschitz constant using a backtracking technique to choose a next iterate improves the average runtime of the algorithm in practice. 
    They showed that the convergence rate is bounded by the estimates of the Lipschitz constant. 
    Restart is a technique pioneer early by ??? \todo{[?]}. 
    Necoara et al. \cite{necoara_linear_2019} showed that there exists an optimal restarting interval to achieve fast line convergence rate for all functions with quadratic growth condition. 
    \par
    In this chapter, we will go through the details of these enhancements of FISTA and discuss why they are important in theories, and in practice. 
    \section{Preliminaries}
        This section introduces the full scope of the theories in our analysis. 
        \subsection{smooth plus nonsmooth weakly convex}
            \begin{assumption}[sum of weakly convex smooth and nonsmooth]\;\label{ass:sum-of-wcnvx}\\
                Let $F: \RR^n \rightarrow \overline\RR:= f + g$ such that $f, g$ satisfy 
                \begin{enumerate}
                    \item $f$ is $L$ Lipschitz smooth and $q_f$ weakly convex. 
                    \item $g$ is $L$ Lipschitz smooth and $q_g$ weakly convex. 
                \end{enumerate}
            \end{assumption}
            \begin{remark}
                If a function is $L$ smooth, it also be $L$ weakly convex. 
                Here we defined $q_f$ because the actual weakly convex constant may be much smaller than $L$, and it is true in the case when $g$ is in fact convex.  
            \end{remark}
            \begin{definition}[weakly convex function]\;\label{def:wcnvx-fxn}\\
                Let $f: \RR^n \rightarrow\overline \RR$ be an l.s.c proper function. 
                We define $F$ to be $q$ weakly convex if there exists $q \in \ge 0$ such that the function $F + q/2\Vert \cdot\Vert^2$ is a convex function and $q$ is the infimum of all such possible parameters. 
            \end{definition}
            \begin{remark}
                If $q = 0$, $f$ is convex or strongly convex. 
            \end{remark}
            \begin{definition}[gradient mapping]
                Suppose $F = f + g$ satisfies Assumption \ref{ass:sum-of-wcnvx}, define the gradient mapping for all $x \in \RR^n$
                \begin{align*}
                    \mathcal G_{\beta^{-1}, f, g}(x) = \beta(x - T_{\beta^{-1}, f, g}(x)). 
                \end{align*}
                If $f, g$ are clear in the context then we omit subscript and present $\mathcal G_\beta$. 
            \end{definition}
            \begin{lemma}[weakly convex monotone descent]\;\label{lemma:mono-wcnvx-descent}\\
                Let $F = f + g$ satisfies Assumption \ref{ass:sum-of-wcnvx}. 
                Let $\bar x = T_{\beta, f, g}(x)$. 
                Then, for all $x \in \RR^n$, it has the following inequality: 
                $$
                \begin{aligned}
                    0 \le F(x) - F(\bar x) - (\beta - q/2 - L/2)\Vert x - \bar x\Vert^2. 
                \end{aligned}
                $$
                And descent is possible when $\beta \ge (L + q)/2$. 
                The maximum amount of descent is achieved when $\beta = L + q$. 
            \end{lemma}
            \begin{proof}
                Use Theorem \ref{thm:pg-ineq-wcnvx-generic}. 
            \end{proof}
        \subsection{smooth plus nonsmooth convex}
            The following assumption is strictly stronger than \ref{ass:sum-of-wcnvx}. 
            \begin{assumption}[convex smooth and nonsmooth]\label{ass:standard-fista}
                Let $F = f + g$ where $f:\RR^n\rightarrow \overline \RR$ is $L$ Lipschitz smooth, $g$ is convex. 
                Suppose that $\argmin_{x \in \RR^n} F(x)\neq \emptyset$. 
            \end{assumption}
            \begin{lemma}[proximal gradient inequality]
                If $F = f + g$ satisfies Assumption \ref{ass:standard-fista}, then for all $x \in \RR^n, z \in \RR^n$, define $\bar x = T_{L^{-1}, f, g}(x)$ it has 
                \begin{align*}
                    0 &\le F(z) - F(\bar x) + \frac{L}{2}\Vert z - x\Vert^2 - \frac{L}{2}\Vert z - \bar x\Vert^2. 
                \end{align*}
            \end{lemma}
            \begin{proof}
                Use Theorem \ref{thm:cnvx-pg-ineq}. 
            \end{proof}
            
    \section{FISTA made simple}
        We make the proofs for FISTA  with common enhancement technique available in simple proofs. 
        We showcase the theories using a generic similar triangle representations of the algorithm which tremendously simplifies the arguments. 
        The following definition can capture monotone variants of FISTA with line search, including backtracking strategies. 
        \par
        ``MAPG'' stands for monotone accelerated gradient. 
        We refer to ``Generic Monotone Accelerated Proximal Gradient with line search" as ``GMAPG''. 
        \begin{definition}[GMAPG]\label{def:generic-mapg-ls}\;\\ 
            Initialize any $x_0, v_0 \in \RR^n$. 
            Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_k \in (0, 1) \;\forall k \ge 0$ and $\alpha_0 \in (0, 1]$. 
            \begin{tcolorbox}
                The algorithm makes sequences $(x_k, v_k, y_k)_{k \ge 1}$, such that for all $k = 1, 2, \ldots$ they satisfy: 
                \begin{align*}
                    & y_k = \alpha_k v_{k - 1} + (1 - \alpha_k) x_{k - 1}, \\
                    & \tilde x_k = T_{L_k^{-1}}(y_{k}), \\ 
                    & v_k = x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1}), \\
                    & D_{f}(\tilde x_k, y_k) \le \frac{L_k}{2}\Vert \tilde x_k - y_k\Vert^2, \\
                    & \text{Choose any } x_k: F(x_k) \le \min(F(\tilde x_k), F(x_{k - 1})). 
                \end{align*}    
            \end{tcolorbox}
        \end{definition}
        \begin{lemma}[acceerated proximal gradient iterates relation]\;\label{lemma:apg-iterates}\;\\
            The iterates $(x_k, v_k, y_k)_{k \ge 1}$ generated by Definition \ref{def:generic-mapg-ls}. 
            Let $z_k = \alpha_k x^+ + (1 - \alpha_k)x_{k - 1}$. 
            Then it has for all $k \ge 1$ that: 
            \begin{align*}
                z_k - \tilde x_k &= \alpha_k(x^+ - v_k)
                \\
                x_k - y_k &= \alpha_k(x^+ - v_{k - 1}). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            It's direct from the algorithm. 
            \begin{align*}
                z_k - \tilde x_k &= (\alpha_k x^+ + (1 - \alpha_k)x_{k - 1}) - \tilde x_k
                \\
                &= \alpha_k (x^+ + \alpha_k^{-1}(1 - \alpha_k)x_{k - 1} - \alpha_k^{-1}\tilde x_k)
                \\
                &= \alpha_k(x^+ + \alpha_k^{-1}x_{k - 1} - x_{k - 1} - \alpha_k^{-1}\tilde x_k)
                \\
                &= \alpha_k(x^+ + \alpha_k^{-1}(x_{k - 1} - \tilde x_k) - x_{k - 1})
                \\
                &= \alpha_k(x^+ - v_{k}), 
                \\
                z_k - y_k &= 
                (\alpha_k x^+ + (1 - \alpha_k)x_{k - 1}) - \left(
                    \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                \right)
                \\
                &= \alpha_k(x^+ + \alpha_k^{-1}(1 - \alpha_k)x_{k - 1} - v_{k - 1} - \alpha_k^{-1}(1 - \alpha_k)x_{k - 1})
                \\
                &= \alpha_k(x^+ - v_{k - 1}). 
            \end{align*}
        \end{proof}
        \begin{theorem}[generic GMAPG convergence]\; \label{thm:gmapg-ls-convergence}\;\\
            Let $F = f + g$ satisfy Assumptions \ref{ass:standard-fista}. 
            Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_k \in (0, 1)$ for all $k \ge 1$ and $\alpha_0 \in (0, 1]$. 
            Let $\rho_k = (1 - \alpha_{k + 1})^{-1}\alpha_{k + 1}^2 \alpha_k^{-2}$ for all $k \ge 0$. 
            Then, for all $x^+ \in \RR^n, k \ge 1$, the convergence rate of GMAPG-LS (Definition \ref{def:generic-mapg-ls}) is given by: 
            \begin{align*}
                & \beta_k := \prod_{i = 0}^{k - 1} (1 - \alpha_{i + 1})
                \max\left(1, \rho_i L_{i + 1}L_i^{-1}\right), 
                \\
                & F(x_k) - F(x^+) + \frac{L_k\alpha_k}{2}\Vert x^+ - v_k\Vert^2
                \le 
                \beta_k
                \left(
                    F(x_0) - F(x^+) + \frac{L_0\alpha_0}{2} \Vert x^+ - v_0\Vert^2
                \right). 
            \end{align*}
            If in addition, the algorithm is initialized with $L_0 \ge L$, $\alpha_0 = 1, x_0 = v_0 = T_{L_{0}}x_{-1} \in \text{dom}\; F$ and $x^+$ is a minimizer of $F$.
            Then, the convergence rate simplifies: 
            \begin{align*}
                F(x_k) - F(x^+) + \frac{L_k\alpha_k}{2}\Vert x^+ - v_k\Vert^2
                & \le 
                \frac{\beta_kL_0}{2}\Vert x^+ - x_{-1}\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Define $z_k = \alpha_k x^+ + (1 - \alpha_k)x_{k - 1}$ for all $k \ge 1$. 
            In the proof follows, the follow facts will be used. 
            We list them in advance, and they will be labeled during the proof. 
            \begin{enumerate}
                \item Lemma \ref{lemma:apg-iterates}. 
                \item The sequence $(\alpha_k)_{k \ge 1}$ has for all $k \ge 1$, $1 - \alpha_k = \alpha_k^2\alpha_{k - 1}^2\rho_{k - 1}$, $\alpha_k \in (0, 1)$. 
                \item $F$ is convex and hence $F(z_k) \le \alpha_k F(x^+) + (1 - \alpha_k)F(x_{k - 1})$. 
                \item $F(x_k) \le F(\tilde x_k)$ which is true by definition of GMAPG. 
            \end{enumerate}
            Now, using Theorem \ref{thm:cnvx-pg-ineq}, it has for all $k \in \N$: 
            {\allowdisplaybreaks\small
            \begin{align*}
                0 &\le 
                F(z_k) 
                - F(\tilde x_k) - \frac{L_k}{2}\Vert z_k - \tilde x_k\Vert^2 + 
                \frac{L_k}{2}\Vert z_k - y_k\Vert^2
                \\
                &\underset{\text{(i)}}{=}
                F(\alpha_k x^+ + (1 - \alpha_k)x_{k - 1}) - F(\tilde x_k)
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k \Vert^2 
                + \frac{L_k\alpha_k^2}{2}\Vert(x^+ - v_{k - 1})\Vert^2
                \\
                &\hspace{-0.3em}\underset{\text{(iii)}}{\le} 
                \alpha_k F(x^+) + (1 - \alpha_k) F(x_{k - 1}) - F(\tilde x_k)
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k \Vert^2 
                + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1} \Vert^2
                \\
                &= 
                (\alpha_k - 1)F(x^+) + (1 - \alpha_k) F(x_{k - 1}) + F(x^+) - F(\tilde x_k)
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k \Vert^2 
                + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1}\Vert^2
                \\
                &= 
                (1 - \alpha_k)(F(x_{k - 1}) - F(x^+)) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1}\Vert^2
                - \left(
                    F(\tilde x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \right)
                \\
                &\hspace{-0.3em}\underset{\text{(iv)}}{\le} 
                (1 - \alpha_k)(F(x_{k - 1}) - F(x^+)) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_{k - 1}\Vert^2
                - \left(
                    F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \right)
                \\
                &= 
                (1 - \alpha_k)(F(x_{k - 1}) - F(x^+)) + 
                \left(
                    \frac{\alpha_k^2}{\alpha_{k - 1}^2\rho_{k - 1}}
                \right)
                \frac{L_{k - 1}\alpha_{k - 1}^2(\rho_{k - 1}L_kL_{k - 1}^{-1})}{2}\Vert x^+ - v_{k-1}\Vert^2 \\
                    &\quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right)
                \\
                &= 
                \left(
                    1 - \alpha_k
                \right)\left(
                    F(x_{k - 1}) - F(x^+) + \frac{L_{k - 1}\alpha_{k - 1}^2(\rho_{k - 1}L_kL_{k - 1}^{-1})}{2}
                    \Vert x^+ - v_{k - 1}\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right)
                \\
                &\le 
                \left(
                    1 - \alpha_k
                \right)\left(
                    F(x_{k - 1}) - F(x^+) + \frac{L_{k - 1}\alpha_{k - 1}^2\max(1, \rho_{k - 1}L_kL_{k - 1}^{-1})}{2}
                    \Vert x^+ - v_{k - 1}\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right)
                \\
                &\le 
                \left(
                    1 - \alpha_k
                \right)\max(1, \rho_{k - 1}L_kL_{k - 1}^{-1})
                \left(
                    F(x_{k - 1}) - F(x^+) + \frac{L_{k - 1}\alpha_{k - 1}^2}{2}
                    \Vert x^+ - v_{k - 1}\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right). 
            \end{align*}
            }
            Unroll recursively for $k, k-1, \ldots, 0$, it implies: 
            \begin{align*}
                0
                &\le 
                \left(
                    \prod^{k - 1}_{i = 0} (1 - \alpha_{i + 1})\max(1, \rho_{i}L_{i + 1}L^{-1}_i)
                \right)\left(
                    F(x_0) - F(x^+) + \frac{L_0 \alpha_0}{2}\Vert x^+ - v_0\Vert^2
                \right) \\
                    & \quad 
                    - \left(
                        F(x_k) - F(x^+) + \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                    \right). 
            \end{align*}
            If in addition, we assume that $x^+$ is a minimizer of $F$, and $\alpha_0 = 1, x_0 = v_0 = T_{L_0}x_{-1}$. 
            Using Theorem \ref{thm:cnvx-pg-ineq} it gives: 
            \begin{align*}
                0 &\le 
                F(x^+) - F(T_{L_{-1}}x_{-1}) - \frac{L_0}{2}\Vert x^+ - T_{L_0}x_{-1}\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2
                \\
                &= 
                F(x^+) - F(x_0) - \frac{L_0}{2}\Vert x^+ - v_0\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2. 
            \end{align*}
            Substituting it back to the previous inequality it yields the desired results. 
        \end{proof}
        \begin{remark}
            The sequence has explicit update formula: 
            \begin{align*}
                \alpha_k = 
                \frac{1}{2}
                \left(
                    \alpha_{k - 1}\sqrt{\alpha_{k -1}^2 + 4 \rho_{k - 1}} - \alpha^2_{k - 1}
                \right)
            \end{align*}
        \end{remark}
        \begin{theorem}[generic GMAPG gradient mapping convergence]\;\label{thm:gmapg-generic-gradmap-convergence}\\
            Suppose that $F = f + g$ satisfies Assumption \ref{ass:standard-fista}. 
            Let the sequences $(x_k, y_k, v_k)$ satisfy GMAPG (Definition \ref{def:generic-mapg-ls}). 
            If in addition, the sequence $(\alpha_k)_{k \ge 0}$ has $\alpha_0 = 1$ and, GMAPG is initialized with $L_0 \ge L$, $v_0=x_0 = T_{1/L_0, f, g}(x_{-1})$ for any $x_{-1} \in\RR^n$ and there exists $x^+$ which is a minimizer of $F$. 
            Then, the convergence of gradient mapping $\Vert \mathcal G_{1/L_k}(y_k)\Vert$ is described by the sequence $(\beta)_{k \ge 0}$ where $\beta_0 = 1$ and for all $k \ge 1$:
            \begin{align*}
                \beta_k := \prod_{i = 0}^{k - 1}(1 - \alpha_{i + 1}) \max\left(1, \rho_i L_{i + 1}L_i^{-1}\right). 
            \end{align*}
            It satisfies for all $k \ge 1$ the inequality:
            \begin{align*}
                \Vert \mathcal G_{1/L_k} (y_k)\Vert &\le 
                \sqrt{\beta_k}L_k L_0 \left(
                    1 - 
                    \min(\rho_{k - 1}, L_k^{-1} L_{k - 1})^{1/2}
                \right)\Vert x^+ - v_0\Vert. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            The following intermediate results will be used in the proof. 
            We stated prior to the proof for a sleeker exposition. 
            \begin{enumerate}
                \item[(a)] From Definition \ref{def:generic-mapg-ls}, the gradient mapping satisfies for all $k \ge 1$ that $\Vert \mathcal G_{1/L_k} y_k\Vert = L_k\alpha_k \Vert v_k - v_{k - 1}\Vert$.
                \item[(b)] We have $(a_k)_{k \ge 1}$ satisfying $\forall k \ge 1$ that $(1 - \alpha_k)\rho_{k - 1} = \alpha_k^2/\alpha_{k - 1}^2$ from the statement hypothesis. We assumed $\alpha_0 = 0, \beta_0 = 1$, $x^+$ is a minimizer of $F$. Then using these it has for all $k \ge 0$ it has $\frac{\alpha_k}{\sqrt{\beta_k L_0}}\Vert x^+ - v_k\Vert \le \Vert x^+ - v_0\Vert$. 
                \item[(c)] The sequence $(\alpha_k)_{k \ge 0}$ has $(1 - \alpha_k)\rho_{k - 1} = \alpha_k^2/\alpha_{k - 1}^2$ from the statement hypothesis so $\alpha_k/\alpha_{k - 1} = \sqrt{\rho_{k - 1}(1 - \alpha_k)}$ for all $k \ge 1$. 
                \item[(d)] The definition of the sequence $(\beta_k)_{k \ge 0}$ stated in the statement hypothesis. 
            \end{enumerate}
            From these intermediate results, the convergence can be derived. 
            From (a) it has for all $k \ge 0$: 
            \begin{align*}
                \Vert \mathcal G_{1/L_k} y_k\Vert 
                &= L_k\alpha_k \Vert v_k - v_{k - 1}\Vert
                \\
                &\le 
                L_k\alpha_k(\Vert v_k - x^+\Vert + \Vert v_{k - 1} - x^+\Vert)
                \\
                &\underset{\text{(b)}}{\le} 
                L_k \alpha_k \left(
                    \frac{\sqrt{\beta_kL_0}}{\alpha_k}\Vert x^+ - v_0\Vert
                    +
                    \frac{\sqrt{\beta_{k - 1}L_0}}{\alpha_{k - 1}}\Vert x^+ - v_0\Vert
                \right) 
                \\
                &= L_k\sqrt{L_0} \left(
                    \sqrt{\beta_k}
                    +
                    \frac{\alpha_k\sqrt{\beta_{k - 1}}}{\alpha_{k - 1}}
                \right)\Vert x^+ - v_0\Vert
                \\
                &= \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \frac{\alpha_k}{\alpha_{k - 1}}\sqrt{\frac{\beta_{k - 1}}{\beta_k}}
                \right)\Vert x^+ - v_0\Vert
                \\
                &\underset{\text{(d)}}{=} \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \frac{\alpha_k}{\alpha_{k - 1}}
                    \left((1 - \alpha_k)\max(1, \rho_{k - 1}L_k L_{k - 1}^{-1})\right)^{-1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &\underset{\text{(c)}}{=} 
                \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    ((1 - \alpha_k)\rho_{k - 1})^{1/2}
                    \left((1 - \alpha_k)\max(1, \rho_{k - 1}L_k L_{k - 1}^{-1})\right)^{-1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &= 
                \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \left(\rho_{k - 1}^{-1}\max(1, \rho_{k - 1}L_k L_{k - 1}^{-1})\right)^{-1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &=
                \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \max(\rho_{k - 1}^{-1}, L_k L_{k - 1}^{-1})^{-1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &= 
                \sqrt{\beta_k L_0}L_k \left(
                    1 +
                    \min(\rho_{k - 1}, L_k^{-1} L_{k - 1})^{1/2}
                \right)\Vert x^+ - v_0\Vert. 
            \end{align*}
            Now, \textbf{let's proof intermediate results (a)}. 
            From the definition of GMAPG it has 
            \begin{align*}
                y_k &= \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                \iff 
                v_{k - 1} = \alpha_k^{-1}(y_k - (1 - \alpha_k)x_{k - 1}). 
            \end{align*}
            Using the above, and definition of GMAPG, it yields 
            \begin{align*}    
                v_k - v_{k - 1} &= 
                (x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})) - \alpha_k^{-1}(y_k - (1 - \alpha_k)x_{k - 1})
                \\
                &= 
                x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})
                - \alpha_k^{-1}y_k + (\alpha_k^{-1} - 1)x_{k - 1}
                \\
                &= \alpha_k^{-1}(\tilde x_k - x_{k - 1}) - \alpha_k^{-1}y_k + \alpha_k^{-1} x_{k - 1}
                \\
                &= \alpha_k^{-1}\tilde x_k - \alpha_k^{-1} y_k 
                = \alpha^{-1}_k(\tilde x_k - y_k) = \alpha_k^{-1}(T_{L_k} y_k - y_k)
                \\
                &= -\alpha_k^{-1}L_k^{-1}(\mathcal G_{1/L_k}(y_k)). 
            \end{align*}
            \textbf{We now prove result (b)}. 
            The base case $k = 1$ is verified by the assumption that $x_0 = v_0 = T_{L_0} x_{-1}$. 
            Apply the proximal gradient inequality with $x^+$ as a minimizer it yields: 
            \begin{align*}
                0 &\le 
                F(x^+) - F(T_{L_{-1}}x_{-1}) - \frac{L_0}{2}\Vert x^+ - T_{L_0}x_{-1}\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2
                \\
                &= 
                F(x^+) - F(x_0) - \frac{L_0}{2}\Vert x^+ - v_0\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2
                \\
                &\le 
                - \frac{L_0}{2}\Vert x^+ - v_0\Vert^2 + 
                \frac{L_0}{2}\Vert x^+ - x_{-1}\Vert^2
                \\
                &= \frac{L_0}{2}\left(
                    \Vert x^+ - x_{-1}\Vert- \Vert x^+ - v_0\Vert 
                \right). 
            \end{align*}
            Because $\beta_0 = \alpha_0 = 1$, the base case holds. 
            For all $k \ge 1$, we consider the convergence claim and use the assumption that $x^+$ is a minimizer of $F$ so, it has from Theorem \ref{thm:gmapg-ls-convergence} that 
            \begin{align*}
                0 &\le \frac{L_0\beta_k }{2}\Vert x^+ - x_{-1}\Vert^2 
                - F(x_k) + F(x^+) - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \\
                &\le 
                \frac{L_0\beta_k }{2}\Vert x^+ - x_{-1}\Vert^2 
                - \frac{L_k\alpha_k^2}{2}\Vert x^+ - v_k\Vert^2
                \\
                &= \frac{\alpha_k^2L_k}{2}\left(
                    \frac{\beta_k}{\alpha_k^2L_0}
                    \Vert x^+ - x_{-1}\Vert^2 
                    - \Vert x^+ - v_k\Vert^2
                \right)
                \\
                \iff 
                0 &\le 
                \Vert x^+ - x_{-1}\Vert - \frac{\alpha_k}{\sqrt{\beta_k L_0}}\Vert x^+ - v_k\Vert. 
            \end{align*}
        \end{proof}
        \begin{lemma}[specialized GMAPG momentum sequence]\;\label{lemma:gmapg-seq-bnd}\\
            Suppose that the sequence $(\alpha_k)_{k \ge 0}$ has $\alpha_0 \in (0, 1]$ and for all $k \ge 1$, it satisfies $\alpha_{k - 1}^2\rho_{k - 1}(1 - \alpha_k) = \alpha_k^2$. 
            Suppose that $(\beta_k)_{k \ge 0}$ is a sequence such that $\beta_0 = 1$ and for all $k \ge 1$ it's defined as: 
            \begin{align*}
                \beta_k := \prod_{i = 0}^{k - 1}(1 - \alpha_{i + 1}) \max\left(1, \rho_i L_{i + 1}L_i^{-1}\right). 
            \end{align*}
            If, we set $\rho_{k - 1} = L_{k}^{-1}L_{k - 1}$ such that $L_k >0$ for all $k \ge 1$, then for all $k \ge 1$ it has the inequality: 
            \begin{align*}
                \beta_k \le \left(
                    1 + \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                \right)^{-2}. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            We state the following intermediate results needed to construct the proof. 
            They will be proved at the end. 
            \begin{enumerate}
                \item[(a)] $(\beta_k)_{k \ge 0}$ is monotone decreasing and it's strictly larger than zero. 
                \item[(b)] Because $\rho_{k}L_{k + 1}L_k = 1$ for all $k \ge 0$, the definition of $(\beta_k)_{k\ge 0}$ simplifies and $\beta_k = (\alpha_k^2/\alpha_0^2)(L_k/L_0)$. As a consequence it also gives for all $k \ge 1$ that: 
                \begin{align*}
                    \alpha_k^2 &= \alpha_0^2\beta_kL_0L_k^{-1},
                    \\
                    \alpha_k &= 1 - \beta_k / \beta_{k - 1}. 
                \end{align*}
            \end{enumerate}
            Starting with results (b), and combine the two equality it gives for all $k \ge 1$ the equality 
            \begin{align*}
                0 &=
                (1 - \beta_k/\beta_{k - 1})^2 - \alpha_0^2L_0L_k^{-1}\beta_k 
                \\
                \iff 
                0 &= 
                (1 - \beta_k/\beta_{k - 1}) - \alpha_0\sqrt{L_0L_k^{-1}\beta_k}
                \\
                \iff 
                0 &= 
                (\beta_k^{-1} - \beta_{k - 1}^{-1}) - \alpha_0 \sqrt{L_0 L_k^{-1}\beta_k^{-1}}
                \\
                &= \left(
                    \sqrt{\beta_k^{-1}} + \sqrt{\beta_{k - 1}^{-1}}
                \right)\left(
                    \sqrt{\beta_k^{-1}} - \sqrt{\beta_{k - 1}^{-1}}
                \right)
                - \alpha_0 \sqrt{L_0L_k^{-1}\beta_k^{-1}}
                \\
                &\underset{\text{(a)}}{\le} 
                2\sqrt{\beta_k^{-1}}\left(
                    \sqrt{\beta_k^{-1}} - \sqrt{\beta_{k - 1}^{-1}}
                \right) - \alpha_0 \sqrt{L_0L_k^{-1}\beta_k^{-1}}
                \\
                \iff
                0 &\le 
                2\left(
                    \sqrt{\beta_k^{-1}} - \sqrt{\beta_{k - 1}^{-1}} 
                \right) - \alpha_0 \sqrt{L_0L_k^{-1}}. 
            \end{align*}
            Since this is true for all $k \ge 1$, taking a telescoping sum of the above series gives
            \begin{align*}
                0 &\le 
                \left(
                    \sum_{i = 1}^{k} \sqrt{\beta_i^{-1}} - \sqrt{\beta_{i - 1}^{-1}}
                \right)
                - \sum_{i = 1}^{k} \frac{\alpha_0}{2} \sqrt{L_0L_k^{-1}}
                \\
                &= 
                \sqrt{\beta_k^{-1}} - \sqrt{\beta_0^{-1}} 
                - \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{ L_k^{-1}}
                \\
                &= 
                \sqrt{\beta_k^{-1}} - 1
                - \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{ L_k^{-1}}. 
            \end{align*}
            Therefore, transforming the inequality it has: 
            \begin{align*}
                \beta_k &\le 
                \left(
                    1 + \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{ L_k^{-1}}
                \right)^{-2}. 
            \end{align*}
            \textbf{Let's now justify (a)}.
            When $\rho_{i} = L_{i + 1}L_i^{-1}$, the big product simplifies and, it has: 
            \begin{align*}
                \beta_k &= \prod_{i = 0}^{k - 1}(1 - \alpha_{i + 1}) 
                = (1 - \alpha_k)\beta_{k - 1}. 
            \end{align*}
            Since $\alpha_k \in (0, 1)$, $\beta_k$ is monotonically decreasing. 
            \textbf{To see (b)}, it has from the above which also justifies $1 - \alpha_k = \beta_k / \beta_{k - 1}$. 
            Recall that sequence $(\alpha_k)_{k \ge 0}$ has $\forall k \ge 1$ that $\alpha_{k - 1}^2\rho_{k - 1}(1 - \alpha_k) = \alpha_k^2$, using it we can simplify the product for $\beta_k$, it follows that 
            \begin{align*}
                \beta_k &= \prod_{i = 0}^{k - 1}\left(
                    1 - \alpha_{i + 1}
                \right)
                = 
                \prod_{i = 1}^{k}\alpha_i^2\alpha_{i - 1}^{-2}\rho_{i - 1}^{-1}
                = 
                \prod_{i = 1}^{k}\alpha_i^2\alpha_{i - 1}^{-2} L_i^{-1}L_{i - 1}
                \\
                &= \left(
                    \frac{\alpha_k^2\alpha_{k - 1}^2\ldots \alpha_1^2}
                    {\alpha_{k -1}^2\alpha_{k - 2}^2\ldots \alpha_0^2}
                \right)\left(
                    \frac{L_kL_{k - 1}\ldots L_1}{L_{k - 1}L_{k - 1}\ldots L_0}
                \right)
                = \frac{\alpha_k^2}{\alpha_0^2}\frac{L_k}{L_0}. 
            \end{align*}
            Rearranging it gives: $\alpha_0^2L_0 \beta_kL_k^{-1} = \alpha_k^2$.
        \end{proof}
        \begin{remark}
            A simpler upper bound is more useful in practice. 
            For all $k \ge 1$ let $\hat L_k = \max_{i = 0, 1, \ldots, k} L_i$ then 
            \begin{align*}
                \beta_k 
                &\le \left(
                    1 + 
                    \frac{\alpha_0 \sqrt{L_0}}{2}\sum_{i = 1}^{k}\sqrt{\hat L^{-1}_k}
                \right)^{-2}
                \le \left(
                    1 + \frac{1}{2}\alpha_0 \sqrt{L_0}k\sqrt{\hat L^{-1}_k}
                \right)^{-2}
                \\
                &= \left(
                    1 + \frac{k\alpha_0\sqrt{L_0 \hat L^{-1}_k}}{2}
                \right)^{-2} = L^{-1}_0\hat L\left(
                    \sqrt{L_0^{-1}\hat L_k} + \frac{k\alpha_0}{2}
                \right)^{-2}
                \\
                &\le 
                L^{-1}_0\hat L_k\left(
                    1 + \frac{k\alpha_0}{2}
                \right)^{-2} 
                = \frac{4\hat L_k}{L_0(2 + k \alpha_0)^2}. 
            \end{align*}
            This simplifies the convergence claim back in Theorem \ref{thm:gmapg-ls-convergence}. 
            The above inequality would work the same if we set: 
            \begin{align*}
                \hat L_k &= \max\left(
                    L_0, \frac{1}{k} \sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                \right). 
            \end{align*}
        \end{remark}
        \begin{theorem}[specialized GMAPG convergence rate]\label{thm:gmapg-specialized-convergence}
            Suppose that $F = f + g$ satisfy Assumption \ref{ass:standard-fista}. 
            Let the sequence $(x_k, v_k, v_k)$ satisfy GMAPG, definition \ref{def:generic-mapg-ls} and, assume that the GMAPG is initialized by $x_0 = v_0 = T_{1/L_0}(x_{-1})$ and, assume $\rho_{k - 1} = L_{k}^{-1}L_{k}$, $\alpha_0 = 1$ so the sequence $(\alpha_k)_{k \ge 0}$ satisfies for all $k\ge 1$: $\alpha_{k - 1}^2L_k^{-1}L_{k - 1}(1 - \alpha_k) = \alpha_k^2$. 
            Let $x^+$ be a minimizer of $F$, define 
            \begin{align*}
                \hat L_k = \max\left(
                    L_0, \frac{1}{k}\sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                \right). 
            \end{align*}
            Then, we have convergence claim: 
            \begin{enumerate}
                \item \begin{align*}
                    F(x_k) - F(x^+) + \frac{L_k\alpha_k}{2}\Vert x^+ - v_k\Vert^2 \le 
                    \frac{2\hat L_k}{(2 + k)^2}\Vert x^+ - x_{-1}\Vert^2.
                \end{align*}
                \item 
                \begin{align*}
                    \Vert \mathcal G_{1/L_k}(y_k)\Vert \le 
                    \frac{2\hat L_k L_k}{2 + k}
                    \left(
                        1 - L_k^{-1/2}L_{k - 1}^{1/2}
                    \right)
                    \Vert x^+ - v_0\Vert. 
                \end{align*}
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            To see (i), use \ref{lemma:gmapg-seq-bnd} and the remark with Theorem \ref{thm:gmapg-ls-convergence} because the assumptions of $x^+, (\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ is the same. 
            To see (ii), the convergence claim from \ref{thm:gmapg-generic-gradmap-convergence} simplifies with $\hat L_k \ge L_0$ and, it has 
            \begin{align*}
                \Vert \mathcal G_{1/L_k}(y_k) \Vert
                &\le 
                \left(
                    \frac{2\sqrt{\hat L_kL_0}L_k}{2 + k}
                \right)\left(
                    1 + \min(\rho_{k - 1}, L_k^{-1}L_{k - 1})^{1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &= 
                \left(
                    \frac{2\sqrt{\hat L_kL_0}L_k}{2 + k}
                \right)\left(
                    1 + L_k^{-1/2}L_{k - 1}^{1/2}
                \right)\Vert x^+ - v_0\Vert
                \\
                &\le 
                \left(
                    \frac{2\hat L_k L_k}{2 + k}
                \right)\left(
                    1 + L_k^{-1/2}L_{k - 1}^{1/2}
                \right)\Vert x^+ - v_0\Vert. 
            \end{align*}
        \end{proof}
    
    \section{Algorithmic description of GMAPG}
        There are several components to the GMAPG algorithm. 
        This section will introduce various type of implementations that can be fitted into GMAPG in Definition \ref{def:generic-mapg-ls}. 
        \subsection{Line search routines}
            % ARMIJO LINE SEARCH -----------------------------------------------
            \begin{algorithm}[H]
                {\small
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function ArmijoLS: }
                        \begin{tabular}{|ll}
                            $f : \RR^n \rightarrow \RR$ & Convex Lipschitz smooth\\ 
                            $g: \RR^n \rightarrow \overline \RR$  &  Convex\\ 
                            $x \in \RR^n$ & Vector\\
                            $v \in \RR^n$ & Vector\\
                            $L \in \RR$ & $L > 0$\\
                            $\alpha \in \RR$  & $\alpha \in (0, 1]$\\
                            $\cdots$ & Ignore extra inputs
                        \end{tabular}\vspace{0.5em}
                    }
                    \STATE{$\alpha^+ := (1/2)\left(\alpha\sqrt{\alpha^2 + 1} - \alpha^2\right)$.}
                    \STATE{$y^+ := \alpha^+ v + (1 - \alpha^+)x$.}
                    \STATE{$L^+ := L$. }
                    \FOR{$i = 1, 2,\ldots, 53$}
                        \STATE{$L^+ := 2L^+$. }
                        \STATE{$x^+ := T_{1/L^+, f, g}(y^+)$. }
                        \IF{$D_f(x^+, y^+) \le (L^+/2)\Vert x^+ - y^+\Vert^2$}
                            \STATE{\textbf{break}}
                        \ENDIF
                        \STATE{$L^+ := 2^{i}L$}
                    \ENDFOR
                    \STATE{\textbf{Return:} $x^+, y^+, \alpha^+, L^+$}
                    \caption{Armijo Line Search}\label{alg:armijo-ls}
                \end{algorithmic}
                }
            \end{algorithm}
            % CHAMBOLLE'S BACKTRACKING LINE SEARCH -----------------------------
            \begin{algorithm}[H]
                {\small
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function ChamBT Inputs: }
                        \begin{tabular}{|ll}
                            $f:\RR^n \rightarrow \RR$ & Convex Lipschitz smooth\\ 
                            $g:\RR^n \rightarrow \overline\RR$ & Convex\\ 
                            $x\in \RR^n$ & Vector\\
                            $v\in \RR^n$ & Vector\\
                            $L\in \RR$ & Number, $L > 0$ \\
                            $\alpha \in \RR$ & Vector\\
                            $L_{\min} \in \RR$ & Number, $L_{\min} > 0$\\
                            $\rho \in \RR$ & Number, $\rho \in (0, 1)$\\
                        \end{tabular}\vspace{0.5em}
                    }
                    \STATE{$L^+ := \max(L_{\min}, \rho L)$.}
                    \FOR{$i = 1, 2, \ldots, 53$}
                        \STATE{$\alpha^+ := (1/2)\left(\alpha\sqrt{\alpha^2 + L/L^+} - \alpha^2\right)$. }
                        \STATE{$y^+ := \alpha^+ v + (1 - \alpha^+) x$. }
                        \STATE{$x^+ := T_{1/L^+, f, g}(y^+)$.}
                        \IF{$2D_f(x^+, y^+) \le \Vert x^+ - y^+\Vert^2$}
                            \STATE{\textbf{break}}
                        \ENDIF
                        \STATE{$L^+ := 2^iL^+$.}
                    \ENDFOR
                    \STATE{\textbf{Return: }$x^+, \alpha^+, L^+$}
                    \caption{Chambolle's Backtracking}\label{alg:chambolle-btls}
                \end{algorithmic}
                }
            \end{algorithm}
        \subsection{Monotone routines}
            % BECK'S MONO ------------------------------------------------------
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function BeckMono Inputs: }
                        \begin{tabular}{|ll}
                            $f: \RR^n \rightarrow \RR$ & \\
                            $g: \RR^n \rightarrow \overline \RR$ & \\
                            $\tilde x \in \RR^n$ & \\
                            $x \in \RR^n$ & \\
                            $\rho$ & Number $\rho \in (0, 1)$ \\
                            $G \in \RR$ & Number 
                        \end{tabular}\vspace{0.5em}
                    }
                    \STATE{\noindent
                        $x^+ = \argmin \{(f + g)(z): z \in \{\tilde x, x\}\}$. 
                    }
                    \STATE{\textbf{Return: } $x^+, x_{k - 1} ,\eta$}
                    \caption{Beck's monotone routine}\label{alg:beck-mono}
                \end{algorithmic}
            \end{algorithm}
            % NES'S MONO -------------------------------------------------------
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function NesMono Inputs: }
                        \begin{tabular}{|ll}
                            $f:\RR^n\rightarrow \RR$ & Lipschitz Smooth \\ 
                            $g: \RR^n \rightarrow \overline \RR$ & Weakly Convex \\ 
                            $\tilde x \in \RR^n$ & Vector \\ 
                            $x \in \RR^n$ & Vector \\
                            $\eta \in \RR$ & Number $\eta > 0$\\
                            $G \in \RR$  & Number
                        \end{tabular}\vspace{0.5em}
                    }
                    \STATE{\noindent
                        $\hat y := \argmin\{(f + g)(z), x \in \{\tilde x, x\}\}$. 
                    }
                    \STATE{\noindent
                        $x^+ := T_{1/\eta}(\hat y)$. 
                    }
                    \FOR {i = 1, 2, \ldots, 53}
                        \IF{$(f + g)(x^+) - (f + g)(\hat y) \le -1/(2\eta)\Vert \mathcal G_{1/\eta}(\hat y)\Vert^2$}
                            \STATE{\textbf{Break}}
                        \ENDIF
                        \STATE{$\eta := 2\eta$. }
                            \STATE{$x^+ := T_{1/\eta}(\hat y)$.}
                    \ENDFOR
                    \STATE{$G := \eta (x^+ - \hat y).$}
                    \STATE{\textbf{return: } $x^+, \eta$ }
                    \caption{Nesterov's monotone routine}\label{alg:nes-mono}
                \end{algorithmic}
            \end{algorithm}
        \subsection{GMAPG main algorithm}
            % GMAPG ------------------------------------------------------------
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                    \STATE{\noindent
                        \textbf{Function GMAPG Inputs: }
                        \begin{tabular}{|ll}
                            $f:\RR^n \rightarrow \RR$ & Lipschitz Smooth \\
                            $g: \RR^n \rightarrow \overline\RR$ & Weakly Convex \\ 
                            $x_{-1}$ & Vector\\
                            $L\in \RR$ & $L > 0$\\ 
                            $r $ & $r \in (0, 1)$ \\ 
                            $\rho \in \RR$ & $\rho \in (0, 1)$ \\
                            $N_{\min}\in \N$ & $N \ge 1$ \\
                            $N \in \N$ & $N \ge N_{\min}$ \\
                            $\epsilon \in \RR$ & Number\\
                            $\mathbf L$ & Algorithm \ref{alg:armijo-ls} or \ref{alg:chambolle-btls}\\
                            $\mathbf M$ & Algorithm \ref{alg:beck-mono} or \ref{alg:nes-mono}\\
                            $\mathbf E_\chi$ & Exit Condition 
                        \end{tabular}
                    }
                    \STATE{$\alpha_0 := 1$.}
                    \STATE{$x_0, y_0,\alpha_1, L_0 := \textbf{ArmijoLS}(f, g, x_{-1}, x_{-1}, L, \alpha_0)$.}
                    \STATE{$\eta_0 := L_0; v_0 := x_0; G_0 = L_0(x_0 - y_0).$}
                    \FOR{$k := 1, 2,\ldots, N$}
                        \STATE{\noindent
                            $\tilde x_{k}, y_k, \alpha_{k + 1}, L_{k} := \mathbf{L}(f, g, v_{k - 1}, x_{k - 1}, L_{k - 1}, \alpha_{k}, rL_{k - 1}, \rho)$.
                        }
                        \STATE{\noindent
                            $\rho := \rho^{1/2} \textbf{ if } L_{k} > L_{k - 1} \textbf{ else } \rho$.
                        }
                        \STATE{$G_k:= L_k(\tilde x_k - y_k)$}
                        \STATE{\noindent
                            $x_k, \hat y_k, \eta_{k + 1}, G_k := \mathbf M(f, g, \tilde x_k, x_{k - 1}, \eta_k, G_k)$.
                        }
                        \IF{$G_k < \epsilon \textbf{ or } (\mathbf E_\chi  \textbf{ and } k > N_{\min})$}
                            \STATE{\textbf{break}}
                        \ENDIF
                    \ENDFOR
                    \STATE{\textbf{Return: }$x_k, k$}
                \caption{GMAPG with Chambolle's backtracking}
                \end{algorithmic}\label{alg:gmapg}
            \end{algorithm}
    % ==========================================================================
    % SECTION | EXAMPLES OF GMAPG IN THE LITERATURE
    % ==========================================================================
    \section{Examples of GMAPG in the literature}
        \begin{example}[MFISTA with Armijo line search]\;\\\vspace{-1em}
            \begin{algorithm}[H]
                \begin{algorithmic}[1]
                    \STATE{\textbf{Input:} $x_{-1} \in \RR^n, L_0 \in \RR^n, f:\RR^n \rightarrow \RR, g: \mathbb \RR^n \rightarrow \overline \RR$} 
                    \STATE{
                        $x_0 := y_0, t_0 := 1$.
                    }
                    \FOR{$k = 0, 1, 2, \ldots$}
                        \STATE{$\tilde x_{k + 1} := T_{L_k^{-1}}(y_k)$.}
                        \IF{$D_f(\tilde x_{k + 1}, y_k) > L_k/2\Vert \tilde x_{k + 1} - y_k\Vert^2$}
                            \STATE{\noindent
                                $L_k := \argmin_{i = 1,2, \ldots} \left\{ i: 
                                    D_f(T_{2^{-i}L_k^{-1}}(y_k), y_k) 
                                    \le 2^{i-1}L_k\left\Vert 
                                        T_{2^{-i}L^{-1}}y_k - y_k
                                    \right\Vert^2
                                \right\}$.
                            }
                            \STATE{$\tilde x_{k + 1} := T_{L_k^{-1}}y_k$.}
                        \ENDIF
                        \STATE{\noindent
                            Choose $x_{k + 1} \in \{\tilde x_{k + 1}, x_k\}$ such that $F(x_{k + 1}) \le \min(F(x_k), F(\tilde x_{k + 1}))$. 
                        }
                        \STATE{\noindent
                            $t_{k + 1} := (1/2)\left(1 + \sqrt{1 + 4t_k^2}\right)$. 
                        }
                        \STATE{\noindent
                            $y_{k + 1} := x_{k + 1} + t_kt_{k + 1}^{-1}(\tilde x_{k + 1} - x_{k + 1})+ (t_k - 1)t_{k + 1}^{-1}(x_{k + 1} - x_k)$. 
                        }
                    \ENDFOR
                \end{algorithmic}
                \caption{MFISTA with Armijo Line Search}
                \label{alg:mfista-armijo}
            \end{algorithm}
            We now demonstrate that Algorithm \ref{alg:mfista-armijo} is a special case of Definition \ref{def:generic-mapg-ls}.
            Let's consider $y_{k + 1}$ produced the GMAPG. 
            If $x_k = x_{k - 1}$ then replacing all instance of $x_k$ by $x_{k - 1}$ it has: 
            \begin{align*}
                y_{k + 1} &= \alpha_{k + 1}(v_k) + (1 - \alpha_{k + 1})x_{k - 1}
                \\
                &= \alpha_{k + 1}(x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})) + (1 - \alpha_{k + 1})x_{k - 1}
                \\
                &= \alpha_{k + 1} x_{k - 1} + \alpha_{k + 1}\alpha_k^{-1}(\tilde x_k - x_{k - 1}) + (1 - \alpha_{k + 1}) x_{k - 1}
                \\
                &= x_{k - 1} + \alpha_{k + 1}\alpha_k^{-1}(\tilde x_k - x_{k - 1})
                % \\
                % &= \tilde x_k + (\alpha_{k + 1} \alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1}). 
            \end{align*}
            Similarly when $x_k = \tilde x_k$ it produces: 
            \begin{align*}
                y_{k + 1} &= 
                \alpha_{k + 1}v_k + (1 - \alpha_{k + 1})\tilde x_k
                \\
                &= 
                \alpha_{k + 1}(x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})) + (1 - \alpha_{k + 1})x_k
                \\
                &= 
                \alpha_{k + 1}\left(
                    (1 - \alpha_{k}^{-1})x_{k - 1} + (\alpha_k^{-1} - 1)\tilde x_k + \tilde x_k
                \right) + 
                (1 - \alpha_{k + 1})\tilde x_k
                \\
                &= 
                \alpha_{k + 1}\left(
                    (\alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1}) + \tilde x_k
                \right) + 
                (1 - \alpha_{k + 1})\tilde x_k. 
                \\
                &= \tilde x_k + \alpha_{k + 1}(\alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1}). 
            \end{align*}
            Let's denote $y'_{k}, x'_{k}, \tilde x_k'$ as the $y_k, x_k, \tilde x_k$ produced by Algorithm \ref{alg:mfista-armijo}.
            Observe that if $x_0'$ is not the minimizer then it has $\tilde x_1' = T_{L_0^{-1}}(y_0') = T_{L_0^{-1}}(x_0')$. 
            Then $F(\tilde x_1') < F(x_0')$ is true. 
            So $x_1' = \tilde x_1 = T_{L_0^{-1}}(x_0')$. 
            Since $t_0 = 1$, it has $y_1' =\tilde x_1' + (t_0 - 1)t_1^{-1}(\tilde x_1' - x_0')= \tilde x_1'$. 
            \par
            Summarize the above results compactly, it has for all $k \ge 0$
            \begin{align}\label{eqn:emp:result-item-1}
                y_{k + 1} = \begin{cases}
                    x_{k - 1} + \alpha_{k + 1}\alpha_k^{-1}(\tilde x_k - x_{k - 1})
                    & \text{if } x_k = x_{k - 1} \wedge k \ge 1,
                    \\
                    \tilde x_k + \alpha_{k + 1}(\alpha_k^{-1} - 1)(\tilde x_k - x_{k - 1})
                    & \text{if } x_k = \tilde x_k \wedge k \ge 1,
                    \\
                    \alpha_1v_0 + (1 - \alpha_1)x_0 & \text{if } k = 0. 
                \end{cases}
            \end{align}
            Then it has for all $k \ge 0$: 
            \begin{align}\label{eqn:emp:result-item-2}
                y'_{k + 1} &= 
                \begin{cases}
                    x'_k + t_kt_{k + 1}^{-1}(\tilde x_{k + 1} - x_k) 
                    & \text{if } x_{k + 1}' = x_k' \wedge k \ge 1,
                    \\
                    x_{k + 1}' + (t_k - 1)t_{k + 1}^{-1}(\tilde x_{k + 1}' - x_k')  
                    & \text{if } x_{k + 1}' = \tilde x_{k + 1}'\wedge k \ge 1, 
                    \\
                    \tilde x_1'
                    & 
                    \text{if } k = 0. 
                \end{cases}
            \end{align}
            Let $x_{-1} \in \RR^n$. 
            If we choose $v_0 = x_0 = T_{L_0^{-1}} x_{-1}$, then $y_1 = \alpha_1 x_0 + (1 - \alpha_1)x_0 = x_0 = T_{L_0^{-1}}(x_{-1})$.
            Next, we make $\alpha_k^{-1} = t_k$, then \eqref{eqn:emp:result-item-1}, \eqref{eqn:emp:result-item-2} are equivalent. 
        \end{example}
        \begin{example}[Nesterov's monotone scheme with generic line search]\;\\
            The following is (2.2.32) in Nesterov's book, phrased in our GMAPG framework. 
            \begin{algorithm}\label{alg:nesterov-mono-generic-ls}
            \begin{algorithmic}[1]
            \STATE{\textbf{Input: } }
            \end{algorithmic}\caption{Nesterov's monotone scheme with generic line search}
            \end{algorithm}
        \end{example}

    \section{Practical enhancement from the Nesterov's Monotone Variant}
        Firstly, we will introduce some theories in for nonconvex functions. 
        This is necessary because Nesterov's monotone variants can adapt to weakly convex objective functions. 
        The following assumption will characterize the full scope of discussion in nonconvexity objective function. 
        \begin{definition}[nonconvex Nesterov's monotone scheme]\;\label{def:nes-monotone-scheme}\\

            
        \end{definition}
    \section{Restarting with function values}
    \section{Applying them to large scale LP}
        
    
    
        


\section{Primal Dual Lagrangian for LP}
    


% ==============================================================================

\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}