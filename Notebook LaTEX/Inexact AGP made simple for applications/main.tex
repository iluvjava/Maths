\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% NOTATIONS FOR THIS PAPER 
\DeclareMathOperator{\dist}{\mathop{dist}}
\DeclareMathOperator{\rng}{\mathop{rng}}



\title{{
    \fontfamily{ptm}\selectfont 
    Error Bound Can Give Near Optimal Convergence Rate for Inexact Accelerated Proximal Gradient Method
    }
}

\author{
    Author 1 Name, Author 2 Name
    \thanks{
        Subject type, Some Department of Some University, Location of the University,
        Country. E-mail: \texttt{author.namee@university.edu}.
    }
}

\begin{document}

% TITLE, ABSTRACT ==============================================================
\date{\today}
\maketitle
\todoinline{This paper is currently in draft mode. Check source to change options. }
\begin{abstract} 
    \noindent
    This is still a draft. \cite{zhang_robust_2022}. 
\end{abstract}
\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

\section{Introduction}
    \textbf{Notations.}
    Let $g: \RR^n \rightarrow\overline \RR$, we denote $g^\star$ to be the Fenchel conjugate. 
    $I: \RR^n \rightarrow \RR^n$ denotes the identity operator.
    For a multivalued mapping $T: \RR^n \rightarrow 2^{\RR^n}$, $\gra T$ denotes the graph of the operator, defined as $\{(x, y)\in \RR^n \times \RR^n : y \in Tx\}$. 
    \subsection{Epsilon subgradient and inexact proximal point}
        \begin{definition}[$\epsilon$-subgradient]\label{def:esp-subgrad}
            Let $g: \RR^n \rightarrow \overline \RR$ be proper, lsc. 
            Let $\epsilon \ge 0$. 
            Then the $\epsilon$-subgradient of $g$ at some $\bar x \in \dom g$ is given by: 
            $$
            \begin{aligned}
                \partial g_\epsilon(\bar  x) := 
                \left\lbrace
                    v \in \RR^n \left| \; 
                        \langle v, x - \bar  x\rangle \le 
                        g(x) - g(\bar x) + \epsilon \;\forall x \in \RR^n
                    \right. 
                \right\rbrace.
            \end{aligned}
            $$
            When $\bar x \not \in \dom g$, it has $\partial g_\epsilon(\bar x) = \emptyset$. 
        \end{definition}
        \begin{remark}
            $\partial_\epsilon g$ is a multivalued operator and, it's not monotone, unless $\epsilon = 0$, which makes it equivalent to Fenchel subgradient $\partial g$. 
        \end{remark}
        If we assume lsc, proper and convex $g$, we will now introduce results in the literatures that we will use. 
        \begin{fact}[$\epsilon$-Fenchel inequality]\label{fact:esp-fenchel-ineq}
            Let $\epsilon \ge 0$, then:
            \begin{align*}
                x^* \in \partial_\epsilon f(\bar x)\iff f^\star(x^*) + f(\bar x) \le \langle x^*, \bar x\rangle + \epsilon \implies \bar x \in \partial_\epsilon f^\star(x^*).
            \end{align*}
            They are all equivalent if $f^{\star\star}(\bar x) = f(\bar x)$. 
        \end{fact}
        \begin{remark}
            The above fact is taken from Zalinascu \cite[Theorem 2.4.2]{zalinescu_convex_2002}. 
        \end{remark}
        % \begin{fact}[strong epsilon subgradient sum rule]\label{fact:esp-subgrad-sum-rule}
        %     Take both $f, g: \RR^n\rightarrow \overline \RR$. 
        %     Let $\epsilon \ge 0$. 
        %     Then the epsilon subgradient sum rule states that 
        %     \begin{align*}
        %         \partial_\epsilon[f + g](\bar x) = 
        %         \bigcup \left\lbrace
        %             \partial_{\epsilon_1} f(\bar x) + 
        %             \partial_{\epsilon_2} g(\bar x) \left|\; 
        %                 \min(\epsilon_1, \epsilon_2) \ge 0\wedge 
        %                 \epsilon_1 + \epsilon_2 = \epsilon
        %             \right.
        %         \right\rbrace
        %     \end{align*}
        %     Under the scenario that: $\reli\dom f \cap \reli\dom g \neq \emptyset$. 
        % \end{fact}
        % \begin{remark}
        %     The above is taken from Morduchovich, Mau Nam \cite[Theorem 5.19]{mordukhovich_convex_2022}. 
        % \end{remark}
        We will now define inexact proximal point based on $\epsilon$-subgradient
        \begin{definition}[inexact proximal point]\label{def:inxt-pp}
            For all $x \in \RR^n, \epsilon \ge 0, \lambda > 0$, $\tilde x$ is an inexact evaluation of proximal point at $x$, if and only if it satisfies: 
            \begin{align*}
                \lambda^{-1}(x - \tilde x) \in \partial_{\epsilon} g(\tilde x). 
            \end{align*}
            We denote it by $\tilde x \approx_\epsilon \hprox_{\lambda g}(x)$. 
        \end{definition}
        \begin{remark}
            This definition is nothing new, for example see Villa et al. \cite[Definition 2.1]{villa_accelerated_2013}
        \end{remark}
        \begin{fact}[the resolvant identity]\label{fact:resv-identity}
            Let $T: \RR^n \rightarrow 2^{\RR^n}$, then it has: 
            \begin{align*}
                (I + T)^{-1} = (I - (I + T^{-1})^{-1}).
            \end{align*}
        \end{fact}
        \begin{theorem}[inexact Moreau decomposition]\label{thm:inxt-moreau-decomp}
            Let $g: \RR^n \rightarrow \overline \RR$ be a closed, convex and proper function. 
            It has the equivalence
            \begin{align*}
                \tilde y \approx_\epsilon \hprox_{\lambda^{-1}g^\star}(\lambda^{-1}y)
                \iff 
                y - \lambda \tilde y \approx_\epsilon \hprox_{\lambda g}(y). 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Consider $\tilde y \approx_\epsilon \hprox_{\lambda^{-1}g^\star}(\lambda^{-1}y)$, then it has: 
            \begin{align*}
                & 
                \tilde y 
                \in (I + \lambda^{-1}\partial_\epsilon g^\star)^{-1}(\lambda^{-1}y)
                \\
                \iff &
                (\lambda^{-1}y, \tilde y)\in 
                \gra(I + \lambda^{-1}\partial_\epsilon g^\star)^{-1}
                \\
                \underset{(1)}{\iff} &
                (\lambda^{-1}y, \tilde y)\in 
                \gra(I - (I + \partial_\epsilon g\circ(\lambda I))^{-1})
                \\
                \iff &
                (\lambda^{-1}y, \lambda^{-1}y - \tilde y)\in 
                \gra(I + \partial_\epsilon g\circ(\lambda I))^{-1}
                \\
                \iff &
                (\lambda^{-1}y - \tilde y, \lambda^{-1}y)\in 
                \gra(I + \partial_\epsilon g\circ(\lambda I))
                \\
                \iff &
                (y - \lambda\tilde y, \lambda^{-1}y)\in 
                \gra(\lambda^{-1}I + \partial_\epsilon g)
                \\
                \iff &
                (y - \lambda\tilde y, y)\in 
                \gra(I + \lambda\partial_\epsilon g)
                \\
                \iff& 
                y - \lambda \tilde y \in 
                (I + \lambda \partial_\epsilon g)^{-1}y
                \\
                \iff& 
                y - \lambda \tilde y \approx_\epsilon \hprox_{\lambda g}(y). 
            \end{align*}
            At (1) we can use Fact \ref{fact:resv-identity}, and it has $(\lambda^{-1}\partial_\epsilon g^\star)^{-1} = \partial_\epsilon g\circ(\lambda I)$ by Fact \ref{fact:esp-fenchel-ineq} and the assumption that $g$ is closed, convex and proper. 
        \end{proof}

    \subsection{Inexact proximal gradient inequality}
        \begin{assumption}[for inexact proximal gradient]\label{ass:for-inxt-pg-ineq}
            The assumption is about $(f, g, L)$. 
            We assume that 
            \begin{enumerate}[nosep]
                \item $f: \RR^n \rightarrow \RR$ is a convex, $L$ Lipschitz function. 
                \item $g: \RR^n \rightarrow \overline\RR$ is a convex, proper, and lsc function which we do not have its exact proximal operator. 
            \end{enumerate}
        \end{assumption}
        No, we develop the theory based on the use of epsilon subgradient as in Definition \ref{def:esp-subgrad}. 
        Let $\rho > 0$, the exact proximal gradient operator defined for $(f, g, L)$ satisfying Assumption \ref{ass:for-inxt-pg-ineq} has
        \begin{align*}
            T_{\rho}(x) &= \argmin_{z\in\RR^n}\left\lbrace g(z) + \langle \nabla f(x), z\rangle + \frac{\rho}{2}\Vert z - x\Vert^2 \right\rbrace
            \\
            &= \hprox_{\rho^{-1} g}\left(x - \rho^{-1}\nabla f(x)\right). 
        \end{align*}
        The following definition extends the proximal gradient operator to the inexact case using the concept of $\epsilon$-subgradient as given by Definition \ref{def:esp-subgrad}. 
        \begin{definition}[inexact proximal gradient]\label{def:inxt-pg}
            Let $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}. 
            Let $\epsilon \ge 0, \rho > 0$. 
            Then, $\tilde x \approx_\epsilon T_\rho(x)$ is an inexact proximal gradient if it satisfies variational inequality: 
            \begin{align*}
                \mathbf 0 \in \nabla f(x) + \rho(x - \tilde x) + \partial_{\epsilon} g(\tilde x). 
            \end{align*}
        \end{definition}
        \begin{remark}
            We assumed that we can get exact evaluation of $\nabla f$ at any points $x \in \RR^n$. 
        \end{remark}
        \begin{lemma}[other representations of inexact proximal gradient]\;\label{lemma:other-repr-inxt-pg}\\
            Let $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, $\epsilon \ge 0, \rho > 0$, then for all $\tilde x \approx_\epsilon T_\rho(x)$, it has the following equivalent representations: 
            \begin{align*}
                & (x - \rho^{-1}\nabla f(x)) - \tilde x 
                \in \rho^{-1} \partial_\epsilon g(\tilde x)
                \\
                \iff 
                & \tilde x \in (I + \rho^{-1}\partial_\epsilon g(\tilde x))^{-1}
                (x - \rho^{-1}\nabla f(x))
                \\
                \iff 
                & x \approx_\epsilon \hprox_{\rho^{-1} g}
                \left(x - \rho^{-1}\nabla f(x)\right)
            \end{align*}
        \end{lemma}
        \begin{proof}
            It's direct. 
        \end{proof}
        \begin{theorem}[inexact over-regularized proximal gradient inequality]\;\label{thm:inxt-pg-ineq}\\
            Let $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, $\epsilon \ge 0, B \ge 0, \rho > 0$. 
            Consider $\tilde x \approx_\epsilon T_{B + \rho}(x)$. 
            Denote $F = f + g$. 
            If in addition, $\tilde x, B$ satisfies the line search condition $D_f(\tilde x, x) \le B/2\Vert x - \tilde x\Vert^2$, then it has $\forall z \in \RR^n$: 
            \begin{align*}
                - \epsilon &\le 
                F(z) - F(\tilde x)
                + \frac{B + \rho}{2}\Vert x - z\Vert^2
                - \frac{B + \rho}{2}\Vert z - \tilde x\Vert^2
                - \frac{\rho}{2}\Vert \tilde x - x\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            By Definition \ref{def:inxt-pg} write the variational inequality that describes $\tilde x \approx_\epsilon T_B(x)$, and the definition of epsilon subgradient (Definition \ref{def:esp-subgrad}) it has for all $z \in \RR^n$: 
            \begin{align*}
                - \epsilon &\le 
                g(z) - g(\tilde x) - \langle (B + \rho)(\tilde x - x) - \nabla f(x), z - \tilde x\rangle
                \\
                &= 
                g(z) - g(\tilde x) 
                - (B + \rho)\langle \tilde x - x, z - \tilde x\rangle
                + \langle \nabla f(x), z - \tilde x\rangle
                \\
                &\underset{(1)}{\le} 
                g(z) + f(z) - g(\tilde x) - f(\tilde x)
                - (B + \rho)\langle \tilde x - x, z - \tilde x\rangle
                - D_f(z, x) + D_f(\tilde x, x)
                \\
                &\underset{(2)}{\le} 
                F(z) - F(\tilde x)
                - (B + \rho)\langle \tilde x - x, z - \tilde x\rangle
                + \frac{B}{2}\Vert \tilde x - x\Vert^2
                \\
                &=
                F(z) - F(\tilde x) + \frac{B + \rho}{2}\left(
                    \Vert x - z\Vert^2
                    - \Vert \tilde x - x\Vert^2
                    - \Vert z - \tilde x\Vert^2
                \right)
                + \frac{B}{2}\Vert \tilde x - x\Vert^2
                \\
                &= 
                F(z) - F(\tilde x)
                + \frac{B + \rho}{2}\Vert x - z\Vert^2
                - \frac{B + \rho}{2}\Vert z - \tilde x\Vert^2
                - \frac{\rho}{2}\Vert \tilde x - x\Vert^2. 
            \end{align*}
            At (1), we used considered the following: 
            \begin{align*}
                \langle \nabla f(x), z - x\rangle &= \langle \nabla f(x), z - x + x - \tilde x\rangle
                \\
                &= \langle \nabla f(x), z - x\rangle + \langle \nabla f(x), x - \tilde x\rangle
                \\
                &= -D_f(z, x) + f(z) - f(x) + D_f(\tilde x, x) - f(\tilde x) + f(x)
                \\
                &= -D_f(z, x) + f(z) + D_f(\tilde x, x) - f(\tilde x). 
            \end{align*}
            At (2), we used the fact that $f$ is convex hence $- D_f(z, x) \le 0$ always, and in the statement hypothesis we assumed that $B$ has $D_f(\tilde x, x) \le B/2\Vert \tilde x - x\Vert^2$. 
            We also used $F = f + g$. 
        \end{proof}
        \begin{remark}
            When $\epsilon = 0, \rho = 0$, this reduces to proximal gradient inequality in the exact case. 
            In this inequality, observe that the parameter $\epsilon$ controls the inexactness of the proximal gradient evaluation. 
            More specifically, $\epsilon_k$ controls the absolute perturbations of the proximal gradient inequality compared to its exact counterpart. 
            $\rho$ on the other hand, it is the over-relaxation of proximal gradient operator, and it compensates the perturbations caused by $\epsilon$ relative to the term $\Vert \tilde x - x\Vert^2$. 
        \end{remark}
    \subsection{Optimizing the inexact proximal point problem}\label{sec:optz-inxt-pp-problem}
        In this section we will present the optimization problem that obtains a $\tilde x$ such that $\tilde x \approx_\epsilon \hprox_{\lambda g}(z)$. 
        Eventually we want to evaluate $T_{\rho}(x)$ of some $F = f + g$ inexactly using Lemma \ref{lemma:other-repr-inxt-pg}. 
        To do that one would need to evaluate $\hprox_{\rho^{-1}g}$ inexactly which is defined in Definition \ref{def:inxt-pp}. 
        \par
        Most of these results that will follow are from the literature. 
        To start, we must assume the following about a function $g: \RR^n \rightarrow \overline \RR$, with $g$ closed, convex and proper. 
        \begin{assumption}[for inexact proximal operator]\;\label{ass:for-inxt-prox}\\
            This assumption is about $(g, \omega, A)$. 
            Let $m \in \N, n \in \RR^n$, we assume that 
            \begin{enumerate}[nosep]
                \item $A\in \RR^{m \times n}$ is a matrix. 
                \item $\omega: \RR^n \rightarrow \overline \RR$ is a closed and convex function such that it admits proximal operator $\hprox_{\lambda\omega}$ and, its conjugate $\omega^\star$ is known. 
                \item $g := \omega(Ax)$ such that $\rng A \cap \reli\dom g \neq\emptyset$. 
            \end{enumerate}
        \end{assumption}
        Now, we are ready to discuss how to choose $\tilde x \approx_\epsilon \hprox_{\lambda g}(x)$. 
        Fix $y \in \RR^n, \lambda > 0$, we are ultimately interested in minimizing: 
        \begin{align}\label{eqn:primal-pp}
            \Phi_\lambda(u) &:= \omega(Au) + \frac{1}{2\lambda} \Vert u - y\Vert^2
        \end{align}
        This problem admits dual objective in $\RR^m$: 
        \begin{align}\label{eqn:dual-pp}
            \Psi_\lambda(v) &:=
            \frac{1}{2\lambda}\Vert \lambda A^\top v - y\Vert^2
            + \omega^\star(v) - \frac{1}{2\lambda}\Vert y\Vert^2. 
        \end{align}
        We define the duality gap
        \begin{align}\label{eqn:duality-gap-pp}
            \mathbf G_\lambda(u, v) &:= \Phi_\lambda(u) + \Psi_\lambda(v). 
        \end{align}
        If strong duality holds, it exists $(\hat u, \hat v)$ such that we have the following: 
        \begin{align*}
            \mathbf G_\lambda(\hat u, \hat v) = 0 = \min_{u} \Phi_\lambda(u) + \min_v \Psi_\lambda(v)
        \end{align*}
        The following theorem quantifies a sufficient conditions for $\tilde x \approx_\epsilon \hprox_{\lambda g}(x)$. 
        The theorem below is from \cite[Proposition 2.2]{villa_accelerated_2013}. 
        \begin{theorem}[primal translate to dual {\cite[Proposition 2.2]{villa_accelerated_2013}}]\label{thm:primal-dual-trans}
            Let $(g, \omega, A)$ satisfies assumption \ref{ass:for-inxt-prox}, $\epsilon \ge 0$, then 
            \begin{align*}
                \left(
                    \forall z \approx_\epsilon \hprox_{\lambda g}(y) 
                \right)(\exists v \in \dom \omega^\star): z = y - \lambda A^\top v. 
            \end{align*}
        \end{theorem}
        This theorem that follows is from Villa et al. \cite[Proposition 2.3]{villa_accelerated_2013}. 
        \begin{theorem}[duality gap of inexact proximal problem {\cite[Proposition 2.3]{villa_accelerated_2013}}]\;\label{thm:dlty-gap-inxt-pp}\\
            Let $(g, \omega, A)$ satisfies Assumption \ref{ass:for-inxt-prox}, for all $\epsilon \ge 0$, $v \in \RR^n$ consider the following conditions: 
            \begin{enumerate}[nosep]
                \item $\mathbf G_\lambda(y - \lambda A^\top v, v) \le \epsilon$. 
                \item $A^\top v \approx_\epsilon \hprox_{\lambda^{-1}g^\star}(\lambda^{-1}y)$. 
                \item $y - \lambda A^\top v \approx_{\epsilon} \hprox_{\lambda g}(y)$. 
            \end{enumerate}
            They have $(a)\implies (b) \iff (c)$. 
            If in addition $\omega^\star(v) = g^\star(A^\top v)$, then all three conditions are equivalent. 
        \end{theorem}
        \begin{proof}
            The proof of $(a) \implies (b)$, and the case when $(a)\iff (b)$, we refer readers to Villa et al. \cite[ Proposition 2.3]{villa_accelerated_2013}, and to show $(b) \iff (c)$ use Theorem \ref{thm:inxt-moreau-decomp}. 
        \end{proof}
        \par
        The following fact from the literature indicates that it's sufficient to minimize the dual problem $\Psi_\lambda$ to obtain an element of the inexact proximal point operator. 
        The following fact is Proposition is from Villa et al. \cite[Theorem 5.1]{villa_accelerated_2013}. 
        \begin{fact}[minimizing dual of the proximal problem {\cite[Theorem 5.1]{villa_accelerated_2013}}]\label{fact:minimizing-dual-pp}
            Let $\bar v$ be a solution of $\Psi_\lambda$. 
            Suppose that $(v_n)_{n \ge 0}$ is a minimizing sequence for $\Psi_\lambda$. 
            Let $z_n = y - \lambda A^\top v_n$, and $\bar z = y - \lambda A^\top \bar v$. 
            If in addition, $\Phi_\lambda$ is $L_1$ Lipschitz continuous, then it has for all $k \ge 0$ the inequality: 
            \begin{align*}
                \Phi_{\lambda}(z_n) - \Phi_\lambda(\bar z) 
                &\le L_1 \Vert z_n - \bar z\Vert 
                \le L_1\sqrt{2\lambda}(\Psi_\lambda(v_n) - \Psi_\lambda(\bar v))^{1/2}. 
            \end{align*}
        \end{fact}
        We remark that the above fact translates any algorithm that optimizes the function value of the dual problem $\Psi_\lambda$ into optimizing duality gap $\mathbf G(z_n, v_n)$. 
        For this reason, the number of iterations of the inner loop required to achieve $\mathbf G(z_n, v_n) < \epsilon$ for a given $\epsilon$ is related to the convergence rate of the algorithms used to optimize $\Psi_\lambda(v_n)$. 
        With the theorem derived above, and using Theorem \ref{thm:dlty-gap-inxt-pp} it implies that any algorithm which can optimize function value $\Psi_\lambda$ will produce iterates sufficient to achieve $\approx_\epsilon \hprox_{\lambda g}(y)$. 
    \subsection{Literature reviews}

    \subsection{Our contributions}


\section{The inexact accelerated proximal gradient with controlled errors}
    In this section, we present an accelerated algorithm with controlled error using Definition \ref{def:inxt-pg}, and show that it can have a convergence rate under certain error conditions. 
    \begin{definition}[our inexact accelerated proximal gradient]\;\label{def:inxt-apg}\\
        Suppose that $(F, f, g, L)$ and, sequences $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$ satisfies the following
        \begin{enumerate}[nosep]
            \item $(\alpha_k)_{k \ge 0}$ is a sequence such that $\alpha \in (0, 1]$ for all $k \ge 0$. 
            \item $(B_k)_{k \ge 0}$ has $B_k > 0\; \forall k$, it characterizes any potential line search, back tracking routine. 
            \item $(\rho_k)_{k \ge 0}$ be a sequence such that $\rho_k \ge 0$, characterizing the over-relaxation of the proximal gradient operator. 
            \item $(\epsilon_k)_{k \ge 0}$ has $\epsilon_k > 0$ for all $k \ge 0$, it characterizes the errors of inexact proximal evaluation.
            \item $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, and let $F = f + g$. 
        \end{enumerate}
        Denote $L_k = B_k + \rho_k$ for short. 
        Given any initial condition $v_{-1}, x_{-1} \in \RR^n$, the algorithm generates the sequences $(y_k, x_k, v_k)_{k \ge 0}$ such that they satisfy for all $k \ge 0$: 
        \begin{align*}
            & y_k = \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}, 
            \\
            & x_k \approx_{\epsilon_k} T_{L_k}(y_k), 
            \\
            & D_f(x_k, y_k) \le \frac{B_k}{2}\Vert x_k - y_k\Vert^2, 
            \\
            & v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}). 
        \end{align*}
    \end{definition}
    \begin{lemma}[inexact accelerated proximal gradient preparation stage I]\; \label{lemma:inxt-apg-cnvg-prep1}\\
        Let $(F, f, g, L)$, and $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, be given by Definition \ref{def:inxt-apg}. 
        Denote $L_k = B_k + \rho_k$. 
        Then, for any $\bar x \in \RR^n$, the sequences $(y_k, x_k, v_k)_{k \ge 0}$ generated satisfy for all $k \ge 1$ the inequality: 
        \begin{align*}
            & \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k
            \\
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            \\ &
            + \max\left(
                    1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
                \right)\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
        \end{align*}
        When, $k = 1$ it instead has: 
        \begin{align*}
            & \frac{\rho_0}{2}\Vert x_0 - y_0\Vert^2 - \epsilon_0 
            \\
            &\le 
            (1 - \alpha_0)(F(x_{-1}) - F(\bar x)) + F(\bar x) - F(x_0) 
            + \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_{-1}\Vert^2
            - \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_0\Vert^2. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Two intermediate results are in order before we can prove the inequality. 
        Define $z_k := \alpha_k \bar x + (1 - \alpha_k)x_{k - 1}$ for short. 
        It has for all $k \ge 1$ the equality: 
        \begin{align}\tag{a}\label{eqn:inxt-apg-cnvg-prep1-a}\begin{split}
            z_k - x_k &= 
            \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - x_k
            \\
            &= \alpha_kx^+ + (x_{k - 1} - x_k) - \alpha_kx_{k - 1}
            \\
            &= \alpha_k \bar x - \alpha_k v_k. 
        \end{split}\end{align}
        It also has for all $k \ge 1$ the equality: 
        \begin{align}\tag{b}\label{eqn:inxt-apg-cnvg-prep1-b}\begin{split}
            z_k - y_k &= 
            \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - y_k
            \\
            &= \alpha_k \bar x - \alpha_k v_{k - 1}. 
        \end{split}\end{align}
        Let's denote $L_k = B_k + \rho_k$ for short. 
        Recall that $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, if we choose $x = y_k$ so $\tilde x = x_k \approx_{\epsilon_k} T_{L_k}(y_k)$, and set $z = z_k, \epsilon = \epsilon_k$ then Theorem \ref{thm:inxt-pg-ineq} has: 
        \begin{align*}
            & \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k
            \\
            &\le
            F(z_k) - F(x_k) + \frac{L_k}{2}\Vert y_k - z_k\Vert^2 - \frac{L_k}{2}\Vert z_k - x_k\Vert^2
            \\
            &\underset{(1)}{\le} \alpha_k F(\bar x) + (1 - \alpha_k)F(x_{k - 1}) - F(x_k)
            + \frac{L_k}{2}\Vert y_k - z_k\Vert^2 - \frac{L_k}{2}\Vert z_k - x_k\Vert^2
            \\
            &\underset{(2)}{=} 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_{k}\Vert^2 
            \\
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            \\ &
            + \max\left(
                    1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
                \right)\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
        \end{align*}
        At (1) we used the fact that $F = f + g$ hence $F$ is convex. 
        At (2) we used \eqref{eqn:inxt-apg-cnvg-prep1-a}, \eqref{eqn:inxt-apg-cnvg-prep1-b}. 
        Finally, if $k = 0$, then take the RHS of $\underset{(1)}{=}$ then:
        \begin{align*}
            & \frac{\rho_0}{2}\Vert x_0 - y_0\Vert^2 - \epsilon_0 
            \\
            &\le 
            (1 - \alpha_0)(F(x_{-1}) - F(\bar x)) + F(\bar x) - F(x_0) 
            + \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_{-1}\Vert^2
            - \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_0\Vert^2. 
        \end{align*}
    \end{proof}
    \par
    The following assumption encapsulate assumptions on the errors such that a near optimal convergence rate is still attainable by an algorithm that satisfies Definition \ref{def:inxt-apg}. 
    \begin{assumption}[valid error schedule]\label{ass:valid-err-schedule}
        The following assumption is about an algorithm satisfying Definition \ref{def:inxt-apg}, its parameters $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$ in relation to its iterates $(y_k, x_k, v_k)_{k\ge 0}$ and, some additional parameters $(\beta_k)_{k\ge 0}, \mathcal E_0$ and $p$. 
        Let 
        \begin{enumerate}[nosep]
            \item $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}, (F, f, g, L)$ and $(y_k, x_k, v_k)_{k\ge 0}$ be given by Definition \ref{def:inxt-apg}. 
            \item $\mathcal E_0 \ge 0$ be arbitrary;
            \item the sequence $(\beta_k)_{k\ge 0}$ be defined as $\beta_k := \prod_{i = 1}^{k} \max\left(1 - \alpha_i, \frac{\alpha_i^2L_i}{\alpha_{i - 1}^2L_{i - 1}}\right)$ for all $k \ge 1$, with the base case being $\beta_0 = 1$; 
            \item $p \ge 1$ is some constant which will bound the error $\epsilon_k$ relative to $\rho_k\Vert x_k - y_k\Vert^2, \beta_k$ and, $k$. 
        \end{enumerate}
        In addition, we assume that the error parameter $\epsilon_k \ge 0$ and over-relaxation parameter $\rho_k$, iterates $x_k, y_k$ and $\beta_k$ together satisfies for all $k \ge 0$ the relations:
        \begin{align*}
            \frac{- \mathcal E_0\beta_k}{k^p} &\le 
            \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k. 
        \end{align*}
        
    \end{assumption}
    \par
    The following proposition is a prototype of the convergence rate together with the error schedule that delivers convergence of algorithms satisfying Definition \ref{def:inxt-apg}. 
    \begin{proposition}[generic convergence rate under valid error schedule]\;\label{prop:inxt-apg-cnvg-generic}\\
        Let $(F, f, g, L)$, $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, $(\beta_k)_{k\ge 0}, \mathcal E_0, p$ as assumed in Assumption \ref{ass:valid-err-schedule}. 
        Fix any $\bar x \in \RR^n$ for all $k \ge 0$ and assume that $\alpha_0 = 1$. 
        Then for the iterates generated $(y_k, x_k, v_k)_{k \ge 0}$ by the algorithm, for all $k \ge 0$ they will satisfy: 
        \begin{align*}
            & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
            \le 
            \beta_k \left(
                \frac{L_0}{2}\Vert \bar x - v_{-1}\Vert^2
                + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}
        Consider results from Lemma \ref{lemma:inxt-apg-cnvg-prep1} has $\forall k \ge 1$: 
        \begin{align*}
            & \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k
            \\
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            \\ &
            + \max\left(
                    1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
                \right)\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
            \\
            &\le \max\left(
                1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
            \right)\left(
                F(x_{k - 1}) - F(\bar x)
                + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            \right)
            \\&\quad 
                + F(\bar x) - F(x_k) - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
        \end{align*}
        For notation brevity, we introduce $\beta_k, \Lambda_k$: 
        \begin{align*}
            \beta_0 &= 1, 
            \\
            \beta_k &:= \prod_{i = 1}^{k} \max\left(
                1 - \alpha_i, \frac{\alpha_i^2L_i}{\alpha_{i - 1}^2L_{i - 1}}
            \right),
            \\
            \Lambda_k &:= 
            - F(\bar x) + F(x_k) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
        \end{align*}
        Now, suppose that in addition there is a non-negative sequence $(\mathcal  E_k)_{k \ge 0}$ such that 
        \begin{enumerate}[nosep]
            \item For all $k \ge 0$, it has $\frac{-\mathcal E_k}{k^p} \le \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k$ where $p \ge 1$, 
            \item For all $k \ge 1$, it has $\mathcal E_k = \frac{\beta_k}{\beta_{k - 1}}\mathcal E_{k - 1}$, with $\mathcal E_0 \ge 0$. 
        \end{enumerate}
        These conditions are equivalent to the assumption that $\frac{- \mathcal E_0\beta_k}{k^p} \le \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k$ (which was stated in Assumption \ref{ass:valid-err-schedule}). 
        One can show that by unrolling recurrence on $\mathcal E_k$. 
        Then \eqref{ineq:inxt-apg-cnvg-generic-pitem-1} implies $\forall k \ge 1$: 
        \begin{align}\label{ineq:inxt-apg-cnvg-generic-pitem-1}
            \frac{- \mathcal E_k}{k^p} &\le 
            \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} - \Lambda_k
            \iff 
            \Lambda_k \le 
            \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} + \frac{\mathcal E_k}{k^p}. 
        \end{align}
        Now, we show the convergence of $\Lambda_k$, using the relations of $\mathcal E_k, \Lambda_k, \beta_k$ above. 
        \begin{align*}\begin{split}
            \Lambda_k &\le 
            \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} + \frac{\mathcal E_k}{k^p}
            \\
            &\le \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} 
            + \frac{\beta_k}{\beta_{k -1}}\frac{\mathcal E_{k - 1}}{k^p}
            \\
            &= 
            \frac{\beta_k}{\beta_{k - 1}}
            \left(
                \Lambda_{k - 1} + \frac{\mathcal E_{k - 1}}{k^p}
            \right)
            \\
            &\le 
            \frac{\beta_k}{\beta_{k - 1}}
            \left(
                \frac{\beta_{k - 1}}{\beta_{k - 2}}\Lambda_{k - 2}
                + \frac{\mathcal E_{k - 1}}{(k - 1)^p}
                + \frac{\mathcal E_{k - 1}}{k^p}
            \right)
            \\
            &= 
            \frac{\beta_k}{\beta_{k - 2}}
            \left(
                \Lambda_{k - 2}
                + \frac{\mathcal E_{k - 2}}{(k - 1)^p}
                + \frac{\mathcal E_{k - 2}}{k^p}
            \right)
            \\
            & ...
            \\
            &\le \frac{\beta_k}{\beta_1}\left(
                \Lambda_1 + \mathcal E_1\sum_{n = 2}^{k} \frac{1}{n^p}
            \right)
            \\
            &\le \frac{\beta_k}{\beta_1}\left(
                \frac{\beta_1}{\beta_0}\Lambda_0 
                + \mathcal E_1\sum_{n = 1}^{k} \frac{1}{n^p}
            \right)
            \\
            &= \frac{\beta_k}{\beta_0}\left(
                \Lambda_0 
                + \mathcal E_0\sum_{n = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{split}\end{align*}
        Therefore, it points to the following inequality: 
        \begin{align*}
            & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
            \\
            &\le 
            \beta_k \left(
                F(x_0) - F(\bar x) + \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_0\Vert^2
                + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{align*}
        Finally, when $\alpha_0 = 1$, then the results from \ref{lemma:inxt-apg-cnvg-prep1} with $k = 0$ simplifies the above inequality and give: 
        \begin{align*}
            & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
            \le 
            \beta_k \left(
                \frac{L_0}{2}\Vert \bar x - v_{-1}\Vert^2
                + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{align*}
    \end{proof}
    \par
    Now, it only remains to determine the sequence $\alpha_k$ to derive a type of convergence rate for the algorithm because from the above theorem, we have the convergence rate $\beta_k$ and, the error parameters $\epsilon_k, \rho_k$ both controlled by the sequence $(\alpha_k)_{k \ge 0}$. 
    
    \subsection{Convergence results of the outer loop}
        This section will give specific instances of the error control sequence $(\epsilon_k)_{k \ge 0}, (\rho_k)_{k \ge0}$ and, momentum sequence $(\alpha_k)_{k \ge0}$ such that an optimal convergence rate of $\mathcal O(1/k^2)$ can be achieved. 
        \begin{assumption}[the optimal momentum sequence]\;\label{ass:opt-mmntm-seq}\\
            Keeping everything assumed in Assumption \ref{ass:valid-err-schedule} about $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, $(F, f, g, L)$, $(y_k, x_k, v_k)_{k \ge 0}$, $(\beta_k)_{k \ge 0}$, $\mathcal E_0$ and $p$, we assume in addition that the sequence $(\alpha_k)_{k \ge 0}$ satisfies for all $k \ge 0$ the equality: $(1 - \alpha_k) = \alpha_{k}^2L_k\alpha_{k - 1}^{-2}L_{k - 1}^{-1}$. 
        \end{assumption}
        \begin{lemma}[the optimal momentum sequence is indeed valid and optimal]\;\label{lemma:opt-mmntm-seq}\\
            Let $(\alpha_k)_{k \ge 0}, (\beta_k)_{k \ge 0}$ be given by Assumption \ref{ass:opt-mmntm-seq}. 
            If we choose $\alpha_0 \in (0, 1]\;$ then it satisfies $\alpha_k \in (0, 1)\;\forall k\ge 1$, and it's given by: 
            \begin{align*}
                \alpha_k &=
                \frac{L_{k - 1}}{2L_k}\left(
                    - \alpha_{k - 1}^2 + \left(
                        \alpha_{k - 1}^4 + 4 \alpha_{k - 1}\frac{L_k}{L_{k - 1}}
                    \right)^{1/2}
                \right). 
            \end{align*}
            In addition, the sequence $(\beta_k)_{k \ge 0}$ has
            \begin{align*}
                \left(
                    1 + \alpha_0\sqrt{L_0}\sum_{i = 1}^{k}\sqrt{L_i^{-1}}
                \right)^{-2}
                \hspace{-1em}\le 
                \beta_k
                \le 
                \left(
                    1 + \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k}\sqrt{L_i^{-1}}
                \right)^{-2}. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Firstly, we will show that for all $\alpha_0 \in (0, 1]$, and $\alpha_k \in (0, 1)\;\forall k \ge 1$. 
            We will prove using induction.
            Assume inductively that $\alpha_k \in (0, 1]$. 
            We can solve for $\alpha_k$ using the recursive equality from Assumption \ref{ass:opt-mmntm-seq} which always has one of the strictly positive root given by: 
            \begin{align*}
                \alpha_k &=
                \frac{L_{k - 1}}{2L_k}\left(
                    - \alpha_{k - 1}^2 + \left(
                        \alpha_{k - 1}^4 + 4 \alpha_{k - 1}\frac{L_k}{L_{k - 1}}
                    \right)^{1/2}
                \right)
                \\
                &= \frac{\alpha_{k - 1}^2L_{k - 1}}{2L_k}\left(
                    - 1 + \left(
                        1 + \frac{4 L_k}{L_{k - 1}\alpha_{k-1}^2}
                    \right)^{1/2}
                \right)
                \\
                &\underset{(1)}{<} \frac{\alpha_{k - 1}^2L_{k - 1}}{2L_k}\left(
                    - 1 + 1 + 2 \alpha_{k - 1}^{-2}\frac{L_k}{L_{k - 1}}
                \right)
                \\
                &= 1
            \end{align*}
            At (1) we completed a square inside the radical, and use the assumption that $\alpha_k > 0$ and, $L_k > 0, L_{k - 1} > 0$ because we had $B_k > 0$: 
            \begin{align*}
                1 + 4 L_{k}L_{k - 1}^{-1}\alpha_{k - 1}^{-2}
                &= 
                1 + 4 L_{k}L_{k - 1}^{-1}\alpha_{k - 1}^{-2}
                + 2L_k^2L_{k - 1}^{-2}\alpha_{k - 1}^{-4}
                - 2L_k^2L_{k - 1}^{-2}\alpha_{k - 1}^{-4}
                \\
                &= (1 + 2L_kL_{k - 1}^{-1}\alpha_{k - 1}^{-2})^2
                - 2\alpha_{k - 1}^{-4}L_k^2L_{k - 1}^{-2}
                \\
                &< \left(1 + 2L_kL_{k - 1}^{-1}\alpha_{k - 1}^{-2}\right)^2. 
            \end{align*}
            To see that $\alpha_k > 0$, recall the same fact that $L_k > 0$, and use the inductive hypothesis that $\alpha_{k - 1} \in (0, 1]$ then $\alpha_k > 0$ because: 
            \begin{align*}
                \alpha_k &=
                \frac{L_{k - 1}}{2L_k}\left(
                    - \alpha_{k - 1}^2 + \left(
                        \alpha_{k - 1}^4 + 4 \alpha_{k - 1}\frac{L_k}{L_{k - 1}}
                    \right)^{1/2}
                \right) 
                \\
                &\ge \frac{L_{k - 1}}{2L_k}\left(
                    - \alpha_{k - 1}^2 + \left(
                        \alpha_{k - 1}^4
                    \right)^{1/2}
                \right) 
                \\
                &> 0. 
            \end{align*}
            Therefore, inductively it holds that $\alpha_k \in (0, 1)$ too. 
            \par
            Now, we focus on $(\beta_k)_{k \ge 0}$. 
            From the assumption that $(\alpha_k)_{k \ge 0}$ has $(1 - \alpha_k) = \alpha_k^2L_k \alpha_{k - 1}^{-2}L_{k - 1}^{-1}$ for all $k \ge 0$ and, the definition of $\beta_k$, it yields the following equalities: 
            \begin{align*}
                \beta_k = \prod_{i = 1}^k \max\left(
                    1 - \alpha_i, \frac{\alpha_i^2L}{\alpha_{i - 1}^2L_{i - 1}}
                \right) 
                = \prod_{i = 1}^k(1 - \alpha_i) 
                = \prod_{i = 1}^k \frac{\alpha_i^2L}{\alpha_0^2L_0} = \frac{\alpha_k^{2}L_k}{\alpha_0^2L_0}. 
            \end{align*}
            With the above relation, and the definitions of the sequences $(\alpha_k)_{k \ge 0}, (\beta_k)_{k\ge 0}$ it satisfies for all $k \ge 1$ the properties: 
            \begin{enumerate}[nosep]
                \item[(a)] $\beta_k$ is monotone decreasing and $\beta_k > 0$ for all $k \ge 0$ because $\beta_k = \prod_{i = 1}^{k} (1 - \alpha_i)$ and, $\alpha_k \in (0, 1]$. 
                \item[(b)] It has the equalities $\beta_k/\beta_{k - 1} = (1 - \alpha_k) = \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2 L_{k - 1}}$ for all $k \ge 1$. 
            \end{enumerate}
            Using the above observations, we can show the chain of equalities $\alpha_k^{2} = (1 - \beta_k/\beta_{k - 1})^2 = \beta_kL_0\alpha_0^2L_k^{-1}$ for all $k \ge 0$. 
            This is true by first considering the relations $\prod_{i = 1}^k(1 - \alpha_i) = \beta_k$: 
            \begin{align}\label{eqn:opt-mmntm-seq-pitem1}\begin{split}
                (1 - \alpha_k) &= \beta_k/\beta_{k - 1}
                \\
                \iff 
                \alpha_k &= 1 - \beta_k / \beta_{k - 1}
                \\
                \implies 
                \alpha_k^2 &= (1 - \beta_k / \beta_{k - 1})^2.     
            \end{split}\end{align}
            Next, the recursive relation of $(\alpha_k)_{k \ge 0}$ gives
            \begin{align}\label{eqn:opt-mmntm-seq-pitem2}\begin{split}
                \alpha_k^2&= (1 - \alpha_k)\alpha_{k - 1}^2L_{k - 1}L_k^{-1}
                \\
                &= 
                (1 - \alpha_k)
                \frac{\alpha_{k - 1}^2L_{k - 1}}{\alpha_0^2L_0}
                \frac{L_{k - 1}\alpha_0^2L_0}{L_k}
                \\
                &= 
                (\beta_k\beta_{k - 1}^{-1})\beta_{k - 1}L_0\alpha_0^2L_k^{-1}
                \\
                &= \beta_kL_0\alpha_0^2L_k^{-1}.         
            \end{split}\end{align}
            Combining \eqref{eqn:opt-mmntm-seq-pitem1}, \eqref{eqn:opt-mmntm-seq-pitem2} and the fact that $\beta_k > 0\;\forall k \ge 0$, it would mean for all $i \ge 1$ it has: 
            \begin{align*}
                L_0 \alpha_0^2 L_i^{-1} &= 
                \beta_i^{-1}\left(
                    1 - \frac{\beta_k}{\beta_{k - 1}}
                \right)^2
                \\
                &= 
                \beta_i \left(
                    \beta_i^{-1} - \beta_{i - 1}^{-1}
                \right)^2
                \\
                &=
                \beta_i \left(
                    \beta_i^{-1/2} - \beta_{i - 1}^{-1/2}
                \right)^2
                \left(
                    \beta_i^{-1/2} + \beta_{i - 1}^{-1/2}
                \right)^2
                \\
                &= \left(
                    \beta_i^{-1/2} - \beta_{i - 1}^{-1/2}
                \right)^2\left(
                    1 + \beta_i^{1/2}\beta_{i - 1}^{-1/2}
                \right)^2. 
            \end{align*}
            Since $\beta_i$ is monotone decreasing, it has $0 < \beta_i^{-1/2}\beta_{i - 1}^{-1/2} \le 1$, this gives: 
            \begin{align*}
                \beta_i^{-1/2} - \beta_{i - 1}^{-1/2} 
                &\le \alpha_0\sqrt{\frac{L_0}{L_i}} 
                \le 2 \left(
                    \beta_i^{-1/2} - \beta_{i - 1}^{-1/2}
                \right).
            \end{align*}
            Performing a telescoping sum, use the fact that $\beta_0 = 1$ will yield the desired results after some algebraic manipulations. 
        \end{proof}
        \begin{proposition}[$\mathcal O(1/k^2)$ optimal convergence rate of the outer loop]\;\label{prop:opt-cnvg-outr-loop}\\
            Let $(f, g, L)$, $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, $(\beta_k)_{k\ge 0}, \mathcal E_0, p$ be given by Assumption \ref{ass:opt-mmntm-seq}. 
            Assume in addition that
            \begin{enumerate}[nosep]
                \item there exists $\bar x \in \RR^n$ that is a minimizer of $F = f + g$;
                \item it has $p > 0$;
                \item the sequence $L_k := B_k + \rho_k$ is bounded, and there exists an $L_{\max}$ such that for all $k \ge 0$ it has $L_{\max} \ge \max_{k\ge i\ge 0} L_i$. 
            \end{enumerate}
            Then, $\alpha_k \in (0, 1]\;\forall k \ge 0$ hence it's always valid and, it has $\forall k \ge 0$: 
            \begin{align*}
                & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
                \le 
                \left(
                    1 + \frac{k\alpha_0\sqrt{L_0}}{2\sqrt{L_{\max}}}
                \right)^{-2}\left(
                    \frac{L_0}{2}\Vert \bar x - v_{-1}\Vert^2
                    + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
                \right). 
            \end{align*}
            Since, $p > 0$ the sum is convergent and hence the above inequality claims an overall convergence rate $\mathcal O(1/k^2)$. 
        \end{proposition}
        \begin{proof}
            We use the results from \ref{lemma:opt-mmntm-seq} because the sequence has the same assumption, then using the fact that $L_k$ is bounded then it gives:
            \begin{align*}
                \beta_k &\le 
                \left(
                    1 + \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                \right)^{-2}
                \hspace{-0.7em}\le 
                \left(
                    1 + \frac{k\alpha_0\sqrt{L_0}}{2\sqrt{L_{\max}}}
                \right)^{-2}. 
            \end{align*}
            Then, apply Theorem \ref{prop:inxt-apg-cnvg-generic} to obtain the desired results.
        \end{proof}
        \begin{remark}
            In this remark, we assert the fact that all assumptions made in the theorem can be satisfied on practice, and we will also bring attention to the current, and future roles played by each of the parameters used in the inexact algorithm. 
            \par
            Pay attention that $\alpha_k$ had been determined in the above theorem (as seemed in Lemma \ref{lemma:opt-mmntm-seq}), and $(B_k)_{k \le 0}$ is reserved for potential line search routine, the only parameter left undetermined in Definition \ref{def:inxt-apg} is the over-relaxation sequence $(\rho_k)_{k \ge 0}$, we only need the entire sequence to be bounded above. 
            We give the freedom to the practitioners to choose $(\rho_k)_{k \ge 0}$. 
            However, this sequence $\rho_k$ is not useless because it relaxes the upper bound for $\epsilon_k$, this has huge impact in the earlier stage (the first few iterations) of the algorithm $\Vert x_k - y_k\Vert$ is large. 
            Of course, the parameter $\mathcal E_0$ is also free to choose. 
            \par
            Finally, observe that from the above proof, in case when $p = 1$, the convergence rate would be $\mathcal O(\log(k)/k^2)$. 
        \end{remark}
        In the next subsection, we will show that under the assumption of the above theorem, there exists an error sequence $\epsilon_k$ such that it can never approach $0$ at an arbitrarily fast rate. 
    \subsection{The fastest rate of which the error schedule can shrink}
        To have overall convergence claim of the algorithm, it's necessary to prevent the error schedule $(\epsilon_k)_{k \ge 0}$ from crashing into zero too quickly. 
        Following Assumption \ref{ass:valid-err-schedule}, in this section, we will provide the lower bound results for $\epsilon_k$ in Theorem \ref{prop:opt-cnvg-outr-loop} to show that in the worst case it cannot approach zero arbitrarily fast, if we choose the largest possible $\epsilon_k$ using $\beta_k$. 
        \begin{lemma}[error schedule lower bound]\;\label{lemma:err-schedule-lbnd}\\
            Let, $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, $(\beta_k)_{k\ge 0}, \mathcal E_0, p$ be given by Assumption \ref{ass:opt-mmntm-seq}, $L_k := \rho_k + B_k$. 
            Let $(\epsilon_k)_{k \ge 0}$ be given by $\epsilon_k := \frac{\mathcal E_0 \beta_k}{k^p} + \rho_k \frac{\Vert x_k - y_k\Vert^2}{2}$, then it will be a valid error sequence and so that it satisfies the assumption. 
            If in addition, there exists $L_{\min}$ such that for all $k\ge 0$ such that it has $L_{\min} \le \min_{1 \le i \le k}L_i$ and, we assume $\mathcal E > 0$, then $\epsilon_k$ admits the non-trivial lower bound: 
            \begin{align*}
                \epsilon_k \ge \frac{\mathcal E_0}{k^p}\left(
                    1 + k \alpha_0 \sqrt{L_0} \sqrt{L_{\min}^{-1}}
                \right)^{-2} \ge \mathcal O(k^{-2-p}). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Recall Assumption \ref{ass:valid-err-schedule}, the largest valid error schedule is $\epsilon_k = \frac{\mathcal E_0 \beta_k}{k^p} + \rho_k \frac{\Vert x_k - y_k\Vert^2}{2}$. 
            Then it has 
            \begin{align*}
                \epsilon_k &\ge \frac{\mathcal E_0 \beta_k}{k^p}
                \\
                &\underset{(1)}{\ge} 
                \left(
                    1 + \alpha_0\sqrt{L_0}\sum_{i = 1}^{k}\sqrt{L_i^{-1}}
                \right)^{-2} 
                \frac{\mathcal E_0\beta_k}{k^p}
                \\
                &\underset{(2)}{\ge}
                \frac{\mathcal E_0\beta_k}{k^p}
                \left(
                    1 + k\alpha_0\sqrt{L_0}\sqrt{L_{\min}^{-1}}
                \right)^{-2} 
                \\
                &\ge \mathcal O(k^{-2-p}). 
            \end{align*}
            At (1), we used Lemma \ref{lemma:opt-mmntm-seq}. 
            At (2), we used that $L_{\min} \le L_i$ for all $i = 0, 1, 2, \ldots$. 
        \end{proof}

\section{Linear convergence for the proximal problem in the inner loop}
    In this section, we continue the discussion from Section \ref{sec:optz-inxt-pp-problem}. 
    As an important reminder, we will fix the vector $y \in \RR^n$, which is in the inexact proximal problem as a constant in this entire section. 
    \par
    The inner loop of the algorithm is another algorithm that evaluates $x_k \approx_\epsilon T_{(B_k + \rho_k)}(y_k)$ for a given value of $\epsilon, B + \rho$ and at the point $y_k$. 
    To accomplish, let $\lambda = (B_k + \rho_k)^{-1}$, the algorithm needs to resolve the following equivalent inexact proximal point problem: 
    \begin{align*}
        x_k \approx_\epsilon \hprox_{\lambda g}(y_k - \lambda \nabla f(y_k)). 
    \end{align*}
    Unfortunately recall that $g = \omega \circ A$ in the context of the outer loop hence it's impossible to directly evaluate the proximal operator of $g$ and hence we optimize the function $\Phi_\lambda$ as given by \eqref{eqn:primal-pp}. 
    \par
    In this section, we will show that there exists an algorithm generating the sequences $z_n, v_n$ such that $\mathbf G_\lambda (z_n, v_n)$ converges linearly if $\Psi_\lambda$ satisfies the error bound conditions. 
    Using results available in the literature, we will characterize the exact scenarios of $\omega \circ A$ when this is possible to achieve. 
    To start, the following assumption is the general error bound condition of a convex with additive composite structure. 
    \begin{assumption}[gradient mapping error bound]\;\label{ass:pg-eb}\\
        The following assumption is about $(F, f, g, L, S, \gamma)$. 
        Assume that
        \begin{enumerate}[nosep]
            \item $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, 
            \item let $\tau > 0$ be the step size inverse, let $T_{\tau}$ be the proximal operator of $f + g$ as given by $T_{\tau}(x) := \hprox_{\tau^{-1}g}(x - \tau^{-1}\nabla f(x))$, 
            \item $S= \argmin_{x}{f(x) + g(x)} \neq \emptyset$, 
            \item the objective function is given by $F = f + g$. 
        \end{enumerate}   
        Let the gradient mapping $\mathcal G_{\tau}$ be defined as: $\mathcal G_\tau(x) := \tau (x - T_\tau(x))$     
        In addition, assume that the optimization problem $F$ satisfies the error bound condition if it has for all $\tau \ge L, x \in \RR^n$ there exists $\gamma > 0$: 
        \begin{align*}
            \Vert \mathcal G_\tau(x)\Vert \ge \gamma\dist(x|S). 
        \end{align*}
    \end{assumption}
    \begin{definition}[proximal gradient method]\label{def:ista}
        Suppose that $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}. 
        Let $\tau \ge L$, and $x_0 \in \RR^n$. 
        Then an algorithm is a proximal gradient method if it generates iterates $(x_k)_{k \ge 0}$ such that they satisfy for all $k \ge 1$: 
        \begin{align*}
            x_{k + 1} = \hprox_{\tau^{-1} g}\left(x_k + \tau^{-1}\nabla f(x_k)\right). 
        \end{align*}
    \end{definition}
    \begin{assumption}[error bound for proximal problem]\;\label{ass:eb-for-pp}\\
        This assumption is about $(g, \omega, A, \Psi_\lambda, \gamma_\lambda)$
        Here are the assumptions
        \begin{enumerate}[nosep]
            \item Assume function $\Psi_\lambda$ as given by \eqref{eqn:dual-pp} which satisfies gradient mapping error bound (Assumption \ref{ass:pg-eb}) where, $f(v) = \frac{1}{2\lambda}\Vert \lambda A^\top v - y\Vert^2 - \frac{1}{2\lambda}\Vert y\Vert^2, g(v) = \omega^\star(v)$. 
            \item Assume the primal objective $\Phi_\lambda$ is a $L_1$ Lipschitz continuous on its domain. 
            \item Assume $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-prox}, with $\gamma = \gamma_\lambda$ we can do this because $f$ is quadratic and has a Lipschitz continuous gradient. 
        \end{enumerate}
    \end{assumption}
    \subsection{Error bound and linear convergence}
        The following theorem characterizes linear convergence of the proximal gradient method under gradient mapping error bound condition. 
        \begin{theorem}[linear convergence under gradient mapping error bound]\;\label{thm:lin-cnvg-ista-eb}\\
            Assume that $(F, f, g, L, S, \gamma)$ is given by Assumption \ref{ass:pg-eb}. 
            Under this assumption, the iterates $(x_k)_{k \ge 0}$ given by Definition \ref{def:ista} satisfies for all $k \ge 0, \bar x \in S$ and $\tau \ge L$ the inequality: 
            \begin{align*}
                F(x_{k + 1}) - F(\bar x)
                &\le 
                \left(
                    1 - \frac{\gamma}{2\tau}
                \right)(F(x_k) - F(\bar x)). 
            \end{align*}
            Hence, the algorithm generates $F(x_k) - F(\bar x)\le \mathcal O((1 - \gamma/(2\tau))^k)$. 
        \end{theorem}
        \begin{proof}
            Two important immediate results will be presented first.
            Consider the proximal gradient inequality from Theorem \ref{thm:inxt-pg-ineq}, but with $\rho = 0, \epsilon = 0, B = \tau$, then for all $x$ such that $\Vert \mathcal G_\tau(x)\Vert > 0$ it has for $\tilde x = T_{\tau}(x), z \in \RR^n$ the inequality 
            \begin{align*}
                F(\tilde x) - F(z) 
                &\le 
                \frac{\tau}{2}\Vert x - z\Vert^2 - \frac{\tau}{2}\Vert z - \tilde x\Vert^2
                \\
                &=  
                - \frac{\tau}{2}\Vert x - \tilde x\Vert^2
                + \tau\langle x - z, x - \tilde x\rangle
                \\
                &=  - \frac{1}{2\tau}\Vert \mathcal G_\tau(x) \Vert^2
                + \langle x - z, \mathcal G_\tau(x)\rangle
                \\
                &\le  - \frac{1}{2\tau}\Vert \mathcal G_\tau(x)\Vert^2 
                + \Vert x - z\Vert \Vert \mathcal G_\tau(x)\Vert
                \\
                &=
                \Vert \mathcal G_\tau(x)\Vert^2\left(
                    \frac{\Vert x - z \Vert}{\Vert \mathcal G_\tau(x)\Vert} - \frac{1}{2\tau}
                \right). 
            \end{align*}
            Now, for all $z = \bar x \in S$, from Assumption \ref{ass:eb-for-pp} $\exists \gamma > 0$ such that: 
            \begin{align*}
                \frac{\Vert x - z \Vert}{\Vert \mathcal G_\tau(x)\Vert}
                \le 
                \frac{\Vert x - z \Vert}{\gamma \dist(x | S)} \le \frac{1}{\gamma}. 
            \end{align*}
            Hence, for all $\bar x \in S$ it has 
            \begin{align}\label{ineq:lin-cnvg-ista-eb-pitem1}
                0\le F(\tilde x) - F(\bar x)&\le 
                \Vert \mathcal G_\tau(x)\Vert^2\left(
                    \frac{1}{\gamma} - \frac{1}{2\tau}
                \right). 
            \end{align}
            Obviously it has $\gamma^{-1} - (1/2)\tau^{-1} > 0$. 
            When $z = x$, we have the inequality: 
            \begin{align}\label{ineq:lin-cnvg-ista-eb-pitem2}
                F(\tilde x) - F(x) &\le - \frac{1}{2\tau}\Vert \mathcal G_\tau(x)\Vert^2. 
            \end{align}
            To derive the linear convergence, we use \eqref{ineq:lin-cnvg-ista-eb-pitem1} with $x = x_k, \tilde x = x_{k + 1}$:
            {\allowdisplaybreaks
            \begin{align*}
                0 &\le 
                \Vert \mathcal G_\tau(x_k)\Vert^2\left(
                    \frac{1}{\gamma} - \frac{1}{2\tau} 
                \right)
                - F(x_{k + 1}) + F(\bar x)
                \\
                &= 
                \frac{1}{2\tau}\Vert \mathcal G_\tau(x_k)\Vert^2\left(
                    \frac{2\tau}{\gamma} - 1
                \right)
                - F(x_{k + 1}) + F(\bar x)
                \\
                &\underset{(1)}{\le}
                \left(
                    \frac{2\tau}{\gamma} - 1
                \right)
                \left(
                    F(x_k) - F(x_{k + 1})
                \right)
                - F(x_{k + 1}) + F(\bar x)
                \\
                &= 
                \left(
                    \frac{2\tau}{\gamma} - 1
                \right)
                \left(
                    F(x_k) - F(\bar x) + F(\bar x) - F(x_{k + 1})
                \right)
                - F(x_{k + 1}) + F(\bar x)
                \\
                &= \frac{2\tau}{\gamma}(F(\bar x) - F(x_{k + 1}))
                + \left(
                    \frac{2\tau}{\gamma} - 1
                \right)(F(x_k) - F(\bar x)). 
            \end{align*}
            }
            At (1) we used \eqref{ineq:lin-cnvg-ista-eb-pitem2}. 
            Multiple both side by $\frac{\gamma}{2\tau}$ then we are done. 
        \end{proof}
    \subsection{Conditions for linear convergence of the proximal problem}
        In this section, we will focus on the sufficient characterization of the proximal problem which allows proximal gradient method to achieve linear convergence rate. 
        We can immediately claim quadratic growth condition when $\omega = \Vert \cdot\Vert_1$. 
        The next proposition will characterize a precise case where Assumption \ref{ass:eb-for-pp} is true, and it's a case widely available in applications. 
        \begin{proposition}[1-norm problem]\label{prop:1nrm-prox-problem}
            Let $(g, \omega, A)$ satisfies Assumption \ref{ass:for-inxt-prox}. 
            In addition, if $g := \Vert \cdot\Vert_1$, then the function $\Psi_\lambda$ satisfies Assumption \ref{ass:eb-for-pp}. 
        \end{proposition}
        \begin{proof}
            Take note that the dual has closed form $g^\star(z) =\delta(z | \{x : \Vert x\Vert_1 \le 1\})$. 
            The $g^\star$ is an indicator of a polytope constraint. 
            The objective function $\Psi_\lambda(v) = \frac{1}{2\lambda}\Vert \lambda A^\top v - y\Vert^2 + \omega^\star(v) - \frac{1}{2\lambda}\Vert y\Vert^2$ so it's Lipschitz smooth, and it must have a set of minimizers $S$ because its domain is compact. 
            In addition, $\Psi_\lambda$ fits the assumption of Necoara et al. \cite[Theorem 8]{necoara_linear_2019}. 
            Therefore, $\Psi_\lambda$ is quasi-strongly convex then by \cite[Theorem 4]{necoara_linear_2019}, and \cite[Theorem 7]{necoara_linear_2019}, it satisfies error bound condition as given in Assumption \ref{ass:pg-eb}, hence Assumption \ref{ass:eb-for-pp} also. 
            \par
            Let $\mu_f = \Vert A\Vert^2/\kappa_f$, let $\kappa_f = \theta^{-2}(A, C)$ be the Hoffman Constant as presented by Necoara et al. in \cite[Section 4]{necoara_linear_2019} where $C$ is the constraint matrix of the inequality set $\delta(z | \{x : \Vert x\Vert_1 \le 1\})$, then the error bound constant can be satisfied with
            \begin{align*}
                \gamma_\lambda = \frac{\kappa_f}{1 + \lambda\mu_f + \sqrt{1 + \lambda\mu_f}}. 
            \end{align*}
        \end{proof}
        \begin{remark}
            It is very difficult to obtain a lower estimate for $\gamma$ in practice. 
        \end{remark}
        \par
        The following propositions precisely show that the linear convergence is achievable for the inner loop when $g = \Vert \cdot\Vert_1$ for our optimization objective. 
        \begin{proposition}[sufficient conditions of linear convergence of the inner loop]\;\label{prop:inn-loop-lin-cnvg}\\
            Let $(g, \omega, A)$ satisfies Assumption \ref{ass:for-inxt-prox}. 
            Consider the following proximal problem: 
            \begin{enumerate}[nosep]
                \item The primal proximal problem $\Phi_\lambda(u) = \frac{1}{2\lambda}\Vert u - y\Vert^2 + \omega(Au)$.
                \item The dual proximal problem $\Psi_\lambda(v) = f(v) + g(v)$ where \\ $f(v) := \frac{1}{2\lambda}\left\Vert \lambda A^\top v - y\right\Vert^2 - \frac{1}{2\lambda}\Vert y\Vert^2, g(v) := \omega^\star(v)$. 
            \end{enumerate}
            If, $\Psi_\lambda$ satisfies Assumption \ref{ass:eb-for-pp}. 
            Let $L_1$ be the global Lipschitz continuity constant of $\Phi_\lambda$ which it has in this case. 
            Let $\tau \ge L, \epsilon \ge 0$, $v_0 \in \dom \Psi_\lambda$ be an initial condition for an algorithm that satisfies Definition \ref{def:ista} with $F(v) = f(v) + g(v)$ and let the iterates $(v_n)_{n \ge 0}$ be generated by it. 
            Then, there exists a minimizer $\bar v$ for $\Psi_\lambda$, and the followings are true: 
            \begin{enumerate}[nosep]
                \item The sequence $\Psi_\lambda(v_n) - \Psi_\lambda(\bar v)$ converges linearly to zero. \label{prop:inn-looplin-cnvg-item1}
                \item If we choose $z_n := y - \lambda A^\top v_n$, and $\bar z := y - \lambda A^\top \bar v$, then $\Phi_\lambda(z_n) - \Phi_\lambda(\bar z)$ converges linearly. \label{prop:inn-looplin-cnvg-item2}
                \item The duality gap satisfies for all $n \ge 0$ the inequality 
                $$\mathbf G_\lambda(z_n, v_n) \le \left(1 - \frac{\gamma_\lambda}{2\tau}\right)^{n/2}\left(1 + L_1\sqrt{2 \lambda}\right)(\Psi_\lambda(v_0) - \Psi_\lambda(\bar v)).$$
                \label{prop:inn-looplin-cnvg-item3}
                \item If $\mathbf G_\lambda(z_n, v_n) \le \epsilon$, then $z_n \approx_\epsilon \hprox_{\lambda g}(y)$. 
                \label{prop:inn-looplin-cnvg-item4}
            \end{enumerate}
        \end{proposition}
        \begin{proof}
            To see \ref{prop:inn-looplin-cnvg-item1}, the sequence $\Psi_\lambda(v_n) - \Psi_\lambda(\bar v)$ has linear convergence by Proposition \ref{prop:1nrm-prox-problem} because we assumed that $\Psi_\lambda$ satisfies Assumption \ref{ass:eb-for-pp}. 
            More precisely it has: 
            \begin{align}\label{ineq:inn-loop-lin-cnvg-pitem1}
                \Psi_\lambda(v_n) - \Psi_\lambda(\bar v) &\le 
                \left(
                    1 - \frac{\gamma_\lambda}{2\tau}
                \right)^n
                \left(
                    \Psi_\lambda(v_0) - \Psi_\lambda(\bar v)
                \right). 
            \end{align}
            \par
            We will now proceed to proving \ref{prop:inn-looplin-cnvg-item2}. 
            Because the iterates are given by $z_n = y - \lambda A^\top v_n$, and $\bar z = y - \lambda A^\top \bar v$, it provides $\Phi_\lambda(z_n) - \Phi_\lambda(v_0)$, we can use Theorem \ref{fact:minimizing-dual-pp} which gives: 
            \begin{align*}
                \Phi_\lambda(z_n) - \Phi_\lambda(\bar z) &\le L_1 \sqrt{2\lambda}\left(
                    \Psi_\lambda(v_n) - \Psi_\lambda (\bar v)
                \right)^{1/2}
                \\
                &\underset{(1)}{\le} 
                L_1\sqrt{2\lambda} \left(
                    1 - \frac{\gamma_\lambda}{2\tau}
                \right)^{n/2}\left(
                    \Psi_\lambda(v_0) - \Psi_\lambda(\bar v)
                \right)^{1/2}. 
            \end{align*}
            At (1), we used convergence results from Theorem \ref{thm:lin-cnvg-ista-eb}. 
            \par
            To prove \ref{prop:inn-looplin-cnvg-item3}, by definition of duality gap in \eqref{eqn:duality-gap-pp} we have:
            \begin{align*}
                \mathbf G_\lambda(z_n, v_n) 
                &\underset{(?)}{=} \Phi_\lambda(z_n) + \Psi_\lambda(v_n) + 0
                \\
                &= \Phi_\lambda(z_n) + \Psi_\lambda(v_n) - \Phi_\lambda(\bar z) - \Psi_\lambda(\bar v)
                \\
                &= \Phi_\lambda(z_n)- \Phi_\lambda(\bar z) + \Psi_\lambda(v_n) - \Psi_\lambda(\bar v)
                \\
                &\underset{(2)}{\le} 
                L_1 \sqrt{2\lambda}\left(
                    1 - \frac{\gamma_\lambda}{2\tau}
                \right)^{n/2}\left(
                    \Psi_\lambda(v_0) - \Psi_\lambda(\bar v)
                \right)^{1/2}
                + \left(
                    1 - \frac{\gamma_\lambda}{2\tau}
                \right)^{n}\left(
                    \Psi_\lambda(v_0) - \Psi_\lambda(\bar v)
                \right)
                \\
                &= \left(
                \right)
            \end{align*}
        \end{proof}
        \begin{remark}
            To choose a feasible $v_0$, we can...
            \todoinline{
                REMEMBER TO FINISH IT. 
            }
        \end{remark}
        \begin{definition}[piecewise linear-quadratic function {\cite[Definition 10.20]{rockafellar_variational_1998}}]
            A function $f: \RR^n \rightarrow \overline \RR$ is piecewise linear-quadratic when $\dom f$ is the union of finitely many piecewise polyhedral set, such that on each polyhedral partition of $f$ there exists $\alpha \in \RR, a \in \RR^n$, $C\in \RR^{n \times n}$ as a symmetric matrix is where $f$ has the representation $x \rightarrow \langle x, Cx\rangle + \langle a, x\rangle + c$. 
        \end{definition}
        The following proposition characterizes another case where error bound conditions of problem holds. 
        \begin{proposition}[convex PLQ proximal problem]\label{prop:plq-pp}
            
        \end{proposition}
        \begin{proposition}[scenario II, convex PLQ linear composite]\label{prop:plq-prox-problem}
            
        \end{proposition}

        \begin{example}[inner loop linear convergence rate examples]\label{example:inn-lp-lin-cnvg}
            
        \end{example}
    \subsection{Inner loop complexity}
        In this section, we will estimate the total number of iterations in the inner loop to achieve a given accuracy $\epsilon$ on primal objective $\Phi_\lambda$ of the proximal problem for some given $\lambda > 0$. 
        We will specify the context for parameters $\epsilon, \lambda$ in relation to the outer problem of obtaining $x_k \approx_{\epsilon_k} T_{B_k + \rho_k}(y_k)$. 


\section{Total complexity of the algorithm}
    This section puts results regarding the total complexity of the proposed inexact proximal gradient algorithm. 


\bibliographystyle{siam}
\bibliography{references/refs.bib}
\end{document}