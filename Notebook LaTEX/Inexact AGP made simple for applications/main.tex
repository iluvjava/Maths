\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% NOTATIONS FOR THIS PAPER 
\DeclareMathOperator{\dist}{\mathop{dist}}
\DeclareMathOperator{\rng}{\mathop{rng}}



\title{{\fontfamily{ptm}\selectfont Inexect Accelerated Proximal Gradient }}

\author{
    Author 1 Name, Author 2 Name
    \thanks{
        Subject type, Some Department of Some University, Location of the University,
        Country. E-mail: \texttt{author.namee@university.edu}.
    }
}

\begin{document}

% TITLE, ABSTRACT ==============================================================
\date{\today}
\maketitle
\todoinline{This paper is currently in draft mode. Check source to change options. }
\begin{abstract} 
    \noindent
    This is still a draft. \cite{zhang_robust_2022}. 
\end{abstract}
\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

\section{Introduction}
    \textbf{Notations.}
    Let $g: \RR^n \rightarrow\overline \RR$, we denote $g^\star$ to be the Fenchel conjugate. 
    $I: \RR^n \rightarrow \RR^n$ denotes the identity operator.
    For a multivalued mapping $T: \RR^n \rightarrow 2^{\RR^n}$, $\gra T$ denotes the graph of the operator, defined as $\{(x, y)\in \RR^n \times \RR^n : y \in Tx\}$. 
    \subsection{Epsilon subgradient and inexact proximal point}
        \begin{definition}[$\epsilon$-subgradient]\label{def:esp-subgrad}
            Let $g: \RR^n \rightarrow \overline \RR$ be proper, lsc. 
            Let $\epsilon \ge 0$. 
            Then the $\epsilon$-subgradient of $g$ at some $\bar x \in \dom g$ is given by: 
            $$
            \begin{aligned}
                \partial g_\epsilon(\bar  x) := 
                \left\lbrace
                    v \in \RR^n \left| \; 
                        \langle v, x - \bar  x\rangle \le 
                        g(x) - g(\bar x) + \epsilon \;\forall x \in \RR^n
                    \right. 
                \right\rbrace.
            \end{aligned}
            $$
            When $\bar x \not \in \dom g$, it has $\partial g_\epsilon(\bar x) = \emptyset$. 
        \end{definition}
        \begin{remark}
            $\partial_\epsilon g$ is a multivalued operator and, it's not monotone, unless $\epsilon = 0$, which makes it equivalent to French subgradient $\partial g$. 
        \end{remark}
        If we assume lsc, proper and convex $g$, we will now introduce results in the literatures that we will use. 
        \begin{fact}[$\epsilon$-Fenchel inequality]\label{fact:esp-fenchel-ineq}
            Let $\epsilon \ge 0$, then:
            \begin{align*}
                x^* \in \partial_\epsilon f(\bar x)\iff f^\star(x^*) + f(\bar x) \le \langle x^*, \bar x\rangle + \epsilon \implies \bar x \in \partial_\epsilon f^\star(x^*).
            \end{align*}
            They are all equivalent if $f^{\star\star}(\bar x) = f(\bar x)$. 
        \end{fact}
        \begin{remark}
            The above fact is taken from Zalinascu \cite[Theorem 2.4.2]{zalinescu_convex_2002}. 
        \end{remark}
        % \begin{fact}[strong epsilon subgradient sum rule]\label{fact:esp-subgrad-sum-rule}
        %     Take both $f, g: \RR^n\rightarrow \overline \RR$. 
        %     Let $\epsilon \ge 0$. 
        %     Then the epsilon subgradient sum rule states that 
        %     \begin{align*}
        %         \partial_\epsilon[f + g](\bar x) = 
        %         \bigcup \left\lbrace
        %             \partial_{\epsilon_1} f(\bar x) + 
        %             \partial_{\epsilon_2} g(\bar x) \left|\; 
        %                 \min(\epsilon_1, \epsilon_2) \ge 0\wedge 
        %                 \epsilon_1 + \epsilon_2 = \epsilon
        %             \right.
        %         \right\rbrace
        %     \end{align*}
        %     Under the scenario that: $\reli\dom f \cap \reli\dom g \neq \emptyset$. 
        % \end{fact}
        % \begin{remark}
        %     The above is taken from Morduchovich, Mau Nam \cite[Theorem 5.19]{mordukhovich_convex_2022}. 
        % \end{remark}
        We will now define inexact proximal point based on $\epsilon$-subgradient
        \begin{definition}[inexact proximal point]\label{def:inxt-pp}
            For all $x \in \RR^n, \epsilon \ge 0, \lambda > 0$, $\tilde x$ is an inexact evaluation of proximal point at $x$, if and only if it satisfies: 
            \begin{align*}
                \lambda^{-1}(x - \tilde x) \in \partial_{\epsilon} g(\tilde x). 
            \end{align*}
            We denote it by $\tilde x \approx_\epsilon \hprox_{\lambda g}(x)$. 
        \end{definition}
        \begin{remark}
            This definition is nothing new, for example see Villa et al. \cite[Definition 2.1]{villa_accelerated_2013}
        \end{remark}
        \begin{fact}[the resolvant identity]\label{fact:resv-identity}
            Let $T: \RR^n \rightarrow 2^{\RR^n}$, then it has: 
            \begin{align*}
                (I + T)^{-1} = (I - (I + T^{-1})^{-1}).
            \end{align*}
        \end{fact}
        \begin{theorem}[inexact Moreau decomposition]
            Let $g: \RR^n \rightarrow \overline \RR$ be a closed, convex and proper function. 
            It has the equivalence
            \begin{align*}
                \tilde y \approx_\epsilon \hprox_{\lambda^{-1}g^\star}(\lambda^{-1}y)
                \iff 
                y - \lambda \tilde y \approx_\epsilon \hprox_{\lambda g}(y). 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Consider $\tilde y \approx_\epsilon \hprox_{\lambda^{-1}g^\star}(\lambda^{-1}y)$, then it has: 
            \begin{align*}
                & 
                \tilde y 
                \in (I + \lambda^{-1}\partial_\epsilon g^\star)^{-1}(\lambda^{-1}y)
                \\
                \iff &
                (\lambda^{-1}y, \tilde y)\in 
                \gra(I + \lambda^{-1}\partial_\epsilon g^\star)^{-1}
                \\
                \underset{(1)}{\iff} &
                (\lambda^{-1}y, \tilde y)\in 
                \gra(I - (I + \partial_\epsilon g\circ(\lambda I))^{-1})
                \\
                \iff &
                (\lambda^{-1}y, \lambda^{-1}y - \tilde y)\in 
                \gra(I + \partial_\epsilon g\circ(\lambda I))^{-1}
                \\
                \iff &
                (\lambda^{-1}y - \tilde y, \lambda^{-1}y)\in 
                \gra(I + \partial_\epsilon g\circ(\lambda I))
                \\
                \iff &
                (y - \lambda\tilde y, \lambda^{-1}y)\in 
                \gra(\lambda^{-1}I + \partial_\epsilon g)
                \\
                \iff &
                (y - \lambda\tilde y, y)\in 
                \gra(I + \lambda\partial_\epsilon g)
                \\
                \iff& 
                y - \lambda \tilde y \in 
                (I + \lambda \partial_\epsilon g)^{-1}y
                \\
                \iff& 
                y - \lambda \tilde y \approx_\epsilon \hprox_{\lambda g}(y). 
            \end{align*}
            At (1) we can use Fact \ref{fact:resv-identity}, and it has $(\lambda^{-1}\partial_\epsilon g^\star)^{-1} = \partial_\epsilon g\circ(\lambda I)$ by Fact \ref{fact:esp-fenchel-ineq} and the assumption that $g$ is closed, convex and proper. 
        \end{proof}

    \subsection{Inexact proximal gradient inequality}
        \begin{assumption}[for inexact proximal gradient]\label{ass:for-inxt-pg-ineq}
            The assumption is about $(f, g, L)$. 
            We assume that 
            \begin{enumerate}[nosep]
                \item $f: \RR^n \rightarrow \RR$ is a convex, $L$ Lipschitz function. 
                \item $g: \RR^n \rightarrow \overline\RR$ is a convex, proper, and lsc function which we do not have its exact proximal operator. 
            \end{enumerate}
        \end{assumption}
        We develop the theory based on the use of epsilon subgradient as in Definition \ref{def:esp-subgrad}. 
        \begin{definition}[inexact proximal gradient]\label{def:inxt-pg}
            Let $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}. 
            Let $\epsilon \ge 0, \rho > 0$. 
            Then, $\tilde x \approx_\epsilon T_\rho(x)$ is an inexact proximal gradient if it satisfies variational inequality: 
            \begin{align*}
                \mathbf 0 \in \nabla f(x) + \rho(x - \tilde x) + \partial_{\epsilon} g(\tilde x). 
            \end{align*}
        \end{definition}
        \begin{remark}
            We assumed that we can get exact evaluation of $\nabla f$ at any points $x \in \RR^n$. 
        \end{remark}
        \begin{lemma}[other representations of inexact proximal gradient]\;\\
            Let $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, $\epsilon \ge 0, \rho > 0$, then for all $x \approx_\epsilon T_\rho(x)$, it has the following equivalent representations: 
            \begin{align*}
                & (x - \rho^{-1}\nabla f(x)) - \tilde x 
                \in \rho^{-1} \partial_\epsilon g(\tilde x)
                \\
                \iff 
                & \tilde x \in (I + \rho^{-1}\partial_\epsilon g(\tilde x))^{-1}
                (x - \rho^{-1}\nabla f(x))
                \\
                \iff 
                & x \approx_\epsilon \hprox_{\rho^{-1} g}
                \left(x - \rho^{-1}\nabla f(x)\right)
            \end{align*}
        \end{lemma}
        \begin{proof}
            It's direct. 
        \end{proof}
        \begin{theorem}[inexact over-regularized proximal gradient inequality]\;\label{thm:inxt-pg-ineq}\\
            Let $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, $\epsilon \ge 0, B \ge 0, \rho > 0$. 
            Consider $\tilde x \approx_\epsilon T_{B + \rho}(x)$. 
            Denote $F = f + g$. 
            If in addition, $\tilde x, B$ satisfies the line search condition $D_f(\tilde x, x) \le B/2\Vert x - \tilde x\Vert^2$, then it has $\forall z \in \RR^n$: 
            \begin{align*}
                - \epsilon &\le 
                F(z) - F(\tilde x)
                + \frac{B + \rho}{2}\Vert x - z\Vert^2
                - \frac{B + \rho}{2}\Vert z - \tilde x\Vert^2
                - \frac{\rho}{2}\Vert \tilde x - x\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            By Definition \ref{def:inxt-pg} write the variational inequality that describes $\tilde x \approx_\epsilon T_B(x)$, and the definition of epsilon subgradient (Definition \ref{def:esp-subgrad}) it has for all $z \in \RR^n$: 
            \begin{align*}
                - \epsilon &\le 
                g(z) - g(\tilde x) - \langle (B + \rho)(\tilde x - x) - \nabla f(x), z - \tilde x\rangle
                \\
                &= 
                g(z) - g(\tilde x) 
                - (B + \rho)\langle \tilde x - x, z - \tilde x\rangle
                + \langle \nabla f(x), z - \tilde x\rangle
                \\
                &\underset{(1)}{\le} 
                g(z) + f(z) - g(\tilde x) - f(\tilde x)
                - (B + \rho)\langle \tilde x - x, z - \tilde x\rangle
                - D_f(z, x) + D_f(\tilde x, x)
                \\
                &\underset{(2)}{\le} 
                F(z) - F(\tilde x)
                - (B + \rho)\langle \tilde x - x, z - \tilde x\rangle
                + \frac{B}{2}\Vert \tilde x - x\Vert^2
                \\
                &=
                F(z) - F(\tilde x) + \frac{B + \rho}{2}\left(
                    \Vert x - z\Vert^2
                    - \Vert \tilde x - x\Vert^2
                    - \Vert z - \tilde x\Vert^2
                \right)
                + \frac{B}{2}\Vert \tilde x - x\Vert^2
                \\
                &= 
                F(z) - F(\tilde x)
                + \frac{B + \rho}{2}\Vert x - z\Vert^2
                - \frac{B + \rho}{2}\Vert z - \tilde x\Vert^2
                - \frac{\rho}{2}\Vert \tilde x - x\Vert^2. 
            \end{align*}
            At (1), we used considered the following: 
            \begin{align*}
                \langle \nabla f(x), z - x\rangle &= \langle \nabla f(x), z - x + x - \tilde x\rangle
                \\
                &= \langle \nabla f(x), z - x\rangle + \langle \nabla f(x), x - \tilde x\rangle
                \\
                &= -D_f(z, x) + f(z) - f(x) + D_f(\tilde x, x) - f(\tilde x) + f(x)
                \\
                &= -D_f(z, x) + f(z) + D_f(\tilde x, x) - f(\tilde x). 
            \end{align*}
            At (2), we used the fact that $f$ is convex hence $- D_f(z, x) \le 0$ always, and in the statement hypothesis we assumed that $B$ has $D_f(\tilde x, x) \le B/2\Vert \tilde x - x\Vert^2$. 
        \end{proof}
    \subsection{Optimizing the inexact proximal point problem}
        In this section we will show an optimization problem that allows us to solve for some $\tilde x \approx_\epsilon \hprox_{\lambda g}(z)$. 
        Most of these results are from the literature. 
        To start, we must assume the following about a function $g: \RR^n \rightarrow \overline \RR$, with $g$ closed, convex and proper. 
        \begin{assumption}[for inexact proximal operator]\;\label{ass:for-inxt-prox}\\
            This assumption is about $(g, \omega, A)$. 
            Let $m \in \N, n \in \RR^n$, we assume that 
            \begin{enumerate}[nosep]
                \item $A\in \RR^{m \times n}$ is a matrix. 
                \item $\omega: \RR^n \rightarrow \overline \RR$ is a closed and convex function such that it admits proximal operator $\hprox_{\lambda\omega}$ and, its conjugate $\omega^\star$ is known. 
                \item $g := \omega(Ax)$ such that $\rng A \cap \reli\dom g \neq\emptyset$. 
            \end{enumerate}
        \end{assumption}
        Now, we are ready to discuss how to choose $\tilde x \approx_\epsilon \hprox_{\lambda g}(x)$. 
        Fix $y \in \RR^n, \lambda > 0$, we are ultimately interested in minimizing: 
        \begin{align}
            \Phi_\lambda(u) &:= \omega(Au) + \frac{1}{2\lambda} \Vert u - y\Vert^2
        \end{align}
        This problem admits dual objective in $\RR^m$: 
        \begin{align}
            \Psi_\lambda(v) &:=
            \frac{1}{2\lambda}\Vert \lambda A^\top v - y\Vert^2
            + \omega^\star(v) - \frac{1}{2\lambda}\Vert y\Vert^2. 
        \end{align}
        We define the duality gap
        \begin{align}
            \mathbf G_\lambda(u, v) &:= \Phi_\lambda(u) + \Psi_\lambda(v). 
        \end{align}
        If strong duality holds, it exists $(\hat u, \hat v)$ such that we have the following: 
        \begin{align*}
            \mathbf G_\lambda(\hat u, \hat v) = 0 = \min_{u} \Phi_\lambda(u) + \min_v \Psi_\lambda(v)
        \end{align*}
        The following theorem quantifies a sufficient conditions for $\tilde x \approx_\epsilon \hprox_{\lambda g}(x)$. 
        The theorem below is from \cite[Proposition 2.2]{villa_accelerated_2013}. 
        \begin{theorem}[primal translate to dual]\label{thm:primal-dual-trans}
            Let $(g, \omega, A)$ satisfies Assumptiom \ref{ass:for-inxt-prox}, $\epsilon \ge 0$, then 
            \begin{align*}
                \left(
                    \forall z \approx_\epsilon \hprox_{\lambda g}(y) 
                \right)(\exists v \in \dom \omega^\star): z = y - \lambda A^\top v. 
            \end{align*}
        \end{theorem}
        This theorem that follows is from Villa et al. \cite[Proposition 2.3]{villa_accelerated_2013}, but put into our symbols and, Definition 
        \begin{theorem}[duality gap of inexact proximal problem]\label{thm:dlty-gap-inxt-pp}
            Let $(g, \omega, A)$ satisfies Assumption \ref{ass:for-inxt-prox}, for all $\epsilon \ge 0$, $v \in \RR^n$ consider the following conditions: 
            \begin{enumerate}[nosep]
                \item $\mathbf G_\lambda(y - \lambda A^\top v, v) \le \epsilon$. 
                \item $A^\top v \approx_\epsilon \hprox_{\lambda^{-1}g^\star}(\lambda^{-1}y)$. 
                \item $y - \lambda A^\top v \approx_{\epsilon} \hprox_{\lambda g}(y)$. 
            \end{enumerate}
            They have $(a)\implies (b) \iff (c)$. 
            If in addition $\omega^\star(v) = g^\star(A^\top v)$, then all three conditions are equivalent. 
        \end{theorem}
        Next, let's explore some options for minimizing the duality gap of the proximal problem. 
        \todoinline{
            STILL WRITING AND NOT FINISHED YET! 
        }
    \subsection{Literature reviews}
    \subsection{Our contributions}

\section{The accelerated proximal gradient with controlled errors}
    In this section, we present an accelerated algorithm with controlled error using Definition \ref{def:inxt-pg}, and show that it can have a convergence rate under certain error conditions. 
    \begin{definition}[our inexact accelerated proximal gradient]\;\label{def:inxt-apg}\\
        Suppose that $(F, f, g, L)$ and, sequences $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$ satisfies the following
        \begin{enumerate}[nosep]
            \item $(\alpha_k)_{k \ge 0}$ is a sequence such that $\alpha \in (0, 1]$ for all $k \ge 0$. 
            \item $(B_k)_{k \ge 0}$ is a non-negative sequence, characterizing the potential line search routine. 
            \item $(\rho_k)_{k \ge 0}$ be a sequence such that $\rho_k > 0$, characterizing the over-relaxation of the proximal gradient operator. 
            \item $(\epsilon_k)_{k \ge 0}$ is a non-negative sequence characterizing the errors of inexact proximal evaluation.
            \item $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, and let $F = f + g$. 
        \end{enumerate}
        Denote $L_k = B_k + \rho_k$ for short. 
        Given any initial condition $v_{-1}, x_{-1} \in \RR^n$, the algorithm generates the sequences $(y_k, x_k, v_k)_{k \ge 0}$ such that they satisfies for all $k \ge 0$: 
        \begin{align*}
            & y_k = \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}, 
            \\
            & x_k \approx_{\epsilon_k} T_{L_k}(y_k), 
            \\
            & D_f(x_k, y_k) \le \frac{B_k}{2}\Vert x_k - y_k\Vert^2, 
            \\
            & v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}). 
        \end{align*}
    \end{definition}
    \begin{lemma}[inexact accelerated proximal gradient preparation stage I]\; \label{lemma:inxt-apg-cnvg-prep1}\\
        Let $(f, g, L)$, and $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, be given by Definition \ref{def:inxt-apg}. 
        Denote $L_k = B_k + \rho_k$. 
        Then, for any $\bar x \in \RR^n$, the sequences $(y_k, x_k, v_k)_{k \ge 0}$ generated satisfy for all $k \ge 1$ the inequality: 
        \begin{align*}
            & \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k
            \\
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            \\ &
            + \max\left(
                    1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
                \right)\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
        \end{align*}
        When, $k = 1$ it instead have: 
        \begin{align*}
            & \frac{\rho_0}{2}\Vert x_0 - y_0\Vert^2 - \epsilon_0 
            \\
            &\le 
            (1 - \alpha_0)(F(x_{-1}) - F(\bar x)) + F(\bar x) - F(x_0) 
            + \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_{-1}\Vert^2
            - \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_0\Vert^2. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Two intermediate results are in order before we can prove the inequality. 
        Define $z_k := \alpha_k \bar x + (1 - \alpha_k)x_{k - 1}$ for short. 
        It has for all $k \ge 1$ the equality: 
        \begin{align}\tag{a}\label{eqn:inxt-apg-cnvg-prep1-a}\begin{split}
            z_k - x_k &= 
            \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - x_k
            \\
            &= \alpha_kx^+ + (x_{k - 1} - x_k) - \alpha_kx_{k - 1}
            \\
            &= \alpha_k \bar x - \alpha_k v_k. 
        \end{split}\end{align}
        It also has for all $k \ge 1$ the equality: 
        \begin{align}\tag{b}\label{eqn:inxt-apg-cnvg-prep1-b}\begin{split}
            z_k - y_k &= 
            \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - y_k
            \\
            &= \alpha_k \bar x - \alpha_k v_{k - 1}. 
        \end{split}\end{align}
        Let's denote $L_k = B_k + \rho_k$ for short. 
        Recall that $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, if we choose $x = y_k$ so $\tilde x = x_k \approx_\epsilon T_{L_k}(y_k)$, and set $z = z_k, \epsilon = \epsilon_k$ then Theorem \ref{thm:inxt-pg-ineq} has: 
        \begin{align*}
            & \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k
            \\
            &\le
            F(z_k) - F(x_k) + \frac{L_k}{2}\Vert y_k - z_k\Vert^2 - \frac{L_k}{2}\Vert z_k - x_k\Vert^2
            \\
            &\underset{(1)}{\le} \alpha_k F(\bar x) + (1 - \alpha_k)F(x_{k - 1}) - F(x_k)
            + \frac{L_k}{2}\Vert y_k - z_k\Vert^2 - \frac{L_k}{2}\Vert z_k - x_k\Vert^2
            \\
            &\underset{(2)}{=} 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_{k}\Vert^2 
            \\
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            \\ &
            + \max\left(
                    1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
                \right)\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
        \end{align*}
        At (1) we used the fact that $F = f + g$ hence $F$ is convex. 
        At (2) we used \eqref{eqn:inxt-apg-cnvg-prep1-a}, \eqref{eqn:inxt-apg-cnvg-prep1-b}. 
        Finally, if $k = 0$, then take the RHS of $\underset{(1)}{=}$ then:
        \begin{align*}
            & \frac{\rho_0}{2}\Vert x_0 - y_0\Vert^2 - \epsilon_0 
            \\
            &\le 
            (1 - \alpha_0)(F(x_{-1}) - F(\bar x)) + F(\bar x) - F(x_0) 
            + \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_{-1}\Vert^2
            - \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_0\Vert^2. 
        \end{align*}
    \end{proof}
    \par
    The following proposition is a prototype of the convergence rate together with the error schedule that delivers convergence of algorithms satisfying Definition \ref{def:inxt-apg}. 
    \begin{proposition}[valid error schedule and convergence rate]\;\label{prop:inxt-apg-cnvg-generic}\\
        Let $(f, g, L)$, $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$ be given by Definition \ref{def:inxt-apg}. 
        Fix any $\bar x \in \RR^n$ for all $k \ge 0$ and assume that $\alpha_0 = 1$. 
        Denote for brevity $\beta_0 = 1$, $\beta_k = \prod_{i = 1}^{k} \max\left(1 - \alpha_i, \frac{\alpha_i^2L_i}{\alpha_{i - 1}^2L_{i - 1}}\right)$ and $L_k = B_k + \rho_k$. 
        If for some fixed $\mathcal E_0 \ge 0, p \ge 1$ the parameter $\rho_k, \epsilon_k$ can satisfy for all $k \ge 0$ the condition
        \begin{align*}
            \frac{- \mathcal E_0\beta_k}{k^p} &\le 
            \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k. 
        \end{align*}
        Then for the sequence generated $(y_k, x_k, v_k)_{k \ge 0}$ by the algorithm, for all $k \ge 0$ they satisfy: 
        \begin{align*}
            & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
            \le 
            \beta_k \left(
                \frac{L_0}{2}\Vert \bar x - v_{-1}\Vert^2
                + \mathcal E_0 \sum_{i = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}
        Conside results from Lemma \ref{lemma:inxt-apg-cnvg-prep1} has $\forall k \ge 1$: 
        \begin{align*}
            & \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k
            \\
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            \\ &
            + \max\left(
                    1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
                \right)\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
            \\
            &\le \max\left(
                1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
            \right)\left(
                F(x_{k - 1}) - F(\bar x)
                + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            \right)
            \\&\quad 
                + F(\bar x) - F(x_k) - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
        \end{align*}
        For notation brevity, we introduce $\beta_k, \Lambda_k$: 
        \begin{align*}
            \beta_0 &= 1, 
            \\
            \beta_k &:= \prod_{i = 1}^{k} \max\left(
                1 - \alpha_i, \frac{\alpha_i^2L_i}{\alpha_{i - 1}^2L_{i - 1}}
            \right),
            \\
            \Lambda_k &:= 
            - F(\bar x) + F(x_k) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
        \end{align*}
        Now, suppose that in addition there is a non-negative sequence $(\mathcal  E_k)_{k \ge 0}$ such that 
        \begin{enumerate}[nosep]
            \item For all $k \ge 0$, it has $\frac{-\mathcal E_k}{k^p} \le \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k$ where $p \ge 1$, 
            \item For all $k \ge 1$, it has $\mathcal E_k = \frac{\beta_k}{\beta_{k - 1}}\mathcal E_{k - 1}$, with $\mathcal E_0 \ge 0$. 
        \end{enumerate}
        These conditions are equivalent to the assumption that $\frac{- \mathcal E_0\beta_k}{k^p} \le \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k$. 
        One can show that by unrolling recurrence on $\mathcal E_k$. 
        Then \eqref{ineq:inxt-apg-cnvg-generic-pitem-1} implies $\forall k \ge 1$: 
        \begin{align}\label{ineq:inxt-apg-cnvg-generic-pitem-1}
            \frac{- \mathcal E_k}{k^p} &\le 
            \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} - \Lambda_k
            \iff 
            \Lambda_k \le 
            \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} + \frac{\mathcal E_k}{k^p}. 
        \end{align}
        Now, we show the convergence of $\Lambda_k$, using the relations of $\mathcal E_k, \Lambda_k, \beta_k$ above. 
        \begin{align*}\begin{split}
            \Lambda_k &\le 
            \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} + \frac{\mathcal E_k}{k^p}
            \\
            &\le \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} 
            + \frac{\beta_k}{\beta_{k -1}}\frac{\mathcal E_{k - 1}}{k^p}
            \\
            &= 
            \frac{\beta_k}{\beta_{k - 1}}
            \left(
                \Lambda_{k - 1} + \frac{\mathcal E_{k - 1}}{k^p}
            \right)
            \\
            &\le 
            \frac{\beta_k}{\beta_{k - 1}}
            \left(
                \frac{\beta_{k - 1}}{\beta_{k - 2}}\Lambda_{k - 2}
                + \frac{\mathcal E_{k - 1}}{(k - 1)^p}
                + \frac{\mathcal E_{k - 1}}{k^p}
            \right)
            \\
            &= 
            \frac{\beta_k}{\beta_{k - 2}}
            \left(
                \Lambda_{k - 2}
                + \frac{\mathcal E_{k - 2}}{(k - 1)^p}
                + \frac{\mathcal E_{k - 2}}{k^p}
            \right)
            \\
            & ...
            \\
            &\le \frac{\beta_k}{\beta_1}\left(
                \Lambda_1 + \mathcal E_1\sum_{n = 2}^{k} \frac{1}{n^p}
            \right)
            \\
            &\le \frac{\beta_k}{\beta_1}\left(
                \frac{\beta_1}{\beta_0}\Lambda_0 
                + \mathcal E_1\sum_{n = 1}^{k} \frac{1}{n^p}
            \right)
            \\
            &= \frac{\beta_k}{\beta_0}\left(
                \Lambda_0 
                + \mathcal E_0\sum_{n = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{split}\end{align*}
        Therefore it points to the following inequality: 
        \begin{align*}
            & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
            \\
            &\le 
            \beta_k \left(
                F(x_0) - F(\bar x) + \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_0\Vert^2
                + \mathcal E_0 \sum_{i = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{align*}
        Finally, when $\alpha_0 = 1$, then the results from \ref{lemma:inxt-apg-cnvg-prep1} with $k = 0$ simplifies the above inequality and give: 
        \begin{align*}
            & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
            \le 
            \beta_k \left(
                \frac{L_0}{2}\Vert \bar x - v_{-1}\Vert^2
                + \mathcal E_0 \sum_{i = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{align*}
    \end{proof}
    \par
    Now, it only remains to determine the sequence $\alpha_k$ to derive a type of convergence rate for the algorithm because from the above theorem, we have the convergence rate $\beta_k$ and, the error parameters $\epsilon_k, \rho_k$ both controlled by the sequence $(\alpha_k)_{k \ge 0}$. 

\bibliographystyle{siam}
\bibliography{references/refs.bib}
\end{document}