
@article{chambolle_introduction_2016,
	series = {Acta {Numerica}},
	title = {An introduction to continuous optimization for imaging},
	volume = {25},
	url = {https://hal.science/hal-01346507},
	doi = {10.1017/S096249291600009X},
	abstract = {A large number of imaging problems reduce to the optimization of a cost function , with typical structural properties. The aim of this paper is to describe the state of the art in continuous optimization methods for such problems, and present the most successful approaches and their interconnections. We place particular emphasis on optimal first-order schemes that can deal with typical non-smooth and large-scale objective functions used in imaging problems. We illustrate and compare the different algorithms using classical non-smooth problems in imaging, such as denoising and deblurring. Moreover, we present applications of the algorithms to more advanced problems, such as magnetic resonance imaging, multilabel image segmentation, optical flow estimation, stereo matching, and classification.},
	urldate = {2023-10-19},
	journal = {Acta Numerica},
	author = {Chambolle, Antonin and Pock, Thomas},
	year = {2016},
	keywords = {convex analysis, Nonsmooth Optimization},
	pages = {161--319},
	file = {Chambolle and Pock - 2016 - An introduction to continuous optimization for imaging:/Volumes/T6/Zotero Library/storage/WYGGWYVC/Chambolle and Pock - 2016 - An introduction to continuous optimization for imaging.pdf:application/pdf},
}

@book{rockafellar_variational_1998,
	address = {Berlin, Heidelberg},
	series = {Grundlehren der mathematischen {Wissenschaften}},
	title = {Variational {Analysis}},
	volume = {317},
	isbn = {978-3-540-62772-2 978-3-642-02431-3},
	url = {http://link.springer.com/10.1007/978-3-642-02431-3},
	urldate = {2024-01-19},
	publisher = {Springer},
	author = {Rockafellar, R. Tyrrell and Wets, Roger J. B.},
	editor = {Berger, M. and De La Harpe, P. and Hirzebruch, F. and Hitchin, N. J. and Hörmander, L. and Kupiainen, A. and Lebeau, G. and Ratner, M. and Serre, D. and Sinai, Y. G. and Sloane, N. J. A. and Vershik, A. M. and Waldschmidt, M.},
	year = {1998},
	doi = {10.1007/978-3-642-02431-3},
	keywords = {convex analysis, optimization, epi-convergence, non-smooth analysis, variational analysis},
	file = {Rockafellar and Wets - 1998 - Variational Analysis.pdf:/Volumes/T6/Zotero Library/storage/GCHP4MC7/Rockafellar and Wets - 1998 - Variational Analysis.pdf:application/pdf},
}

@book{bauschke_convex_2017,
	address = {Cham},
	series = {{CMS} {Books} in {Mathematics}},
	title = {Convex {Analysis} and {Monotone} {Operator} {Theory} in {Hilbert} {Spaces}},
	isbn = {978-3-319-48310-8 978-3-319-48311-5},
	url = {https://link.springer.com/10.1007/978-3-319-48311-5},
	language = {en},
	urldate = {2023-11-29},
	publisher = {Springer International Publishing},
	author = {Bauschke, Heinz H. and Combettes, Patrick L.},
	year = {2017},
	keywords = {Convex analysis, Monotone Operator, Nonexpansive Operator, Operator Splitting, Proximal Algorithm},
	file = {Bauschke and Combettes - 2017 - Convex Analysis and Monotone Operator Theory in Hi.pdf:/Volumes/T6/Zotero Library/storage/57IWCTZJ/Bauschke and Combettes - 2017 - Convex Analysis and Monotone Operator Theory in Hi.pdf:application/pdf},
}

@article{necoara_linear_2019,
	title = {Linear convergence of first order methods for non-strongly convex optimization},
	volume = {175},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-018-1232-1},
	doi = {10.1007/s10107-018-1232-1},
	abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
	language = {en},
	number = {1},
	urldate = {2023-10-11},
	journal = {Mathematical Programming},
	author = {Necoara, I. and Nesterov, Yu. and Glineur, F.},
	month = may,
	year = {2019},
	pages = {69--107},
	file = {Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:/Volumes/T6/Zotero Library/storage/7X79PGLC/Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:application/pdf},
}

@book{mordukhovich_convex_2022,
	address = {Cham},
	series = {Springer {Series} in {Operations} {Research} and {Financial} {Engineering}},
	title = {Convex {Analysis} and {Beyond}: {Volume} {I}: {Basic} {Theory}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-030-94784-2 978-3-030-94785-9},
	shorttitle = {Convex {Analysis} and {Beyond}},
	url = {https://link.springer.com/10.1007/978-3-030-94785-9},
	language = {en},
	urldate = {2024-12-27},
	publisher = {Springer International Publishing},
	author = {Mordukhovich, Boris S. and Mau Nam, Nguyen},
	year = {2022},
	keywords = {Convex Analysis, Convex Functions, Convex Sets, Generalized Differentiation, Locally Convex Topological Vector Spaces, Topological Vector Spaces, Variational Analysis, Variational Methods of Nonlinear Analysis},
	file = {Convex Analysis and Beyond Volume I Basic Theory:/Volumes/T6/Zotero Library/storage/NLAFYGG3/Mordukhovich and Mau Nam - 2022 - Convex Analysis and Beyond Volume I Basic Theory.pdf:application/pdf},
}

@book{hiriart-urruty_convex_1993,
	address = {Berlin, Heidelberg},
	series = {Grundlehren der mathematischen {Wissenschaften}},
	title = {Convex {Analysis} and {Minimization} {Algorithms} {II}: {Advanced} {Theory} and {Bundle} {Methods}},
	volume = {306},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-642-08162-0 978-3-662-06409-2},
	shorttitle = {Convex {Analysis} and {Minimization} {Algorithms} {II}},
	url = {http://link.springer.com/10.1007/978-3-662-06409-2},
	language = {en},
	urldate = {2024-12-27},
	publisher = {Springer},
	author = {Hiriart-Urruty, Jean-Baptiste and Lemaréchal, Claude},
	editor = {Artin, M. and Chern, S. S. and Coates, J. and Fröhlich, J. M. and Hironaka, H. and Hirzebruch, F. and Hörmander, L. and Moore, C. C. and Moser, J. K. and Nagata, M. and Schmidt, W. and Scott, D. S. and Sinai, Ya. G. and Tits, J. and Waldschmidt, M. and Watanabe, S. and Berger, M. and Eckmann, B. and Varadhan, S. R. S.},
	year = {1993},
	doi = {10.1007/978-3-662-06409-2},
	keywords = {Optimization, Nonsmooth Optimization, Convex Analysis, algorithms, calculus, Mathhematical Programming, Numerical Algorithms},
	file = {Convex Analysis and Minimization Algorithms II Advanced Theory and Bundle Methods:/Volumes/T6/Zotero Library/storage/BGSV6MM7/Hiriart-Urruty and Lemaréchal - 1993 - Convex Analysis and Minimization Algorithms II Advanced Theory and Bundle Methods.pdf:application/pdf},
}

@misc{drusvyatskiy_error_2016,
	title = {Error bounds, quadratic growth, and linear convergence of proximal methods},
	url = {http://arxiv.org/abs/1602.06661},
	doi = {10.48550/arXiv.1602.06661},
	abstract = {The proximal gradient algorithm for minimizing the sum of a smooth and a nonsmooth convex function often converges linearly even without strong convexity. One common reason is that a multiple of the step length at each iteration may linearly bound the "error" -- the distance to the solution set. We explain the observed linear convergence intuitively by proving the equivalence of such an error bound to a natural quadratic growth condition. Our approach generalizes to linear convergence analysis for proximal methods (of Gauss-Newton type) for minimizing compositions of nonsmooth functions with smooth mappings. We observe incidentally that short step-lengths in the algorithm indicate near-stationarity, suggesting a reliable termination criterion.},
	urldate = {2024-03-19},
	publisher = {arXiv},
	author = {Drusvyatskiy, Dmitriy and Lewis, Adrian S.},
	month = jun,
	year = {2016},
	note = {arXiv:1602.06661 [math]},
	keywords = {Mathematics - Optimization and Control, 90C25, 90C31, 90C55, 65K10},
	file = {arXiv Fulltext PDF:/Volumes/T6/Zotero Library/storage/MLYSVWJ6/Drusvyatskiy and Lewis - 2016 - Error bounds, quadratic growth, and linear convergence of proximal methods.pdf:application/pdf;arXiv.org Snapshot:/Volumes/T6/Zotero Library/storage/GK4FBD5X/1602.html:text/html;Snapshot:/Volumes/T6/Zotero Library/storage/5BFJ49ND/1602.html:text/html},
}

@unpublished{chambolle_first-order_2010,
	title = {A first-order primal-dual algorithm for convex problems with applications to imaging},
	url = {https://hal.science/hal-00490826},
	abstract = {We study a first-order primal-dual algorithm for convex optimization problems with known saddle-point structure. We prove convergence to a saddle-point with rate O(1/N) in finite dimensions, which is optimal for the complete class of non-smooth problems we are considering in this paper. We further show accelerations of the proposed algorithm to yield optimal rates on easier problems. In particular we show that we can achieve O(1/N²) convergence on problems, where the primal or the dual objective is uniformly convex, and we can show linear convergence, i.e. O(1/e{\textasciicircum}N) on problems where both are uniformly convex. The wide applicability of the proposed algorithm is demonstrated on several imaging problems such as image denoising, image deconvolution, image inpainting, motion estimation and image segmentation.},
	urldate = {2024-02-07},
	author = {Chambolle, Antonin and Pock, Thomas},
	month = jun,
	year = {2010},
	keywords = {Convex optimization, Dual approaches, Image reconstruction, Inverse problems, Total variation},
	file = {Full Text PDF:/Volumes/T6/Zotero Library/storage/WS36B6QW/Chambolle and Pock - 2010 - A first-order primal-dual algorithm for convex problems with applications to imaging.pdf:application/pdf},
}

@article{boyd_distributed_2011,
	title = {Distributed {Optimization} and {Statistical} {Learning} via the {Alternating} {Direction} {Method} of {Multipliers}},
	volume = {3},
	issn = {1935-8237, 1935-8245},
	url = {https://www.nowpublishers.com/article/Details/MAL-016},
	doi = {10.1561/2200000016},
	abstract = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
	language = {English},
	number = {1},
	urldate = {2023-12-06},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
	month = jul,
	year = {2011},
	note = {Publisher: Now Publishers, Inc.},
	keywords = {ADMM},
	pages = {1--122},
	file = {ADMM Monograph.pdf:/Volumes/T6/Zotero Library/storage/E3GYL5JJ/ADMM Monograph.pdf:application/pdf},
}

@article{zhang_robust_2022,
	title = {Robust brain {MR} image compressive sensing via re-weighted total variation and sparse regression},
	volume = {85},
	issn = {0730-725X},
	url = {https://www.sciencedirect.com/science/article/pii/S0730725X21001983},
	doi = {10.1016/j.mri.2021.10.031},
	abstract = {Total variation (TV) and non-local self-similarity (NSS) are powerful tools for successfully enhancing compressive sensing performance. However, standard TV approaches often over-smooth detailed edges in the image, due to the uniform regularization of gradient magnitude. In this paper, a novel compressed sensing method for the reconstruction of medical images is proposed, the image edges are well preserved with the proposed reweighted TV. The redundancy of the NSS patch also is leveraged through the sparse regression model. The proposed model was solved with an efficient strategy of the Alternating Direction Method of Multipliers (ADMM) algorithm. Experimental results on thesimulated phantom, brain Magnetic resonance imaging (MRI) show that the proposed method outperforms the state-of-the-art compressed sensing approaches.},
	urldate = {2025-08-30},
	journal = {Magnetic Resonance Imaging},
	author = {Zhang, Mingli and Zhang, Mingyan and Zhang, Fan and Chaddad, Ahmad and Evans, Alan},
	month = jan,
	year = {2022},
	keywords = {ADMM, Compressive sensing (CS), Nonlocal self-similarity (NSS), Re-weighted TV, Sparse regression, Total Variation (TV)},
	pages = {271--286},
	file = {ScienceDirect Snapshot:/Volumes/T6/Zotero Library/storage/MDXB6CPN/S0730725X21001983.html:text/html},
}

@article{joshi_mri_2009,
	title = {{MRI} resolution enhencement using total variation regularization},
	volume = {2009},
	issn = {1945-7928},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2990698/},
	doi = {10.1109/ISBI.2009.5193008},
	abstract = {We propose a novel method for resolution enhancement for volumetric images based on a variational-based reconstruction approach. The reconstruction problem is posed using a deconvolution model that seeks to minimize the total variation norm of the image. Additionally, we propose a new edge-preserving operator that emphasizes and even enhances edges during the up-sampling and decimation of the image. The edge enhanced reconstruction is shown to yield significant improvement in resolution, especially preserving important edges containing anatomical information. This method is demonstrated as an enhancement tool for low-resolution, anisotropic, 3D brain MRI images, as well as a pre-processing step to improve skull-stripping segmentation of brain images.},
	urldate = {2025-08-30},
	journal = {Proceedings / IEEE International Symposium on Biomedical Imaging: from nano to macro. IEEE International Symposium on Biomedical Imaging},
	author = {Joshi, Shantanu H. and Marquina, Antonio and Osher, Stanley J. and Dinov, Ivo and Van Horn, John D. and Toga, Arthur W.},
	month = aug,
	year = {2009},
	pmid = {21113426},
	pmcid = {PMC2990698},
	pages = {161--164},
	file = {PubMed Central Full Text PDF:/Volumes/T6/Zotero Library/storage/6N2P8SDZ/Joshi et al. - 2009 - MRI RESOLUTION ENHANCEMENT USING TOTAL VARIATION REGULARIZATION.pdf:application/pdf},
}

@article{villa_accelerated_2013,
	title = {Accelerated and inexact forward-backward algorithms},
	volume = {23},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/110844805},
	doi = {10.1137/110844805},
	abstract = {The forward-backward algorithm is a powerful tool for solving optimization problems with an additively separable and smooth plus nonsmooth structure. In the convex setting, a simple but ingenious acceleration scheme developed by Nesterov improves the theoretical rate of convergence for the function values from the standard \${\textbackslash}mathcal O(k{\textasciicircum}\{-1\})\$ down to \${\textbackslash}mathcal O(k{\textasciicircum}\{-2\})\$. In this short paper, we prove that the rate of convergence of a slight variant of Nesterov's accelerated forward-backward method, which produces convergent sequences, is actually \$o(k{\textasciicircum}\{-2\})\$, rather than \${\textbackslash}mathcal O(k{\textasciicircum}\{-2\})\$. Our arguments rely on the connection between this algorithm and a second-order differential inclusion with vanishing damping.},
	number = {3},
	urldate = {2025-09-09},
	journal = {SIAM Journal on Optimization},
	author = {Villa, Silvia and Salzo, Saverio and Baldassarre, Luca and Verri, Alessandro},
	month = jan,
	year = {2013},
	pages = {1607--1633},
	file = {PDF:/Volumes/T6/Zotero Library/storage/836H4NP9/Villa et al. - 2013 - Accelerated and Inexact Forward-Backward Algorithms.pdf:application/pdf},
}

@book{zalinescu_convex_2002,
	address = {River Edge, N.J. ; London},
	title = {Convex analysis in general vector spaces},
	isbn = {978-981-238-067-8},
	language = {en},
	publisher = {World Scientific},
	author = {Zalinescu, C.},
	year = {2002},
	keywords = {Convex functions, Convex sets, Functional analysis, Vector spaces},
	file = {Zalinescu - 2002 - Convex analysis in general vector spaces.pdf:/Volumes/T6/Zotero Library/storage/G9TLNKLL/Zalinescu - 2002 - Convex analysis in general vector spaces.pdf:application/pdf},
}
