
@article{dollner_geospatial_2020,
	title = {Geospatial {Artificial} {Intelligence}: {Potentials} of {Machine} {Learning} for {3D} {Point} {Clouds} and {Geospatial} {Digital} {Twins}},
	volume = {88},
	issn = {2512-2819},
	shorttitle = {Geospatial {Artificial} {Intelligence}},
	url = {https://doi.org/10.1007/s41064-020-00102-3},
	doi = {10.1007/s41064-020-00102-3},
	abstract = {Artificial intelligence (AI) is changing fundamentally the way how IT solutions are implemented and operated across all application domains, including the geospatial domain. This contribution outlines AI-based techniques for 3D point clouds and geospatial digital twins as generic components of geospatial AI. First, we briefly reflect on the term “AI” and outline technology developments needed to apply AI to IT solutions, seen from a software engineering perspective. Next, we characterize 3D point clouds as key category of geodata and their role for creating the basis for geospatial digital twins; we explain the feasibility of machine learning (ML) and deep learning (DL) approaches for 3D point clouds. In particular, we argue that 3D point clouds can be seen as a corpus with similar properties as natural language corpora and formulate a “Naturalness Hypothesis” for 3D point clouds. In the main part, we introduce a workflow for interpreting 3D point clouds based on ML/DL approaches that derive domain-specific and application-specific semantics for 3D point clouds without having to create explicit spatial 3D models or explicit rule sets. Finally, examples are shown how ML/DL enables us to efficiently build and maintain base data for geospatial digital twins such as virtual 3D city models, indoor models, or building information models.},
	language = {en},
	number = {1},
	urldate = {2023-12-29},
	journal = {PFG – Journal of Photogrammetry, Remote Sensing and Geoinformation Science},
	author = {Döllner, Jürgen},
	month = feb,
	year = {2020},
	keywords = {Machine learning, 3D city models, 3D point clouds, Deep learning, Geospatial artificial intelligence, Geospatial digital twins},
	pages = {15--24},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/C2RY9V3X/Döllner - 2020 - Geospatial Artificial Intelligence Potentials of Machine Learning for 3D Point Clouds and Geospatia.pdf:application/pdf},
}

@misc{black_interpreting_2022,
	title = {Interpreting {Neural} {Networks} through the {Polytope} {Lens}},
	url = {http://arxiv.org/abs/2211.12312},
	abstract = {Mechanistic interpretability aims to explain what a neural network has learned at a nuts-and-bolts level. What are the fundamental primitives of neural network representations? Previous mechanistic descriptions have used individual neurons or their linear combinations to understand the representations a network has learned. But there are clues that neurons and their linear combinations are not the correct fundamental units of description: directions cannot describe how neural networks use nonlinearities to structure their representations. Moreover, many instances of individual neurons and their combinations are polysemantic (i.e. they have multiple unrelated meanings). Polysemanticity makes interpreting the network in terms of neurons or directions challenging since we can no longer assign a specific feature to a neural unit. In order to find a basic unit of description that does not suffer from these problems, we zoom in beyond just directions to study the way that piecewise linear activation functions (such as ReLU) partition the activation space into numerous discrete polytopes. We call this perspective the polytope lens. The polytope lens makes concrete predictions about the behavior of neural networks, which we evaluate through experiments on both convolutional image classifiers and language models. Specifically, we show that polytopes can be used to identify monosemantic regions of activation space (while directions are not in general monosemantic) and that the density of polytope boundaries reflect semantic boundaries. We also outline a vision for what mechanistic interpretability might look like through the polytope lens.},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Black, Sid and Sharkey, Lee and Grinsztajn, Leo and Winsor, Eric and Braun, Dan and Merizian, Jacob and Parker, Kip and Guevara, Carlos Ramón and Millidge, Beren and Alfour, Gabriel and Leahy, Connor},
	month = nov,
	year = {2022},
	note = {arXiv:2211.12312 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Black et al. - 2022 - Interpreting Neural Networks through the Polytope Lens.pdf:/Users/hongdali/Zotero/storage/7IG27CUU/Black et al. - 2022 - Interpreting Neural Networks through the Polytope Lens.pdf:application/pdf},
}

@inproceedings{balestriero_spline_2018,
	title = {A {Spline} {Theory} of {Deep} {Learning}},
	url = {https://proceedings.mlr.press/v80/balestriero18b.html},
	abstract = {We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.},
	language = {en},
	urldate = {2024-01-05},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Balestriero, Randall and baraniuk},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {374--383},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/M6Z6N2MD/Balestriero and baraniuk - 2018 - A Spline Theory of Deep Learning.pdf:application/pdf},
}

@misc{fan_deep_2023,
	title = {Deep {ReLU} {Networks} {Have} {Surprisingly} {Simple} {Polytopes}},
	url = {http://arxiv.org/abs/2305.09145},
	doi = {10.48550/arXiv.2305.09145},
	abstract = {A ReLU network is a piecewise linear function over polytopes. Figuring out the properties of such polytopes is of fundamental importance for the research and development of neural networks. So far, either theoretical or empirical studies on polytopes only stay at the level of counting their number, which is far from a complete characterization of polytopes. To upgrade the characterization to a new level, here we propose to study the shapes of polytopes via the number of simplices obtained by triangulating the polytope. Then, by computing and analyzing the histogram of simplices across polytopes, we find that a ReLU network has relatively simple polytopes under both initialization and gradient descent, although these polytopes theoretically can be rather diverse and complicated. This finding can be appreciated as a novel implicit bias. Next, we use nontrivial combinatorial derivation to theoretically explain why adding depth does not create a more complicated polytope by bounding the average number of faces of polytopes with a function of the dimensionality. Our results concretely reveal what kind of simple functions a network learns and its space partition property. Also, by characterizing the shape of polytopes, the number of simplices be a leverage for other problems, {\textbackslash}textit\{e.g.\}, serving as a generic functional complexity measure to explain the power of popular shortcut networks such as ResNet and analyzing the impact of different regularization strategies on a network's space partition.},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Fan, Feng-Lei and Huang, Wei and Zhong, Xiangru and Ruan, Lecheng and Zeng, Tieyong and Xiong, Huan and Wang, Fei},
	month = may,
	year = {2023},
	note = {arXiv:2305.09145 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Multimedia},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/XABGRT9N/Fan et al. - 2023 - Deep ReLU Networks Have Surprisingly Simple Polytopes.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/4AS8C6BT/2305.html:text/html},
}

@misc{ramsauer_hopfield_2021,
	title = {Hopfield {Networks} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2008.02217},
	doi = {10.48550/arXiv.2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	month = apr,
	year = {2021},
	note = {arXiv:2008.02217 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/Z66R8Q6N/Ramsauer et al. - 2021 - Hopfield Networks is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/2CB85YUF/2008.html:text/html},
}

@misc{domingos_every_2020,
	title = {Every {Model} {Learned} by {Gradient} {Descent} {Is} {Approximately} a {Kernel} {Machine}},
	url = {http://arxiv.org/abs/2012.00152},
	doi = {10.48550/arXiv.2012.00152},
	abstract = {Deep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Domingos, Pedro},
	month = nov,
	year = {2020},
	note = {arXiv:2012.00152 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, I.2.6, I.5.1},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/TJ55ITB2/Domingos - 2020 - Every Model Learned by Gradient Descent Is Approximately a Kernel Machine.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/23Z9W45D/2012.html:text/html},
}

@article{khosravi_comprehensive_2011,
	title = {Comprehensive {Review} of {Neural} {Network}-{Based} {Prediction} {Intervals} and {New} {Advances}},
	volume = {22},
	issn = {1045-9227, 1941-0093},
	url = {http://ieeexplore.ieee.org/document/5966350/},
	doi = {10.1109/TNN.2011.2162110},
	number = {9},
	urldate = {2024-01-05},
	journal = {IEEE Transactions on Neural Networks},
	author = {Khosravi, A. and Nahavandi, S. and Creighton, D. and Atiya, A. F.},
	month = sep,
	year = {2011},
	pages = {1341--1356},
	file = {Khosravi et al. - 2011 - Comprehensive Review of Neural Network-Based Prediction Intervals and New Advances.pdf:/Users/hongdali/Zotero/storage/G9INM4PT/Khosravi et al. - 2011 - Comprehensive Review of Neural Network-Based Prediction Intervals and New Advances.pdf:application/pdf},
}

@misc{power_grokking_2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/hongdali/Zotero/storage/VVXMBMPD/2201.html:text/html;Full Text PDF:/Users/hongdali/Zotero/storage/JZQPE2YR/Power et al. - 2022 - Grokking Generalization Beyond Overfitting on Small Algorithmic Datasets.pdf:application/pdf},
}

@article{wan_regularization_2013,
	title = {Regularization of {Neural} {Networks} using {DropConnect}},
	volume = {30},
	abstract = {We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.},
	language = {en},
	journal = {Proceedings of the 30th International Conference on Machine Learning},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and LeCun, Yann and Fergus, Rob},
	month = jun,
	year = {2013},
	file = {Wan et al. - Regularization of Neural Networks using DropConnect.pdf:/Users/hongdali/Zotero/storage/VYEFQFNA/Wan et al. - Regularization of Neural Networks using DropConnect.pdf:application/pdf},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	urldate = {2024-01-09},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	doi = {10.1007/978-0-387-84858-7},
	keywords = {machine learning, Random Forest, Averaging, Boosting, classification, clustering, data mining, Projection pursuit, supervised learning, Support Vector Machine, unsupervised learning},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/R38U24QD/Hastie et al. - 2009 - The Elements of Statistical Learning.pdf:application/pdf},
}

@misc{noauthor_tensorflow_nodate,
	title = {{TensorFlow} {Probability}},
	url = {https://www.tensorflow.org/probability/overview},
	language = {en},
	urldate = {2024-01-09},
	journal = {TensorFlow},
	file = {Snapshot:/Users/hongdali/Zotero/storage/TYKSLD9I/overview.html:text/html},
}

@article{kingma_introduction_nodate,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	url = {https://arxiv.org/abs/1906.02691\},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	language = {en},
	author = {Kingma, Diederik P and Welling, Max},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/hongdali/Zotero/storage/3WVI2DMW/1906.html:text/html;Kingma and Welling - An Introduction to Variational Autoencoders.pdf:/Users/hongdali/Zotero/storage/WHMMMKF8/Kingma and Welling - An Introduction to Variational Autoencoders.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
	file = {Deep Learning - Goodfellow, Ian, Yoshua Bengio.pdf:/Users/hongdali/Zotero/storage/HCUAGNMK/Deep Learning - Goodfellow, Ian, Yoshua Bengio.pdf:application/pdf;Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org).pdf:/Users/hongdali/Zotero/storage/2S8JY53K/Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville (z-lib.org).pdf:application/pdf},
}

@book{murphy_probabilistic_2022,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning series},
	title = {Probabilistic machine learning: an introduction},
	isbn = {978-0-262-04682-4},
	shorttitle = {Probabilistic machine learning},
	url = {https://probml.github.io/pml-book/},
	abstract = {"This book provides a detailed and up-to-date coverage of machine learning. It is unique in that it unifies approaches based on deep learning with approaches based on probabilistic modeling and inference. It provides mathematical background (e.g. linear algebra, optimization), basic topics (e.g., linear and logistic regression, deep neural networks), as well as more advanced topics (e.g., Gaussian processes). It provides a perfect introduction for people who want to understand cutting edge work in top machine learning conferences such as NeurIPS, ICML and ICLR"--},
	publisher = {The MIT Press},
	author = {Murphy, Kevin P.},
	year = {2022},
	keywords = {Machine learning, Probabilities},
	file = {book1.pdf:/Users/hongdali/Zotero/storage/R7TJL28P/book1.pdf:application/pdf;book2.pdf:/Users/hongdali/Zotero/storage/HDBL9YNJ/book2.pdf:application/pdf;Machine Learning A Probabilisti - Kevin P. Murphy.pdf:/Users/hongdali/Zotero/storage/EIX85EI3/Machine Learning A Probabilisti - Kevin P. Murphy.pdf:application/pdf},
}

@misc{wu_pointconv_2020,
	title = {{PointConv}: {Deep} {Convolutional} {Networks} on {3D} {Point} {Clouds}},
	shorttitle = {{PointConv}},
	url = {http://arxiv.org/abs/1811.07246},
	abstract = {Unlike images which are represented in regular dense grids, 3D point clouds are irregular and unordered, hence applying convolution on them can be difficult. In this paper, we extend the dynamic filter to a new convolution operation, named PointConv. PointConv can be applied on point clouds to build deep convolutional networks. We treat convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions. With respect to a given point, the weight functions are learned with multi-layer perceptron networks and density functions through kernel density estimation. The most important contribution of this work is a novel reformulation proposed for efficiently computing the weight functions, which allowed us to dramatically scale up the network and significantly improve its performance. The learned convolution kernel can be used to compute translation-invariant and permutation-invariant convolution on any point set in the 3D space. Besides, PointConv can also be used as deconvolution operators to propagate features from a subsampled point cloud back to its original resolution. Experiments on ModelNet40, ShapeNet, and ScanNet show that deep convolutional neural networks built on PointConv are able to achieve state-of-the-art on challenging semantic segmentation benchmarks on 3D point clouds. Besides, our experiments converting CIFAR-10 into a point cloud showed that networks built on PointConv can match the performance of convolutional networks in 2D images of a similar structure.},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Wu, Wenxuan and Qi, Zhongang and Fuxin, Li},
	month = nov,
	year = {2020},
	note = {arXiv:1811.07246 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/hongdali/Zotero/storage/LVPK3V5D/1811.html:text/html;Full Text PDF:/Users/hongdali/Zotero/storage/EXM7V4A2/Wu et al. - 2020 - PointConv Deep Convolutional Networks on 3D Point Clouds.pdf:application/pdf},
}

@misc{qi_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	doi = {10.48550/arXiv.1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	urldate = {2024-01-19},
	publisher = {arXiv},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv:1612.00593 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/X7D56A6I/Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Classification and Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/J6VWAXTJ/1612.html:text/html;Qi_PointNet_Deep_Learning_2017_CVPR_supplemental.pdf:/Users/hongdali/Zotero/storage/P7EB372P/Qi_PointNet_Deep_Learning_2017_CVPR_supplemental.pdf:application/pdf},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2024-01-20},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/RCW76U6Z/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/2H42UZYK/1312.html:text/html},
}

@book{bishop_deep_2024,
	address = {Cham},
	title = {Deep {Learning}: {Foundations} and {Concepts}},
	isbn = {978-3-031-45467-7 978-3-031-45468-4},
	shorttitle = {Deep {Learning}},
	url = {https://link.springer.com/10.1007/978-3-031-45468-4},
	language = {en},
	urldate = {2024-01-20},
	publisher = {Springer International Publishing},
	author = {Bishop, Christopher M. and Bishop, Hugh},
	year = {2024},
	doi = {10.1007/978-3-031-45468-4},
}

@misc{noauthor_dbscan_2023,
	title = {{DBSCAN}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=DBSCAN&oldid=1185206201},
	abstract = {Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.
It is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).
DBSCAN is one of the most common, and most commonly cited, clustering algorithms.In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, ACM SIGKDD. As of July 2020, the follow-up paper "DBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN" appears in the list of the 8 most downloaded articles of the prestigious ACM Transactions on Database Systems (TODS) journal.The popular follow-up HDBSCAN* was initially published by Ricardo J. G. Campello, David Moulavi, and Jörg Sander in 2013, then expanded upon with Arthur Zimek in 2015. It revises some of the original decisions such as the border points and produces a hierarchical instead of a flat result.},
	language = {en},
	urldate = {2024-01-24},
	journal = {Wikipedia},
	month = nov,
	year = {2023},
	note = {Page Version ID: 1185206201},
	file = {Snapshot:/Users/hongdali/Zotero/storage/ES9HH2BE/DBSCAN.html:text/html},
}

@misc{ng_pytorch_2024,
	title = {Pytorch {Tutorial} {Mega} {Repository}},
	copyright = {MIT},
	url = {https://github.com/ritchieng/the-incredible-pytorch},
	abstract = {The Incredible PyTorch: a curated list of tutorials, papers, projects, communities and more relating to PyTorch.},
	urldate = {2024-01-24},
	author = {Ng, Ritchie},
	month = jan,
	year = {2024},
	note = {original-date: 2017-02-11T08:33:11Z},
	keywords = {deep-learning, deep-learning-library, deep-learning-tutorial, deep-neural-networks, python, pytorch},
}

@article{tockner_automatic_2022,
	title = {Automatic tree crown segmentation using dense forest point clouds from {Personal} {Laser} {Scanning} ({PLS})},
	volume = {114},
	issn = {1569-8432},
	url = {https://www.sciencedirect.com/science/article/pii/S1569843222002138},
	doi = {10.1016/j.jag.2022.103025},
	abstract = {Among digital-based technologies to monitor forest ecosystems, personal laser scanning (PLS) has high potential to characterize even complex deciduous and rainforests. PLS data include a complete and detailed 3D representation of forest stands, but tree individuals need to be segmented accurately before retrieving tree characteristics. As manual on-screen segmentation is time-consuming and labor intensive, we suggest an automatic voxel-based region growing crown segmentation algorithm. Diameter at breast height (dbh), tree height, crown base height (cbh), crown projection area (cpa) and crown volume were automatically extracted from single tree point clouds. The methodology was validated on previously published PLS raw data in terms of segmentation accuracy and measurement precision. Manual segmentation, field measurements, and geometrical crown models were used as reference data. The overall segmentation accuracy of the crowns was 87.02\%and tree height was accurately measured with a bias of −0.05 m and a root mean square deviation (RMSD) of 1.21 m (6.33\%). Existing geometric crown models proved to be a realistic approximation of the true crown architecture and matched the measured tree crown volume with a bias of −4.62 m3 and a RMSD of 63.02 m3 (31.72\%). Tree height and cpa were not affected by segmentation accuracy, but a major challenge remained in estimating cbh. The proposed methodology provides an efficient and low-cost solution for a fully automatic and digital forest inventory.},
	urldate = {2024-01-30},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Tockner, Andreas and Gollob, Christoph and Kraßnitzer, Ralf and Ritter, Tim and Nothdurft, Arne},
	month = nov,
	year = {2022},
	keywords = {Automatic tree segmentation, Crown models, Forest inventory, Forest point cloud data, Personal laser scanning, Region growing},
	pages = {103025},
	file = {ScienceDirect Snapshot:/Users/hongdali/Zotero/storage/LUZMCFLM/S1569843222002138.html:text/html},
}

@article{wang_manifold_2011,
	title = {Manifold {Alignment}},
	issn = {978-1-4398-7109-6},
	url = {https://www.researchgate.net/publication/265107566_Manifold_Alignment},
	doi = {10.1201/b11431-6},
	author = {Wang, Chang and Krafft, P M and Mahadevan, Sridhar},
	month = dec,
	year = {2011},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/XYZBJWHT/Wang et al. - 2011 - Manifold Alignment.pdf:application/pdf},
}

@misc{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	doi = {10.48550/arXiv.1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv:1506.02640 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/Z2HL42IG/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/M4PXMXLL/1506.html:text/html},
}

@misc{redmon_yolo9000_2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO9000}},
	url = {http://arxiv.org/abs/1612.08242},
	doi = {10.48550/arXiv.1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {arXiv:1612.08242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/K8EE33P9/Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/S222WW9R/1612.html:text/html},
}

@misc{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	doi = {10.48550/arXiv.1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv:1804.02767 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/QA5CZPGA/Redmon and Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/G294PXRB/1804.html:text/html},
}

@misc{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	doi = {10.48550/arXiv.1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv:1703.06870 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/5LP6JLFM/He et al. - 2018 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/C9V8FLBL/1703.html:text/html},
}

@misc{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2024-03-16},
	publisher = {arXiv},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1502.03167 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/hongdali/Zotero/storage/KWFTHCTL/1502.html:text/html;Full Text PDF:/Users/hongdali/Zotero/storage/2CF5U5IK/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:application/pdf},
}

@misc{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	doi = {10.48550/arXiv.1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv:1506.01497 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/USY88JPC/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/3L9LRCWL/1506.html:text/html},
}

@misc{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	doi = {10.48550/arXiv.1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv:1311.2524 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/9GHFY2ZF/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/WFASGRAR/1311.html:text/html},
}

@misc{noauthor_transpose_nodate,
	title = {Transpose {Convolutions} and {Autoencoders}},
	url = {https://www.cs.toronto.edu/~lczhang/321/lec/autoencoder_notes.html},
	urldate = {2024-04-03},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2024-04-10},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/BCWP7WUJ/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/V8MDLHH7/1512.html:text/html},
}
