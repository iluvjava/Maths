\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{anyfontsize} % fix font size warning. 
\usepackage{url} 
\urlstyle{same} % fix wacky url links in bib entries. 
% \usepackage{minted}

% Basic Type Settings ----------------------------------------------------------
\usepackage[margin=1in,footskip=0.25in]{geometry}
\linespread{1}  % double spaced or single spaced
\usepackage[fontsize=12pt]{fontsize}
\usepackage{authblk}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}       % Theorem counter global 
\newtheorem{prop}{Proposition}[section]  % proposition counter is section
\newtheorem{lemma}{Lemma}[subsection]  % lemma counter is subsection
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}[subsection]
\newtheorem{remark}{Remark}[subsection]
{
    % \theoremstyle{plain}
    \newtheorem{assumption}{Assumption}
}
\numberwithin{equation}{subsection}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\usepackage[final]{graphicx}
\usepackage{listings}
\usepackage{courier}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\newcommand{\indep}{\perp \!\!\! \perp}
\usepackage{wrapfig}
\graphicspath{{.}}
\usepackage{fancyvrb}

%%
%% Julia definition (c) 2014 Jubobs
%%
\usepackage[T1]{fontenc}
\usepackage{beramono}
\usepackage[usenames,dvipsnames]{xcolor}
\lstdefinelanguage{Julia}%
  {morekeywords={abstract,break,case,catch,const,continue,do, else, elseif,%
      end, export, false, for, function, immutable, import, importall, if, in,%
      macro, module, otherwise, quote, return, switch, true, try, type, typealias,%
      using, while},%
   sensitive=true,%
   alsoother={$},%
   morecomment=[l]\#,%
   morecomment=[n]{\#=}{=\#},%
   morestring=[s]{"}{"},%
   morestring=[m]{'}{'},%
}[keywords,comments,strings]%
\lstset{%
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries\color{blue},
    stringstyle      = \color{magenta},
    commentstyle     = \color{ForestGreen},
    showstringspaces = false,
}

\title{Machine Learning, a Review for Tree Segmentations Tasks}
\author{Hongda Li}

\begin{document}
\maketitle

\begin{abstract}
    This is my own notes. 
\end{abstract}


\section{Introduction}
    Artificial Neural networks represents modes of numerical computations that are differentiable programs. 
    We skips the basics and assume the reader already know something about Deep Neural Network, their components, and the automatic differentiation program on modern deep learning frameworks. 
    For our discussion we introduce some definitions to make for a better presentation of computataional concepts occured in Artificial Neural Networks (ANNs). 
    \par
    \subsection{Note}
        The index starts with ``1'' in our writings. 
        But it starts with zero if we are using programming languages such as python. 

    \begin{definition}[Component]
        A component is a function $f(x; p|w): \mathbb R^m \mapsto \mathbb R^n$. 
        $x$ is the inputs and $w$ represent trainable parameters, usually in the form of a multi-dimensional array. 
        And $p$ represents parameters that are not trainable parameters. 
        
    \end{definition}
    \begin{definition}[Connection]
        Let $f:\mathbb R^n \mapsto \mathbb R^m, g: \mathbb R^m \mapsto \mathbb R^k$ be two components, then a connection between is a $\mathbb R^m \mapsto \mathbb R^m$ function $h(x; p | w)$ with trainable parameters $w$, and parameter $p$. 
    \end{definition}


    \begin{example}[Dense Layer]
        Let $m, n \in \mathbb N$, let $A \in \mathbb R^{n\times m}$, $b \in \mathbb R^n$, then a Dense layer is a $\mathbb R^m \mapsto \mathbb R^n$ functions with a list of activation functions $\sigma_i$ for $i = 1, \cdots, n$. 
        Let $x \in \mathbb R^m$ be the input, then a dense layer is a component. We define its computation: 
        $$
        \begin{aligned}
            \text{DnsLyr}(x ; m, n | (A, b), \{\sigma_i\}_{i=1}^n) = 
            \left[
                z \mapsto \bigoplus_{i = 1}^n\sigma_i(z_i)
            \right]
            \left(
                Ax + b
            \right). 
        \end{aligned}
        $$
        Where, inside of $[\cdot]$, we denote the definition of a anonymous function. 
    \end{example}
    
    \begin{example}[Multi-Layer Perceptron]
        Let $l_1, l_2, \cdots, l_N$ be integers. 
        We define the Multi-Layer Perceptron to be a composition of dense layer mapping from $l_{i}$ to $l_{i + 1}$ for $i = 1, \cdots, N - 1$. 
        Let $\sigma_{i, j}$ represent the activation function for the $j$ th output in the $i$ th layer. 
        Then a Multi-Layer Perceptron (MLP) is a component admit representation
        $$
        \begin{aligned}
            & \text{MLP}\left(x ; l_1, \cdots, l_n | \{(A_i, b_i)\}_{i=1}^N\right): \mathbb R^{l_1} \mapsto \mathbb R^{l_N}
            \\
            :=&
            \left[
            \bigodot_{i = 1}^n \text{DnsLyr}
            \left(
                (\cdot) ; l_i, l_{i + 1} \left| (A_i, b_i), \{\sigma_{j, i}\}_{j=1}^{l_i} \right.
            \right)
            \right](x). 
        \end{aligned}
        $$
        Where $\bigodot$ is functional composition and it represents $\bigodot_{i=1}^n f_i = f_n\circ\cdots\circ f_1(x)$, and $(\cdot)$ represents the input of the anonymous function, in this case it's the dense layer. 
    \end{example}

\section{Preliminaries}
    These concepts and components are relevant to the architecture of Vision Networks. 
    \begin{definition}[Convolution 2D]
        Let $u, v$ be multi-array of dimension $m \times n$ and $k \times l$. 
        We assume that $m \le  k$ and $n \le l$.
        Then the convolution operator $*$ is a mapping from $(\mathbb C^M \times \mathbb C^n)\times (\mathbb C^k \times \mathbb C^l) \mapsto \mathbb C^{(m - k)\times (n - l)}$. 
        Then the convolution is defined as 
        $$
        \begin{aligned}
            (u* v)_{t, \tau} = 
            \sum_{i = 1}^{k}\sum_{j = 1}^{l}u_{i, j}v_{i + t, j + \tau}. 
        \end{aligned}
        $$
    \end{definition}

    \subsection{Convolutional Layers}
        In this section we talk about \href{https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html}{2D convolution component (pytorch link)} inside of an ANNs. 
        The convolution operations module contains more detailed parameters. 
        
        \begin{definition}[2D Convolution Layers]
            Assuming that we have a single sample. 
            Let $(C, H, W)$ be the shape of the input tensor. 
            $C$ is the number of channel, and $H, W$ are the height and width. 
            We use this because image tensors are usually in the shape of $(3, H, W)$. 
            Define the component to be a function, mapping from $(C', H', W')$. 
            Let $(C, K, L)$ denotes the dimension of the kernel: $\mathcal K$. 
            Then mathematically, the computation of the output tensor $Y$ given input tensor $X$ can be computed as
            $$
            \begin{aligned}
                Y_{c', h', w'} = 
                \text{ReLU}\left( b_{c'} + 
                \sum_{n = 1}^{C} (\mathcal K_{c'} * X)_{h', w'}\right). 
            \end{aligned}
            $$
        \end{definition}
        \begin{remark}
            Observe that each of the output channel is the sum of $C$ many kernels convluted with all inputs channels and summed up. 
            For more information about using it in pytorch, visit 
            The following parameters are extra and can be used to alter the computations. 
            \begin{enumerate}
                \item ``stride'', striding means ignoring some of the element of the convoluted vectors. In the case of `stride=2', the dimension is odd or even will matter. 
                
                \item ``padding'', padding adds zero elements to the input vector/matrix. 
                
                \item ``dialation'', dilation dilate the filter, making it bigger and fills it with more zeros. The trainable weights will distributed with integers spacing between them. 
                
                \item  ``group'', see \href{https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148}{here} for an explanations. I am not sure what this parameter is doing really.  
            \end{enumerate}
            The activaton function above is ``ReLU'', but it doesn't have to be. 
            See \href{https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html}{here} for the pytorch documentations. 
        \end{remark}
    \subsection{Pooling Layer}
        Let's define these quantities 
        \begin{enumerate}
            \item $(N, C, H, W)$ is the size of the input signal. 
            \item $(N, C, H', W')$ is the signal of the output layer.
            \item $(k_1, k_2)$ is the size of the kernel. 
            \item $N$ is usually the size of the Batched samples. 
            \item $[s_1, s_2]$ be the stride parameters for the kernels. 
        \end{enumerate}
            
        \begin{definition}[2D Max Pooling Layers]
            Let $X$ be the signal of size $(N,C, H, W)$, let the output signal be $Y$ of size $(N, C, H', W')$, then the output can be precisely described by the following formula: 
            {
            \large
            \[
                \begin{aligned}
                    Y_{i,c, h, w} = 
                    \max_{
                        \substack{m = 1, \;\cdots, H\\n = 1, \; \cdots \; , W}
                    }
                    \{
                        X_{i, c, hs_1 + m, s_2w + n}
                    \}. 
                \end{aligned} 
            \]
            }
        \end{definition}
    
    

\appendix
\section{Appendix Section 1}
    This is the appendix section. 




\bibliographystyle{plain}
% \bibliography{refs.bib}


\end{document}