
@article{su_differential_2016,
	title = {A differential equation for modeling nesterov's accelerated gradient method: {Theory} and {Insights}},
	volume = {17},
	issn = {1533-7928},
	shorttitle = {A {Differential} {Equation} for {Modeling} {Nesterov}'s {Accelerated} {Gradient} {Method}},
	url = {http://jmlr.org/papers/v17/15-084.html},
	abstract = {We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
	language = {en},
	number = {153},
	urldate = {2023-10-09},
	journal = {Journal of Machine Learning Research},
	author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
	year = {2016},
	keywords = {ODEs, dynamical system},
	pages = {1--43},
	file = {Full Text PDF:C\:\\Users\\alto\\Zotero\\storage\\JMSC3D9R\\Su et al. - 2016 - A Differential Equation for Modeling Nesterov's Accelerated Gradient Method Theory and Insights.pdf:application/pdf;Su et al. - 2015 - A differential equation for modeling nesterov's ac.pdf:C\:\\Users\\alto\\Zotero\\storage\\ANWIF5NT\\Su et al. - 2015 - A differential equation for modeling nesterov's ac.pdf:application/pdf},
}

@article{aujol_parameter-free_2024,
	title = {Parameter-{Free} {FISTA} by adaptive restart and backtracking},
	url = {https://epubs.siam.org/doi/10.1137/23M158961X},
	abstract = {We consider a combined restarting and adaptive backtracking strategy for the popular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for accelerating the convergence speed of large-scale structured convex optimization problems. Several variants of FISTA enjoy a provable linear convergence rate for the function values \$F(x\_n)\$ of the form \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}{\textasciitilde}n\})\$ under the prior knowledge of problem conditioning, i.e. of the ratio between the ({\textbackslash}L ojasiewicz) parameter \${\textbackslash}mu\$ determining the growth of the objective function and the Lipschitz constant \$L\$ of its smooth component. These parameters are nonetheless hard to estimate in many practical cases. Recent works address the problem by estimating either parameter via suitable adaptive strategies. In our work both parameters can be estimated at the same time by means of an algorithmic restarting scheme where, at each restart, a non-monotone estimation of \$L\$ is performed. For this scheme, theoretical convergence results are proved, showing that a \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}n\})\$ convergence speed can still be achieved along with quantitative estimates of the conditioning. The resulting Free-FISTA algorithm is therefore parameter-free. Several numerical results are reported to confirm the practical interest of its use in many exemplar problems.},
	language = {en},
	urldate = {2023-10-09},
	journal = {SIAM Journal on Optimization},
	author = {Aujol, Jean-François and Calatroni, Luca and Dossal, Charles and Labarrière, Hippolyte and Rondepierre, Aude},
	month = dec,
	year = {2024},
	file = {Full Text PDF:C\:\\Users\\alto\\Zotero\\storage\\3YBXJH4F\\Aujol et al. - 2023 - Parameter-Free FISTA by Adaptive Restart and Backt.pdf:application/pdf},
}

@book{nesterov_lectures_2018,
	address = {Cham},
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	volume = {137},
	isbn = {978-3-319-91577-7 978-3-319-91578-4},
	url = {http://link.springer.com/10.1007/978-3-319-91578-4},
	urldate = {2023-10-11},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	keywords = {Fast Gradient Methods, Interior-Point Methods, Optimization in Relative Scale, Self-Concordant Functions, Smoothing Technique, Optimization, Numerical Optimization, Algorithmic Complexity},
	file = {Nesterov - 2018 - Lectures on Convex Optimization.pdf:C\:\\Users\\alto\\Zotero\\storage\\HSCCPYL9\\Nesterov - 2018 - Lectures on Convex Optimization.pdf:application/pdf},
}

@article{necoara_linear_2019,
	title = {Linear convergence of first order methods for non-strongly convex optimization},
	volume = {175},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-018-1232-1},
	doi = {10.1007/s10107-018-1232-1},
	abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
	language = {en},
	number = {1},
	urldate = {2023-10-11},
	journal = {Mathematical Programming},
	author = {Necoara, I. and Nesterov, Yu. and Glineur, F.},
	month = may,
	year = {2019},
	pages = {69--107},
	file = {Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:C\:\\Users\\alto\\Zotero\\storage\\7X79PGLC\\Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:application/pdf},
}

@article{beck_fast_2009,
	title = {Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems},
	volume = {18},
	issn = {1941-0042},
	url = {https://ieeexplore.ieee.org/document/5173518},
	doi = {10.1109/TIP.2009.2028250},
	abstract = {This paper studies gradient-based schemes for image denoising and deblurring problems based on the discretized total variation (TV) minimization model with constraints. We derive a fast algorithm for the constrained TV-based image deburring problem. To achieve this task, we combine an acceleration of the well known dual approach to the denoising problem with a novel monotone version of a fast iterative shrinkage/thresholding algorithm (FISTA) we have recently introduced. The resulting gradient-based algorithm shares a remarkable simplicity together with a proven global rate of convergence which is significantly better than currently known gradient projections-based methods. Our results are applicable to both the anisotropic and isotropic discretized TV functionals. Initial numerical results demonstrate the viability and efficiency of the proposed algorithms on image deblurring problems with box constraints.},
	number = {11},
	urldate = {2023-10-19},
	journal = {IEEE Transactions on Image Processing},
	author = {Beck, Amir and Teboulle, Marc},
	month = nov,
	year = {2009},
	pages = {2419--2434},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\alto\\Zotero\\storage\\G4ZXMUZG\\5173518.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\alto\\Zotero\\storage\\J2T9LVVK\\Beck and Teboulle - 2009 - Fast Gradient-Based Algorithms for Constrained Tot.pdf:application/pdf},
}

@book{beck_first-order_2017,
	address = {israel},
	series = {{MOS}-{SIAM} {Series} in {Optimization}},
	title = {First-order {Methods} in {Optimization}},
	isbn = {978-1-61197-498-0},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974997},
	language = {en},
	urldate = {2023-10-19},
	publisher = {SIAM},
	author = {Beck, Amir},
	year = {2017},
	keywords = {Optimization, Numerical Optimization, Non-smooth Optimization, First-order Methods},
	file = {First-Order Methods in Optimization  SIAM Publication.pdf:C\:\\Users\\alto\\Zotero\\storage\\P2HFAVVQ\\First-Order Methods in Optimization  SIAM Publication.pdf:application/pdf;Snapshot:C\:\\Users\\alto\\Zotero\\storage\\88BHKZ6Y\\1.html:text/html},
}

@inproceedings{ahn_understanding_2022,
	title = {Understanding {Nesterov}'s acceleration via proximal point method},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977066.9},
	doi = {https://doi.org/10.1137/1.9781611977066},
	abstract = {The proximal point method (PPM) is a fundamental method in optimization that is often used as a building block for designing optimization algorithms. In this work, we use the PPM method to provide conceptually simple derivations along with convergence analyses of different versions of Nesterov's accelerated gradient method (AGM). The key observation is that AGM is a simple approximation of PPM, which results in an elementary derivation of the update equations and stepsizes of AGM. This view also leads to a transparent and conceptually simple analysis of AGM's convergence by using the analysis of PPM. The derivations also naturally extend to the strongly convex case. Ultimately, the results presented in this paper are of both didactic and conceptual value; they unify and explain existing variants of AGM while motivating other accelerated methods for practically relevant settings.},
	urldate = {2023-11-04},
	booktitle = {Symposium on {Simplicity} in {Algorithms}},
	publisher = {SIAM},
	author = {Ahn, Kwangjun and Sra, Suvrit},
	month = jun,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Optimization and Control},
	file = {Ahn and Sra - 2022 - Understanding nesterov's acceleration via proximal.pdf:C\:\\Users\\alto\\Zotero\\storage\\PZBWUWW5\\Ahn and Sra - 2022 - Understanding nesterov's acceleration via proximal.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alto\\Zotero\\storage\\WSSGK9Q4\\2005.html:text/html},
}

@article{beck_fast_2009-1,
	title = {A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
	volume = {2},
	issn = {1936-4954},
	url = {http://epubs.siam.org/doi/10.1137/080716542},
	doi = {10.1137/080716542},
	abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be signiﬁcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
	language = {en},
	number = {1},
	urldate = {2023-11-16},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Beck, Amir and Teboulle, Marc},
	month = jan,
	year = {2009},
	pages = {183--202},
	file = {Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:C\:\\Users\\alto\\Zotero\\storage\\H7CGKLL3\\Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:application/pdf},
}

@article{chambolle_convergence_2015,
	title = {On the convergence of the iterates of the "{Fast} iterative shrinkage/thresholding algorithm"},
	volume = {166},
	issn = {1573-2878},
	url = {https://doi.org/10.1007/s10957-015-0746-4},
	doi = {10.1007/s10957-015-0746-4},
	abstract = {We discuss here the convergence of the iterates of the “Fast Iterative Shrinkage/Thresholding Algorithm,” which is an algorithm proposed by Beck and Teboulle for minimizing the sum of two convex, lower-semicontinuous, and proper functions (defined in a Euclidean or Hilbert space), such that one is differentiable with Lipschitz gradient, and the proximity operator of the second is easy to compute. It builds a sequence of iterates for which the objective is controlled, up to a (nearly optimal) constant, by the inverse of the square of the iteration number. However, the convergence of the iterates themselves is not known. We show here that with a small modification, we can ensure the same upper bound for the decay of the energy, as well as the convergence of the iterates to a minimizer.},
	language = {en},
	number = {3},
	urldate = {2023-11-18},
	journal = {Journal of Optimization Theory and Applications},
	author = {Chambolle, A. and Dossal, Ch.},
	month = sep,
	year = {2015},
	keywords = {First-order Methods, Forward–backward Splitting, Heavy Ball Momentum, Optimization},
	pages = {968--982},
	file = {Chambolle and Dossal - 2015 - On the Convergence of the Iterates of the “Fast It.pdf:C\:\\Users\\alto\\Zotero\\storage\\P7LSJUWM\\Chambolle and Dossal - 2015 - On the Convergence of the Iterates of the “Fast It.pdf:application/pdf},
}

@book{ryu_large-scale_2022,
	address = {Cambridge},
	title = {Large-scale {Convex} {Optimization}: {Algorithms} \& {Analyses} via {Monotone} {Operators}},
	isbn = {978-1-009-16085-8},
	shorttitle = {Large-{Scale} {Convex} {Optimization}},
	url = {https://large-scale-book.mathopt.com/},
	abstract = {Starting from where a first course in convex optimization leaves off, this text presents a unified analysis of first-order optimization methods – including parallel-distributed algorithms – through the abstraction of monotone operators. With the increased computational power and availability of big data over the past decade, applied disciplines have demanded that larger and larger optimization problems be solved. This text covers the first-order convex optimization methods that are uniquely effective at solving these large-scale optimization problems. Readers will have the opportunity to construct and analyze many well-known classical and modern algorithms using monotone operators, and walk away with a solid understanding of the diverse optimization algorithms. Graduate students and researchers in mathematical optimization, operations research, electrical engineering, statistics, and computer science will appreciate this concise introduction to the theory of convex optimization algorithms.},
	urldate = {2024-01-22},
	publisher = {Cambridge University Press},
	author = {Ryu, Ernest K. and Yin, Wotao},
	year = {2022},
	doi = {10.1017/9781009160865},
	file = {Ryu and Yin - 2022 - Large-Scale Convex Optimization Algorithms & Analyses via Monotone Operators.pdf:C\:\\Users\\alto\\Zotero\\storage\\JPZLJBL8\\Ryu and Yin - 2022 - Large-Scale Convex Optimization Algorithms & Analyses via Monotone Operators.pdf:application/pdf;Snapshot:C\:\\Users\\alto\\Zotero\\storage\\PNYCF4FI\\2A7F8E7428BFA4EDB8AFACA11AB97E4C.html:text/html},
}

@article{tibshirani_regression_1996,
	title = {Regression shrinkage and selection via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2024-03-29},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
	file = {Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:C\:\\Users\\alto\\Zotero\\storage\\K4KSLBP7\\Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:application/pdf},
}

@inproceedings{lee_geometric_2021,
	title = {A {Geometric} structure of acceleration and its role in making gradients small fast},
	volume = {34},
	url = {https://papers.nips.cc/paper/2021/hash/647c722bf90a49140184672e0d3723e3-Abstract.html},
	urldate = {2024-06-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Jongmin and Park, Chanwoo and Ryu, Ernest},
	year = {2021},
	pages = {11999--12012},
	file = {Lee et al. - 2021 - A Geometric Structure of Acceleration and Its Role.pdf:C\:\\Users\\alto\\Zotero\\storage\\F7Z9BVUA\\Lee et al. - 2021 - A Geometric Structure of Acceleration and Its Role.pdf:application/pdf},
}

@article{nesterov_method_1983,
	title = {A method for solving the convex programming problem with convergence rate {O}(1/k{\textasciicircum}2)},
	url = {https://www.semanticscholar.org/paper/A-method-for-solving-the-convex-programming-problem-Nesterov/8d3a318b62d2e970122da35b2a2e70a5d12cc16f},
	abstract = {Semantic Scholar extracted view of "A method for solving the convex programming problem with convergence rate O(1/k{\textasciicircum}2)" by Y. Nesterov},
	urldate = {2024-10-10},
	journal = {Proceedings of the USSR Academy of Sciences},
	author = {Nesterov, Y.},
	year = {1983},
}

@article{apidopoulos_convergence_2018,
	title = {Convergence rate of inertial {Forward}-{Backward} algorithm beyond {Nesterov}'s rule},
	url = {https://hal.science/hal-01551873},
	doi = {10.1007/s10107-018-1350-9},
	abstract = {In this paper we study the convergence of an Inertial Forward-Backward algorithm, with a particular choice of an over-relaxation term. In particular we show that for a sequence of overrrelaxation parameters, that do not satisfy Nesterov’s rule one can still expect some relatively fast convergence properties for the objective function. In addition we complement this work by studying the convergence of the algorithm in the case where the proximal operator is inexactly computed with the presence of some errors and we give sufficient conditions over these errors in order to obtain some convergence properties for the objective function .},
	urldate = {2024-12-20},
	journal = {Mathematical Programming, Series A},
	author = {Apidopoulos, Vassilis and Aujol, Jean-François and Dossal, Charles H},
	month = nov,
	year = {2018},
	note = {Publisher: Springer},
	keywords = {Convex optimization, inertial FB algorithm, Nesterov's rule, proximal operator, rate of convergence},
	pages = {1--20},
	file = {Convergence rate of inertial Forward-Backward algorithm beyond Nesterov rule:C\:\\Users\\alto\\Zotero\\storage\\U3PBW5UM\\Apidopoulos et al. - 2018 - Convergence rate of inertial Forward-Backward algorithm beyond Nesterov's rule.pdf:application/pdf},
}

@article{maulen_speed_2023,
	title = {A speed restart scheme for a dynamics with hessian-driven damping},
	volume = {199},
	issn = {1573-2878},
	url = {https://doi.org/10.1007/s10957-023-02290-5},
	doi = {10.1007/s10957-023-02290-5},
	abstract = {In this paper, we analyze a speed restarting scheme for the inertial dynamics with Hessian-driven damping, introduced by Attouch et al. (J Differ Equ 261(10):5734–5783, 2016). We establish a linear convergence rate for the function values along the restarted trajectories. Numerical experiments suggest that the Hessian-driven damping and the restarting scheme together improve the performance of the dynamics and corresponding iterative algorithms in practice.},
	language = {en},
	number = {2},
	urldate = {2024-12-16},
	journal = {Journal of Optimization Theory and Applications},
	author = {Maulén, Juan José and Peypouquet, Juan},
	month = nov,
	year = {2023},
	keywords = {34A12 (secondary), 37N40, 65K10 (primary), 90C25, Convex optimization, Differential equations, First-order methods, Hessian-driven damping, Restarting},
	pages = {831--855},
	file = {peed Restart Scheme for a Dynamics with Hessian-Driven Damping:C\:\\Users\\alto\\Zotero\\storage\\G8QLYLH3\\Maulén and Peypouquet - 2023 - A Speed Restart Scheme for a Dynamics with Hessian-Driven Damping.pdf:application/pdf},
}

@article{attouch_first-order_2022,
	title = {First-order optimization algorithms via inertial systems with {Hessian} driven damping},
	volume = {193},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-020-01591-1},
	doi = {10.1007/s10107-020-01591-1},
	abstract = {In a Hilbert space setting, for convex optimization, we analyze the convergence rate of a class of first-order algorithms involving inertial features. They can be interpreted as discrete time versions of inertial dynamics involving both viscous and Hessian-driven dampings. The geometrical damping driven by the Hessian intervenes in the dynamics in the form \$\${\textbackslash}nabla {\textasciicircum}2 f (x(t)) {\textbackslash}dot\{x\} (t)\$\$. By treating this term as the time derivative of \$\$ {\textbackslash}nabla f (x (t)) \$\$, this gives, in discretized form, first-order algorithms in time and space. In addition to the convergence properties attached to Nesterov-type accelerated gradient methods, the algorithms thus obtained are new and show a rapid convergence towards zero of the gradients. On the basis of a regularization technique using the Moreau envelope, we extend these methods to non-smooth convex functions with extended real values. The introduction of time scale factors makes it possible to further accelerate these algorithms. We also report numerical results on structured problems to support our theoretical findings.},
	language = {en},
	number = {1},
	urldate = {2024-12-29},
	journal = {Mathematical Programming},
	author = {Attouch, Hedy and Chbani, Zaki and Fadili, Jalal and Riahi, Hassan},
	month = may,
	year = {2022},
	keywords = {90C25, 65K10, 65K05, 37N40, 46N10, 49M30, 65B99, 90B50, Hessian driven damping, Inertial optimization algorithms, Nesterov accelerated gradient method, Ravine method, Time rescaling},
	pages = {113--155},
	file = {First-order optimization algorithms via inertial systems with Hessian driven damping:C\:\\Users\\alto\\Zotero\\storage\\89LNXY5T\\Attouch et al. - 2022 - First-order optimization algorithms via inertial systems with Hessian driven damping.pdf:application/pdf},
}
