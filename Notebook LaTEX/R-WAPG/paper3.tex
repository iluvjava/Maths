\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}


\begin{document}


\title{{\fontfamily{ptm}\selectfont 
        A Parameter Free Accelerated Proximal Gradient Method Without Restarting
    }}

\author{
    Hongda Li
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~ and ~Xianfu Wang
    \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
    % and ~Heinz H. Bauschke~
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{heinz.bauschke@ubc.ca}.
    % }
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    We study the accelerated proximal gradient method where the sequence that controls the momentum term doesn't follow the Nesterov's rule. 
    The paper proposed R-WAPG, a generic algorithm that unifies the convergence results for strongly convex and convex where the extrapolation constant is characterized by a sequence that is much weaker than the Nesterov's rule. 
    The R-WAPG framework is able to describe several notable Euclidean variants of FISTA and verify their convergence. 
    In addition we also proposed the convergence rate of strongly convex objective with a constant momentum term. 
    Inspired by a small detail in the proof, we give a formuatlion of R-WAPG that doesn't require any parameter without using the idea of restarting which we call ``Free R-WAPG". 
    Explorative numerical experiments were conducted to show its competitive convergence with numerical experiments. 
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 90C30, 90C25, 65K10; Secondary 65Y20. 
\noindent{\bfseries Keywords: } Convex optimization, Nesterov's acceleration, Proximal gradient, Convergence rate. 

\section{Introduction}
    \subsection{Motivations}
        For simplicity consider minimizing a differentiable convex function $F: \RR^n \rightarrow \RR$. 
        Let $F$ be $L$-Lipschitz smooth and $\mu \ge 0$ strongly convex meaning that there exists $L \in \RR$ such that for all $x \in \RR^n, y\in \RR^n$ it has $\mu\Vert x - y\Vert\le \Vert \nabla F(x) - \nabla F(y)\Vert \le L \Vert x - y \Vert$. 
        \par 
        The Nesterov's acceleration scheme which was originally proposed in 1983 \cite{nesterov_method_1983} is a celebrated first order method for solving the minimization problem. 
        Initialize $x_1 = y_1$ and $t_0 = 1$, the algorithm finds $(x_k)_{k \ge 1}$ for all $k \ge 1$ by: 
        \begin{align}
            & x_{k + 1} = y_k - L^{-1}\nabla F(y_k), 
            \\
            & t_{k + 1} = 1/2\left(1 + \sqrt{1 + 4t_{k}^2}\right), 
            \\
            & \theta_{k + 1} = (t_{k} - 1)/t_{k + 1}, 
            \\
            & y_{k + 1} = x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k). 
        \end{align}\label{eqn:example_algorithm}
        If the minimizer $x^*$ exists, then the optimality gap $F(x_k) - F(x^*)$ decreases at a rate of $\mathcal O(1/k^2)$, a faster rate compared to gradient descent which is $\mathcal O(1/k)$. 
        It's consider optimal in some sense and see Chapter 2 of Nesterov's book \cite{nesterov_lectures_2018} for details. 
        However, things are not sunshine, rainbow and happily ever after because the above scheme is associated with the issue that if $F$ is $\mu > 0$ strongly convex, the optimality gap $F(x_k) - F(x^*)$ oscillates and converges sub-linearly, making it worse than gradient descent. See page 9 of Su et al. \cite{su_differential_2016}. 
        \par
        This issue motivated a vast amount of literatures aims at improving, interpreting, and extending this method. 
        Restarting is a popular solution to address the issue of obtaining better convergence rate when the objective function is strongly convex. 
        Beck and Teboulle \cite{beck_fast_2009} mitigate the issue by restarting and showed that it has an $\mathcal O(1/k^2)$ convergence still, but it performs empirically better. 
        See \cite{necoara_linear_2019}\cite{aujol_parameter-free_2024} and references within for recent advancements in restarting accelerated proximal gradient algorithm. 
        
        \subsection{Prior works}
            Apidopoulos et al. \cite{apidopoulos_convergence_2018} showed that a relaxed choice of the sequence that doesn't strictly follow the Nesterov's rule still leads to the convergence of the momentum algorithm. 
            Besides restarting, See Attouch, et al. \cite{attouch_first-order_2022}, Maulen and Peypouquet \cite{maulen_speed_2023} and references within.
            They present the idea of Hessian Damping for stabilizing the Nesterov's accelerated gradient without/with restarting. 

    \subsection{Contents}
        Inspired specifically by the technique of Nestrov's estimating sequence \cite{nesterov_lectures_2018}, firstly we present a unified framework of Accelerated Proximal Gradient (APG) which we call Relaxed Weak Accelerated Proximal Gradient (R-WAPG) in Section \ref{sec:rwapg-formulation-convergence}.
        It has the ability to upper bound $F(x_k) - F(x^*)$ for sequences $(t_k)_{k \ge 0}$ that follows a rule much weaker than Nester's update rule. 
        Secondly, we present an alternative to restarting that performs well empirically inspired by a small detail in the convergence proof of R-WAPG. 
        In addition to a convergence claim of $F(x_k)- F(x^*)$ for a much more flexible choice of $(t_k)_{k \ge 1}$. 
        It also has descriptive power to describe several variants of FISTA in the literatures.  
        \par
        Our contributions are two folds, theoretical and practical. 
        Our results are based the assumption $F = f + g$ where $g:\RR^n \rightarrow \overline\RR$ is convex proper and closed, and $f$ is an $L$-Lipschitz smooth and $\mu \ge 0$ strongly convex function. 
        \par
        \textbf{A summary of our main results follow. } 
        Nesterov's acceleration extrapolate $y_{k + 1} = x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k)$ where $\theta_{k + 1} = (t_{k} - 1)/t_{k + 1} \in (0, 1)$ is the ``momentum''. 
        The choices for $\theta_k$ varies for different variants of the accelerated proximal gradient algorithm. 
        In Chambolle, Dossal \cite{chambolle_convergence_2015}, it has $t_k = (n + a - 1)/a$ for all $a > 2$ which gives weak convergence of the iterates $x_k$ in Hilbert space. 
        In Chapter 10 of Beck's Book \cite{beck_first-order_2017}, a variant called V-FISTA can achieve the faster linear convergence rate: $\mathcal O((1 - \sqrt{\mu/L})^k)$ on the optimality gap for $\mu > 0$ strongly convex $F$. 
        V-FISTA has $\theta_t = (\sqrt{\kappa} - 1)/(\sqrt{\kappa} + 1)$ where $\kappa = \mu/L$. 
        \par
        We relax the traditional choice of the sequence $\theta_k$ in Equation \ref{eqn:example_algorithm} and showed an upper bound of the optimal gap. 
        Let $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ be two sequences that satisfy
        \begin{align*}
            \alpha_0 &\in (0, 1], 
            \\
            \alpha_k &\in (\mu/L, 1) \quad (\forall k \ge 1), 
            \\
            \rho_k &:= \frac{\alpha_{k + 1}^2 - (\mu/L)\alpha_{k + 1}}{(1 - \alpha_{k + 1})\alpha_k^2} \quad \forall (k \ge 0). 
        \end{align*}
        Our first main result shows that if $\theta_{k + 1} = (\rho_k\alpha_k(1 - \alpha_k)/(\rho_k\alpha_k^2 + \alpha_{k + 1}))$, using the R-WAPG we proposed in Definition \ref{def:wapg} with Proposition \ref{prop:wagp-convergence}, \ref{prop:r-wapg-momentum-repr}, we can show that the gap $F(x_k) - F(x^*)$ is bounded by:
        \begin{align*}
            \mathcal O\left(
                \left(
                    \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
                \right)
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right). 
        \end{align*}
        Our second main result shows that for any $\alpha_k \ge 0$ the choice of sequence $\alpha_k = a/(a + k)$ results in $\rho_k > 1$ for all $k \in \N$ such that R-WAPG reduces to a variant of FISTA proposed in Chambolle, Dossal \cite{chambolle_convergence_2015}, and we are able to show the same convergence rate in Theorem \ref{thm:r-wapg-on-cham-doss}. 
        When $\rho_k = 1, \mu = 0$, R-WAPG reduces perfectly to FISTA by Beck \cite{beck_first-order_2017}, if $\mu > 0, \rho_k = 1$, it reduces to the V-FISTA by Beck \cite{beck_first-order_2017}. 
        In Theorem \ref{thm:fixed-momentum-fista}, it demonstrates that R-WAPG frameworks gives a linear convergence claim for all fixed momentum method where $\alpha_k := \alpha \in (\mu/L, 1)$ and  $F$ is $\mu > 0$ strongly convex. 
        Finally, we did the tedious work and present three equivalent forms of R-WAPG in Section \ref{sec:rwapg-equiv-repr} that are comparable to notable Euclidean variants of FISTA and beyond. This shows the descriptive power of our R-WAPG framework. 
        \par
        Our practical contribution is an algorithm inspired by a detail in our convergence proof which we call it ``Parameter Free R-WAPG'' (See Algorithm \ref{alg:free-rwapg}). 
        The algorithm is parameter free, meaning that it doesn't require knowing $L, \mu$ in advance, and it determines the value of $\theta_t$ by estimating the local concavity using iterates $y_{k}, y_{k + 1}$ from the Bregman divergence of $f$ with minimal computational cost. 
        We conducted ample amount of numerical experiments to show that it has a favorable convergence rate in practice and behaves similarly to the FISTA with monotone restart.
        \par
        \textbf{Organizations now follow.}
        Section \ref{sec:preliminaries} introduces the proximal gradient inequality which is instrumental to developing the stepwise convergence claim in the next section. 
        Section \ref{sec:stepwise-stuff} states and prove an inequality based on a generic iterative algorithm for just one iteration. 
        Section \ref{sec:rwapg-formulation-convergence} formulates the full R-WAPG algorithm and characterize sufficient conditions for R-WAPG sequence to derive the optimality upper bound in Proposition \ref{prop:wagp-convergence}. 
        Section \ref{sec:rwapg-equiv-repr} gives three equivalent representations of the R-WAPG algorithms that are comparable to instances of APG found in the literatures. 
        Section \ref{sec:rwapg-literatures} formulates FISTA, and V-FISTA sequences as instance of the R-WAPG sequences.
        The section proves the convergence rate of several variants of FISTA using the equivalent forms introduced in Section \ref{sec:rwapg-equiv-repr} and the convergence rate developed Section \ref{sec:rwapg-formulation-convergence}. 
        Finally, in Section \ref{sec:free-rwapg} gives a formulation of a parameter free version of R-WAPG algorithm and showcase the numerical experiments for regression, LASSO. 
        The numerical experiments expose very interesting behaviors of the algorithm of our own creativity. 


\section{Preliminaries}\label{sec:preliminaries}
    Throughout, we make the following assumption unless specified. 
    \begin{assumption}[Smooth nonsmooth additive]\label{ass:smooth-nsmooth-additive}
        \;
        \begin{enumerate}
            \item Define $F := f + g$. 
            \item $f: \RR^n \rightarrow \RR$ is $L$ Lipschitz smooth and $\mu \ge 0$ strongly convex. 
            \item $g: \RR^n \rightarrow \overline \RR$ is proper, closed and convex. 
            \item Minimizer exists for the optimization problem: $F^* = \min_x \left\lbrace f(x) + g(x)\right\rbrace$. 
        \end{enumerate}
    \end{assumption}
    For $x \in \RR^n, y \in \RR^n$, we define: 
    \begin{align}
        \widetilde{\mathcal M}^{L^{-1}}
        (x; y) 
        &:=
        g(x) + f(y) + \langle \nabla f(y), x - y\rangle 
        + \frac{L}{2}\Vert x - y\Vert^2,  \label{eqn:pg_model-func}
        \\
        \mathcal M^{L^{-1}}(x; y) &:= F(x) + \frac{L}{2}\Vert x - y\Vert^2. 
        \label{eqn:pp_model-func}
    \end{align}
    Define the proximal gradient operator and gradient mapping operator: $T_L, \mathcal G_L$: 
    \begin{align*}
        T_Ly &= \argmin_{x \in \RR^n} \left\lbrace
            g(x) + \langle \nabla f(y), x\rangle + L/2\Vert x - y\Vert^2
        \right\rbrace = [I + L^{-1}\partial g]^{-1}\circ [I - L^{-1}\nabla f], 
        \\
        \mathcal G_L 
        &:= L(I - T_L). 
    \end{align*}
    $T_L$ is a single-valued mapping, and it has its domain on the entire $\RR^n$. 
    Finally, define the Bregman divergence of $f$: 
    \begin{align*}
        D_f(x, y): \RR^n \times \RR^n \rightarrow \RR 
        \defeq f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
    \end{align*}
    The following lemma provides important properties of $\mathcal M^{L^{-1}}(\cdot; y), \widetilde{\mathcal M}^{L^{-1}}(\cdot; y)$. 
    \begin{lemma}[Proximal gradient envelope]\label{lemma:pg-envelope}
        With $\mathcal M^{L^{-1}}, \widetilde{\mathcal M}^{L^{-1}}$ as given by \eqref{eqn:pg_model-func}, \eqref{eqn:pp_model-func}, 
        we have for all $x \in \RR^n$, $y \in \RR^n$:
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y)
            &= 
            \mathcal M^{L^{-1}}(x; y)- D_f(x, y) \le \mathcal M^{L^{-1}}(x; y). 
        \end{align*}
    \end{lemma}
    \begin{proof}
        For $x \in \dom g$, by definition we have: 
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) 
            &= 
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            g(x) + f(x) - f(x) + f(y) 
            + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            F(x) - D_f(x, y) + \frac{L}{2}\Vert x - y\Vert^2 
            \\
            &= \mathcal M^{L^{-1}}(x; y) - D_f(x, y). 
        \end{align*}
        The equality is trivially true when $x \in \RR^n \setminus \dom g$. 
    \end{proof}
    \begin{theorem}[Proximal inequality]\label{thm:prox-grad-ineq}
        Let $F$ be given by Assumption \ref{ass:smooth-nsmooth-additive}. 
        For $x,y\in \RR^n$, we have: 
        \begin{align*}
            F(x) - F(T_Ly) - \langle L(y - T_Ly), x - y\rangle
            - \frac{L}{2}\Vert y - T_L y\Vert^2
            - \frac{\mu}{2}\Vert x - y\Vert^2
            &\ge  
            0.
        \end{align*}
    \end{theorem}
    \begin{proof}
        For simplicity of notations, we write $T$ for $T_L$. 
        The model function $\widetilde{\mathcal M}^{L^{-1}}(\cdot; y)$ is an $L + \mu$ strongly convex function, so it admits quadratic growth conditions over minimizer $Ty$ and for all $x \in \RR^n$: 
        {\smaller
        \begin{align*}
            0 &\le 
            \widetilde{\mathcal M}^{L^{-1}}(x; y) - 
            \widetilde{\mathcal M}^{L^{-1}}(Ty; y)
            - 
            \frac{L + \mu}{2}\Vert x - Ty\Vert^2
            \\
            &= 
            \left(
                \mathcal M^{L^{-1}}(x; y) - D_f(x, y)
            \right) - 
            \mathcal M^{L^{-1}}(Ty; y) 
            - 
            \frac{L + \mu}{2}\Vert x - Ty\Vert^2
            + D_f(Ty; y)
            \\
            &=
            \left(
                \mathcal M^{L^{-1}}(x; y)
                - 
                \mathcal M^{L^{-1}}(Ty; y)
            \right)
            - 
            D_f(x, y) 
            - \frac{L + \mu}{2}\Vert x - Ty\Vert^2
            + D_f(Ty; y)
            \\
            &=
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}\Vert x - y\Vert^2 - 
                \frac{L}{2}\Vert Ty - y\Vert^2
            \right)
            - D_f(x, y) 
            + D_f(Ty; y)
            - \frac{L + \mu}{2}\Vert x - Ty\Vert^2
            \\
            &=  
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - Ty + Ty - y\Vert^2
                    - 
                    \Vert y - Ty\Vert^2
                \right)
            \right)
            - D_f(x, y) 
            + D_f(Ty; y)
            - \frac{L + \mu}{2}\Vert x - Ty\Vert^2
            \\
            &= 
            \left(
                F(x) - F(Ty) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - Ty\Vert^2 + 
                    2\langle x - Ty, Ty - y\rangle
                \right)
            \right)
            - D_f(x, y) 
            + D_f(Ty; y)
            - \frac{L + \mu}{2}\Vert x - Ty\Vert^2
            \\
            & = 
            \left(
                F(x) - F(Ty) + \frac{L}{2}\Vert x - Ty\Vert^2 
                - L\langle  x - Ty, y - Ty\rangle
            \right)
            - D_f(x, y) 
            + D_f(Ty; y)
            - \frac{L + \mu}{2}\Vert x - Ty\Vert^2
            \\
            &\le 
            F(x) - F(Ty)
            - \langle L(y - Ty), x - Ty\rangle
            - D_f(x, y) 
            + D_f(Ty, y). 
        \end{align*}
        }
        Now, consider the lower bound of $D_f(x, y) \ge \mu/2\Vert x - y\Vert^2$ which is true for all $x, y \in \RR^n$ by strong convexity of $f$. 
        Then it has for all $x \in \RR^n, y \in \RR^n$: 
        \begin{align*}
            0 &\le 
            F(x) - F(Ty)
            - \langle L(y - Ty), x - Ty\rangle
            - D_f(x, y) 
            + D_f(Ty, y)
            \\
            &\le 
            F(x) - F(Ty)
            - \langle L(y - Ty), x - Ty\rangle
            - \frac{\mu}{2}\Vert x - y\Vert^2
            + \frac{L}{2}\Vert y - Ty\Vert^2
            \\
            &\le 
            F(x) - F(Ty)
            - \langle L(y - Ty), x - y  + y - Ty\rangle
            - \frac{\mu}{2}\Vert x - y\Vert^2
            + \frac{L}{2}\Vert y - Ty\Vert^2
            \\
            &\le 
            F(x) - F(Ty) - \langle L(y - Ty), x - y\rangle - \frac{\mu}{2}\Vert x - y \Vert^2
            - \frac{L}{2}\Vert y - Ty\Vert^2. 
        \end{align*}
        The proof is done. 
    \end{proof}
    \begin{remark}
        This proof works for all $L$ that is larger than the smallest Lipschitz modulus of $f$ and $\mu$ less than the strong convexity modulus of $f$. 
    \end{remark}


\section{Stepwise formulation of weak accelerated proximal gradient}\label{sec:stepwise-stuff}
    This entire section is instrumental to build the R-WAPG algorithm which is described in the Definition \ref{def:wapg} of the next section. 
    \par 
    Definition \ref{def:stepwise-wapg} which describes what happens at a single iteration of the R-WAPG algorithm. 
    It defines a procedure of generating $x_{k + 1}, v_{k + 1}$ given any $x_k, v_k$. 
    Proposition \ref{prop:stepwise-lyapunov} states the inequality that describes a decreasing quantity that involves $F(x_k), F(x_{k + 1})$ at each single iteration. 
    \begin{assumption}
        Given $x_k, y_k, v_k$ where $k \in \mathbb Z_+$, we define the following quantities
        \begin{align}
            g_k &\defeq L(y_k - T_L y_k), 
            \label{eqn:grad-map}
            \\
            l_F(x; y_k) &\defeq F(T_Ly_k) + \langle g_k, x - y_k\rangle + \frac{1}{2L}\Vert g_k\Vert^2, 
            \label{eqn:lower-linearization}
            \\
            \epsilon_{k} &\defeq F(x_k) - l_F(x_k; y_k), 
            \label{eqn:regret}
        \end{align}
        Observe that by convexity of $F$, $\epsilon_k \ge 0$ for all $x_k, L > 0$. 
        To see, use Theorem \ref{thm:prox-grad-ineq} and let $y = y_k, x = x_k$ which gives: 
        \begin{align*}
            F(x_k) - F(T_Ly_k)
            - \langle L(y_k - T_Ly_k),x_k - y_k \rangle
            - \frac{L}{2}\Vert y_k - T_Ly_k\Vert^2
            - \frac{\mu}{2}\Vert x_k - y_k\Vert^2
            &\ge 0
            \\
            \iff 
            F(x_k) - F(T_Ly_k)
            - \langle g_k,x_k - y_k \rangle
            - \frac{1}{2L}\Vert g_k\Vert^2
            &\ge 0. 
        \end{align*}
    \end{assumption}
    
    \begin{definition}[Stepwise weak accelerated proximal gradient]\label{def:stepwise-wapg}\;\\
        Assume $0 \le \mu < L$.
        Fix any $k \in \mathbb Z_+$. 
        For any $(v_k, x_k), \alpha_k \in (0, 1), \gamma_k > 0$, let $\hat \gamma_{k + 1}$, and vectors $y_k, v_{k + 1}, x_{k + 1}$ be given by: 
        \begin{align}
            \hat \gamma_{k + 1} &= (1 - \alpha_k)\gamma_k + \mu \alpha_k, \label{eqn:stepwise-wapg-eqn1}
            \\
            y_k &= 
            (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k), \label{eqn:stepwise-wapg-eqn2}
            \\
            g_k &= \mathcal G_L y_k, \label{eqn:stepwise-wapg-eqn3}
            \\
            v_{k + 1} &= \hat\gamma^{-1}_{k + 1}
            (\gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), \label{eqn:stepwise-wapg-eqn4}
            \\
            x_{k + 1} &= T_L y_k. \label{eqn:stepwise-wapg-eqn5}
        \end{align}
    \end{definition}
    \begin{observation}\label{obs:stepwise-wapg}
        Let's observe Definition \ref{def:stepwise-wapg} closely. 
        We make two crucial observations here. 
        We have $x_k, y_k, v_k$ lie on the same line because and they have the following equivalent equalities: 
        \begin{align*}
            y_k - v_k &= 
            \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k}(x_k - y_k), 
            \tag{Q1}\label{eqn:Q1}
            \\
            y_k - x_k &= 
            \frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu}(v_k - x_k). 
            \tag{Q2}\label{eqn:Q2}
        \end{align*}
    \end{observation}
        To see \eqref{eqn:Q1}, observe 
        \begin{align*}
            y_k - v_k &= 
            \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k}(x_k - y_k)
            \\
            \iff 
            -(\alpha_k \gamma_k \hat \gamma^{-1}_{k + 1} + 1)y_k
            &= 
            - \alpha_k \gamma_k \hat \gamma^{-1}_{k + 1}v_k - x_k
            \\
            \iff 
            y_k &= 
            \frac{
                \alpha_k \gamma_k \hat \gamma_{k + 1}^{-1}v_k + x_k
            }{1 + \alpha_k \gamma_k \hat \gamma_{k + 1}^{-1}}
            \\
            &=  
            \frac{\alpha_k \gamma_k v_k + \hat \gamma_{k + 1} x_k}{\gamma_k + \alpha_k \mu}.
        \end{align*}
        On the first equality \eqref{eqn:Q1}, we multiplied both side of the equation by $\hat\gamma_{k + 1}/\alpha_k \gamma_k$ and then grouped $y_k$. 
        The last equality comes by multiplying both the numerator and denominator by $\hat \gamma_{k + 1}$, then simplifies the denominator by rearranging \eqref{eqn:stepwise-wapg-eqn1} into $\hat \gamma_{k + 1} + \alpha_k \gamma_k = \gamma_k + \alpha_k \mu$. 
        To see the second equality \eqref{eqn:Q2}, consider \eqref{eqn:stepwise-wapg-eqn2}: 
        \begin{align*}
            y_k &= (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k)
            \\
            \iff
            y_k - x_k &= 
            (\gamma_k + \alpha_k \mu)^{-1}
            (\alpha_k \gamma_k v_k - (\gamma_k + \alpha_k \mu)x_k + \hat \gamma_{k + 1} x_k)
            \\
            \iff 
            (\gamma_k + \alpha_k \mu)(y_k - x_k)
            &= 
            \alpha_k\gamma _kv_k + 
            (\hat \gamma_k - \gamma_k - \alpha_k \mu) x_k
            \\
            &= \alpha_k \gamma_k v_k - \alpha_k \gamma_k x_k 
            \\
            &= \alpha_k \gamma_k(v_k - x_k)
            \\
            \iff 
            y_k - x_k &= 
            \frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu}(v_k - x_k). 
        \end{align*}
        On the second equality that follows, we substituted $\hat\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$ given by \eqref{eqn:stepwise-wapg-eqn1}. 
        Therefore, $x_k, v_k, y_k$ lies on the same line because \eqref{eqn:Q1} indicates $y_k - v_k$ parallels to $-(y_k - x_k)$ and both vector shares the same head which anchors at $y_k$. 
    
        \begin{lemma}\label{lemma:ineq-q3}
        Let $v_k, x_k, y_k, v_{k + 1}, x_{k + 1}$ and $\alpha_k, \hat \gamma_{k + 1}, \gamma_{k}$ be given by Definition \ref{def:stepwise-wapg}. 
        Then for any $x^* \in \RR^n$ it satisfies: 
        \begin{align}
            - \alpha_k(v_k - x^*) - \frac{\alpha_k^2 \mu}{\hat \gamma}(y_k - v_k) - (x_k - y_k)
            &= \alpha_k(x^* - x_k). 
            \tag{Q3}\label{eqn:Q3}
        \end{align}
    \end{lemma}
    \begin{proof}
        The proof is direct algebra: 
        \begin{align*}
            &  
            - \alpha_k(v_k - x^*) - \frac{\alpha_k^2 \mu}{\hat \gamma_{k + 1}}(y_k - v_k) - (x_k - y_k)
            \\
            & \underset{\eqref{eqn:Q1}}{=}
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k^2\mu}{\hat \gamma_{k + 1}}
            \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k}(x_k - y_k)
            - (x_k - y_k) 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \frac{\alpha_k \mu}{\gamma_k}(x_k - y_k)
            - (x_k - y_k) 
            \\
            &= 
            -\alpha_k(v_k - x^*) -
            \left(
                1 + \frac{\alpha_k \mu}{\gamma_k}
            \right)(x_k - y_k)
            \\
            &\underset{\eqref{eqn:Q2}}{=}
            -\alpha_k(v_k - x^*) - 
            \frac{\alpha_k \mu + \gamma_k}{\gamma_k}
            \frac{\alpha_k \gamma_k}{\gamma_k + \alpha_k \mu}(x_k - v_k)
            \\
            &= 
            -\alpha_k(v_k - x^*)
            - \alpha_k(x_k - v_k)
            \\
            &= \alpha_k(x^* - x_k). 
        \end{align*}
    \end{proof}
    \begin{proposition}[Stepwise Lyapunov]\label{prop:stepwise-lyapunov}\;\\
        Let $k \in \mathbb Z_+$, $R_k \in \RR$. 
        Given any $v_k, x_k$ and $\gamma_k > 0$ and $v_{k + 1}, x_{k + 1}, y_k, \hat \gamma_{k + 1}, \alpha_k$ that satisfies Definition \ref{def:stepwise-wapg}. 
        Define: 
        \begin{align}\label{eqn:stepwise-lya-1}
            R_{k + 1}
            \defeq
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right). 
        \end{align}
        Then for all $x^* \in \RR^n$, we have:
        {\small
        \begin{align}\label{ineq:stepwise-lya-2}
            F(x_{k + 1}) - F(x^*) + R_{k + 1} + \frac{\hat \gamma_{k + 1}}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F(x^*) + R_k + \frac{\gamma_{k}}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align}
        }
    \end{proposition}
    \begin{proof}
        We use proof by induction. 
        For notation simplicity we drop the subscript $k$, $k + 1$ on $\gamma_{k}, \hat \gamma_{k + 1}$ because they are fixed throughout the proof. 
        \par 
        We started with $F(x_{k + 1}) + R_{k + 1} \iff \eqref{eqn:stepwise-lya-proof-eqn1}$, 
        then $\hat \gamma/2\Vert v_{k + 1} - x^*\Vert^2 = \eqref{expr:stepwise-lya-expr2.2}$ via \eqref{eqn:stepwise-lya-proof-eqn2}. 
        We add (\ref{eqn:stepwise-lya-proof-eqn1}), (\ref{expr:stepwise-lya-expr2.2}) to get \eqref{eqn:stepwise-lya-proof-eqn3.1}. 
        Finally, the entire LHS of \eqref{ineq:stepwise-lya-2} equals to the RHS of \eqref{eqn:stepwise-lya-proof-eqn3.2}. 
        Going from \eqref{eqn:stepwise-lya-proof-eqn3.2} to \eqref{ineq:stepwise-lya-proof-ineq4}, we used Theorem \ref{thm:prox-grad-ineq} which yields the desired inequality. 
        \par
        Start by considering the first and the third term on LHS of \eqref{ineq:stepwise-lya-2} summed up: 
        \begin{align}\label{eqn:stepwise-lya-proof-eqn1}
            F(x_{k + 1}) &\underset{\eqref{eqn:regret}}{=}
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2,
            \notag
            \\
            R_{k + 1}
            &\underset{\eqref{eqn:stepwise-lya-1}}{=} 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right), 
            \notag
            \\
            \implies 
            F(x_{k + 1}) + R_{k + 1}
            &= 
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            \notag
            \\
            &\quad 
                + 
                (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right). 
        \end{align}
        % Coefficient of $\epsilon_k$ and $\Vert g_k\Vert^2$ are grouped. 
        Next, we have: 
        \begin{align}
        \label{eqn:stepwise-lya-proof-eqn2}
        \begin{split}
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^* \Vert^2
            &= 
            \frac{\hat \gamma}{2}\Vert 
                \hat \gamma^{-1}
                (
                    \gamma(1 - \alpha_k)v_k - 
                    \alpha_k g_k + \mu \alpha_k y_k
                )
                - x^* 
            \Vert^2
            \\
            &=  
            \frac{\hat \gamma}{2}
            \Vert 
                \hat \gamma^{-1}
                (
                \hat \gamma v_k + \mu \alpha_k(y_k - v_k)
                    - \alpha_k g_k
                )
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert 
                v_k + \hat \gamma^{-1} \mu \alpha_k (y_k - v_k)
                - \hat \gamma^{-1}\alpha_k g_k
                - x^* 
            \Vert^2
            \\
            &= 
            \frac{\hat \gamma}{2}
            \Vert v_k - x^*\Vert^2 
            + 
            \frac{\alpha_k^2}{2\hat \gamma}\Vert \mu(y_k - v_k) - g_k\Vert^2 
            \\ &\quad 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            \\ &\quad
                + 
                \frac{\alpha_k^2}{2\hat \gamma}
                \Vert \mu(y_k - v_k) - g_k\Vert^2 
                + 
                \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle. 
        \end{split}
        \end{align}
        On the above derivation, the first equality comes by substituting the definition of $v_{k + 1}$ given by \eqref{eqn:stepwise-wapg-eqn4}; the second equality simplifies using $\hat \gamma = (1 - \alpha_k)\gamma + \mu \alpha_k$ given by \eqref{eqn:stepwise-wapg-eqn1} and  because:
        \begin{align*}
            \gamma(1 - \alpha_k) v_k &= 
            (\hat \gamma  - \mu \alpha_k)v_k
            = \hat \gamma v_k - \mu\alpha_k v_k
            \\
            \iff 
            \gamma(1 - \alpha_k) v_k + \mu \alpha_k y_k
            &= 
            \hat \gamma v_k + \mu \alpha_k(y_k - v_k). 
        \end{align*}
        Focusing on the last two terms by the end of expression \eqref{eqn:stepwise-lya-proof-eqn2}, we have  
        \begin{align}
         \label{eqn:stepwise-lya-proof-eqn2.1}
        \begin{split}
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            & = 
            \frac{\alpha_k^2\mu}{\hat \gamma}
            \left(
                \frac{\mu}{2}\Vert y_k - v_k\Vert^2 
                - \langle y_k - v_k, g_k\rangle
            \right)
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2, 
            \\
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            &= 
            \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle 
            - \alpha_k \langle v_k - x^*, g_k\rangle. 
        \end{split} % \tag{2.1*}
        \end{align}
        Adding them gives: 
        {\small
        \begin{align*}
            & \quad 
            \frac{\alpha^2_k}{2\hat \gamma} 
            \Vert \mu(y_k - v_k) - g_k\Vert^2
            + 
            \langle v_k - x^*, \mu \alpha_k(y_k - v_k) - \alpha_k g_k\rangle
            \\
            &= 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
            + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \end{align*}
        }
        With the above \eqref{eqn:stepwise-lya-proof-eqn2} simplifies to 
        {\small
        \begin{align}\label{expr:stepwise-lya-expr2.2}
        \begin{split}
            & 
            \left(
            \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
            \right)\Vert v_k - x^*\Vert^2
            + 
            \left\langle g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
            \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle. 
        \end{split}
        %\tag{2.2*}
        \end{align}
        }
        Adding \eqref{expr:stepwise-lya-expr2.2} to \eqref{eqn:stepwise-lya-proof-eqn1} gives: 
        \begin{align}\label{eqn:stepwise-lya-proof-eqn3}
        \begin{split}
            &
            F(x_k) - \epsilon_k - \langle  g_k, x_k - y_k\rangle
            - \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
            + (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\quad 
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
                + 
                \left\langle g_k, 
                    - \alpha_k(v_k - x^*) 
                    - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                \right\rangle
            \\
            & \quad 
                + \frac{\alpha_k^2}{2\hat \gamma}\Vert g_k\Vert^2
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \epsilon_k 
            + \left\langle 
                g_k, 
                - \alpha_k(v_k - x^*) 
                - \frac{\alpha_k^2\mu}{\hat \gamma}(y_k - v_k)
                - (x_k - y_k)
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &\underset{\eqref{eqn:Q3}}{=} 
            F(x_k) - \epsilon_k 
            + \alpha_k\left\langle 
                g_k, 
                x^* - x_k
            \right\rangle
            \\
            &\quad 
                + (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma}{2\hat \gamma}
                    \Vert v_k - y_k\Vert^2
                \right)
                + 
                \left(
                \frac{(1 - \alpha_k)\gamma + \mu \alpha_k}{2} 
                \right)\Vert v_k - x^*\Vert^2
            \\
            & \quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k\epsilon_k + \alpha_k\langle g_k, x^* - x_k\rangle
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat\gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle.     
        \end{split}
        %\tag{3*}
        \end{align}
        On the first equality, coefficients of $\Vert g_k\Vert^2$ cancels out to zero and the inner product containing $g_k$ are grouped
        using \eqref{eqn:Q3} derived earlier. 
        The last equality re-arranged terms and grouped the coefficients of $\epsilon_k$ together. 
        Continuing on \eqref{eqn:stepwise-lya-proof-eqn3} we have
        \begin{align}\label{eqn:stepwise-lya-proof-eqn3.1}
        \begin{split}
            &
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\&\quad 
                + \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}\Vert v_k - y_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2
            \\&\quad 
                + \frac{\alpha_k^2 \mu^2}{2\hat \gamma}\Vert y_k - v_k\Vert^2
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            &= 
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \left(
                    \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma}
                    + 
                    \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
                \right)\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\
            & =
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                + 
                \frac{\mu \alpha_k}{2}\Vert y_k - v_k\Vert^2
                + \frac{\mu \alpha_k}{2}\Vert v_k - x^*\Vert^2 
                + \langle v_k - x^*, \mu\alpha_k(y_k - v_k)\rangle
            \\ &=
            F(x_k) - \alpha_k(\epsilon_k + \langle g_k, x_k - x^*\rangle)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\ &\quad 
                +
                \frac{\mu\alpha_k}{2} \Vert y_k - x^*\Vert^2
            \\&= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right).         
        \end{split} % \tag{3.1*}
        \end{align}
        On the first equality, coefficients of $\Vert y_k - v_k\Vert^2$ are grouped together. 
        On the second to the third equality, its coefficients simplified using: 
        \begin{align*}
            \frac{(1 - \alpha_k)\mu\alpha_k\gamma}{2\hat \gamma} + 
            \frac{\alpha_k^2 \mu^2}{2\hat \gamma}
            &= 
            \frac{\mu\alpha_k}{2}\left(
                \frac{(1 - \alpha_k)\gamma_k + \alpha_k \mu}{\hat \gamma}
            \right)
            \\
            &\underset{\eqref{eqn:stepwise-wapg-eqn1}}{=} \frac{\mu\alpha_k}{2}\left(
                \frac{\hat \gamma}{\hat \gamma}
            \right) = \frac{\mu\alpha_k}{2}. 
        \end{align*}
        We have \eqref{eqn:stepwise-lya-proof-eqn3.1} $\impliedby$ \eqref{eqn:stepwise-lya-proof-eqn1} + \eqref{expr:stepwise-lya-expr2.2}. 
        By \eqref{eqn:stepwise-lya-proof-eqn2} $\iff$ \eqref{expr:stepwise-lya-expr2.2},
        \eqref{eqn:stepwise-lya-proof-eqn3.1} says: 
        \begin{align*}
            & F(x_{k + 1}) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_k) - \alpha_k\left(
                \epsilon_k + \langle g_k, x_k - x^*\rangle
                - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \right)
            + 
            (1 - \alpha_k)\left(
                R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
        \end{align*}
        Subtracting $F(x^*)$ from both side of the equation gives: 
        \begin{align}\label{eqn:stepwise-lya-proof-eqn3.2}
            \begin{split}
                \\
                & F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
                \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
                \\
                &= 
                F(x_k) - F(x^*) - \alpha_k\left(
                    \epsilon_k + \langle g_k, x_k - x^*\rangle
                    - \frac{\mu}{2}\Vert y_k - x^*\Vert^2
                \right)
                + 
                (1 - \alpha_k)\left(
                    R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
                \right)
                \\
                &= (1 - \alpha_k)(F(x_k) - F(x^*))
                + \alpha_k\left(
                    F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
                \right)
                \\ &\quad 
                    + 
                    (1 - \alpha_k)\left(
                        R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
                    \right).     
            \end{split}
        % \tag{3.2*}
        \end{align}
        Focusing on the second term, we simplify the multiplier inside: 
        {\small
        \begin{align}\label{ineq:stepwise-lya-proof-ineq4}
        \begin{split}
            & F(x_k) - F(x^*) - \epsilon_k - \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= 
            F(x_k) - F(x^*) - \left(
                F(x_k) - F(T_L y_k) - \langle g_k, x_k - y_k\rangle - \frac{1}{2L}\Vert g_k\Vert^2
            \right)- \langle g_k, x_k - x^*\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            \\
            &= F(T_L y_k) - F(x^*) + \langle g_k, x^* - y_k\rangle + \frac{\mu}{2}\Vert y_k - x^*\Vert^2
            + \frac{1}{2L}\Vert g_k\Vert^2 \underset{\text{Theorem }\ref{thm:prox-grad-ineq}}{\le} 0.     
        \end{split}
        \end{align}
        }
        % On the last line, we expand the definition of $g_k$ and then used the
        % \hyperref[thm:prox-grad-ineq]{Theorem \ref*{thm:prox-grad-ineq}}. 
        Therefore
        {\small
        \begin{align*}
            F(x_{k + 1}) - F(x^*) + R_{k + 1} + 
            \frac{\hat \gamma}{2}\Vert v_{k + 1} - x^*\Vert^2
            &\le 
            (1 - \alpha_k)\left(
                F(x_k) - F(x^*) + R_k + \frac{\gamma}{2}\Vert v_k - x^*\Vert^2
            \right). 
        \end{align*}
        }
    \end{proof}
    \begin{remark}
        Given $y_k$, we can choose to increase $\mu_\kappa = 2D_f(x^*, y_k)/\Vert y_k - x^*\Vert^2 \ge \mu$ which also works on \eqref{ineq:stepwise-lya-proof-ineq4}.
        This is true by the intermediate steps taken to prove Theorem \ref{thm:prox-grad-ineq}. 
        $\mu$ is a pessimistic choice for the inequality above. 
        But in general the choice of $\mu$ remains the strong convexity modulus or equivalently, any value that is smaller than the true strong convexity constant for claiming the convergence rate for all initial guesses. 
    \end{remark}

\section{R-WAPG and its convergence rates}\label{sec:rwapg-formulation-convergence}
    In this section we propose R-WAPG  see Definition \ref{def:wapg}.
    R-WAPG algorithm generates iterates $(x_k, y_k, v_k)$ and admits an upper bound on $F(x_k) - F^*$ described in Proposition \ref{prop:stepwise-lyapunov}. 
    Definition \ref{def:rwapg-seq} introduces the concept of an R-WAPG sequences: $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ which is crucial.
    The sequences parameterize the R-WAPG algorithm stated in Definition \ref{def:wapg}, it connects the step-wise formulation of R-WAPG (Definition \ref{def:stepwise-wapg}) and can describe the convergence claim of R-WAPG in Proposition \ref{prop:wagp-convergence}. 
    In the next section, it continues to play a crucial role in describing several equivalent forms of the R-WAPG algorithm, and their corresponding convergence claim. 
    \begin{definition}[R-WAPG sequences]\label{def:rwapg-seq}\;\\
        Assume $0 \le \mu < L$. 
        The sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ are valid for R-WAPG if all the following holds: 
        \begin{align*}
            \alpha_0 &\in (0, 1], 
            \\
            \alpha_k &\in (\mu/L, 1) \quad (\forall k \ge 1), 
            \\
            \rho_k &:= \frac{\alpha_{k + 1}^2 - (\mu/L)\alpha_{k + 1}}{(1 - \alpha_{k + 1})\alpha_k^2} \quad \forall (k \ge 0). 
        \end{align*}
        We call $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ the \textbf{R-WAPG Sequences}. 
    \end{definition}
    \begin{observation}\label{obs:r-wapg-observation-1}
        The following is true: 
        \begin{enumerate}
            \item $\rho_k > 0$ for all $k \ge 0$. 
            \item $\forall k \ge 1: L\alpha_k^2 = (1 - \alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2 +\mu\alpha_k$. 
        \end{enumerate}
        
        % If $\alpha_1 \in (0, 1)$ which is true by $\gamma_1 \in (0, L]$, then the entire sequence $\alpha_k \in (0, 1)$. 
        % Suppose inductively that $\alpha_{k - 1}, \rho_{k - 1}$ are given such that they satisfy $\alpha_{k -1} \in (0, 1)$ and $0 < \rho_{k - 1} \alpha_{k - 1}^2 < 1$. 
        % Solving the quadratic $L\alpha_k^2=(1 - \alpha_k)\rho_{k - 1}L\alpha_{k - 1}^2 + \mu \alpha_k$ for $\alpha_k$ yields candidates. 
        % \begin{align*}
        %     \alpha_k &= 
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
        %         \pm
        %         \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
        %     \right). 
        % \end{align*}
        % Choosing the positive sign which is the larger one of the roots of the quadratic. 
        % We have $\alpha_k > 0$ because: 
        % \begin{align*}
        %     \alpha_k &=
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
        %         +
        %         \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
        %     \right)
        %     \\
        %     &\ge 
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
        %         +
        %         \left|\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L\right|
        %     \right) + \alpha_{k - 1}\sqrt{\rho_{k - 1}}
        %     \\
        %     & > 0. 
        % \end{align*}
        % On the last strict inequality we used the fact that $\rho_{k - 1}> 0, \alpha_{k - 1} > 0$. 
        % An upper bound can be identified by using inductive hypothesis and considering: 
        % \begin{align*}
        %     \alpha_k &= 
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2 
        %         +
        %         \sqrt{(\rho_{k - 1}\alpha_{k - 1}^2 - \mu/L)^2 + 4\rho_{k - 1}\alpha_{k - 1}^2}
        %     \right)
        %     \\
        %     &
        %     \frac{1}{2}\left(
        %         \frac{\mu}{L} + 
        %         \sup_{x \in (0, 1)}
        %         \left\lbrace
        %             -x + \sqrt{(x - \mu/L)^2 + 4x}
        %         \right\rbrace
        %     \right)
        %     \\
        %     & \le \frac{1}{2}\left(
        %         \mu/L + \max\left(\mu/L, -1 + \sqrt{(1 - \mu/L)^2 + 4}\right)
        %     \right) \le 1. 
        % \end{align*}
        % Going to the first inequality, we used $\rho_{k - 1} \alpha_{k - 1}^2 < 1$ to get the strict inequality. 
        % Going from the second to the third inequality, we maximized $\mu/L$ by monotone increasing of linear function and the square root function. 
        % Therefore, inductively, $\alpha_k \in (0, 1)$ holds. 
        % However, the limit of $\alpha_k$ could be 1. 
    \end{observation}

    \begin{definition}[Relaxed weak accelerated proximal gradient (R-WAPG)]\label{def:wapg}\;\\
        Choose any $x_1 \in \RR^n, v_1 \in \RR^n$. 
        Let $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ be given by Definition \ref{def:rwapg-seq}. 
        The algorithm generates a sequence of vector $(y_k, x_{k + 1}, v_{k + 1})_{k \ge 1}$ for $k\ge 1$ by the procedures:  
        \begin{tcolorbox}
            For $k=1, 2, 3, \cdots$
            \begin{align*}
                \gamma_k &:= \rho_{k -1}L\alpha_{k - 1}^2, 
                \\
                \hat \gamma_{k + 1} & := (1 - \alpha_k)\gamma_k + \mu \alpha_k = L\alpha_k^2, 
                \\
                y_k &= 
                (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k), 
                \\
                g_k &= \mathcal G_L y_k, 
                \\
                v_{k + 1} &= 
                \hat\gamma^{-1}_{k + 1}
                (\gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
                \\
                x_{k + 1} &= T_L y_k. 
            \end{align*}    
        \end{tcolorbox}
    \end{definition}
    \begin{observation}\label{obs:r-wapg-observation-2}
        Observe that if $\rho_k = 1$ for all $k\ge 0$, then the above algorithm is similar to (2.2.7) in Nesterov's book \cite{nesterov_lectures_2018} because $L\alpha_k^{2} = (1 - \alpha_k)L\alpha_{k - 1}^2 + \mu \alpha_k$. 
        \par
        For all $k \ge 1$, the algorithm chains together the sequence of $(\hat \gamma_{k+1})_{k \ge 1}$ and $(\gamma_k)_{k \ge 1}$ in Definition \ref{prop:stepwise-lyapunov} through the equalities: 
        \begin{align*}
            \hat \gamma_{k + 1} 
            &= L\alpha_k^2 
            \\
            &= (1 - \alpha_k)\gamma_k + \alpha_k \mu
            \\
            &= (1 - \alpha_k)L\rho_{k - 1}\alpha_{k - 1}^2 + \alpha_k \mu
            \\
            &= (1 - \alpha_k)\rho_{k - 1}\hat \gamma_{k} + \alpha_k \mu. 
        \end{align*}
        The third equality from Observation \ref{obs:r-wapg-observation-1}. 
    \end{observation}
    Here is the main result of this section. 
    \begin{proposition}[R-WAPG convergence claim]\label{prop:wagp-convergence}\; \\
        Fix any arbitrary $x^* \in \RR^n, N \in \mathbb N$. 
        Let vector sequence $(y_k, v_{k}, x_{k})_{k \ge 1}$ and R-WAPG sequences $\alpha_k, \rho_k$ be given by Definition \ref{def:wapg}. 
        Define $R_1 = 0$ and suppose that for $k = 1, 2, \ldots, N$, we have $R_k$ recursively given by: 
        \begin{align*}
            R_{k + 1}
            := 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right). 
        \end{align*}
        Then for all $k = 1, 2, \ldots, N$: 
        \begin{align*}
            & F(x_{k + 1}) - F(x^*) + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{i})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
    \end{proposition}
    
    \begin{proof}  
        The proof is by induction. 
        Let's first consider $k = 1$, $\hat \gamma_2 = (1 - \alpha_1)\gamma_1 + \mu \alpha_1$, $R_1 = 0$. 
        From the above definition, $R_2$ is the same as \eqref{eqn:stepwise-lya-1}.
        Invoking Proposition \ref{prop:stepwise-lyapunov}, we have: 
        \begin{align*}
            & F(x_{2}) - F(x^*) + R_2 + \frac{L \alpha_1^2}{2}\Vert v_{2} - x^*\Vert^2
            \\
            & = F(x_{2}) - F(x^*) + R_2 + \frac{\hat \gamma_{2}}{2}\Vert v_{2} - x^*\Vert^2
            & \hat \gamma_2 = L \alpha_1^2 , \text{ Definition \ref{def:wapg}}
            \\
            &\le 
            (1 - \alpha_1)\left(
                F(x_1) - F(x^*) + R_1 + \frac{\gamma_1}{2}\Vert v_1 - x^*\Vert^2
            \right) 
            & \quad \text{By Proposition \ref{prop:stepwise-lyapunov}}
            \\
            &= 
            (1 - \alpha_1)\left(
                F(x_1) - F(x^*) + R_1 + \frac{L\rho_{0}\alpha_{0}^2}{2}\Vert v_1 - x^*\Vert^2
            \right) 
            \\
            &= \max\left(\rho_0, 1\right)
            (1 - \alpha_1)\left(
                F(x_1) - F(x^*) + R_1 + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
        We had demonstrated the base case. 
        Next, for all $k = 2, 3, \ldots, N$, from Observation \ref{obs:r-wapg-observation-2}: 
        \begin{align*}
            \hat \gamma_{k + 1} = L\alpha_{k}^2 
            &=(1 - \alpha_k)\rho_{k - 1}L\alpha_{k - 1}^2 + \mu\alpha_k
            \\
            &= (1 - \alpha_k)\gamma_k + \mu\alpha_k. 
        \end{align*}
        So $\gamma_k, \hat \gamma_{k + 1}, \alpha_k$ fits Definition \ref{def:stepwise-wapg}. 
        For all $k \ge 1$, $R_k$ satisfies \eqref{eqn:stepwise-lya-1}, hence unroll recursively using Proposition \ref{prop:stepwise-lyapunov}: 
        {\small
        \begin{align*}
            &
            F(x_{k + 1}) - F^* + R_{k + 1} + \frac{\hat\gamma_{k + 1}}{2}\Vert v_{k + 1} - x^*\Vert^2 
            \\
            &=
            F(x_{k + 1}) - F^* + R_{k + 1} + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{\rho_{k - 1}L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            (1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \max(1, \rho_{k - 1})\frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \max(1, \rho_{k - 1})(1 - \alpha_k)
            \left(
                F(x_k) - F^* + R_k + \frac{L \alpha_{k - 1}^2}{2}\Vert v_k - x^*\Vert^2
            \right)
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{i})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F^* + R_1 + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
        }
        We had demonstrated in the inductive case for $k=1, 2, \ldots, N$. 
        Additionally, for all $k = 1, 2, \ldots, N$ it has $R_{k + 1} \ge 0$ because: 
        \begin{align*}
            R_{k + 1}
            &= 
            \frac{1}{2}\left(
                L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left(
                \epsilon_k + R_k + 
                \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &= (1 - \alpha_k)
            \left(
                \epsilon_k + R_k 
                + \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                \Vert v_k - y_k\Vert^2
            \right)
            \\
            &\ge 
            (1 - \alpha_k) R_k
            \\
            &\ge R_1 \prod_{i = 1}^{k} \left(1 - \alpha_i\right) = 0. 
        \end{align*}
        Going from the left to the right on the first equality, we used the fact that $\hat \gamma_{k + 1} = L \alpha_{k}^2$.
        This makes coefficient of $\Vert g_k\Vert^2$ zero. 
        The first inequality is by $\epsilon_k \ge 0$ and the non-negativity of the one remaining term. 
        The last equality is by the assumption that $R_1 = 0$. 
        Therefore: 
        {\small
        \begin{align*}
            & 
            F(x_{k + 1}) - F^* +
            \frac{L\alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{i})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F^* + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right). 
        \end{align*}
        }
    \end{proof}
    \begin{remark}        
        The choice of $\rho_k = 1$ reproduces traditional choice of sequences for the APG algorithm. 
        The above claim showed that there exists $\rho_k > 1$, or $\rho_k < 1$ where convergence of $F(x_k) - F(x^*)$ remains possible. 
    \end{remark}


\section{Equivalent representations of R-WAPG}\label{sec:rwapg-equiv-repr}
    This section reduces Definition \ref{def:wapg} into simpler forms that are comparable to what commonly appears in the literatures. 
    In the literatures, variants of Accelerated Proximal Gradient algorithm such as FISTA, V-FISTA has different representations. 
    This shows that R-WAPG provides a unified framework. 
    These equivalent representations are listed in Definition \ref{def:r-wapg-intermediate}, \ref{def:r-wapg-st-form} and \ref{def:r-wapg-momentum-form}. 
    These forms are equivalent under a subset of initial conditions. 
    \par 
    Proposition \ref{prop:wapg-first-equivalent-repr} simplifies Definition \ref{def:wapg} and finds a representation without using auxiliary sequence $\gamma_k, \hat \gamma_k$. 
    Definition \ref{def:r-wapg-intermediate} states the first simplified form of the R-WAPG algorithm which we call: ``R-WAPG intermediate form''. 
    Following a similar pattern, Proposition \ref{prop:wagp-st-form}, \ref{prop:r-wapg-momentum-repr} demonstrates two more equivalent representations of the R-WAPG intermediate form (Definition \ref{def:r-wapg-intermediate}) which are formulated into Definition \ref{def:r-wapg-st-form}, \ref{def:r-wapg-momentum-form}. 
    Convergence results from Proposition \ref{prop:wagp-convergence} applies to all these equivalent forms of R-WAPG. 
    In brief, different equivalent reformulations are summarized as following: 
    \begin{align*}
        &\text{Definition \ref{def:wapg}}   \iff 
        \text{Definition \ref{def:r-wapg-intermediate}}  & \text{By Proposition \ref{prop:wapg-first-equivalent-repr}.}
        \\
        & \text{Definition \ref{def:r-wapg-intermediate}}
        \iff \text{Definition \ref{def:r-wapg-st-form}} & \text{By Proposition \ref{def:r-wapg-st-form}.}
        \\
        & 
        \text{Definition \ref{def:r-wapg-st-form}}\implies 
        \text{Definition }\ref{def:r-wapg-momentum-form}. 
        & \text{By Proposition }\ref{prop:r-wapg-momentum-repr}.
    \end{align*}
    To start, the following proposition on ``abstract similar triangle form'' were made to simplify arguments and notations to make nicer proofs. 
    % SECTION INTRODUCTION ENDS 
    \begin{proposition}[Abstract similar triangle form]\label{prop:abs-st-form}\;\\
        Given any initial $(x_1, v_1)$, and a sequence $(\tau_k)_{k \ge 1}, (\xi_k)_{k \ge 1}$ such that for all $k \ge 1$ $\tau_k \in (0, 1), \xi_k \in (0, 1)$, and the iterates $(y_k, z_{k + 1}, x_{k + 1})_{k \ge 1}$ satisfies recursively that: 
        \begin{align*}
            y_k &= (1 + \tau_k)^{-1}(v_k + \tau_k x_k),
            \\
            v_{k + 1} &= (1 + \xi_k)^{-1}(v_k + \xi_k y_k) - (1 + \xi_k)^{-1}\delta_k g_k,
            \\
            x_{k + 1} &= y_k - L^{-1} g_k. 
        \end{align*}
        If $1 + \xi_k + \tau_k = L\delta_k\; \forall k \ge 1$, then for all $k \ge 1$: 
        $$
            v_{k + 1} - x_{k + 1} = (1 + \xi_k)^{-1}\tau_k(x_{k + 1} - x_k). 
        $$
        Which makes the algorithm a similar triangle form. 
    \end{proposition}
    \begin{proof}
        We are interested in identifying the conditions required for the sequence of $\xi_k, \tau_k, \delta_k$ such that there exists $\theta_k$ satisfying: 
        \begin{align*}
            v_{k + 1} - x_{k + 1} 
            &= \theta_k(x_{k + 1} - x_k).
        \end{align*}
        To verify, we consider:
        \begin{align*}
            v_{k + 1} &= 
            (1 + \xi_k)^{-1}(v_k + \xi_t y_k - \delta_k \mathcal G_L(y_k))
            \\
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k)y_k - \tau_t x_k + \xi_k y_k - \delta_k \mathcal G_L(y_k))
            \\
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k + \xi_k)y_k - \tau_k x_k - \delta_k \mathcal G_L(y_k))
            \\
            v_{k + 1} - x_{k + 1}
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k + \xi_k)y_t - \tau_k x_k - \delta_t \mathcal G_L(y_k))
            - (y_k - L^{-1}\mathcal G_Ly_k)
            \\
            &= 
            (1 + \xi_k)^{-1}(\tau_ky_k - \tau_k x_k - \delta_k \mathcal G_L(y_k))
            + L^{-1}\mathcal G_Ly_k
            \\
            &= 
            (1 + \xi_k)^{-1}
            \left(
                \tau_ty_k - \tau_t x_k + (L^{-1}(1 + \xi_k) - \delta_k) \mathcal G_L(y_k)
            \right)
            \\
            &= 
            (1 + \xi_k)^{-1}\tau_k
            \left(
                y_k - x_k + 
                \tau_k^{-1}(L^{-1}(1 + \xi_k) - \delta_k) \mathcal G_L(y_k)
            \right).
        \end{align*}
        Going between the first and second inequality we used $v_k = (1 + \tau_k)y_k - \tau_k x_k$ which is rearranged $y_k = (1 + \tau_k)^{-1}(v_k + \tau_k x_k)$. 
        The RHS is can be verified through 
        \begin{align*}
            x_{k + 1} - x_k &= 
            y_t - L^{-1}\mathcal G_L(y_k) - x_k
            \\
            &= (y_k - x_k) - L^{-1}\mathcal G_L(y_k). 
        \end{align*}
        It necessitates the condition: 
        \begin{align*}
            \tau_k^{-1}(L^{-1}(1 + \xi_k) - \delta_k) 
            &= - L^{-1}
            \\
            (1 + \xi_k) - L\delta_k
            &= 
            - \tau_k
            \\
            1 + \xi_k + \tau_k
            &=
            L\delta_k. 
        \end{align*}
        Which allows for: 
        \begin{align*}
            v_{k + 1} - x_{k + 1} &= 
            (1 + \xi_k)^{-1}\tau_t
            \left(y_k - x_k - L^{-1}\mathcal G_L(y_k)\right) 
            = 
            (1 + \xi_k)^{-1}\tau_k(x_{k + 1} - x_k). 
        \end{align*}
    \end{proof}
    \subsection{Equivalent representations of R-WAPG}
        This section lists three equivalent representations of the R-WAPG algorithm. 
        They are comparable to existing APG algorithms in the literatures such as Excersise 12.1 in Ryu, Yin \cite{ryu_large-scale_2022}, Similar Triangle from from Lee et al. \cite{lee_geometric_2021}, Ahn Sra \cite{ahn_understanding_2022} and momentum form of (2.2.19) in Nesterov \cite{nesterov_lectures_2018}. 
        Specific instances of Accelerated Proximal Gradient algorithm that has the same form as the Definition \ref{def:r-wapg-intermediate}, Definition \ref{def:r-wapg-st-form} and Definition \ref{def:r-wapg-momentum-form} in the literatures are stated in the remarks that follows the definitions. 

        \begin{definition}[R-WAPG intermediate form]\label{def:r-wapg-intermediate}\;\\
            Assume $\mu < L$ and let $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ given by Definition \ref{def:rwapg-seq}. 
            Initialize any $x_1, v_1$ in $\RR^n$. 
            For $k \ge 1$, the algorithm generates sequence of vector iterates $(y_{k}, v_{k + 1}, x_{k + 1})_{k \ge 1}$ by the procedures: 
            \begin{tcolorbox}
                For $k = 1, 2, \cdots$
                \begin{align*} 
                    & y_{k} = 
                    \left(
                        1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                    \right)^{-1}
                    \left(
                        v_{k + 1} + 
                        \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                    \right), 
                    \\
                    & x_{k + 1} = 
                    y_k - L^{-1} \mathcal G_L y_k, 
                    \\
                    & v_{k + 1} = 
                    \left(
                        1 + \frac{\mu}{L \alpha_k - \mu}
                    \right)^{-1}
                    \left(
                        v_k + 
                        \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                    \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
                \end{align*}
            \end{tcolorbox}
        \end{definition}
        \begin{remark}
            This form of APG is rarely identified in the literatures. 
            The closest algorithm that fits the form but with $\mu = 0$ is Chapter 12 of in Ryu and Yin's Book \cite{ryu_large-scale_2022}, right after Theorem 17. 
            We created this form which makes the math that follows simpler. 
            The inspiration of using this as an intermediate representation was inspired by solving Exercise 12.1 in the same Ryu and Yin's Book. 
        \end{remark}
        \begin{definition}[R-WAPG similar triangle form]\label{def:r-wapg-st-form} \; \\
            Given any $(x_1, v_1)$ in $\RR^n$. 
            Assume $\mu < L$.
            Let the sequence $(\alpha_k)_{k \ge 0}, (\rho_k)_{k\ge 0}$ be given by Definition \ref{def:rwapg-seq}. 
            For $k \ge 1$, the algorithm generates sequences of vector iterates $(y_k, v_{k + 1}, x_{k + 1})_{k \ge 1}$ by the procedures: 
            \begin{tcolorbox}
                For $k=1, 2, \cdots $
                \begin{align*}
                    & y_k = 
                    \left(
                        1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
                    \right)^{-1}
                    \left(
                        v_k + 
                        \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
                    \right), 
                    \\
                    & x_{k + 1} = 
                    y_k - L^{-1} \mathcal G_L y_k, 
                    \\
                    & v_{k + 1} = 
                    x_{k + 1} + (\alpha_k^{-1} -1)(x_{k + 1} - x_k). 
                \end{align*}    
            \end{tcolorbox}
        \end{definition}
        \begin{remark}
            The word similar triangle form can be traced back to several literatures. 
            The term ``Method of Similar Triangle" was used for Algorithm (6.1.19) in Nesterov's book \cite{nesterov_lectures_2018}, but without the necessary graphical illustrations to clarify it. 
            Finally, a similar triangle for formulation of FISTA with $\mu = 0$ can be found in Equation (2), (3), (4) in \cite{chambolle_convergence_2015}. 
            To see graphical visualization on why such term is used to describe the APG algorithm in the literatures, see 
            (3.1, 4.1) in Lee et al. \cite{lee_geometric_2021} and Ahn and Sra \cite{ahn_understanding_2022}. 

        \end{remark}
        \begin{definition}[R-WAPG momentum form]\label{def:r-wapg-momentum-form}
            Given any $y_1 = x_1 \in \RR^n$, and sequences $(\rho_k)_{k \ge 0}, (\alpha_k)_{k\ge 0}$ Definition \ref{def:rwapg-seq}. 
            The algorithm generates iterates $x_{k + 1}, y_{k + 1}$ For $k = 1, 2, \cdots $ by the procedures: 
            \begin{tcolorbox}
                For $k=1, 2,\cdots $
                \begin{align*}
                    & x_{k + 1} = y_k - L^{-1}\mathcal G_Ly_k, 
                    \\
                    & 
                    y_{k + 1} = 
                    x_{k + 1} + 
                    \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
                \end{align*}    
            \end{tcolorbox}
            In the special case where $\mu = 0$, the momentum term can be represented without relaxation parameter $\rho_k$: 
            $$
                (\forall k \ge 1)\quad \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}} 
                = \alpha_{k + 1}(\alpha_k^{-1} - 1). 
            $$
        \end{definition}
        \begin{remark}
            This format fits with (2.2.19) in Nesterov's book \cite{nesterov_lectures_2018}, however, the sequence $(\alpha_k)_{k \ge 0}$ would be given by a different rule. 
            See Theorem \ref{thm:r-wapg-on-cham-doss} and Lemma \ref{lemma:inverted-fista-seq} to see a specific choice of $(\alpha_k)_{k \ge0}, (\rho_k)_{ k\ge 0}$ such this equivalent form of R-WAPG is in fact two possible variants of the FISTA algorithm.
        \end{remark}

    \subsection{Proofs of equivalent representations of R-WAPG}
        \begin{proposition}[First equivalent representation of R-WAPG]\label{prop:wapg-first-equivalent-repr}\;\\
            If the sequence $(y_k, v_k, x_k)_{k \ge 1}$ is produced by R-WAPG (Definition \ref{def:wapg}), 
            then the iterates can be expressed without $(\gamma_k)_{k \ge1},(\hat \gamma_k)_{k \ge 2}$, and for all $k\ge 1$ namely
            \begin{align*}
                & 
                y_{k} = 
                \left(
                    1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                \right)^{-1}
                \left(
                    v_{k} + 
                    \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                \right), 
                \\
                & x_{k + 1} = 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                & v_{k + 1} = 
                \left(
                    1 + \frac{\mu}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
        \end{proposition}
        \begin{proof}
            For all $k \ge 1$, by R-WAPG (Definition \ref{def:wapg}), it has: 
            \begin{align*}
                y_{k} &= 
                (\gamma_k + \alpha_k \mu)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                (\hat \gamma_{k + 1} + \alpha_k \gamma_k)^{-1}
                (\alpha_k \gamma_k v_k + \hat \gamma_{k + 1}x_k)
                \\
                &= 
                \left(
                    \frac{\hat \gamma_{k + 1}}{\alpha_k\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{\hat \gamma_{k + 1}}{\alpha_k \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    \frac{L\alpha_k^2}{\alpha_k\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{L\alpha_k^2}{\alpha_k \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    \frac{L\alpha_k}{\gamma_k} + 1
                \right)^{-1}
                \left(
                    v_k + \frac{L\alpha_k}{ \gamma_k} x_k
                \right)
                \\
                &= 
                \left(
                    1 + \frac{L - L \alpha_k}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{L - L \alpha_k}{L \alpha_k - \mu} x_k
                \right). 
            \end{align*}
            From the left to right of the second equality, we used the fact that $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k\mu$. 
            Going from the left to the right of the second last equality, we did the following: 
            \begin{align*}
                L\alpha_k^2 &= 
                (1 - \alpha_k)\gamma_k + \alpha_k \mu 
                \\
                \iff 
                L \alpha_k^2 - \alpha_k\mu &= 
                (1 - \alpha_k)\gamma_k
                \\
                \iff 
                \gamma_k/L
                &= 
                \frac{L \alpha_k^2 - \alpha_k\mu}{L (1 - \alpha_k)}
                \\
                \iff 
                L/\gamma_k
                &= 
                \frac{L (1 - \alpha_k)}{L \alpha_k^2 - \alpha_k\mu}
                \\
                \iff 
                L\alpha_k/\gamma_k
                &= 
                \frac{L - L\alpha_k}{L\alpha_k - \mu}. 
            \end{align*}
            On the third $\iff$, we can assume $\alpha_k \neq \mu/L\;  \forall k \ge 1$ because from Definition \ref{def:rwapg-seq}: $\alpha_k \in (\mu/L, 1)$ for all $k \ge 1$. 
            % On the third $\iff$, we can assume $\alpha_k \neq \mu/L\; \forall k \ge 2$ because if $\alpha_k = \mu/L$ were impossible, it then has
            % \begin{align*}
            %     L \alpha_k^2 - \alpha_k\mu &= 
            %     (1 - \alpha_k)L \rho_{k - 1}\alpha_{k - 1}^2 = 0 \implies \alpha_{k - 1} = 0. 
            % \end{align*}
            % Which contradict $\alpha_{k - 1} \in (0, 1)\; \forall k \ge 2$ because $\alpha_1 \in (0, 1)$ is given to satisfy the condition in the base case, and $\alpha_k \in (0, 1)\; \forall k \ge 2$ by justifications in Observation \ref{obs:r-wapg-observation-1}. 
            \\
            For all $k \ge 1$, $v_{k + 1}$ has: 
            \begin{align*}
                v_{k + 1} &= 
                \hat \gamma_{k + 1}^{-1}
                ((1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
                \left(
                    (1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k} y_k
                \right)
                - \alpha_k\hat \gamma_{k + 1}^{-1}\mathcal G_L y_k
                \\
                &= 
                \left(
                    1 + \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \frac{\alpha_k \mu}{L \alpha_k^2 - \alpha_k \mu} y_k
                \right)
                - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
            Going from the left to the right of the second equality, we substitute $\hat \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu\alpha_k$. 
            At the end, recall that for all $k \ge 1$, it has $\hat \gamma_{k + 1} = L \alpha_k^2 = (1 - \alpha_k)\gamma_k + \alpha_k \mu$, so: 
            \begin{align*}
                (1 - \alpha_k)\gamma_k
                &= 
                \hat \gamma_{k + 1} - \mu \alpha_k
                = 
                L\alpha_{k}^2 - \alpha_k\mu. 
            \end{align*}
            The proof is now complete. 
            This form doesn't have $\rho_k, \gamma_k, \hat \gamma_k$ in it. 
        \end{proof}
        
        \begin{proposition}[Second equivalent representation of R-WAPG]\label{prop:wagp-st-form}\;\\
            Let iterates $(y_k, x_{k}, v_{k})_{k \ge 1}$ and sequence $(\alpha_k, \rho_k)_{k \ge 0}$ be given by Definition \ref{def:r-wapg-intermediate}. 
            Then for all $k \ge 1$, iterate $y_k, x_{k + 1}, v_{k + 1}$
            satisfy: 
            \begin{align*}
                y_{k} &= 
                \left(
                    1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                \right)^{-1}
                \left(
                    v_{k} + 
                    \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                \right), 
                \\
                x_{k + 1} &= 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                v_{k + 1} &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proposition}
        \begin{proof}
            From Definition \ref{def:r-wapg-intermediate}, define $(\tau_k, \xi_k, \delta_k)_{k \ge 1}$ in Proposition \ref{prop:abs-st-form} to be
            \begin{align*}
                (\forall k \ge 1) \quad \tau_k &= \frac{L(1 - \alpha_k)}{L\alpha_k - \mu},
                \\
                (\forall k \ge 1)\quad 
                \xi_k &= \frac{\mu}{L \alpha_k - \mu},
                \\
                (\forall k \ge 1)\quad 
                \delta_k &\defeq \frac{1 + \xi_k}{L\alpha_k}. 
            \end{align*}
            Then this fits into the format of abstract similar triangle form. 
            We claim that these parameters satisfy $L\delta_k = 1 + \tau_k + \xi_k$. 
            To see this, we have for all $k\ge 1$: 
            \begin{align*}
                1 + \tau_k + \xi_k &= 
                1 + \frac{L(1 - \alpha_k)}{L \alpha_k - \mu} 
                + \frac{\mu}{L \alpha_k - \mu}
                \\
                &= 
                1 + \frac{L - L \alpha_k + \mu}{L\alpha_k - \mu}
                \\
                &= 
                \frac{L - L \alpha_k + \mu + L \alpha_k - \mu}{L\alpha_k - \mu}
                \\
                &= \frac{L}{L\alpha_k - \mu}. 
            \end{align*}
            Next, it also has for all $k \ge 1$: 
            \begin{align*}
                L\delta_k = \frac{1 + \xi_k}{\alpha_k}
                &= 
                \frac{1 + \frac{\mu}{L\alpha_k - \mu}}{\alpha_k}
                = 
                \frac{\frac{L\alpha_k - \mu + \mu}{L \alpha_k - \mu}}{\alpha_k}
                = 
                \frac{L}{L\alpha_k - \mu}.
            \end{align*}
            Hence,  $v_{k + 1} - x_{k + 1} = (1 + \xi_k)^{-1}(x_{k + 1} - x_k)\; \forall k \ge 1$ by Proposition \ref{prop:abs-st-form}.
            Therefore, substituting $\xi_k$ gives: 
            \begin{align*}
                v_{k + 1} &= 
                x_{k + 1} + \left(
                    1 + \frac{\mu}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k}{L\alpha_k - \mu}
                \right)^{-1}\left(
                    \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= 
                x_{k + 1} + \left(
                    \frac{L\alpha_k - \mu}{L\alpha_k}
                \right)\left(
                    \frac{L - L\alpha_k}{L\alpha_k - \mu}
                \right)(x_{k + 1} - x_k)
                \\
                &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proof}
    
    \begin{proposition}[Third equivalent representation of R-WAPG]\label{prop:r-wapg-momentum-repr}
        \;\\
        Let sequence $(\alpha_k, \rho_k)_{k \ge 0}$ and iterates $(x_k, v_k, y_k)_{k\ge 1}$ given by R-WAPG intermediate form (Definition \ref{def:r-wapg-st-form}). 
        Then for all $k \ge 1$, the iterates $(x_{k + 1}, y_{k + 1})_{k \ge 1}$ satisfy: 
        \begin{align*}
            x_{k + 1} &= y_k - L^{-1}\mathcal G_Ly_k, 
            \\
            y_{k + 1} &= 
            x_{k + 1} + 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        If in addition, $v_1 = x_1$ then 
        \begin{align*}
            y_1 = \left(
                1 + \frac{L - L \alpha_1}{L\alpha_1 - \mu}
            \right)^{-1}\left(
                v_1 + \left(
                    \frac{L - L \alpha_1}{L \alpha_1 - \mu}
                \right)x_1
            \right) = x_1. 
        \end{align*}
        If in addition $\mu = 0$, then the momentum term admits a simpler representation 
        \begin{align*}
            (\forall k \ge 1) \quad 
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
            & = \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proposition}
    \begin{proof}
        Start by considering the update rule for $v_k$ from the Definition \ref{def:r-wapg-st-form} which has for all $k \ge 1$: 
        \begin{align}
            v_{k + 1} &= 
            x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \\
            \iff 
            (L \alpha_{k + 1} - \mu)v_{k + 1} 
            &= 
            (L \alpha_{k + 1} - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k).
            \label{eqn:third-eqv-repr-eqn1} 
        \end{align}
        Next, we simplify the update $y_{k}$ by Definition \ref{def:r-wapg-st-form} for all $k \ge 1$: 
        \begin{align*}
            y_k &= 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \left(
            \frac{L - \mu}{L\alpha_k - \mu} 
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \frac{L\alpha_k - \mu}{L - \mu} v_k
            + 
            \frac{L - L \alpha_k}{L - \mu} x_k
            \\
            &= (L - \mu)^{-1}((L \alpha_k - \mu)v_k + (L - L \alpha_k)x_k). 
        \end{align*}
        We have $y_1 = x_1$ when $v_1 = x_1$ because
        \begin{align*}
            y_1 &= (L - \mu)^{-1}(L(\alpha_1 - \mu)x_1 + (L - L\alpha_1)x_1)
            \\
            &= (L - \mu)^{-1}(L(\alpha_1 - \mu)x_1 + (L - L \alpha_1)x_1)
            \\
            &= (L - \mu)^{-1}((L - \mu)x_1) = x_1. 
        \end{align*}
        Using RHS of \eqref{eqn:third-eqv-repr-eqn1} to substitute $L\alpha_{k + 1} - \mu$ into $y_{k + 1}$, so it has for all $k\ge 1$: 
        {\small
        \begin{align*}
            y_{k + 1} &= 
            (L - \mu)^{-1}((L\alpha_{k + 1} - \mu)v_{k + 1} + (L - L \alpha_{k + 1})x_{k + 1})
            \\
            &= (L - \mu)^{-1}
            \left(
                (L\alpha_{k + 1} - \mu)x_{k + 1} + 
                (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
                + (L - L \alpha_{k + 1})x_{k + 1}
            \right)
            \\
            &= 
            (L - \mu)^{-1}
            \left(
                (L - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \right)
            \\
            &= x_{k + 1} + \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}(x_{k + 1} - x_k). 
        \end{align*}
        }
        The coefficient for $(x_{k + 1} - x_k)$ needs some works to formulate it without the parameter $\mu, L$. 
        To do that we have: 
        \begin{align*}
            \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}
            &= \frac{(L\alpha_{k + 1} - \mu)\alpha_k(1 - \alpha_k)}{\alpha_k^2(L - \mu)}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \left(
                \frac{\alpha_k^2(L - \mu)}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \alpha_k(1 - \alpha_k)
            \left(
                \frac{L\alpha_k^2 - \mu\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \rho_k\left(
                \frac{L\rho_k\alpha_k^2 - \mu\rho_k\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \rho_k\alpha_k(1 - \alpha_k)
            \left(
                \frac{(L\alpha_{k + 1} - \mu)(\rho_k\alpha_k^2 + \alpha_{k + 1})}
                {L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}.
        \end{align*}
        Going from the left to right on the fourth equality, we used $L\alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_kL\alpha_k^2 + \mu \alpha_{k + 1}$ to establish: 
        \begin{align*}
            L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2 
            &= 
            (1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            ((1 - \alpha_{k + 1})L \rho_k \alpha_k^2 + \mu \alpha_{k + 1}) - \mu\alpha_{k + 1} + \alpha_{k + 1} L \rho_k \alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= L \alpha_{k + 1}^2 - \mu\alpha_{k + 1} + \alpha_{k + 1}L\rho_k\alpha_k^2 - \mu \rho_k \alpha_k^2
            \\
            &= 
            L\alpha_{k + 1}(\alpha_{k + 1} + \rho_k \alpha_k^2) - \alpha_{k + 1}\mu - \mu \rho_k \alpha_k^2
            \\
            &= (L \alpha_{k + 1} - \mu)(\alpha_{k + 1} + \rho_k \alpha_k^2). 
        \end{align*}
        When $\mu = 0$, things simplify. 
        Consider that $\forall k \ge 1: \alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_k\alpha_k^2$. 
        \begin{align*}
            \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \alpha_{k + 1}^2}
            \\
            &= 
            \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}
            {\rho_k\alpha_{k + 1}\alpha_k^2 + \rho_k(1 - \alpha_{k + 1})\alpha_k^2}
            \\
            &= \frac{\rho_k\alpha_{k + 1}\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2}
            \\
            &= \alpha_{k + 1}(\alpha_k^{-1} - 1). 
        \end{align*}
    \end{proof}


\section{R-WAPG verifies existing accelerations scheme}\label{sec:rwapg-literatures}
    In addition to various equivalent forms of the R-WAPG algorithm, the R-WAPG sequences are much more flexible. 
    They generalize many existing sequences used in accelerated proximal gradient schemes. 
    \par
    This section demonstrates that several variants of FISTA in the literatures reduces to the R-WAPG method by setting up the R-WAPG sequences with additional assumptions. 
    This section will also demonstrate that the convergence claim (Proposition \ref{prop:wagp-convergence}) holds, and it derives convergence rates consistent with results in the literatures.  
    The table follows demonstrates the R-WAPG convergence results specialized into various settings. 
    \begin{table}[H]
        {\scriptsize
        \begin{tabular}{|l|l|l|l|l|}
        \hline
            Algorithm & $\mu$ & $\alpha_k$ & $\rho_k$ & Convergence of $F(x_k) - F^*$ 
        \\ \hline
            R-WAPG in Definition \ref{def:wapg} & 
            $\mu \ge 0$ &
            $\alpha_k \in(\mu/L, 1)$ & 
            $\rho_k > 0$ &  
            \begin{tabular}{l}
                $\mathcal O \left(\prod_{i = 0}^{k-1} \max(1, \rho_i)(1 - \alpha_{i + 1})\right)$
                \\
                (Proposition \ref{prop:wagp-convergence})
            \end{tabular}
        \\ \hline
            Chambolle, Dossal 2015 \cite{chambolle_convergence_2015} &  
            $\mu = 0$  &
            $ 0< \alpha_k^{-2} \le \alpha_{k + 1}^{-1} - \alpha_{k + 1}^{-2}$ &
            $\rho_k \ge 1$ &  
            \begin{tabular}{l}
                $\mathcal O(\alpha_k^{2})$ \\ (Theorem \ref{thm:r-wapg-on-cham-doss})    
            \end{tabular}
        \\ \hline
            V-FISTA Beck (10.7.7) \cite{beck_first-order_2017} &
            $\mu > 0$&
            $\alpha_k = \sqrt{\mu/L}$ & 
            $\rho_k = 1$ &
            \begin{tabular}{l}
                $\mathcal O\left((1 - \sqrt{\mu/L})^k\right)$, 
                \\
                (Theorem \ref{thm:fixed-momentum-fista}, remark)
            \end{tabular}
        \\ \hline
            R-WAPG in Definition \ref{def:wapg} &  
            $\mu > 0$ &
            $\alpha_k = \alpha \in (\mu/L, 1)$ &  
            $\rho_k = \rho > 0$ & 
            \begin{tabular}{l}
                $\mathcal O \left(\max((1 - \alpha), 1 - \mu/(\alpha L))^{k} \right)$\\
                (Theorem \ref{thm:fixed-momentum-fista})
            \end{tabular}
        \\ \hline
        \end{tabular}
        }
    \end{table}
    The lemma follows characterizes momentum sequences appears in the literatures using Definition \ref{def:rwapg-seq}. 
    \begin{lemma}[R-WAPG sequences as inverted FISTA sequence]\label{lemma:inverted-fista-seq}
        Let R-WAPG sequence $(\rho_k)_{k \ge 0}, (\alpha_k)_{k \ge 0}$ given by Definition \ref{def:rwapg-seq}. 
        If $\mu = 0, \rho_k \ge 1\; \forall k \ge 0$, and $\alpha_0 = 1$, then: 
        \begin{enumerate}
            \item $\alpha_k^{-2} \ge \alpha_{k + 1}^{-2} - \alpha_{k + 1}^{-1}\; \forall k \ge 0$
            \item Let $t_k := \alpha_k^{-1}$, then $0 < t_{k + 1} \le (1/2)\left(1 + \sqrt{1 + 4t_k^2}\right)\;\forall k\ge 0$, hence the name: ``Inverted FISTA sequence''. 
            \item $\prod_{i = 1}^k\max(1, \rho_{k - 1})(1 - \alpha_k) = \alpha_k^2 \quad (\forall k \ge 1)$. 
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        We start proving (i). 
        For all $k \ge 1$: 
        \begin{align*}
            \alpha_{k + 1}^2 
            &= (1 - \alpha_{k + 1})\rho_k\alpha_k^2 + (\mu/L) \alpha_k
            \\
            &= (1 - \alpha_{k + 1})\rho_k\alpha_k^2 & \mu = 0 \text{ assumed } 
            \\
            \implies 
            \alpha_{k + 1}^2 
            & \ge (1 - \alpha_{k + 1})\alpha_k^2 
            &  \rho_k \ge 1
            \\
            \iff 
            \alpha_k^{-2} 
            &\ge 
            \alpha_{k + 1}^{-2} - \alpha_{k + 1}^{-1}. 
        \end{align*}
        When $k = 0$, it has $\alpha_0 = 1$. 
        By definition of $\rho_0$ we have 
        \begin{align*}
            \alpha_1^2 &= 
            (1 - \alpha_1)\alpha_0^2\rho_0 \underset{\mu = 0}{=}
            \frac{\alpha_1^2}{\alpha_0^2(1 - \alpha_1)}(1 - \alpha_1)\alpha_0^2 = \alpha_1^2. 
        \end{align*}
        Therefore, (i) holds. 
        (i) is proved. 
        \par
        (i) $\implies$ (ii): Substituting $\alpha_k^{-1} = t_k$ changes (i) into $t_{k + 1}^2 - t_{k + 1} - t_{k}^2 \le 0$ for all $k \ge 0$. 
        Solving the equality yields $t_{k + 1} = (1/2)\left(1 \pm \sqrt{1 + 4 t_k^2}\right)$ for all $k \ge 0$. 
        Since $\alpha_k \in (0, 1)$, it means $t_k > 0$, hence the valid root is the larger root which gives the upper bound for $t_{k + 1}$ so: 
        \begin{align*}
            t_{k + 1} \in \left(
                0, \frac{1}{2}\left(1 + \sqrt{1 + 4t_k^2}\right) 
            \right]
        \end{align*}
        To prove (iii), because $\rho_k \ge 1$, we have $\prod_{i = 1}^{k} \max(1, \rho_{k - 1})(1 - 
        \alpha_k)= \prod_{i = 1}^{k}\rho_{k - 1}(1 - \alpha_k)$ for all $k \ge 1 $. 
        By definition $(\alpha_k)_{k \ge 1}, (\rho_k)_{k \ge 1}$ sequences they have for all $k \ge 1$: 
        \begin{align*}
            \alpha_k^2 &= \rho_{k - 1}(1 - \alpha_k)\alpha_{k - 1}^2
            \\
            \iff 
            \alpha_k^2/\alpha_{k - 1}^2 &= \rho_{k - 1}(1 - \alpha_k)
            \\
            \prod_{i = 1}^{k}\rho_{k - 1}(1 - \alpha_k)
            &= 
            \prod_{i = 1}^{k}\alpha_k^2 /\alpha_{k - 1}^2= \alpha_k^2/\alpha_0. 
        \end{align*}
        Because $\alpha_0 = 1$, (iii) is justified. 
    \end{proof}
    \begin{remark}
        The sequence $t_k$ is exactly the same as in Theorem 3.1 of Chambolle, Dossal \cite{chambolle_convergence_2015}. 
    \end{remark}

    \begin{lemma}[Constant R-WAPG sequences]\label{lemma:constant-rwapg-seq}
        Suppose $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ are R-WAPG sequences given by Definition \ref{def:rwapg-seq} and assume $L > \mu > 0$.
        Define $q := \mu/L$. 
        Then $\forall r \in \left(\sqrt{q},\sqrt{q^{-1}}\right)$, the constant sequence $\alpha_k := r \sqrt{q}$ has the following: 
        \begin{enumerate}
            \item FOr any $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$, the constant sequence $\alpha_k := \alpha \in (q, 1)$ and\\
            $\rho_k := \rho=\left(1-r^{-1}\sqrt{q}\right)\left(1 - r \sqrt{q}\right)^{-1} > 0$, hence it's a pair of valid R-WAPG sequence. 
            \item The momentum terms $\theta_{t + 1}$ in Definition \ref{def:r-wapg-momentum-form}, which we denoted by $\theta$ is the constant:\\ $\theta = (1 - r^{-1}\sqrt{q})(1 - r\sqrt{q})(1- q)^{-1}$
            \item When $r = 1$, $\theta = (1- \sqrt{q})(1 + \sqrt{q})^{-1}$. 
            \item For all $r \in \left(1, \sqrt{q^{-1}}\right)$, $\rho > 1$; for all $r \in \left(\sqrt{q}, 1\right]$ $\rho \le 1$. 
            \item For all $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$, $\max(\rho, 1)(1 - \alpha) = \max\left(1 - r\sqrt{q}, 1 - r^{-1}q\right)$. 
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        To see (i), fix any $\sqrt{q} < r < \sqrt{q^{-1}}$, so
        \begin{align*}
            r &\in \left(\sqrt{q}, \sqrt{q^{-1}}\right)
            \iff 
            r\sqrt{q} \in 
            \left(
                q, 1
            \right). 
        \end{align*}
        Therefore, $\alpha_k \in (\mu/L, 1)$. 
        To see $\rho_k$, by definition it has 
        \begin{align*}
            \rho_k &= \frac{\alpha_{k + 1}^2 - q \alpha_{k + 1}}{(1 - \alpha_{k + 1})\alpha_k^2} 
            = \frac{\alpha^2 - q \alpha}{(1 - \alpha)\alpha^2} 
            \\
            &= \frac{1 - q\alpha^{-1}}{1 - \alpha}
            \\
            &= \frac{1 - q r^{-1}\sqrt{q^{-1}}}{1 - r \sqrt{q}}
            \\
            &= \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}} > 0. 
        \end{align*}
        Simple algebra can show (ii). 
        To start, we substitute the constant sequence $\alpha, \rho$ for $\alpha_k, \rho_k$ into the definition of $\theta$: 
        \begin{align*}
            \theta &= \frac{\rho\alpha(1 - \alpha)}{\rho \alpha^2 + \alpha}
            = (\rho\alpha)\frac{1 - \alpha}{\rho \alpha^2 + \alpha} = \rho \frac{1 - \alpha}{\rho \alpha + 1}
            \\
            &= 
            \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}}
            (1 - r\sqrt{q})
            \left(
                1 + r\sqrt{q}\frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}}
            \right)^{-1}
            \\
            &= (1 - r^{-1} \sqrt{q})\left(
                1 + \frac{r \sqrt{q} - q}{1 - r \sqrt{q}}
            \right)^{-1}
            \\
            &= \left(1 - r^{-1} \sqrt{q}\right)\left(
                \frac{1 - q}{1 - r \sqrt{q}}
            \right)^{-1} = \frac{(1 - r^{-1}\sqrt{q})(1 - r \sqrt{q})}{1 - q}. 
        \end{align*}
        Now it's the perfect opportunity to show (iii) by substituting $r = 1$ which has 
        \begin{align*}
            \theta &= \frac{(1 - \sqrt{q})^2}{1 - q}
            =
            \frac{(1 - \sqrt{q})}{(1 - \sqrt{q})(1 + \sqrt{q})}
            = \frac{1 - \sqrt{q}}{1 + \sqrt{q}}. 
        \end{align*}
        Next for (iv), we determine when $\rho$ switches from $> 1$ to $ \le 1$. 
        For any $r \in \left(1, \sqrt{q^{-1}}\right)$, we show $\rho > 1$: 
        \begin{align*}
            \rho &= \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}} 
            > \frac{1 - \sqrt{q}}{1 - r \sqrt{q}} > \frac{1 - \sqrt{q}}{1 - \sqrt{q}} = 1
        \end{align*}
        The first inequality comes from $r > 1 \iff -r^{-1} > -1$, the second inequality comes from $r > 1 \iff -r < 1$. 
        For any $r \in \left(\sqrt{q}, 1\right]$, we have $\rho \le 1$ by 
        \begin{align*}
            \rho &= \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}} 
            \le \frac{1 - \sqrt{q}}{1 - r \sqrt{q}} \le \frac{1 - \sqrt{q}}{1 - \sqrt{q}} = 1. 
        \end{align*}
        The first inequality comes from $-r^{-1} \le - 1$; the second inequality comes from $-r \ge -1$. 
        \par
        Finally, to show (v), we consider by cases: 
        \begin{enumerate}
            \item[Case A:] When $r \in \left(1, \sqrt{q^{-1}}\right)$, we have $\rho > 1$ by (iii), meaning that $1 - r^{-1}\sqrt{q} > 1 - r \sqrt{q}$. $\rho > 1 \implies \max(\rho, 1) = \rho$ hence 
                \begin{align*}
                    \max(\rho, 1)(1 - \alpha) &= \rho(1- \alpha)
                    = \frac{1 - r^{-1}\sqrt{q}}{1 - r \sqrt{q}}(1 - r \sqrt{q}) = 1 - r^{-1}\sqrt{q}. 
                \end{align*}
                In this case, the larger one of the two values: $\left\lbrace1 - \rho^{-1}\sqrt{q}, 1 - r\sqrt{q}\right\rbrace$ is taken. 
            \item[Case B:] When $r \in \left(\sqrt{q}, 1\right]$, we have $\rho \le 1$ by (iii); hence $1 - r^{-1}\sqrt{q} \le 1 - r \sqrt{q}$ and $\max(1,\rho) = 1$ therefore: 
            \begin{align*}
                \max(1, \rho)(1 - \alpha) = (1 - \alpha) = 1 - r \sqrt{q}. 
            \end{align*}
            Once again, the larger one of the quantity: $1 - r \sqrt{q}$ among $\{1 - r^{-1}\sqrt{q}, 1 - r\sqrt{q}\}$ is taken. 
        \end{enumerate}
        Therefore, for all vales of $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$ are completed by cases: (A), (B).
        The largest of the value among $\{1 - r \sqrt{q}, 1 - r^{-1}\sqrt{q}\}$ is always taken. 
        Therefore, it has: 
        \begin{align*}
            \max(1, \rho)(1 - \alpha) = \max\left(1 - r\sqrt{q}, 1 - r^{-1}\sqrt{q}\right). 
        \end{align*}

    \end{proof}

    \begin{theorem}[FISTA first variant Chambolle, Dossal 2015]\label{thm:r-wapg-on-cham-doss}\;\\
        Fix arbitrary $a \ge 2$.
        Define $\forall k \ge 1$ the sequence $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ by 
        \begin{align*}
            \alpha_k &= a/(k + a), 
            \\
            \rho_k &= \frac{(k + a)^2}{(k + 1)(k + a + 1)}. 
        \end{align*}
        Consider the algorithm given by: 
        \begin{tcolorbox}
            Initialize any $y_1 = x_1$. 
            \\
            For $k = 1, 2, \cdots$, update: 
            \begin{align*}
                & x_{k + 1} := y_k + L^{-1}\mathcal G_L(y_k), 
                \\
                & \theta_{t + 1} := \alpha_{k + 1}(\alpha_k^{-1} - 1),
                \\
                & y_{k + 1} := x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k). 
            \end{align*}    
        \end{tcolorbox}
        If $\mu = 0$, then $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ is a valid pair of R-WAPG sequence from Definition \ref{def:rwapg-seq} and the above algorithm is a valid form of R-WAPG. 
        \par
        Assume minimizer $x^*$ exists for function $F$. 
        Then algorithm produces $(x_k)_{k \ge0}$ such that $F(x) - F(x^*)$ convergences at a rate of $\mathcal O(\alpha_k^2)$. 
    \end{theorem}
    \begin{proof}
        We only need to show that $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ are valid R-WAPG sequences. 
        To do that we verify equality $\alpha_{k + 1}^2 = (1 - \alpha_{k + 1})\rho_k \alpha_k^2$ because it's assumed $\mu = 0$ in this case. 
        For all $k \ge 0$, the right side of the equality evaluates to 
        \begin{align*}
            \alpha_k^2 \rho_k(1 - \alpha_{k + 1}) &= 
            \left(
                \frac{\alpha}{k + 1}
            \right)^2 \left(
                \frac{(k + a)^2}{(k + 1)(k + a + 1)}
            \right)
            \left(
                1 - \frac{a}{k + 1 + a}
            \right)
            \\
            &= \left(
                \frac{\alpha}{k + 1}
            \right)^2 \left(
                \frac{(k + a)^2}{(k + 1)(k + a + 1)}
            \right)
            \left(
                \frac{k + 1}{k + 1 + a}
            \right)
            \\
            &= \frac{a^2}{(k + a + 1)^2} = \alpha_{k + 1}^2. 
        \end{align*}
        Therefore, $(\rho_k)_{k \ge 0}, (\alpha_k)_{k \ge 0}$ is a pair of valid R-WAPG sequence. 
        So, it can be used in the R-WAPG algorithm and represent it in R-WAPG Momentum Form in Definition \ref{def:r-wapg-momentum-form}. 
        Using $\mu = 0$, it simplifies the momentum term in Definition \ref{def:r-wapg-momentum-form} and gives the rules of updates in the theorem statement. 
        \par
        Next, we show the convergence rate of the algorithm. 
        Observe that $(\forall k \ge 0)(\forall a \ge 2)$, $\rho_k$ has 
        \begin{align*}
            \rho_k &= \frac{(k + a)^2}{(k + 1)(k + a + 1)} 
            \\
            &> \frac{(k + 1)^2}{(k + 1)(k + a + 1)} 
            & \text{By } a \ge 2. 
            \\
            &= \frac{(k + 1)}{(k + a + 1)} 
            \\
            & > 1.   & \text{By } a \ge 2. 
        \end{align*}
        Hence, (iii) in Lemma \ref{lemma:inverted-fista-seq} applies. 
        By Proposition \ref{prop:wagp-convergence}, we have:
        {\small\begin{align*}
            & F(x_{k + 1}) - F(x^*) + \frac{L\alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            & = F(x_{k + 1}) - F(x^*) + \frac{L\alpha_k^2}{2}\Vert x_{k + 1} - x^* + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k)\Vert^2
            \\
            & \le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right)
            \\
            &= 
            \alpha_k^2
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right) 
            & \text{By Lemma \ref{lemma:inverted-fista-seq} (iii). }
            \\
            &= 
            \left(\frac{a}{k + a}\right)^2
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert x_1 - x^*\Vert^2
            \right). 
        \end{align*}}
        We can replace $v_1$ to be $x_1$ by Proposition \ref{prop:r-wapg-momentum-repr}. 
    \end{proof}
    \begin{remark}
        This algorithm described here is exactly the same algorithm being analyzed in the paper by Chambolle, Dossal \cite{chambolle_convergence_2015}. 
    \end{remark}

    \begin{theorem}[Fixed momentum APG]\label{thm:fixed-momentum-fista}
        Assume $L > \mu > 0$, let a pair of constant R-WAPG sequence: $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ be given by Lemma \ref{lemma:constant-rwapg-seq}.
        Define $q := \mu/L$ and for any fixed $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$, let $\alpha_k := \alpha = r \sqrt{q}$ be the constant R-WAPG sequence. 
        Consider the algorithm with a constant momentum specified by the following: 
        \begin{tcolorbox}
            Define $\theta = \left(1 - r^{-1}\sqrt{q}\right)(1 - r\sqrt{q})(1 - q)^{-1}$. 
            \\
            Initialize $y_1 = x_1$; for $k = 1, 2, \ldots, N$, update: 
            \begin{align*}
                &x_{k + 1} = y_k + L^{-1}\mathcal G_L y_k, 
                \\
                & y_{k + 1} = x_{k + 1} + \theta(x_{k + 1} - x_k). 
            \end{align*}
        \end{tcolorbox}
        Then the algorithm generates $(x_k)_{k \ge 1}$ such that $F(x) - F(x^*)$ convergences at a rate of $\mathcal O\left(\max(1 - r\sqrt{q}, 1 - r^{-1}\sqrt{q})^k\right)$. 
    \end{theorem}
    \begin{proof}
        The constant sequence $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ is a valid R-WAPG sequence by Lemma \ref{lemma:constant-rwapg-seq}. 
        The algorithm presented here is the same as the R-WAPG momentum form because the term $\theta$ from Lemma \ref{lemma:constant-rwapg-seq} matches up with Definition \ref{def:r-wapg-momentum-form}. 
        Therefore, by Proposition \ref{prop:wagp-convergence} we have: 
        \begin{align*}
            & F(x_{k + 1}) - F^* + \frac{L\alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
            \\
            &= 
            F(x_{k + 1}) - F^* + \frac{L\alpha_k^2}{2}\Vert x_{k + 1}  - x^* + (\alpha^{-1} - 1)(x_{k + 1} - x_k)\Vert^2
            \\
            &\le 
            \left(
                \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
            \right)
            \left(
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right)
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right)
            \\
            &= \left(
                \prod_{i = 1}^{k - 1} \max(1, \rho)(1 - \alpha)
            \right)
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
            \right)
            \\
            &= \max(1 - r\sqrt{q}, 1 - r^{-1}\sqrt{q})^k
            \left(
                F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert x_1 - x^*\Vert^2
            \right). 
            & \text{Lemma \ref{lemma:constant-rwapg-seq} (v)}
        \end{align*}
        The equivalency between the momentum representation of R-WAPG (Definition \ref{def:r-wapg-momentum-form}) and the similar triangle representation (Definition \ref{def:r-wapg-st-form}) allows us to write $v_{k + 1} = x_{k + 1} + (\alpha^{-1} - 1)(x_{k + 1} - x_k)$. 
        By the initialization condition $v_1 = x_1$ specified in Proposition \ref{prop:r-wapg-momentum-repr}, we can write $v_1$ as $x_1$. 
    \end{proof}
    \begin{remark}
        When $r = 1$, the algorithm described above is exactly the same as the V-FISTA algorithm specified in (10.7.7) of his book \cite{beck_first-order_2017}. 
    \end{remark}


\section{The method of Free R-WAPG}\label{sec:free-rwapg}
    In this section, we propose Algorithm \ref{alg:free-rwapg} which estimates the $\mu$ constant as the algorithm executes, and it pools the information using the Bregman Divergence of the smooth part function $f$. 
    It is called Free R-WAPG because it doesn't require any knowledge of $\mu, L$ in prior for the smooth part $f$, making it parameter-free. 
    \begin{algorithm}
        \begin{algorithmic}[1]
        {\footnotesize
        \STATE{\textbf{Input: } $f, g, x_0, L > \mu \ge 0, \in \RR^n, N \in \N$}
        \STATE{\textbf{Initialize: }$y_0 := x_0;L := 1; \mu := 1/2; \alpha_0 = 1;$}
        \STATE{\textbf{Compute: } $f(y_k)$; }
        \FOR{$k = 0, 1, 2, \cdots, N$}
            \STATE{\textbf{Compute: }$\nabla f(y_k); x^+:= [I + L^{-1}\partial g](y_k - L^{-1}\nabla f(y_k))$;}
            \WHILE{$L/2\Vert x^+ - y\Vert^2 < D_f(x^+, y)$}
                \STATE{$L:= 2L$;}
                \STATE{$x^+ = [I + L^{-1}\partial g](y_k - L^{-1}\nabla f(y_k))$; }
            \ENDWHILE
            \STATE{$x_{k + 1} := x^+$;}
            \STATE{$\alpha_{k + 1} := (1/2)\left(\mu/L - \alpha_{k}^2 + \sqrt{(\mu/L - \alpha_{k}^2)^2 + 4\alpha_{k}^2}\right)$;}
            \STATE{$\theta_{k + 1} := \alpha_k(1 - \alpha_k)/(\alpha_k^2 + \alpha_{k + 1})$;}
            \STATE{$y_{k + 1}:= x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k)$; }
            \STATE{\textbf{Compute: } $f(y_{k + 1})$}
            \STATE{$\mu := (1/2)(2D_f(y_{k + 1}, y_{k})/\Vert y_{k + 1} - y_k\Vert^2) + (1/2)\mu$;}
        \ENDFOR
        }
        \end{algorithmic}
        \caption{Free R-WAPG}
        \label{alg:free-rwapg}
    \end{algorithm}
    \par
    Line 5-8 estimates upper bound for the Lipschitz constant and find $x^+$, the next iterates produced by proximal gradient descent on previous $y_k$; 
    Line 9 updates $x_{k + 1}$ to be $x^+$, a successful iterate identified by the Lipschitz line search routine;
    Line 10 updates the R-WAPG sequence $\alpha_k$ for the iterates $y_{k + 1}$;
    Line 13 updates $\mu$ using the Bregman Divergence of $f$ from iterates $y_{k + 1}, y_k$. 
    \par
    Assume $L$ given is an upper bound of the Lipschitz smoothness constant of $f$, so the line search subroutine is never triggered. 
    Then the algorithm calls $f(\cdot)$ two times, and $\nabla f(\cdot)$ once per iteration. 
    The algorithm computes $\nabla f(y_k)$ once for $x^+$, $f(y_{k + 1})$ once for Bregman Divergence because $f(y_{k})$ is evaluated from the previous iteration, and $f(x^+)$ once for Lipschitz constant line search condition. 
    We note that $f(y_0)$ is computed before the start of the for loop. 
    And finally, it evaluates proximal of $g$ at $y_k - L^{-1}\nabla f(y_k)$ once. 
    \par 
    From a theoretical perspective, the convergence result (Proposition \ref{prop:wagp-convergence}) of R-WAPG is relevant to some degree because the sequence $(\alpha_k)_{k \ge 0}$ generated by Algorithm \ref{alg:free-rwapg} is an R-WAPG sequence. 
    Let $\tilde \mu_k$ be an estimate produced by the algorithm then it satisfies that $\tilde \mu_k/ L \in (0, 1)$.
    Assuming inductively that $q = \tilde \mu_k/L$,  $\alpha_k \in (\mu/L, 1)$, then 
    \begin{align*}
        \alpha_{k + 1} 
        &= 
        (1/2)\left(
            q - \alpha_k^2 + \sqrt{(q - \alpha_k^2)^2 + 4 \alpha_k^2}
        \right)
        \\
        &= 
        (1/2)\left(
            q - \alpha_k^2 + \sqrt{
                (\alpha_k^2 - q + 2)^2 + 2q - 4
            }
        \right)
        \\
        &< 
        (1/2)\left(
            q - \alpha_k^2 + \sqrt{
                (\alpha_k^2 - q + 2)^2
            }
        \right) \le 1. 
    \end{align*}
    Here we used the assumption that $q \in (0, 1)$ so $2q - 4 < 0$ always. 
    It is not hard to see that $\alpha_{k + 1} > \mu/L$ as well. 
    Therefore, $\alpha_k \in (0, 1)$. 
    In the very unlikely case when $q = 1$, Algorithm \ref{alg:free-rwapg} has $\theta_k = 0$, and it performs one step of proximal gradient descent. 
    \subsection{Numerical experiments}
        This section we conduct numerical experiments conducted using FR-WAPG algorithm (Algorithm \ref{alg:free-rwapg}), and compare with other APG algorithms in the literatures: the V-FISTA, and M-FISTA algorithm described in Section (10.7.7, 10.7.6) by Beck \cite{beck_first-order_2017}. 
        We implemented in Julia \cite{bezanson_julia_2017} and compare V-FISTA, M-FISTA from Beck, and Algorithm \ref{alg:free-rwapg} given this section. 
        Focusing on iteration complexities of the algorithm, the results of the experiments are visualized and the setup of the numerical experiments are described  in the sections that follows. 
        \par
        The equivalences highlighted in Proposition \ref{prop:r-wapg-momentum-repr} allows us to compare the sequence of iterates $(x_k)_{k \ge 1}, (y_k)_{k \ge0}$ for R-WAPG, VISTA and M-FISTA. 
        \par
        Given the same randomized initial condition for all the algorithm, we measure the aggregate statistics of the base two logarithms of the normalized optimality gap (NOG), at each iteration $k$.  
        Given the iterates $x_k$, and the minimum $F^*$, the normalized optimality gap we defined is: 
        \newcommand{\NOG}{\text{\textbf{NOG}}}
        \begin{align*}
            \delta_k := \log_2\left(
                \NOG_k := \frac{F(x_k) - F^*}{F(x_0) - F^*}
            \right). 
        \end{align*}
        Since it's not the case that $F^*$ is always known in prior, we used the minimum of all $F(x_k)$ across all algorithms, all iterations $k$ as the surrogate for $F^*$. 
        \par 
        For the termination conditions of the algorithm, we consider the norm of the gradient mapping $\Vert \mathcal G_L(y_k)\Vert < \epsilon$. 
        The $L$ can change during each iteration if it's obtained through the specified Lipschitz line search routine. 

        \subsubsection{Simple convex quadratic}
            Consider the minimization problem of $\min_{x \in \RR^n} \{F(x):= f(x) + 0\}$ where the objective function is given by: 
            \begin{align*}
                F(x) = (1/2)\langle x, A x\rangle. 
            \end{align*}
            The matrix $A$ is set to be positive semi-definite and diagonal. 
            Then the optimization problem admits unique minimizer $x^* = \mathbf 0$ and the minimum is zero. 
            \par
            We apply Algorithm \ref{alg:free-rwapg}, M-FISTA, and V-FISTA. 
            The parameters for setting up the problem now follows. 
            \begin{enumerate}
                \item $N$, the dimension of the problem. 
                \item $0 < \mu < L$, the strong convexity and Lipschitz smoothness constant. They are given in prior to construct the problem. 
                \item $A \in \RR^{N\times N}$, a diagonal matrix given by $N- 1$ linearly spaced with equal increment on the interval $[\mu, L]$, and an extra number $0$, i.e: $A = \text{diag}(0, \mu + (L-\mu)(N - 1)^{-1}, \mu + 2(L-\mu)(N - 1)^{-1}, \cdots, \mu + (N - 2)(L - \mu)^{-1}, L)$. 
                \item In this case $f = F = (1/2)\langle x, A x\rangle$ and $g \equiv 0$. 
                \item $\epsilon > 0$, the tolerance value for termination criteria. 
                \item $x_0 \sim \mathcal N(I, \mathbf 0)$ is a vector, and it's the initial condition for all the algorithm. In this case the initial guess is fixed for all R-WAPG, M-FISTA and M-FISTA, but it's randomly generated by the zero mean standard normal distribution for each element in the vector. 
            \end{enumerate}
            The parameter $L=1, \mu=10^{-5}$ are given in prior to produce the diagonal matrix $A$, and we conduct many experiments for $N = 256$ and $N = 1024$. 
            For all R-WAPG, M-FISTA and V-FISTA, 
            we use a different initial guess each time, a set of 30 experiments are performed. 
            The maximum, minimum and median values of $\delta_k$ are measured for all algorithm at each iteration and plotted as a ribbon. 
            Results are shown in Figure \ref{fig:simple-quadratic-NOG}.
            The solid line in the ribbon is the median value of $\delta_k$ across all experiment, the ribbon gives the maximum, minimum value of $\delta_k$ for each iteration across all experiments. 
            FR-WAPG initially behaves similar to M-FISTA, but as the iteration goes on, it started to behave like V-FISTA.  
            \par 
            The most surprising feature here is the monotone descent, however, it's being numerical verified that the method is not monotone in general, it just looks monotone on the figure. 
            \begin{figure}[H]
                \begin{subfigure}[b]{0.47\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{assets/simple_regression_batched-256.png}
                    \caption{$N = 256$, simple convex quadratic.}
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.47\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{assets/simple_regression_batched-1024.png}
                    \caption{$N = 1024$, simple convex quadratic. }
                \end{subfigure}
                \caption{Simple convex quadratic experiments results for V-FISTA, M-FISTA, and R-WAPG. }
                \label{fig:simple-quadratic-NOG}
            \end{figure}
            \par
            Another quantity that maybe interesting other than $\delta_k$ would be the estimated value of $\mu$ during at each iteration $k$. 
            This $\mu$ parameter should converge to the true value. 
            One individual experiment is carried out for the R-WAPG algorithm and the value of $\mu$ at each iteration is being recorded as well. 
            Figure \ref{fig:simple-quadratic-r-wapg-mu-estimates} showcases the results. 
            The values oscillate and converges to the true $\mu$ value. 
            Observe that the iteration when the estimates are nearing the true value corresponds to the iteration when the algorithm plateau away from its initial fast descent. 
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.64\textwidth]{assets/simple_regression_loss_sc_estimates_1024.png}
                \caption{$N = 1024$, the $\mu$ estimates produced by Algorithm \ref{alg:free-rwapg} (R-WAPG) is recorded. }
                \label{fig:simple-quadratic-r-wapg-mu-estimates}
            \end{figure}

        \subsubsection{LASSO}
            This section present results of numerical experiment for solving the (least absolute shrinkage and selection operator) LASSO problem proposed by Tibshirani \cite{tibshirani_regression_1996}. 
            The problem of LASSO has smooth, nonsmooth additive and the problem is given by: 
            \begin{align*}
                \min_{x \in \RR^n}
                \left\lbrace
                    \frac{1}{2}\Vert Ax - b\Vert^2 + \lambda\Vert x\Vert_1
                \right\rbrace. 
            \end{align*}
            The smooth part is $f(x) =\frac{1}{2}\Vert Ax - b\Vert^2$ and the nonsmooth is $g(x) = \lambda\Vert x\Vert_1$. 
            The objective function is coersive and the exact minimum, or minimizers are unknown. 
            We perform numerical experiments using V-FISTA, M-FISTA and R-WAPG on this problem. 
            The parameters for setting up the problem now follow. 
            \begin{enumerate}
                \item $M, N$ are constants. 
                \item $A \in \RR^{M\times N}$ is a matrix of i.i.d random variable, taken from a standard normal distribution. 
                \item $L, \mu$, the Lipschitz constant and the strong convexity constant for the smooth part of the objective are not known prior, and it's estimated through $A$ by $\mu = 1/\Vert (A^TA)^{-1}\Vert$ and $L = \Vert A^TA\Vert$. 
                \item $x^+ = [1\; -1\; 1 \; \cdots ]^T \in \RR^N$, it's a vector with alternating $1, -1$ in it. 
                \item Given $x^+$, it has $b = Ax^+ \in \RR^M$. 
                \item Given $A$, estimations for $L,\mu$ are given by $L = \Vert A^TA\Vert$, $\mu = \Vert (A^TA)^{-1}\Vert^{-1}$. 
                \item $x_0\in \RR^N$ is the initial guess. Its elements are random i.i.d variable realized from the standard normal distribution. 
                \item $\epsilon > 0$ is the tolerance the controls the termination criteria for test algorithms. 
            \end{enumerate}
            Experiments were conducted using V-FISTA, M-FISTA and FR-WAPG with $(M, N) = (64, 256)$ and $(M, N) = (64, 128)$. 
            Matrix $A$ is fixed and the for all test algorithms and all repetitions. 
            The same experiment are repeated 30 times, but each time, we fix a different random initial condition $x_k$ for all test algorithms. 
            The aggregate statistics of $\delta_k$ are collected for all repetitions, and then grouped by the respective algorithm. 
            The results are showcased in Figure \ref{fig:batched-lasso}. 
            The bump on the curve is due to a subset of test instances of the 30 repetition where the algorithms take larger number of iterations to terminate. 
            \begin{figure}[H]
                \begin{subfigure}[b]{0.47\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{assets/lasso_batched_statistics_64-256.png}
                    \caption{LASSO experiment with $M = 64, N = 256$. Plots of minimum, maximum, and median $\delta_k$ with estimated $F^*$. }
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.47\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{assets/lasso_batched_statistics_64-128.png}
                    \caption{LASSO experiment with $M = 64, N = 128$. Plots of minimum, maximum, and median $\delta_k$ with estimated $F^*$. }
                \end{subfigure}
                \caption{LASSO experiments. }
                \label{fig:batched-lasso}
            \end{figure}
            \par
            Another quantity of interest is the estimates of $\mu$ on each iteration of the algorithm. 
            A single experiment were conducted and the estimates and $\delta_k$ are showcased in Figure \ref{fig:single-lass-mu-estimates}
            \begin{figure}[H]
                \begin{subfigure}[b]{0.47\textwidth}
                    \includegraphics[width=\textwidth]{assets/lasso_loss_256.png}
                    \caption{Single lasso experiment plot of $\delta_k$ with.  }
                \end{subfigure}
                \hfill
                \begin{subfigure}[b]{0.47\textwidth}
                    \includegraphics[width=\textwidth]{assets/lasso_sc_estimates_256.png}
                    \caption{The $\mu$ estimated by test algorithms for one LASSO experiment. }
                \end{subfigure}
                \caption{A single LASSO experiment results, with $M = 64, 256$. }
                \label{fig:single-lass-mu-estimates}
            \end{figure}
            For this specific experiment showed in the figure, the estimated value of $\mu, L$ which we feed into V-FISTA are $\mu = 7.432363627613958\times 10^{-18}$ and $L = 2321.737206983643$. 
            One of the most important feature is that the estimate $\mu$ doesn't converge to the true value, but it didn't affect the convergence of $\delta_k$. 

\section{Discussion}
    This paper provides the results of an upper bound (Proposition \ref{prop:wagp-convergence}) on the optimality gap $F(x_k) - F^*$ for smooth plus nonsmooth composite optimization problem with convex objectives where under the much weaker assumption for the momentum parameter specified by Definition \ref{def:rwapg-seq}. 
    The proposed R-WAPG unifies the convergence proof for major Euclidean variants of FISTA with strong convexity parameter $\mu \ge 0$. 
    No analogous convergence framework had been identified yet in the literatures that proves the convergences of several variants (with and without strong convexity parameter) with momentum sequences that doesn't follow the Nesterov's rule. 
    \par
    In addition, we proposed explorative numerical experiments for Free R-WAPG (Algorithm \ref{alg:free-rwapg}) showcasing a parameter free algorithm that doesn't use the idea of restarting to achieve competitive convergence rate. 
    Unfortunately a precise description of the convergence rate of Free R-WAPG remains a mystery. 
    Our convergence framework gives an upper bound using the sequence $(\alpha_k)_{k \ge 0} \in (\mu/L, 1)$, but it requires knowing what the sequence would be in advanced given the initial conditions and the objective function. 
    \par
    From a practical perspective, Algorithm \ref{alg:free-rwapg} shows competitive results of convergence of strongly convex function.  
    The algorithm behaves in way having characteristics of both V-FISTA and M-FISTA which is fascinating. 
    In addition, of being parameter free, the algorithm works well with auto-differentiation framework because it requires the function value at the same point where the gradient is evaluated at $y_k$. 
    We do note that in the case when function is not strongly convex, the function value converges without $\Vert \mathcal G_L(y_k)\Vert$ converging. 


    % We do not claim a strictly better performance of FR-WAPG (Algorithm \ref{alg:free-rwapg}) compared to well-established variants of APGs in literatures. 
    % Our numerical experiments are explorative and we hope to inspire. 
    % The presented theories (Proposition \ref{prop:stepwise-lyapunov}, \ref{prop:wagp-convergence}) are our best attempts at making sense of what is happening, and we did it directly with elementary algebra. 
    % In addition, we provided some helpful results on the equivalent representations of R-WAPG to illustrate the descriptive power of the R-WAPG framework. 
    % Nonetheless, two things are clear from the numerical experiments. 
    % FR-WAPG performs better than V-FISTA on the first hundreds of iterations, and it doesn't require prior knowledge on parameter $L, \mu$. 
    % \par
    % Our theories are somewhat relevant to FR-WAPG since it can partially describe an upper bound of the optimality gap with any sequence of $\alpha_k \in (0, 1)$. 
    % Unfortunately, it is difficult to know the sequence $(\alpha_k)_{k \ge 0}$ without running the algorithm. 
    % It would require knowledge about the trajectory taken for a specific initial guess, and the property of $D_f$, the Bregman Divergence of the smooth part. 
    % We feel that the breakthrough in understanding may not come from elementary results.  
    % \par 
    % From a practical point of view, the numerical experiments show that in the beginning of the algorithm, the estimate for $\mu$ can be overestimated.
    % This is partially explained by the remark of Proposition \ref{prop:stepwise-lyapunov}. 
    % It also showed that the algorithm performs with mixed charactercteristics of M-FISTA and V-FISTA and it's just fascinating. 
    % Finally, we observed that the convergence of gradient mapping $\mathcal G_L(y_k)$ becomes more difficult and the algorithm may not be able to terminate for certain objectives. 


            
            

            

\bibliographystyle{siam}
\bibliography{references/R-WAPG.bib}

\appendix

\end{document}
