
@book{nesterov_lectures_2018,
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	volume = {137},
	isbn = {978-3-319-91577-7 978-3-319-91578-4},
	url = {http://link.springer.com/10.1007/978-3-319-91578-4},
	urldate = {2023-10-11},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	keywords = {Algorithmic Complexity, Fast Gradient Methods, Interior-Point Methods, Numerical Optimization, Optimization, Optimization in Relative Scale, Self-Concordant Functions, Smoothing Technique},
	file = {Nesterov - 2018 - Lectures on Convex Optimization.pdf:C\:\\Users\\alto\\Zotero\\storage\\HSCCPYL9\\Nesterov - 2018 - Lectures on Convex Optimization.pdf:application/pdf},
}

@book{beck_first-order_2017,
	series = {{MOS}-{SIAM} {Series} in {Optimization}},
	title = {First-order {Methods} in {Optimization}},
	isbn = {978-1-61197-498-0},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974997},
	language = {en},
	urldate = {2023-10-19},
	publisher = {SIAM},
	author = {Beck, Amir},
	year = {2017},
	keywords = {First-order Methods, Non-smooth Optimization, Numerical Optimization, Optimization},
	file = {First-Order Methods in Optimization  SIAM Publication.pdf:C\:\\Users\\alto\\Zotero\\storage\\P2HFAVVQ\\First-Order Methods in Optimization  SIAM Publication.pdf:application/pdf;Snapshot:C\:\\Users\\alto\\Zotero\\storage\\88BHKZ6Y\\1.html:text/html},
}

@misc{li_relaxed_2025,
	title = {Relaxed {Weak} {Accelerated} {Proximal} {Gradient} {Method}: a {Unified} {Framework} for {Nesterov}'s {Accelerations}},
	shorttitle = {Relaxed {Weak} {Accelerated} {Proximal} {Gradient} {Method}},
	url = {http://arxiv.org/abs/2504.06568},
	doi = {10.48550/arXiv.2504.06568},
	abstract = {This paper is devoted to the study of accelerated proximal gradient methods where the sequence that controls the momentum term doesn't follow Nesterov's rule. We propose a relaxed weak accelerated proximal gradient (R-WAPG) method, a generic algorithm that unifies the convergence results for strongly convex and convex problems where the extrapolation constant is characterized by a sequence that is much weaker than Nesterov's rule. Our R-WAPG provides a unified framework for several notable Euclidean variants of FISTA and verifies their convergences. In addition, we provide the convergence rate of strongly convex objective with a constant momentum term. Without using the idea of restarting, we also reformulate R-WAPG as ``Free R-WAPG" so that it doesn't require any parameter. Explorative numerical experiments were conducted to show its competitive advantages.},
	urldate = {2025-06-30},
	publisher = {arXiv},
	author = {Li, Hongda and Wang, Xianfu},
	month = apr,
	year = {2025},
	note = {arXiv:2504.06568 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {Full Text PDF:C\:\\Users\\alto\\Zotero\\storage\\G4MCTW2P\\Li and Wang - 2025 - Relaxed Weak Accelerated Proximal Gradient Method a Unified Framework for Nesterov's Accelerations.pdf:application/pdf;Snapshot:C\:\\Users\\alto\\Zotero\\storage\\KWNBJLDR\\2504.html:text/html},
}
