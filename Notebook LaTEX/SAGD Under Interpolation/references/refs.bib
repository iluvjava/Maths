
@article{calatroni_backtracking_2019,
	title = {Backtracking strategies for accelerated descent methods with smooth composite objectives},
	volume = {29},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/17M1149390},
	doi = {10.1137/17M1149390},
	abstract = {Motivated by big data applications, first-order methods have been extremely popular in recent years. However, naive gradient methods generally converge slowly. Hence, much effort has been made to accelerate various first-order methods. This paper proposes two accelerated methods towards solving structured linearly constrained convex programming, for which we assume composite convex objective that is the sum of a differentiable function and a possibly nondifferentiable one. The first method is the accelerated linearized augmented Lagrangian method (LALM). At each update to the primal variable, it allows linearization to the differentiable function and also the augmented term, and thus it enables easy subproblems. Assuming merely convexity, we show that LALM owns \$O(1/t)\$ convergence if parameters are kept fixed during all the iterations and can be accelerated to  \$O(1/t{\textasciicircum}2)\$ if the parameters are adapted, where \$t\$ is the number of total iterations. The second method is the accelerated linearized alternating direction method of multipliers (LADMM). In addition to the composite convexity, it further assumes two-block structure on the objective. Different from classic alternating direction method of multipliers, our method allows linearization to the objective and also augmented term to make the update simple. Assuming strong convexity on one block variable, we show that LADMM also enjoys \$O(1/t{\textasciicircum}2)\$ convergence with adaptive parameters.  This result is a significant improvement over that in [Goldstein et. al, SIAM J. Imag. Sci., 7 (2014), pp. 1588--1623], which requires strong convexity on both block variables and no linearization to the objective or augmented term. Numerical experiments are performed on quadratic programming, image denoising, and support vector machine. The proposed accelerated methods are compared to nonaccelerated ones and also existing accelerated methods. The results demonstrate the validity of acceleration and superior performance of the proposed methods over existing ones.},
	number = {3},
	urldate = {2023-10-19},
	journal = {SIAM Journal on Optimization},
	author = {Calatroni, Luca and Chambolle, Antonin},
	month = jan,
	year = {2019},
	pages = {1772--1798},
	file = {Backtracking strategies for accelerated descent methods with smooth composite objectives, SIAM:/Volumes/T6/Zotero Library/storage/RKZLQL7H/17m1149390.pdf:application/pdf},
}

@book{bauschke_convex_2017,
	address = {Cham},
	series = {{CMS} {Books} in {Mathematics}},
	title = {Convex {Analysis} and {Monotone} {Operator} {Theory} in {Hilbert} {Spaces}},
	isbn = {978-3-319-48310-8 978-3-319-48311-5},
	url = {https://link.springer.com/10.1007/978-3-319-48311-5},
	language = {en},
	urldate = {2023-11-29},
	publisher = {Springer International Publishing},
	author = {Bauschke, Heinz H. and Combettes, Patrick L.},
	year = {2017},
	keywords = {Convex analysis, Monotone Operator, Nonexpansive Operator, Operator Splitting, Proximal Algorithm},
	file = {Bauschke and Combettes - 2017 - Convex Analysis and Monotone Operator Theory in Hi.pdf:/Volumes/T6/Zotero Library/storage/57IWCTZJ/Bauschke and Combettes - 2017 - Convex Analysis and Monotone Operator Theory in Hi.pdf:application/pdf},
}

@book{nesterov_lectures_2018,
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	volume = {137},
	isbn = {978-3-319-91577-7 978-3-319-91578-4},
	url = {http://link.springer.com/10.1007/978-3-319-91578-4},
	urldate = {2023-10-11},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	keywords = {Optimization, Fast Gradient Methods, Algorithmic Complexity, Interior-Point Methods, Numerical Optimization, Optimization in Relative Scale, Self-Concordant Functions, Smoothing Technique},
	file = {Nesterov - 2018 - Lectures on Convex Optimization.pdf:/Volumes/T6/Zotero Library/storage/HSCCPYL9/Nesterov - 2018 - Lectures on Convex Optimization.pdf:application/pdf},
}

@book{beck_first-order_2017,
	series = {{MOS}-{SIAM} {Series} in {Optimization}},
	title = {First-order {Methods} in {Optimization}},
	isbn = {978-1-61197-498-0},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974997},
	language = {en},
	urldate = {2023-10-19},
	publisher = {SIAM},
	author = {Beck, Amir},
	year = {2017},
	keywords = {Optimization, First-order Methods, Numerical Optimization, Non-smooth Optimization},
	file = {First-Order Methods in Optimization  SIAM Publication.pdf:/Volumes/T6/Zotero Library/storage/P2HFAVVQ/First-Order Methods in Optimization  SIAM Publication.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/88BHKZ6Y/1.html:text/html},
}

@misc{li_relaxed_2025,
	title = {Relaxed {Weak} {Accelerated} {Proximal} {Gradient} {Method}: a {Unified} {Framework} for {Nesterov}'s {Accelerations}},
	shorttitle = {Relaxed {Weak} {Accelerated} {Proximal} {Gradient} {Method}},
	url = {http://arxiv.org/abs/2504.06568},
	doi = {10.48550/arXiv.2504.06568},
	abstract = {This paper is devoted to the study of accelerated proximal gradient methods where the sequence that controls the momentum term doesn't follow Nesterov's rule. We propose a relaxed weak accelerated proximal gradient (R-WAPG) method, a generic algorithm that unifies the convergence results for strongly convex and convex problems where the extrapolation constant is characterized by a sequence that is much weaker than Nesterov's rule. Our R-WAPG provides a unified framework for several notable Euclidean variants of FISTA and verifies their convergences. In addition, we provide the convergence rate of strongly convex objective with a constant momentum term. Without using the idea of restarting, we also reformulate R-WAPG as ``Free R-WAPG" so that it doesn't require any parameter. Explorative numerical experiments were conducted to show its competitive advantages.},
	urldate = {2025-06-30},
	publisher = {arXiv},
	author = {Li, Hongda and Wang, Xianfu},
	month = apr,
	year = {2025},
	note = {arXiv:2504.06568 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {Full Text PDF:/Volumes/T6/Zotero Library/storage/G4MCTW2P/Li and Wang - 2025 - Relaxed Weak Accelerated Proximal Gradient Method a Unified Framework for Nesterov's Accelerations.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/KWNBJLDR/2504.html:text/html},
}
