\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% special operators and stuff introduced for this file. 

\newcommand{\cov}{\ensuremath{\operatorname{Cov}}}
\newcommand{\var}{\ensuremath{\operatorname{Var}}}
\newcommand{\expect}{\ensuremath{\mathbb E}}
\newcommand{\prob}{\ensuremath{\mathbb P}}
\newcommand{\dist}{\ensuremath{\operatorname{dist}}}
\usepackage{tikz}\newcommand*\circled[1]{
    \text{
        \hspace{-0.5em}\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.6pt](char){#1};
        }\hspace{-0.5em}
    }
}

\begin{document}


\title{{\fontfamily{ptm}\selectfont Linear Convergence of Stochastic Nesterov's Accelerated Proximal Gradient method under Interpolation Hypothesis, Truth or Just a Dream?}}

\author{
    Author
    \thanks{
        University of British Columbia Okanagan,
        Canada. E-mail: \texttt{alto@mail.ubc.ca}.
    }
}

\date{\today}

\maketitle
\todoinline{This paper is currently in draft mode. Check source to change options. }
% \vskip 8mm

\begin{abstract} 
    This file is for communication purposes between collaborators. 
    In brief, we think that the conditions required for linear convergence rate of Stochastic Nesterov's accelerated gradient (or proximal gradient) is too precarious to hold, even with interpolation hypothesis. 
    Instead of attacking the problem head on, this file will characterize the conditions required for linear convergence rate. 
    We place specific constraints on the random variable representing the error made when estimating gradient via some random variables. 
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}


% ==============================================================================
\section{Introduction}
    \cite{bauschke_convex_2017}
    Previously we got some results, but unfortunately it was incorrect and, it was impossible to recover from the mistake. 
    \par
    Does stochastic accelerated Nesterov's acceleration (SNAG) produces accelerated convergence rate (or, any type of convergence) when the Interpolation Hypothesis is true? 
    \textbf{I don't think that it's true after some mistakes from previous version of the notes and careful investigations.}
    In this file we develop some sufficient conditions for Linear convergence of (SNAG). 
    We will give explanations on why we don't think this is necessarily true. 
    \par
    When we use stochastic gradient to approximate the true graduate, it has an error. 
    Fix some $x\in \RR^n$, let $\tilde \nabla f(x)$ be an estimate of $\nabla f(x)$, the error we consider is $\expect \Vert \nabla f(x) - \nabla f(x)\Vert$. 
    To make the algebra simpler, we assume that the algorithm produced the next iterates $\tilde x$ by a step of gradient descent, and the error of the expectation satisfies a relative error conditions of the form 
    \begin{align*}
        \frac{
            \expect \left[
                \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
            \right]
            }{
            \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
            \right]
            } &= \epsilon. 
    \end{align*}
    Where the variable $z$ will be explained later. 
    We will show that, the value of $\epsilon$ must decreases at a rate convergence relative to the Nesterov's accelerated sequence, under the standard Framework of analysis similar to what is in the literature. 
    Take note that usually in the literature, people analyze the quantity $\expect\left\Vert \tilde \nabla f(x) - \tilde \nabla f(y)\right\Vert^2$ for stochastic gradient type  of method. 
    The above expression is drastically different from what we usually have in the literature. 

\section{In preparations}
    Unless specifically specified in the context, we use the following notations. 
    $\Pi_C$ denotes the projection onto a set $C$. 
    Let $A \in \RR^{m \times n}$ be a matrix. 
    $\sigma_{\min}(A)$ denotes the smallest non-zero absolute value of all singular values of $A$. 
    Let $\Vert A\Vert$ denotes the spectral norm of the matrix $A$. 
    $I$ denotes the identity operator. 
    \par
    When two expressions are connected via non-trivial results, it's expressed with $\underset{(\cdot)}{=}, \underset{(\cdot)}{\ge}$ where $(\cdot)$ is a label of some intermediate results immediately before it, or explained right after a chain of expressions. 
    If the label is letter, like: (a), (b), ..., then they are stated in advanced at the start of the proof and, they are usually non-trivial results. 
    These labels are reused in every proof. 
    If the label is circled numbers, like: \circled{1}, \circled{2},... they are explained right after the chain of relations, and they are often reused right after their explanations. 
    \subsection{Basic definitions}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Proximal gradient operator]\label{def:pg-opt}
            Suppose $F = f + g$ with $\reli(\dom f) \cap \reli(\dom g) \neq \emptyset$, and $f$ is a differentiable function. 
            Let $\beta > 0$. 
            Then, we define the proximal gradient operator $T_{\beta}$ as 
            \begin{align*}
                T_\beta (x | F) &= \argmin_{z} \left\lbrace
                    g(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
                \right\rbrace. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If the function $g \equiv 0$, then it yields the gradient descent operator $T_\beta(x) = x - \beta^{-1}\nabla f(x)$. 
            In the context where it's clear what the function $F = f + g$ is, we simply write $T_\beta(x)$ for short. 
            Note, it also has $T_\beta(x | f + g) = \hprox_{\beta^{-1}g}(x - \beta^{-1}\nabla f(x))$ in optimization literatures. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Bregman Divergence]
            Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
            Then, for all the Bregman divergence $D_f: \RR^n \times \dom\nabla f \rightarrow \RR$ is defined as: 
            \begin{align*}
                D_f(x, y) := f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If, $f$ is $\mu \ge 0$ strongly convex and $L$ Lipschitz smooth then, its Bregman Divergence has for all $x, y \in \RR^n$: $\mu/2 \Vert x - y\Vert^2 \le D_f(x, y) \le L/2 \Vert x - y\Vert^2$. 
            We note that usually the Bregman Divergence is used with a Legendre function, but in here, we do not assume that $f$ has to be Legendre. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Lipschitz smoothness and strongly convex]\label{def:lip-smooth-and-scnvx}
            A differentiable function $f: \RR^n \rightarrow \RR$ is $L$ lipschitz smooth and, $\mu$ strong convex for some $L > \mu \ge 0$ if and only if for all $x, y \in \RR^n$ it satisfies the inequality 
            \begin{align*}
                \frac{\mu}{2}\Vert x - y\Vert^2 \le D_f(x, y) \le \frac{L}{2}\Vert x - y\Vert^2. 
            \end{align*}
        \end{definition}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Relative proximal gradient error ruler]\label{def:pg-err-ruler}
            Let $F$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
            Fix any $x \in \RR^n$, suppose that $\tilde x$ is an estimated of $T_B(x|F)$. 
            Then the relative proximal gradient error is a set defined as
            \begin{align*}
                S_B(\tilde x, x | F) := 
                \partial \left[
                    z \mapsto \partial g(x) + \langle \nabla f(x), z - x\rangle + 
                    \frac{B}{2}\Vert x - z\Vert^2
                \right](\tilde x). 
            \end{align*}
        \end{definition}
        \begin{remark}
            The definition exists to simplifies the notations for the discussions. 
            When $B > 0$ by strong convexity, the set $\{z : \mathbf 0 \in S_B(z, x | F)\}$ is a singleton, conveniently. 
        \end{remark}
    \subsection{Important inequalities}
        \begin{assumption}\label{ass:smooth-plus-nonsmooth}
            Suppose that $F = f + g$ where $f, g$ are both convex, proper and closed. 
            In addition, assume $f$ is $L > \mu \ge 0$ Lipschitz smooth and strongly convex satisfying Definition \ref{def:lip-smooth-and-scnvx}. 
        \end{assumption}
        % --------------------------------------------------------------------------------------------------------------
        \begin{theorem}[Jensen's inequality]\label{thm:jesen}
            Let $F: \RR^n \rightarrow \overline \RR$ be a $\mu \ge 0$ strongly convex function. 
            Then, it is equivalent to the following condition. 
            For all $x, y \in \RR^n$, $\lambda \in (0, 1)$ it satisfies the inequality 
            \begin{align*}
                (\forall \lambda \in [0, 1])\; 
                F(\lambda x + (1 - \lambda)y) \le \lambda F(x) + (1 - \lambda)F(y) -\frac{\mu\lambda(1 - \lambda)}{2} \Vert y - x\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{remark}
            If $x, y$ is out of $\dom F$, the inequality still work by convexity. 
        \end{remark}
        \begin{lemma}[inexact proximal gradient inequality prototype]\label{lemma:inex-pg-ineq-proto}
            Let $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
            Fix any $x \in \RR^n$ there exists a $B \ge 0$,  $\tilde x$ be an estimate of $T_B(x | F)$ such that $D_f(\tilde x, x) \le B/2\Vert x - \tilde x\Vert^2$. 
            Let $S_B(\tilde x, x | F)$ be given by Definition \ref{def:pg-err-ruler}. 
            Then, for all $z \in \RR^n$ and, any $w \in S_B(\tilde x, x | F)$ it satisfies: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - \langle w, z - \tilde x \rangle. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Since $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}, for all $B \ge L$, it will be an obvious choice. 
            The proof is direct algebra. 
            Let $h = z \mapsto g(z) + \langle \nabla f(x), z - x\rangle + B/2\Vert z - x\Vert^2$. 
            $h$ is a $B$ strongly convex function, using the subgradient inequality of a strongly convex function it has for all $z \in \RR^n$: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2
                &\le h(z) - h(\tilde x) - \langle w, z - \tilde x\rangle
                \\
                &= 
                \left(
                    g(z) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right)
                \\ &\quad 
                    - 
                    \left(
                        g(\tilde x) + \langle \nabla f(x), \tilde x - x\rangle + \frac{B}{2}\Vert \tilde x - x\Vert^2
                    \right)
                    - \langle w, z - \tilde x\rangle
                \\
                &= \left(
                    g(z) + f(z) - f(z) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right)
                \\ &\quad 
                    - 
                    \left(
                        g(\tilde x) + f(\tilde x) - f(\tilde x) 
                        + \langle \nabla f(x), \tilde x - x\rangle + \frac{B}{2}\Vert \tilde x - x\Vert^2
                    \right)
                    - \langle w, z - \tilde x\rangle
                \\
                &= \left(
                    F(z) - D_f(z, x) + \frac{B}{2}\Vert z - x\Vert^2
                \right)
                \\ &\quad 
                    - 
                    \left(
                        F(\tilde x) - D_f(\tilde x, x) + \frac{B}{2}\Vert \tilde x - x\Vert^2
                    \right)
                    - \langle w, z - \tilde x\rangle
                \\
                &\underset{\text{\circled{1}}}{\le} F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - 0 - \langle w, z - \tilde x \rangle
            \end{align*}
            At \circled{1}, we used the fact that $f$ is $L > \mu \ge 0$ Lipschitz smooth and strongly convex therefore it has for all $y \in \RR^n$: 
            \begin{align*}
                0 
                \le \frac{L}{2}\Vert z - y\Vert^2  - D_f(z, y)
                \le \frac{L - \mu}{2}\Vert z - y\Vert^2. 
            \end{align*}
        \end{proof}
        % --------------------------------------------------------------------------------------------------------------
        \begin{lemma}[inexact proximal gradient inequality]\label{lemma:inex-pg-ineq}
            Let $F = f + g$ satisfies Definition \ref{def:lip-smooth-and-scnvx} with $L > \mu \ge 0$. 
            Fix arbitrary $x \in \RR^n$. 
            Assume the followings: 
            \begin{enumerate}[nosep]
                \item $\tilde x$ estimates $T_B(x | F)$. 
                \item $B \ge 0$ satisfies $D_f(\tilde x, x) \le B/2\Vert x - \tilde x\Vert^2$. 
                \item Let $S_B(\tilde x, x | F)$ be given by Definition \ref{def:pg-err-ruler}. 
            \end{enumerate}
            Let $\epsilon \ge 0$ be such that $\Vert x - \tilde x\Vert\epsilon \ge \dist(\mathbf 0 |S_B(\tilde x, x | F))$.
            Then, for all $z\in \RR^n$ it satisfies the inequality: 
            \begin{align*}
                0 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\Vert z - \tilde x\Vert^2
                + \frac{\epsilon}{2}\Vert x - \tilde x \Vert^2. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            The error $w$ satisfies Lemma \ref{lemma:inex-pg-ineq-proto} hence, it has for all $z \in \RR^n$ the inequality: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2
                &\le F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - 0 - \langle w, z - \tilde x \rangle
                \\
                &\underset{\text{\circled{1}}}{\le}
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \Vert w\Vert \Vert z - \tilde x\Vert. 
            \end{align*}
            At \circled{1}, we used Cauchy inequality. 
            Since this is true for all $w \in S_B(\tilde x, x | F)$, it has: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2
                &\le F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \dist(\mathbf 0 | S_B(\tilde x, x | F))\Vert z - \tilde x\Vert
                \\
                &\le
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 + \epsilon \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert. 
            \end{align*}
            Continuing it has 
            \begin{align*}
                0 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                + \epsilon \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert 
                - \frac{B}{2}\Vert z - \tilde x\Vert^2
                    \\&\quad 
                    - \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 - \frac{\epsilon}{2}\Vert z - \tilde x \Vert^2
                    + \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 + \frac{\epsilon}{2}\Vert z - \tilde x \Vert^2
                \\
                &= F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                - \frac{1}{2}\left(
                    \sqrt{\epsilon}\Vert z - \tilde x\Vert - \sqrt{\epsilon}\Vert x - \tilde x\Vert
                \right)^2
                - \frac{B}{2}\Vert z - \tilde x\Vert^2
                    \\
                    &\quad 
                    + \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 + \frac{\epsilon}{2}\Vert z - \tilde x \Vert^2
                \\
                &= F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\Vert z - \tilde x\Vert^2
                + \frac{\epsilon}{2}\Vert x - \tilde x \Vert^2. 
            \end{align*}
        \end{proof}
        \begin{remark}
            Usually in practice, the precise value of $F(\tilde x)$ is never known and, function value is also a random variable, therefore, $B$ cannot be easily determined via $D_f(\tilde x, x)$. 
            In that case we can only choose some $B \ge L$ which gives: 
            \begin{align*}
                0 &\le 
                F(z) - F(\tilde x) + \frac{L - \mu}{2}\Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\Vert z - \tilde x\Vert^2
                + \frac{\epsilon}{2}\Vert x - \tilde x \Vert^2.
            \end{align*}
            The smallest possible choice for $\epsilon$ when $x \neq \tilde x$ is $\epsilon = \Vert w\Vert/\Vert x - \tilde x\Vert$ and, if $x = \tilde x$ then $\epsilon = 0$ is the smallest. 
        \end{remark}
        Note, an inexact evaluation of the proximal gradient operator can be caused by an inexact gradient on the smooth part. 
        Suppose that one take $\tilde \nabla f(x)$ to be an estimate of $\nabla f(x)$ and use it for the proximal gradient operator to produce $\tilde x$, then: 
        \begin{align}
            \mathbf 0 
            &\in \partial g(\tilde x) + \tilde \nabla f(x) + B(\tilde x - x)
            \\
            &= 
            \partial g(\tilde x) + \tilde\nabla f(x) - \nabla f(x) 
            + \nabla f(x) + B(\tilde x - x)
            \\
            \iff &
            \nabla f(x) - \tilde \nabla f(x) \in 
            \partial g(\tilde x) 
            + \nabla f(x) + B(\tilde x - x).\label{eqn:stoch-grad-err-vec}
        \end{align}
        In this case, it adds the interpretation that $w = \nabla f(x) - \tilde \nabla f(x)$. 
        It fully characterizes the error made to estimate the true gradient $\nabla f(x)$. 
        In that case, we have the equation: 
        \begin{align*}
            \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert x - \tilde x\Vert
            = \epsilon \Vert x - \tilde x\Vert^2. 
        \end{align*}
        It's very unclear what LHS really is without additional details and assumptions. 
        \textbf{We very much would like $\epsilon$ to be a constant to make the algebra possible when deriving the convergence rate of the algorithm. }
        \par
        The following lemma gives a proximal gradient inequality when $\tilde \nabla f(x)$ is an estimate by some random variable, \textbf{and it is the precursor.} 
        \begin{lemma}[proximal stochastic gradient inequality]\label{lemma:pg-stoch-ineq}
            Let $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
            Fix any $x, z \in \RR^n$. 
            Assume the following
            \begin{enumerate}[nosep]
                \item $\tilde \nabla f(x)$ is a random variable which estimates $\nabla f(x)$, which produces the estimate $\tilde x$. 
                \item There exists $B \ge 0$ such that $D_f(\tilde x,x)\le B/2 \Vert x - \tilde x\Vert^2$. 
            \end{enumerate}
            If in addition, there exists some $\epsilon \ge 0$: 
            \begin{align*}
                \expect \left[
                    \left\Vert \nabla f(x) - \tilde \nabla f(x)\right\Vert 
                    \Vert z - \tilde x\Vert
                \right] &\le 
                \epsilon \expect \left[
                    \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
                \right]. 
            \end{align*}
            Then it has: 
            \begin{align*}
                0 &\le 
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2} \Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\expect\left[\Vert z - \tilde x\Vert^2\right]
                + \frac{\epsilon}{2} \expect\left[\Vert x - \tilde x\Vert^2\right]. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            The can choose $w = \nabla f(x) - \tilde \nabla f(x) \in S_B(\tilde x, x | F)$ which is explained in \eqref{eqn:stoch-grad-err-vec}. 
            Using Lemma \ref{lemma:inex-pg-ineq-proto}, for any fixed $z$ it has: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - \langle w, z - \tilde x\rangle
                \\
                &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 + \Vert w\Vert\Vert z - \tilde x\Vert
                \\
                &= 
                F(z) - F(\tilde x) 
                + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \left\Vert \nabla f(x) - \tilde \nabla f(x)\right\Vert\Vert z - \tilde x\Vert. 
            \end{align*}
            Take note that, since $w = \nabla f(x) - \tilde \nabla f(x)$ is a random variable, it determines that $\tilde x$ is also a random variable related to $w$. 
            Here, $x, z$ is not a random variable. 
            We take the expectation on both sides and move things all to the RHS then it has 
            \begin{align*}
                0&\le 
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \expect\left[
                    \Vert w\Vert\Vert z - \tilde x\Vert
                \right]
                - \frac{B}{2}\expect \Vert z - \tilde x\Vert^2
                \\
                &\le F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \epsilon \expect \left[
                    \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
                \right]
                - \frac{B}{2}\expect \Vert z - \tilde x\Vert^2
                \\
                &= 
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \expect\left[
                    \epsilon\Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
                    - \frac{B}{2} \Vert z - \tilde x\Vert^2
                \right]
                \\
                &=
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                    \\ &\quad 
                    + \expect\left[
                        - \frac{1}{2}\left(
                            \sqrt{\epsilon}\Vert x - \tilde x\Vert - \sqrt{\epsilon}\Vert z - \tilde x\Vert
                        \right)^2
                        + \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 + \frac{\epsilon}{2}\Vert z - \tilde x\Vert^2
                        - \frac{B}{2} \Vert z - \tilde x\Vert^2
                    \right]
                \\
                &\le 
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \expect\left[
                    \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2
                    - \frac{B - \epsilon}{2} \Vert z - \tilde x\Vert^2
                \right]. 
            \end{align*}
        \end{proof}
        In practice, is chosen in prior to satisfies $B \ge L$. 
        In here, $z, x$ is not a random variable, $\epsilon$ just a constant, but it's determined by $z$ and $x$. 
        Assuming $z \neq \tilde x$ and, $\tilde x \neq x$, then one of the smallest choice for it in this lemma is
        \begin{align*}
            \frac{
                \expect \left[
                    \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
                \right]
            }{
            \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
            \right]
            } &= \epsilon. 
        \end{align*}
        It depends on both $z$ and $x$. 
        Let's think about the edge case. 
        When $\expect [\Vert x - \tilde x\Vert\Vert z - \tilde x\Vert] = 0$, it must be that both $\Vert x - \tilde x\Vert,\Vert z - \tilde x\Vert$ are zero, indicating that $\nabla f(x) = \tilde \nabla f(x)$ and, $x = \tilde x = z$. 
        Otherwise, we can assume $\prob(\Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0) > 0$ and, the expectation has: 
        \begin{align*}
            & \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
            \right] 
            \\&= 
            \expect\left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert \;|\; 
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
            \right]\prob(
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
            ). 
        \end{align*}
        Let's suppose that $\tilde \nabla f(x)$ is a random variable comes from the space: $\Omega(x)$, then it has the following: 
        {\small
        \begin{align*}
            & \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert \; | \; 
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
            \right] 
            \\
            &\ge
            \min_{y \in \Omega(x)}\left\lbrace
                \Vert x - \tilde x\Vert
                :x\neq \tilde x = \argmin_{z}\left\lbrace
                    g(z) + \langle y, z\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right\rbrace
            \right\rbrace
            \prob(
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
            )
            \expect\Vert z - \tilde x\Vert, 
            \\
            & \expect \left[
                \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
            \right]
            \\
            &\le 
            \max_{y \in \Omega(x)}\left\lbrace
                \Vert \nabla f(x) - y\Vert 
            \right\rbrace\expect\Vert z - \tilde x \Vert, 
        \end{align*}
        }
        And it would mean: 
        {\small
        \begin{align*}
            \epsilon &= 
            \frac{
                \expect \left[
                    \left\Vert  \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
                \right]
            }{
            \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
            \right]
            }\\
            &\le \frac{
                \max_{y \in \Omega(x)}\Vert \nabla f(x) - y\Vert 
            }{
                \min_{y \in \Omega(x)}\left\lbrace
                    \Vert x - \tilde x\Vert:
                    x\neq \tilde x = \argmin_{z}\left\lbrace
                    g(z) + \langle y, z\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right\rbrace
                \right\rbrace
                \prob(
                    \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
                )
            }. 
        \end{align*}
        }
        \par
        Of course, look, if there is no random variable and $\tilde \nabla f(x)$ is simply not a probabilistic estimate then the expectation is gone and $x\neq \tilde x$, it has: 
        \begin{align*}
            \epsilon = \frac{\left\Vert
                \nabla f(x) - \tilde \nabla f(x)
            \right\Vert}{\Vert x - \tilde x\Vert}.
        \end{align*}
        And in this case, it has nothing to do with $z$. 


\section{Stochastic/Inexact accelerated proximal gradient algorithm}
    The following defines the inexact proximal gradient operator where the gradient of the smooth part of the function is estimated. 
    All algorithms satisfying the following definition will be referred to as Stochastic Nesterov's Accelerated Gradient (SNAG). 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[proximal inexact gradient operator with relative error]
        Let $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}, let $x \in \RR^n$ be fixed. 
        Suppose that $\tilde \nabla f(x)$ estimates $\nabla f(x)$. 
        We define the inexact proximal gradient operator by the relationships between: 
        \begin{enumerate}[nosep]
            \item $\tilde x = \mathbf T_B(x | F)$ is an inexact output of proximal gradient operator by evaluating on $\tilde \nabla f(x)$. 
            \item $B \ge 0$ is any constant such that it satisfies $D_f(\tilde x, x) \le B/2\Vert \tilde x - x\Vert^2$. 
            \item $\epsilon \ge 0$ is a constant that quantifies the error of inexact evaluation. 
        \end{enumerate}
        Then, we define the relative error $\epsilon$ by: 
        \begin{align*}
            \epsilon = \begin{cases}
                \frac{\left\Vert \tilde \nabla f(x) - \nabla f(x)\right\Vert}{\Vert x - \tilde x\Vert} & \text{if } x \neq \tilde x, 
                \\
                \infty & \text{if } x = \tilde x, \nabla f(x) \neq \tilde \nabla f(x), 
                \\
                0 & \text{if } x = \tilde x, \nabla f(x) = \tilde \nabla f(x). 
            \end{cases}
        \end{align*}
        And the inexact output is defined as: 
        \begin{align*}
            \tilde x = \mathbf T_B(x | F) = \argmin_{z \in \RR^n}\left\lbrace
                g(z) + \left\langle \tilde \nabla f(x), z - x\right\rangle
                + \frac{B}{2}\Vert z - x\Vert^2
            \right\rbrace. 
        \end{align*}
    \end{definition}
    The inexact evaluation can be caused by a random variable. 
    The definition that follows characterize algorithm in which the errors are related to a random variable that estimates the gradient of the objective function. 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[stochastic proximal gradient operator with relative error]\label{def:stoch-pg-opt-rel-err}
        Let $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
        Let $x \in \RR^n$ be fixed. 
        Suppose that $\tilde \nabla f(x)$ is random variable and, it estimates $\nabla f(x)$. 
        Then stochastic proximal gradient operator are the relationships between
        \begin{enumerate}[nosep]
            \item $\tilde x= \mathbf{\widetilde T}_B(x| F)$, an inexact output of proximal gradient operator by evaluating on $\tilde \nabla f(x)$. 
            \item Any $B \ge 0$ such that it satisfies $D_f(\tilde x, x) \le B/2\Vert x - \tilde x\Vert^2$. 
            \item $\epsilon$ is an relative error, determined by $z, x, \tilde\nabla f(x)$ and, defined immediately below. 
        \end{enumerate}
        The relative error $\epsilon$ is defined as: 
        \begin{align*}
            \epsilon 
            &= 
            \begin{cases}
                \frac{
                \expect \left[
                    \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
                \right]
                }{\expect \left[
                    \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
                \right]} 
                & \text{if } \expect\left[\Vert x - \tilde x\Vert\Vert z - \tilde x\Vert\right] \neq 0, 
                \vspace{0.5em}
                \\
                \left.\begin{cases}
                    0 & \expect \left[\left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert\right] = 0, 
                    \\
                    \infty & \text{else. } 
                \end{cases}\right\rbrace
                & \text{else. }
            \end{cases}
        \end{align*}
        Then the inexact proximal gradient operator with relative error $\epsilon$ is the random variable defined as: 
        \begin{align*}
            \tilde x = \mathbf{\widetilde T}_B(x| F) = \argmin_{z \in \RR^n}\left\lbrace
                g(z) + \left\langle \tilde \nabla f(x), z - x\right\rangle
                + \frac{B}{2}\Vert z - x\Vert^2
            \right\rbrace. 
        \end{align*}
    \end{definition}
    \begin{remark}
        For this definition of the stochastic proximal gradient operator, Lemma \ref{lemma:pg-stoch-ineq} is applicable. 
    \end{remark}
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[inexact/stochastic SNAG]\label{def:SNAG}
        Suppose that $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
        Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_k \in (0, 1]$. 
        Let $(\epsilon_k)_{k \ge 0}$ be a sequence of errors. 
        Given initial conditions $v_{-1}, x_{- 1}$. 
        An algorithm satisfying the SNAG definition if it generates a sequence $(y_k, x_k, v_k)_{k \ge 0}$ if for all $k \ge 0$, the following conditions are satisfied: 
        \begin{align*}
            & \tau_k = L(1 - \alpha_k)\left(L \alpha_k - \mu\right)^{-1}, \\
            & y_k = (1 + \tau_k)^{-1}v_{k - 1} + \tau_k(1 + \tau_k)^{-1}x_{k - 1}, \\
            & x_k =  \mathbf T_{L}(y_k | F) \text{ or }, \mathbf{\widetilde T}_L(y_k | F)\\
            & v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}).
        \end{align*}
    \end{definition}
    The following definition gives a momentum sequence where it makes the derivation of the convergence rate easier. 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[relaxed momentum sequence]\label{def:relax-momen-seq}
        Let $(\alpha_k)_{k \ge 0}$ be non-negative sequence. 
        Let $L, \mu$ be some constant such that $L > \mu \ge 0$. 
        It is a relaxed momentum sequence if the following conditions are satisfied: 
        \begin{enumerate}[nosep]
            \item $\alpha_0 \in (0, 1]$ and for all $k \ge 1$, it satisfies that $\alpha_k \in (\mu/L, 1)$. 
        \end{enumerate}
    \end{definition}
    \begin{remark}
        In the context of its usage, the constants $L, \mu$ are the Lipschitz smoothness constant  and, strong convexity constant associated with a smooth and strongly convex function. 
    \end{remark}
    The following lemma defines the conditions required for the momentum sequence to be compatible with convergence claims. 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{lemma}[relaxed momentum sequence conditions]\label{lemma:seq-properties}
        Let $(\alpha_k)_{k \ge 0}$, $L, \mu$ to be two constant such that it has $L > \mu \ge 0$. 
        If we assume that the sequence has for all $k \ge 0: \alpha_k \ge \mu/L$, then we can define a positive sequence 
        \begin{align*}
            (\forall k \ge 1): \rho_{k - 1} = \frac{\alpha_k(\alpha_k - \mu/L)}{(1 - \alpha_k)\alpha_{k - 1}^2}. 
        \end{align*}
        And under this relationship, the followings are true inductively: 
        \begin{enumerate}[nosep]
            \item In general, for all $\rho_{k - 1} \ge 0$, it has $0\le \alpha_k \le \min\left(1, |\rho\alpha^2 - q| + \sqrt{\rho}\alpha\right)$. 
            \item If $\alpha_{k - 1} \ge \mu/L$, then $\alpha_k \ge \mu/L$ too. 
            \item If $\alpha_{k - 1} > \mu/L > 0$, then $\alpha_k \in (\mu/L, 1)$ for all $\rho_{k - 1} > 0$. 
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        \textbf{Proof of item (i), (iii)}. 
        With $\rho_{k - 1}(1 - \alpha_k)\alpha_{k - 1}^2 = \alpha_k(\alpha_k - \mu/L)$ it gives: 
        \begin{align*}
            \alpha_k 
            &= \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2
                + \sqrt{
                    \left(
                        \rho_{k - 1}\alpha_{k - 1}^2 - \frac{\mu}{L}
                    \right)^2 
                    + 4\rho_{k - 1}\alpha_{k - 1}^2
                }
            \right)
            > 0. 
        \end{align*}
        Focusing exclusively on the RHS, we omit the subscript and write $\alpha_{k - 1}, \rho_{k - 1}$ as $\alpha, \rho$. 
        We also just write $q = \mu / L$. 
        By definition, we have $L > \mu \ge 0$ hence, $q \in [0, 1)$.
        We also note that the quantity $(r \alpha^2 + q)^2 + 4\rho \alpha$ is clearly $> 0$. 
        We will show that there is an upper bound for the RHS in terms of $\rho, q$. 
        Completing the square should give
        \begin{align*}
            0 &\le 
            \left(
                \rho \alpha^2 - q
            \right)^2 + 4 \rho \alpha^2
            \\
            &= \rho^2\alpha^4 + q^2 - 2\rho\alpha^2q + 4 \rho \alpha^2
            \\
            &= \rho^2\alpha^4 + 2\rho(2 - q)\alpha^2 + q^2
            \\
            &= 
            \rho^2\alpha^4 + 2\rho(2 - q)\alpha^2 
            + \rho^2(2 - q)^2
            - \rho^2(2 - q)^2
            + q^2
            \\
            &= (\rho\alpha^2 + \rho(2 - q))^2 - \rho^2 (2 - q)^2 + q^2
            \\
            &= \rho^2(\alpha^2 + 2 - q)^2 
            + q^2 
            - \rho^2 (2 - q)^2
            \\
            &\le 
            \rho^2(\alpha^2 + 2 - q)^2 
            + \max(0, q^2 - \rho^2 (2 - q)^2). 
        \end{align*}
        The above is non-negative, taking the square root it has: 
        \begin{align*}
            \sqrt{\left(\rho \alpha^2 - q\right)^2 + 4 \rho \alpha^2} 
            &\le 
            \rho|\alpha^2 + 2 - q| 
            + \sqrt{\max(0, q^2 - \rho^2 (2 - q)^2)}
            \\
            &\le 
            \rho|\alpha^2 + 2 - q| 
        \end{align*}
        But look, if we directly apply the $\sqrt{a + b} \le \sqrt{a} + \sqrt{b}$ it has another upper bound which is: 
        \begin{align}\label{ineq:seq-properties-pr3}
            \sqrt{\left(\rho \alpha^2 - q\right)^2 + 4 \rho \alpha^2} 
            &\le 
            |\rho\alpha^2 - q| + 2\sqrt{\rho}\alpha. 
        \end{align}
        Both upper bounds apply and hence we have: 
        \begin{align*}
            \alpha_k 
            &\le
            \frac{1}{2}\left(
                q - \rho\alpha^2 + \min\left(
                    \rho|\alpha^2 + 2 - q|  , 
                    |\rho\alpha^2 - q| + 2\sqrt{\rho}\alpha
                \right)
            \right)
            \\
            &= 
            \frac{1}{2}\left(
                q - \rho\alpha^2 + \min\left(
                    \rho\alpha^2 + (2 - q)  , 
                    |\rho\alpha^2 - q| + 2\sqrt{\rho}\alpha
                \right)
            \right)
            \\
            &= 
            \frac{1}{2}\left(
                \min\left(
                    2, 
                    |\rho\alpha^2 - q| + 2\sqrt{\rho}\alpha + q - \rho\alpha^2
                \right)
            \right)
            \\
            &= 
            \min\left(
                    1, 
                    (1/2)|\rho\alpha^2 - q| + \sqrt{\rho}\alpha + (1/2)(q - \rho\alpha^2)
                \right)
            \\
            &\le \min\left(
                1, 
                |\rho\alpha^2 - q| + \sqrt{\rho}\alpha
            \right). 
        \end{align*}
        \par
        \textbf{Proof of (ii)}. 
        To see the lower bound, we assume that $\alpha_k \ge \mu/L$ , denote $\mu/L$ by $q$, we have the following: 
        \begin{align}\label{ineq:seq-properties-pr5}
            \alpha_k - q 
            \underset{\circled{1}}{\ge} \alpha_k(\alpha_k - q) 
            = \rho_{k - 1}(1 - \alpha_k)\alpha_{k - 1}^2
            \underset{\circled{2}}{\ge} \rho_{k - 1}(1 - \alpha_k)q^2 \ge 0. 
        \end{align}
        At \circled{1}, we used the fact that $\alpha_k \le 1$ and $\alpha_k \ge 0$ from previous results. 
        At \circled{2}, we used the assumption that $\alpha_{k - 1} \ge q \ge 0$. 
        The last inequality is true because is $\rho_{k - 1} \ge 0, \alpha_k \le 1$. 
        \par
        \textbf{Proof of (iii)}. 
        In addition, if $\alpha > \mu/L >  0$, then the inequality in \eqref{ineq:seq-properties-pr3} is strict, then it would instead make $\alpha_k < 1$. 
        It will also make the inequality chain in \eqref{ineq:seq-properties-pr5} strict. 
        Therefore, Item (iii) is true. 
    \end{proof}
    \subsection{Building up the convergence results}
        In this section we derive a generic convergence results. 
        \par
        The following lemma states two important relationships on the iterates generated by Definition \ref{def:SNAG}. 
        Take note that it's only related to the iterates generated: $x_k, y_k, v_k$, it involves the sequence $(\alpha_k)_{k \ge 0}$, but the sequence can be anything in between $(0, 1]$ and these relations won't change. 
        % --------------------------------------------------------------------------------------------------------------
        \begin{lemma}[SNAG identities]\label{lemma:snag-identities}
            The iterates $(y_k, x_k, v_k)_{k \ge0}$ satisfying Definition \ref{def:SNAG} satisfies for all $k \ge 1$ the identities: 
            \begin{enumerate}[nosep]
                \item $z_k - y_k = (L - \mu)^{-1}((L\alpha_k - \mu)(\bar x - v_{k - 1}) + \mu(1 - \alpha_k)(\bar x - x_{k - 1})).$
                \item $z_k - x_k = \alpha_k (\bar x - v_k).$
            \end{enumerate}
        \end{lemma}
        \begin{proof}
            We prove (i) first. 
            Recall the definitions of $\tau_k$ from Definition \ref{def:SNAG}, it has: 
            \begin{align*}
                (1 + \tau_k)^{-1}
                &=
                \left(
                    1 + \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)^{-1} = \left(
                    \frac{L\alpha_k - \mu + L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)^{-1}
                = \frac{L\alpha_k - \mu}{L - \mu}. 
            \end{align*}
            Therefore, for all $k \ge 0$, $y_k$ has 
            \begin{align*}
                0 &= (1 + \tau_k)^{-1} v_{k - 1} + \tau_k (1 + \tau_k)^{-1} x_{k - 1} - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} 
                \left(
                    v_{k - 1} + \frac{L(1 - \alpha_k)}{L\alpha_k - \mu} x_{k - 1}
                \right) - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1}
                + \frac{L(1 - \alpha_k)}{L - \mu} x_{k - 1} - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                + 
                \left(
                    \frac{L(1 - \alpha_k)}{L - \mu} - (1 - \alpha_k)
                \right) x_{k - 1} - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                + 
                (1 - \alpha_k)\left(
                    \frac{L - L + \mu}{L - \mu}
                \right) x_{k - 1} - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                + 
                \frac{\mu(1 - \alpha_k)}{L - \mu}x_{k - 1} - y_k. 
            \end{align*}
            Therefore, we establish the equality 
            \begin{align*}
                (1 - \alpha_k)x_{k - 1} - y_k &= 
                - \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} 
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}. 
            \end{align*}
            On the second equality below, we will the above equality, it goes: 
            \begin{align*}
                z_k - y_k &= 
                \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - y_k
                \\
                &= \alpha_k \bar x 
                - \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} 
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1})
                + \left(
                    \alpha_k - \frac{L\alpha_k - \mu}{L - \mu}
                \right)\bar x
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1})
                + \left(
                    \frac{\alpha_kL - \alpha_k \mu - L\alpha_k + \mu}{L - \mu}
                \right)\bar x
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1})
                + \frac{\mu(1 - \alpha_k)}{L - \mu}\bar x
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1})
                + \frac{\mu(1 - \alpha_k)}{L - \mu}(\bar x - x_{k - 1}).
            \end{align*}
            To see item (ii), the proof is direct algebra: 
            \begin{align*}
                z_k - x_k &= \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - x_k
                \\
                &= \alpha_k \bar x + x_{k - 1} - x_k - \alpha_k x_{k - 1}
                \\
                &= \alpha_k(\bar x - \alpha_k^{-1}(x_k - x_{k - 1}) - x_{k - 1})
                \\
                &= \alpha_k (\bar x - v_k).
            \end{align*}
        \end{proof}
        Remark the following definitions
        \begin{definition}[conditional expectations]
            Given probability space $(\Omega, \mathcal F_0, \prob)$ and, $\mathcal F \subseteq \mathcal F_0$, and a random variable $X \in \mathcal F$ such that $\expect |X| < \infty$. 
            We define the conditional expectation of $X$ given $\mathcal F$, $\expect [X| \mathcal F]$ to be any random variable $Y$ that has 
            \begin{enumerate}[nosep]
                \item $Y \in \mathcal F$, i.e: $Y$ is $\mathcal F$ measurable. 
                \item For all $A \in \mathcal F$, $\int_A X d\prob = \int_A Y d\prob$. 
            \end{enumerate}
        \end{definition}
        \begin{remark}
            \todoinline{CITATIONS HERE NEEDED. }
        \end{remark}
        The conditional expectation exists and it's unique. 
        The following theorem given an inequality characterizing a descent relation for the SNAG algorithm. 
        % --------------------------------------------------------------------------------------------------------------
        \begin{theorem}[SNAG descent lemma]\label{thm:snag-descent}
            Suppose the iterates sequence $(y_k, x_k, v_k)_{k\ge 0}$ are generated by algorithms satisfying Definition \ref{def:SNAG}. 
            Assume that
            \begin{enumerate}[nosep]
                \item it uses stochastic proximal gradient operator as in Definition \ref{def:stoch-pg-opt-rel-err}, 
                \item it is initialized with $v_{-1} = x_{-1}$, $\alpha_0 = 1$ and, the momentum sequence $(\alpha_k)_{k \ge 0}$ satisfies Definition \ref{def:relax-momen-seq}. 
            \end{enumerate}
            Define $\expect_{k}$ to be the expectation conditioned on $\tilde \nabla f(y_i)$ for $i = 1, 2, \ldots, k - 1$. 
            Then for all $k \ge 1$, for all $\bar x \in \RR^n$ it satisfies the inequality: 
            \begin{align*}
                & \expect_kF(x_k) - F(\bar x) 
                + \frac{\alpha_k^2(L - \epsilon_k)}{2}\expect_k \left[\Vert v_k - \bar x\Vert^2\right]
                \\
                &\le 
                (1 - \alpha_k)\left(
                    F(x_{k - 1}) - F(\bar x)
                    + \frac{\alpha_{k - 1}^2L\rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right) 
                + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right].  
            \end{align*}
            In the edge case of $k = 0$, it has: 
            \begin{align*}
                \expect_0 F(x_0) - F(\bar x)  
                + \frac{L - \epsilon}{2}\expect_0\left[\Vert \bar x - x_0\Vert^2\right]
                &\le 
                \frac{L - \mu}{2} \Vert \bar x - v_{-1}\Vert^2
                + \frac{\epsilon}{2}\expect_0\left[\Vert v_{-1} - x_0\Vert^2\right]. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            The following intermediate results will clear out some algebras, they are all proved by the end of the proof. 
            \begin{enumerate}
                \item [(a)] For all $k \ge 1$, it has $\frac{\mu^2(1 - \alpha_k)^2}{2(L - \mu)} - \frac{\mu\alpha_k(1 - \alpha_k)}{2} = \frac{(\alpha_k - 1)\mu\left(L\alpha_k - \mu\right)}{2\left(L - \mu\right)}$ using some algebra. 
                \item [(b)] We assumed that the sequence $(\alpha_k)_{k \ge 0}$ satisfies\\ for all $k \ge 1$: $\rho_{k - 1}(1 - \alpha_{k})\alpha_{k - 1}^2 = \alpha_{k}(\alpha_{k} - \mu/L)$. 
                \item [(c)] Using (b) and some algebra, we have for all $k \ge 1$ the identity: \\
                $\frac{(L\alpha_k - \mu)^2}{2(L - \mu)} - \frac{\alpha_{k - 1}^2 L \rho_{k - 1}(1 - \alpha_k)}{2} = \frac{(L\alpha_k - \mu)\mu(\alpha_k - 1)}{2(L - \mu)}$. 
                \item [(d)] Using (a), and (c), we can derive for all $k\ge 1$, we have the following identity: 
                \begin{align*}
                    & - \frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                    + \frac{L - \mu}{2}\Vert z_k - y_k\Vert^2
                    \\ &= 
                    \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert x_{k - 1} - v_{k - 1}\Vert^2. 
                \end{align*}
            \end{enumerate}
            Using intermediate results (a), (b), (c), (d), we can prove the claim in just a few steps. 
            For any fixed $\bar x \in \RR^n$. 
            Define $z_k = \alpha_k \bar x + (1 - \alpha_k)x_{k - 1}$ for all $k \ge 0$. 
            Consider the case for all $k \ge 1$. 
            Recall $\expect_{k}$ is the expectation conditioned on all $\tilde \nabla f(y_i)$ for $i = 0, 1, \ldots, k - 1$. 
            We note that under this conditioning the only random variable is $\tilde \nabla f(y_k)$, so iterates $x_{k - 1}, v_{k - 1}, y_k$ are not random variables, but $x_k$, and $v_k$ are. 
            The sequence $(\alpha_k)_{k \ge 0}, (\epsilon_k)_{k \ge 0}$ are also not random variables. 
            \par
            We use Lemma \ref{lemma:pg-stoch-ineq} with $x = y_k, z = z_k, \tilde x = x_{k}$ and, $B = L$ then it means: 
            {\allowdisplaybreaks
            \begin{align*}
                0 &\le 
                F(z_k) - \expect_k F(x_k) 
                + \frac{L - \mu}{2}\Vert z_k - y_k\Vert^2 
                + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    \\&\quad
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\
                &\underset{\circled{1}}{\le} 
                \alpha_k F(\bar x) + (1 - \alpha_k)F(x_{k - 1}) - \expect_k F(x_k) - \frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                    \\&\quad 
                    + \frac{L - \mu}{2}\Vert z_k - y_k\Vert^2 
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\
                &\underset{\text{(d)}}{=}
                \alpha_k F(\bar x) + (1 - \alpha_k)F(x_{k - 1}) - \expect_k F(x_k) 
                    \\&\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert x_{k - 1} - v_{k - 1}\Vert^2
                    \\&\quad 
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\&\underset{\circled{2}}{\le}
                \alpha_k F(\bar x) + (1 - \alpha_k)F(x_{k - 1}) - \expect_k F(x_k) 
                    \\&\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\&= 
                (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - \expect_kF(x_k)
                \\&\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\&= 
                (1 - \alpha_k)\left(
                    F(x_{k - 1}) - F(\bar x)
                    + \frac{\alpha_{k - 1}^2L\rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right)
                    \\ &\quad 
                    + F(\bar x) - \expect_kF(x_k) 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                \\
                &\underset{\circled{3}}{=} 
                (1 - \alpha_k)\left(
                    F(x_{k - 1}) - F(\bar x)
                    + \frac{\alpha_{k - 1}^2L\rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right)
                    \\ &\quad 
                    + F(\bar x) - \expect_kF(x_k) 
                    - \frac{\alpha_k^2(L - \epsilon_k)}{2}\expect_k \left[\Vert v_k - \bar x\Vert^2\right]
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right]. 
            \end{align*}
            }
            At \circled{1}, we used Lemma \ref{thm:jesen}. 
            For \circled{3}, we used Lemma \ref{lemma:seq-properties} (i) and (b) because the sequence $\alpha_k \le 1$, therefore $1 - \alpha_k \le 0$. 
            Next, $\alpha_0 = 1 > \mu/L$, Lemma \ref{lemma:seq-properties} (ii) applies too, hence it also has $L\alpha_k - \mu \ge 0$. 
            Since $L > \mu$ always, the coefficient on $\Vert x_{k - 1} - v_{k - 1}\Vert^2$ is always $\le 0$, therefore the term can be removed to give an inequality. 
            At \circled{3}, we used Lemma \ref{lemma:snag-identities} (ii). 
            Therefore, at the end the chain of inequalities from above produced the following inequality: 
            \begin{align}\begin{split}
                & \expect_kF(x_k) - F(\bar x) 
                + \frac{\alpha_k^2(L - \epsilon_k)}{2}\expect_k \left[\Vert v_k - \bar x\Vert^2\right]
                \\
                &\le 
                (1 - \alpha_k)\left(
                    F(x_{k - 1}) - F(\bar x)
                    + \frac{\alpha_{k - 1}^2L\rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right) + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right]. 
            \end{split}\end{align}
            Next, we handle the Edge case of $k = 0$. 
            The base case has $\alpha_0 = 1$, and $v_{-1} = x_{-1}$. 
            Then, the following consequences will be immediate. 
            Fom Definition \ref{def:SNAG}, $\alpha_0 = 1$ makes $\tau_0 = 0$, so $y_0 = v_{-1}$. 
            Followed by it, $x_0 = \tilde {\mathbf T}_L(y_0 | F) = \widetilde{\mathbf T}_L(v_{-1} | F)$. 
            Therefore, we can apply Lemma \ref{lemma:pg-stoch-ineq} with $z = \bar x$, $\tilde x = x_0$, $x = v_{-1}$, and $\epsilon = \epsilon_0$ it yields: 
            {\small
            \begin{align*}
                0
                &\le 
                F(z) - \expect_0 F(\tilde x)
                + \frac{L - \mu}{2} \Vert z - x\Vert^2
                - \frac{L - \epsilon}{2}\expect_0\left[\Vert z - \tilde x\Vert^2\right]
                + \frac{\epsilon}{2} \expect_0\left[\Vert x - \tilde x\Vert^2\right]
                \\
                &= 
                F(\bar x) - \expect_0 F(x_0)
                + \frac{L - \mu}{2} \Vert \bar x - v_{-1}\Vert^2
                - \frac{L - \epsilon}{2}\expect_0\left[\Vert \bar x - x_0\Vert^2\right]
                + \frac{\epsilon}{2}\expect_0\left[\Vert v_{-1} - x_0\Vert^2\right].
            \end{align*}
            }
            % INTERMEDIATE RESULTS 
            \par \textbf{Proof of (a)}. 
            Using basic algebra: 
            {\allowdisplaybreaks
            \begin{align*}
                & \frac{\mu^2(1 - \alpha_k)^2}{2(L - \mu)} 
                - \frac{\mu\alpha_k(1 - \alpha_k)}{2}
                \\
                &= 
                \frac{1}{2\left(L - \mu\right)}
                \left(
                    \mu^2(1 - \alpha_k)^2
                    - \left(L - \mu\right)\mu \alpha_k(1 - \alpha_k)
                \right)
                \\
                &= \frac{1 - \alpha_k}{2\left(L - \mu\right)}\left(
                    \mu^2 
                    - \mu^2\alpha_k 
                    - \left(L \mu \alpha_k - \mu^2 \alpha_k\right)
                \right)
                \\
                &= 
                \frac{1 - \alpha_k}{2(L - \mu)}\left(
                    \mu^2 - L\left(\mu\right)\alpha_k
                \right)
                \\
                &= 
                \frac{(1 - \alpha_k)\mu\left(\mu - L\alpha_k\right)}
                {2\left(L - \mu\right)}
                \\
                &= \frac{(\alpha_k - 1)\mu\left(L\alpha_k - \mu\right)}
                {2\left(L - \mu\right)}. 
            \end{align*}
            }
            % INTERMEDIATE RESULTS 
            \textbf{Proof of (c)}. 
            Using (b) and some algebra, we can derive: 
            {\allowdisplaybreaks
            \begin{align*}
                & \frac{(L\alpha_k - \mu)^2}{2(L - \mu)} - \frac{\alpha_{k - 1}^2 L \rho_{k - 1}(1 - \alpha_k)}{2}
                \\
                &= \frac{(L\alpha_k - \mu)^2}{2(L - \mu)} - \frac{L\alpha_k(\alpha_k - \mu/L)}{2}
                \\
                &= \frac{1}{2(L - \mu)}\left(
                    (L\alpha_k - \mu)^2 - (L - \mu)L\alpha_k(\alpha_k - \mu/L)
                \right)
                \\
                &= 
                \frac{1}{2(L - \mu)}\left(
                    (L\alpha_k - \mu)^2 - (L - \mu)\alpha_k(L\alpha_k - \mu)
                \right)
                \\
                &= \frac{L\alpha_k - \mu}{2(L - \mu)}\left(
                    L\alpha_k - \mu - (L - \mu)\alpha_k
                \right)
                \\
                &= \frac{L\alpha_k - \mu}{2(L - \mu)}\left(
                    \mu\alpha_k - \mu
                \right)
                \\
                &= \frac{(L\alpha_k - \mu)\mu(\alpha_k - 1)}{2(L - \mu)}. 
            \end{align*}
            }
            % INTERMEDIATE RESULTS 
            \textbf{Proof of (d)}. 
            {\allowdisplaybreaks
            \begin{align*}
                &- \frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                + \frac{L - \mu}{2}\Vert z_k - y_k\Vert^2
                \\
                &\underset{\text{\circled{1}}}{=} 
                -\frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                + \frac{L - \mu}{2}
                \left\Vert 
                    \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1}) + 
                    \frac{\mu(1 - \alpha_k)}{L - \mu}(\bar x - x_{k - 1})
                \right\Vert^2
                \\
                &= 
                - \frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                + \frac{(L\alpha_k - \mu)^2}{2(L - \mu)} \Vert \bar x - v_{k - 1}\Vert^2
                \\
                &\quad
                    + \frac{\mu^2(1 - \alpha_k)^2}{2(L - \mu)} \Vert \bar x - x_{k - 1}\Vert^2
                    + \frac{(L\alpha_k - \mu)\mu(1 - \alpha_k)}{L - \mu}\langle \bar x - x_{k - 1}, \bar x - v_{k - 1}\rangle
                \\
                &= \left(
                    \frac{\mu^2(1 - \alpha_k)^2}{2(L - \mu)} - \frac{\mu\alpha_k(1 - \alpha_k)}{2}
                \right)\Vert \bar x - x_{k - 1}\Vert^2
                    \\ &\quad 
                    + 
                    \left(
                        \frac{(L\alpha_k - \mu)^2}{2(L - \mu)}
                        - \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}
                    \right)\Vert \bar x - v_{k - 1}\Vert^2
                    \\ &\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(L\alpha_k - \mu)\mu(1 - \alpha_k)}{L - \mu}\langle \bar x - x_{k - 1}, \bar x - v_{k - 1}\rangle
                \\
                &\underset{\text{(a)}}{=} 
                \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert \bar x - x_{k - 1}\Vert^2
                + 
                \left(
                    \frac{(L\alpha_k - \mu)^2}{2(L - \mu)}
                    - \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}
                \right)\Vert \bar x - v_{k - 1}\Vert^2
                \\ &\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(L\alpha_k - \mu)\mu(1 - \alpha_k)}{L - \mu}\langle \bar x - x_{k - 1}, \bar x - v_{k - 1}\rangle
                \\
                &\underset{\text{(c)}}{=}
                \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert \bar x - x_{k - 1}\Vert^2
                + 
                \frac{\mu(L\alpha_k - \mu)(\alpha_k - 1)}{2(L - \mu)}\Vert \bar x - v_{k - 1}\Vert^2
                \\ &\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(L\alpha_k - \mu)\mu(1 - \alpha_k)}{L - \mu}\langle \bar x - x_{k - 1}, \bar x - v_{k - 1}\rangle
                \\
                &= 
                \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \\ & \quad
                    + \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\left(
                        \Vert \bar x - x_{k - 1}\Vert^2 + \Vert \bar x - v_{k - 1}\Vert^2 - 2\langle\bar x - x_{k - 1},\bar x - v_{k - 1} \rangle
                    \right)
                \\
                &= \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                + \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert x_{k - 1} - v_{k - 1}\Vert^2. 
            \end{align*}
            }
            At label \circled{1} we used results (i) from Lemma \ref{lemma:snag-identities}. 
        \end{proof}
        Obtaining the convergence results from the lemma requires a recursive relation in expectation. 
        Before we start, here are some techinical details regarding conditional expectations. 
        Conditional expectation wrt to a random variable is not directly defined, it's instead only defined with respect to some sigma algebra. 
        The conditional expectation exists, and it's unique. 
        The following theorem gives necessary results to derive the convergence rate of the algorithm in expectations. 
        \begin{theorem}[SNAG descent lemma in expectations]\label{thm:snag-descent-expe}
            
        \end{theorem}
        \begin{proof}
            
        \end{proof}
\section{Discussion, interpolation helps but, it's not enough}
    \subsection{The no error case}
    \subsection{The degenerate case}

\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}
