\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% special operators and stuff introduced for this file. 

\newcommand{\cov}{\ensuremath{\operatorname{Cov}}}
\newcommand{\var}{\ensuremath{\operatorname{Var}}}
\newcommand{\expect}{\ensuremath{\mathbb E}}

\begin{document}
\title{{\fontfamily{ptm}\selectfont Linear Convergence of Stochastic Nesterov's Accelerated Proximal Gradient method under Interpolation Hypothesis}}

\author{
    Author
    \thanks{
        University of British Columbia Okanagan,
        Canada. E-mail: \texttt{alto@mail.ubc.ca}.
    }
}

\date{\today}

\maketitle
\todoinline{This paper is currently in draft mode. Check source to change options. }
% \vskip 8mm

\begin{abstract} 
    This file is for communication purposes between collaborators. 

\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}


% ==============================================================================
\section{Introduction}
    Does stochastic accelerated Nesterov's acceleration (SNAG) produces accelerated convergence rate (or, any type of convergence) when the Interpolation Hypothesis is true? 
    \cite{bauschke_convex_2017}
    Previously we got some results, but unfortunately it was incorrect the mistake is diffcult to recover. 
    \par
    \textbf{However, I don't think that it's true after some mistakes from previous version of the notes and careful investigations.}
    In this file we develop some sufficient conditions for Linear convergence of (SNAG). 
    We will give explanations on why we don't think this is necessarily true. 

\section{In preparations}
    Unless specifically specified in the context, we use the following notations. 
    $\Pi_C$ denotes the projection onto a set $C$. 
    Let $A \in \RR^{m \times n}$ be a matrix. 
    $\sigma_{\min}(A)$ denotes the smallest non-zero absolute value of all singular values of $A$. 
    Let $\Vert A\Vert$ denotes the spectral norm of the matrix $A$. 
    $I$ denotes the identity operator. 
    When two expressions are connected via non-trivial results, it's expressed with $\underset{(\cdot)}{=}, \underset{(\cdot)}{\ge}$ where $(\cdot)$ is a label of some intermediate results immediately before it, or explained right after a chain of expressions. 
    \subsection{Basic definitions}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Proximal gradient operator]\label{def:pg-opt}
            Suppose $F = f + g$ with $\reli(\dom f) \cap \reli(\dom g) \neq \emptyset$, and $f$ is a differentiable function. 
            Let $\beta > 0$. 
            Then, we define the proximal gradient operator $T_{\beta}$ as 
            \begin{align*}
                T_\beta (x | F) &= \argmin_{z} \left\lbrace
                    g(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
                \right\rbrace. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If the function $g \equiv 0$, then it yields the gradient descent operator $T_\beta(x) = x - \beta^{-1}\nabla f(x)$. 
            In the context where it's clear what the function $F = f + g$ is, we simply write $T_\beta(x)$ for short. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Bregman Divergence]
            Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
            Then, for all the Bregman divergence $D_f: \RR^n \times \dom\nabla f \rightarrow \RR$ is defined as: 
            \begin{align*}
                D_f(x, y) := f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If, $f$ is $\mu \ge 0$ strongly convex and $L$ Lipschitz smooth then, its Bregman Divergence has for all $x, y \in \RR^n$: $\mu/2 \Vert x - y\Vert^2 \le D_f(x, y) \le L/2 \Vert x - y\Vert^2$. 
            We note that usually the Bregman Divergence is used with a Legendre function, but in here, we do not assume that $f$ has to be Legendre. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Lipschitz smoothness and strongly convex]\label{def:lip-smooth-and-scnvx}
            A differentiable function $f: \RR^n \rightarrow \RR$ is $L$ lipschitz smooth and, $\mu$ strong convex for some $L > \mu \ge 0$ if and only if for all $x, y \in \RR^n$ it satisfies the inequality 
            \begin{align*}
                \frac{\mu}{2}\Vert x - y\Vert^2 \le D_f(x, y) \le \frac{L}{2}\Vert x - y\Vert^2. 
            \end{align*}
        \end{definition}
    \subsection{important inequalities}
        \begin{assumption}\label{ass:smooth-plus-nonsmooth}
            Suppose that $F = f + g$ where $f, g$ are both convex, proper and closed. 
            In addition, assume $f$ is $L > \mu \ge 0$ Lipschitz smooth and strongly convex satisfying Definition \ref{def:lip-smooth-and-scnvx}. 
        \end{assumption}
        % --------------------------------------------------------------------------------------------------------------
        \begin{theorem}[Jensen's inequality]\label{thm:jesen}
            Let $F: \RR^n \rightarrow \overline \RR$ be a $\mu \ge 0$ strongly convex function. 
            Then, it is equivalent to the following condition. 
            For all $x, y \in \RR^n$, $\lambda \in (0, 1)$ it satisfies the inequality 
            \begin{align*}
                (\forall \lambda \in [0, 1])\; 
                F(\lambda x + (1 - \lambda)y) \le \lambda F(x) + (1 - \lambda)F(y) -\frac{\mu\lambda(1 - \lambda)}{2} \Vert y - x\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{remark}
            If $x, y$ is out of $\dom F$, the inequality still work by convexity. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{lemma}[inexact proximal gradient inequality]\label{lemma:inex-pg-ineq}
            Let $F = f + g$ satisfies Definition \ref{def:lip-smooth-and-scnvx} with $L > \mu \ge 0$. 
            Let $x \in \RR^n$ be fixed. 
            Suppose an inexact evaluation of proximal gradient operator at $x$ yield an approximation $\tilde x$, characterized by: 
            \begin{align*}
                & D_f(\tilde x, x) \le \frac{B}{2}\Vert \tilde x - x\Vert^2, 
                \\
                & \exists: w \in \partial \left[
                    z \mapsto  g(z) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right](\tilde x)\; \text{s.t: } \Vert w\Vert \le \epsilon \Vert x - \tilde x\Vert. 
            \end{align*}
            Then, it would satisfy for all $z\in \RR^n$ the inequality: 
            \begin{align*}
                0 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\Vert z - \tilde x\Vert^2
                + \frac{\epsilon}{2}\Vert x - \tilde x \Vert^2. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Let $h = z \mapsto g(z) + \langle \nabla f(x), z - x\rangle + B/2\Vert z - x\Vert^2$. 
            $h$ is a $B$ strongly convex function, using the subgradient inequality of a strongly convex function it has for all $z \in \RR^n$: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2
                &\le h(z) - h(\tilde x) - \langle w, z - \tilde x\rangle
                \\
                &= 
                \left(
                    g(z) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right)
                \\ &\quad 
                    - 
                    \left(
                        g(\tilde x) + \langle \nabla f(x), \tilde x - x\rangle + \frac{B}{2}\Vert \tilde x - x\Vert^2
                    \right)
                    - \langle w, z - \tilde x\rangle
                \\
                &= \left(
                    g(z) + f(z) - f(z) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right)
                \\ &\quad 
                    - 
                    \left(
                        g(\tilde x) + f(\tilde x) - f(\tilde x) 
                        + \langle \nabla f(x), \tilde x - x\rangle + \frac{B}{2}\Vert \tilde x - x\Vert^2
                    \right)
                    - \langle w, z - \tilde x\rangle
                \\
                &= \left(
                    F(z) - D_f(z, x) + \frac{B}{2}\Vert z - x\Vert^2
                \right)
                \\ &\quad 
                    - 
                    \left(
                        F(\tilde x) - D_f(\tilde x, x) + \frac{B}{2}\Vert \tilde x - x\Vert^2
                    \right)
                    - \langle w, z - \tilde x\rangle
                \\
                &\underset{\text{(a)}}{\le} F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - 0 - \langle w, z - \tilde x \rangle
                \\
                &\underset{\text{(b)}}{\le}
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - 0 + \epsilon \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert. 
            \end{align*}
            At (a), we used the fact that $f$ is $L > \mu \ge 0$ Lipschitz smooth and strongly convex therefore it has for all $y \in \RR^n$: 
            \begin{align*}
                0 
                \le \frac{L}{2}\Vert z - y\Vert^2  - D_f(z, y)
                \le \frac{L - \mu}{2}\Vert z - y\Vert^2. 
            \end{align*}
            At (b) we used the Cauchy Inequality and, the assumption $\Vert w\Vert \le \epsilon \Vert x - \tilde x\Vert$. 
            Continuing it has 
            \begin{align*}
                0 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                + \epsilon \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert 
                - \frac{B}{2}\Vert z - \tilde x\Vert^2
                    \\&\quad 
                    - \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 - \frac{\epsilon}{2}\Vert z - \tilde x \Vert^2
                    + \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 + \frac{\epsilon}{2}\Vert z - \tilde x \Vert^2
                \\
                &= F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                - \frac{1}{2}\left(
                    \sqrt{\epsilon}\Vert z - \tilde x\Vert - \sqrt{\epsilon}\Vert x - \tilde x\Vert
                \right)^2
                - \frac{B}{2}\Vert z - \tilde x\Vert^2
                    \\
                    &\quad 
                    + \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 + \frac{\epsilon}{2}\Vert z - \tilde x \Vert^2
                \\
                &= F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\Vert z - \tilde x\Vert^2
                + \frac{\epsilon}{2}\Vert x - \tilde x \Vert^2. 
            \end{align*}
        \end{proof}
        \begin{remark}
            Usually in practice, the precise value of $F(\tilde x)$ is never known, therefore, $B$ cannot be easily determined via $D_f(\tilde x, x)$. 
            Hence, in this case we can only choose $B \ge L$ which gives: 
            \begin{align*}
                F(z) - F(\tilde x) + \frac{L - \mu}{2}\Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\Vert z - \tilde x\Vert^2
                + \frac{\epsilon}{2}\Vert x - \tilde x \Vert^2.
            \end{align*}
        \end{remark}
        Note, an inexact evaluation of the proximal gradient operator can be caused by an inexact gradient on the smooth part. 
        Suppose that one take $\tilde \nabla f(x)$ to be an estimate of $\nabla f(x)$ and use it for the proximal gradient operator to produce $\tilde x$, then: 
        \begin{align*}
            \mathbf 0 
            &\in \partial g(\tilde x) + \tilde \nabla f(x) + B(\tilde x - x)
            \\
            &= 
            \partial g(\tilde x) + \tilde\nabla f(x) - \nabla f(x) 
            + \nabla f(x) + B(\tilde x - x)
            \\
            \iff &
            \nabla f(x) - \tilde \nabla f(x) \in 
            \partial g(\tilde x) 
            + \nabla f(x) + B(\tilde x - x).
        \end{align*}
        In this case, it adds the interpretation that $w = \nabla f(x) - \tilde \nabla f(x)$. 
        It fully characterizes the error made to estimate the true gradient $\nabla f(x)$. 

\section{Inexact accelerated proximal gradient algorithm}
    The following defines the inexact proximal gradient operator where the gradient of the smooth part of the function is estimated. 
    All algorithms satisfying the following definition will be referred to as Stochastic Nesterov's Accelerated Gradient (SNAG). 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[inexact proximal gradient operator with relative error]
        Let $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}, let $x \in \RR^n$ be fixed. 
        Suppose that $\tilde \nabla f(x)$ is an estimate of the true gradient $\nabla f(x)$ with relative error $\epsilon \ge 0$. 
        Then, the inexact proximal gradient operator is defined by: 
        \begin{align*}
            T_B^{(\epsilon)}(x | F) = \argmin_{z \in \RR^n}\left\lbrace
                g(z) + \left\langle \tilde \nabla f(x), z - x\right\rangle
                + \frac{B}{2}\Vert z - x\Vert^2
            \right\rbrace. 
        \end{align*}
        And it satisfies 
        \begin{align*}
            \left\Vert \tilde \nabla f(x) - \nabla f(x)\right\Vert \le \epsilon \Vert x - \tilde x\Vert. 
        \end{align*}
    \end{definition}
    The inexact evaluation can be caused by an random variable. 
    The definitions follow characterize algorithm in which the errors are related to a random variable that estimates the gradient of the objective function. 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[stochastic proximal gradient operator with relative error]
        
    \end{definition}
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[inexact/stochastic SNAG]\label{def:SNAG}
        Suppose that $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
        Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_k \in (0, 1]$. 
        Let $(\epsilon_k)_{k \ge 0}$ be a sequence of errors. 
        Given initial conditions $v_{-1}, x_{- 1}$. 
        An algorithm satisfying the SNAG definition if it generates a sequence $(y_k, x_k, v_k)_{k \ge 0}$ if for all $k \ge 0$, the following conditions are satisfied: 
        \begin{align*}
            & \tau_k = L_k(1 - \alpha_k)\left(L_k \alpha_k - \sigma^{(k)}\right)^{-1}, \\
            & y_k = (1 + \tau_k)^{-1}v_{k - 1} + \tau_k(1 + \tau_k)^{-1}x_{k - 1}, \\
            & x_k =  T_{L}^{(\epsilon_k)}(y_k | F_{I_k}), \\
            & v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}).
        \end{align*}
    \end{definition}
    The following lemma states two important relationships on the iterates generated by Definition \ref{def:SNAG}. 
    \begin{lemma}[SNAG identities]\label{lemma:snag-identities}
        
    \end{lemma}


\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}
