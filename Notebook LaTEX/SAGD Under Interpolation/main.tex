\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% special operators and stuff introduced for this file. 

\newcommand{\cov}{\ensuremath{\operatorname{Cov}}}
\newcommand{\var}{\ensuremath{\operatorname{Var}}}

\begin{document}
\title{{\fontfamily{ptm}\selectfont Linear Convergence of Stochastic Nesterov's Accelerated Proximal Gradient method under Interpolation Hypothesis}}

\author{
    Author
    \thanks{
        University of British Columbia Okanagan,
        Canada. E-mail: \texttt{alto@mail.ubc.ca}.
    }
}

\date{\today}

\maketitle
\todoinline{This paper is currently in draft mode. Check source to change options. }
% \vskip 8mm

\begin{abstract} 
    This file is for communication purposes between collaborators. 

\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}


% ==============================================================================
\section{Nesterov's Accelerated Gradient}
    \subsection{In preparations}
        \begin{assumption}[smooth add nonsmooth]\label{ass:smooth-plus-nonsmooth}
            The function $F = f + g$ where $f:\RR^n \rightarrow \RR$ is an $L$ Lipschitz smooth and $\mu \ge 0$ strongly convex function. 
            The function $g:\RR^n \rightarrow \overline \RR$ is a closed convex proper function. 
        \end{assumption}
        \begin{assumption}[admitting minimizers]\label{ass:smooth-plus-nonsmooth-x}
            Let $F = f + g$ and in addition assume that the set of minimizers $X^+ := \argmin_{x}F(x)$ is non-empty. 
        \end{assumption}

        \begin{definition}[Proximal gradient operator]
            Suppose $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
            Let $\beta > 0$. 
            Then, we define the proximal gradient operator $T_{\beta}$ as 
            \begin{align*}
                T_\beta (x | F) &= \argmin{z} \left\lbrace
                    g(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
                \right\rbrace. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If the function $g \equiv 0$, then it yields the gradient descent operator $T_\beta(x) = x - \beta^{-1}\nabla f(x)$. 
            In the context where it's clear what the function $F = f + g$ is, we simply write $T_\beta(x)$ for short. 
        \end{remark}

        \begin{definition}[Bregman Divergence]
            Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
            Then, for all the Bregman divergence $D_f: \RR^n \times \dom\nabla f \rightarrow \RR$ is defined as: 
            \begin{align*}
                D_f(x, y) := f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If, $f$ is $\mu \ge 0$ strongly convex and $L$ Lipschitz smooth then, its Bregman Divergence has for all $x, y \in \RR^n$: $\mu/2 \Vert x - y\Vert^2 \le D_f(x, y) \le L/2 \Vert x - y\Vert^2$. 
        \end{remark}

        \begin{definition}[R-WAPG sequence]\label{def:rwapg-seq}
            Let $(L_k)_{k \ge 0}$ be a sequence such that $L_k > \mu$ for all $k$. 
            Let $\alpha_0 \in (0, 1]$, $(\alpha_k)_{k \ge 1}$ has $\alpha_k \in (\mu/ L_k, 1)$. 
            Then define for all $k \ge 0$: 
            \begin{align*}
                \rho_k(1 - \alpha_{k + 1})\alpha_k^2 = \alpha_{k + 1}(\alpha_{k + 1} - \mu/L_k). 
            \end{align*}
        \end{definition}
        \begin{remark}
            When $\rho_k = 1$, the recursive relation between $\alpha_k, \alpha_{k - 1}$ is the same as the well known Nesterov's sequence used in algorithm such as FISTA and Nesterov's accelerated gradient. 
            See Li and Wang \cite{li_relaxed_2025} for more information. 
        \end{remark}
        % \begin{definition}[similar triangle representation of NAPG]\;\label{def:st-method}\\
        %     Let $(\alpha_k)_{k \ge 0}$ be an R-WAPG sequence. 
        %     Suppose that the base case $v_{-1}, x_{k - 1}\in \RR^n$ is given to initialize the algorithm. 
        %     Then the algorithm produces the sequence of iterates $(y_k, x_k, v_k)_{k \ge 0}$ and auxiliary parameter sequence $L_k, \tau_k$ satisfying these inequalities: 
        %     \begin{align*}
        %         & \tau_k = L_k(1 - \alpha_k)(L_k\alpha_k - \mu)^{-1}, 
        %         \\
        %         & y_k = (1 + \tau_k)^{-1} v_{k - 1} + \tau_k (1 + \tau_k)^{-1} x_{k - 1},
        %         \\
        %         & D_f(x_k, y_k) \le L_k/2\Vert x_k - y_k\Vert^2, 
        %         \\
        %         & x_k = T_{L_k}(y_k),
        %         \\
        %         & v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}). 
        %     \end{align*} 
        % \end{definition}
        % The following theorems are critical in analyzing the behavior of algorithm in Definition \ref{def:st-method}. 
        
        \begin{theorem}[proximal gradient inequality]\label{thm:pg-ineq}
            Let function $F$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}, so it's $\mu \ge 0$ strongly convex. 
            For all $x\in \RR^n$, define $x^+ = T_L(x)$, then there exists a $B \ge 0$ such that $D_f(x^+, x) \le B/2 \Vert x^+ - x\Vert^2$. 
            Then, for all $z \in \RR^n$ it satisfies proximal gradient inequality at point $x$:  
            \begin{align*}
                0&\le F(z) - F(x^+) - \frac{B}{2}\Vert z - x^+\Vert^2  + \frac{B - \mu}{2}\Vert z - x\Vert^2
                \\
                &=  F(z) - F(x^+) - \langle B(x - x^+), z - x\rangle
                - \frac{\mu}{2}\Vert z - x\Vert^2
                - \frac{B}{2}\Vert x - x^+\Vert^2. 
            \end{align*}
            Since $f$ is assumed to be $L$ Lipschitz smooth, the above condition is true for all $x, y \in \RR^n$ for all $B \ge L$. 
        \end{theorem}
        \begin{remark}
            The theorem is the same as in Nesterov's book \cite[Theorem 2.2.13]{nesterov_lectures_2018}, but with the use of proximal gradient mapping and proximal gradient instead of project gradient hence making it equivalent to the theorem in Beck's book \cite[Theorem 10.16]{beck_first-order_2017}. 
            The only generalization here is parameter $B$ which made to accommodate algorithm that implements Definition \ref{def:snapg-v2} with line search routine to determine $L_k$. 
            Each of the reference books gives a proof of the theorem. 
            But for the best consistency in notations, see Theorem 2.3 in Li and Wang \cite{li_relaxed_2025}. 
        \end{remark}
        \begin{theorem}[Jensen's inequality]\label{thm:jesen}
            Let $F: \RR^n \rightarrow \overline \RR$ be a $\mu \ge 0$ strongly convex function. 
            Then, it is equivalent to the following condition. 
            For all $x, y \in \RR^n$, $\lambda \in (0, 1)$ it satisfies the inequality 
            \begin{align*}
                (\forall \lambda \in [0, 1])\; 
                F(\lambda x + (1 - \lambda)y) \le \lambda F(x) + (1 - \lambda)F(y) -\frac{\mu\lambda(1 - \lambda)}{2} \Vert y - x\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{remark}
            If $x, y$ is out of $\dom F$, the inequality still work by convexity. 
        \end{remark}
    
    \subsection{Stochastic accelerated proximal gradient}
        The following assumption about the objective function is fundamental in incremental gradient method for Machine Learning, data science other similar tasks. 
        \begin{assumption}[sum of many]\label{ass:sum-of-many}
            Define $F := (1/n)\sum_{i = 1}^{n} F_i$ where each $F_i = f_i + g_i$.
            Assume that for all $i = 1, \ldots, n$, each $f_i:\RR^n \rightarrow \RR$ are $K^{(i)}$ smooth and $\mu^{(i)} \ge 0$ strongly convex function such that $K^{(i)} > \mu^{(i)}$ and, $g_i:\RR^n \rightarrow \overline \RR$ is a closed convex proper function. 
            Consequently, the function $f$ can be written as $F = g + f$ with $f = (1/n)\sum_{i = 1}^{n} f_i, g = (1/n)\sum_{i = 1}^{n}g_i$ therefore, it also satisfies Assumption \ref{ass:smooth-plus-nonsmooth} with $L = (1/n)\sum_{i = 1}^n K^{(i)}$ and $\mu = (1/n)\sum_{i = 1}^{n}\mu^{(i)}$. 
        \end{assumption}
        This assumption is stronger than Assumption \ref{ass:smooth-plus-nonsmooth}. 
        It still appears in practice, for example if $F_i$ are all indicator function of convex set, then it solves feasibility problem $\bigcap_{i = 1}^n C_i$ and, in this case, the proximal gradient operator becomes a projection onto the convex set $C_i$. 
        In practice, each of the strong convexity constant $\mu^{(i)}$ may not be easily accessible. 
        And we further note that if $\mu > 0$ strongly convex, then there exists at least one $\mu^{(i)} \ge 0$. 
        \par
        The interpolation hypothesis from Machine Learning stated that the model has the capacity to perfect fit all the observed data. 
        The following assumption state the interpolation hypothesis in our context. 
        \begin{assumption}[interpolation hypothesis]\label{ass:interp-hypothesis}
            Suppose that $F := (1/n)\sum_{i = 1}^{n} F_i$ satisfying Assumption \ref{ass:sum-of-many}. 
            In addition, assuming that it has $0 = \inf_{x}F(x)$ and, there exists some $\bar x \in \RR^n$ such that for all $i = 1, \ldots, n$ it satisfies $0 = f_i(\bar x)$. 
        \end{assumption}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[SNAPG-V2]\label{def:snapg-v2}
            Let $F$ satisfies Assumption \ref{ass:sum-of-many}. 
            Let $(I_k)_{k \ge 0}$ be a list of i.i.d random variables uniformly sampled from set $\{0, 1, 2, \cdots, n\}$. 
            Initialize $v_{-1} = x_{-1}, \alpha_0 = 1$. 
            The SNAPG generates the sequence $(y_k, x_k, v_k)_{k \ge 0}$ such that for all $k \ge 0$ they satisfy: 
            \begin{align*}
                & (L_{k - 1}/L_k)(1 - \alpha_{k})\alpha_{k - 1}^2 = \alpha_{k}\left(\alpha_{k} - \mu/L_k\right), \\
                & \tau_k = L_k(1 - \alpha_k)\left(L_k \alpha_k - \mu^{(I_k)}\right)^{-1}, \\
                & y_k = (1 + \tau_k)^{-1}v_{k - 1} + \tau_k(1 + \tau_k)^{-1}x_{k - 1}, \\
                & x_k =  T_{L_k}(y_k | F_{I_k}) \text{ s.t: } D_f(x_k, y_k) \le L_k/2\Vert y_k - x_k\Vert^2, \\
                & v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}). 
            \end{align*}
        \end{definition}
        % --------------------------------------------------------------------------------------------------------------
        \begin{lemma}[range of the momentum sequence in SNAPG-V2]\;\label{lemma:snapg-v2-seq-range}\\
            Suppose that $(L_k)_{k \ge 0}$ is a sequence such that $L_k > 0$ for all $k \ge 0$. 
            Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_0 \in (0, 1]$ and, for all $k \ge 1$, it satisfies recursively the equality: 
            \begin{align*}
                (L_{k - 1}/L_k)(1 - \alpha_{k})\alpha_{k - 1}^2 = \alpha_{k}\left(\alpha_{k} - \mu/L_k\right). 
            \end{align*}
            And, the following items are true: 
            \begin{enumerate}
                \item Solution to the equation with $\alpha_k > 0$ is given by: 
                \begin{align*}
                    \alpha_k = \frac{L_{k - 1}}{2L_k} \left(
                        - \alpha_{k - 1}^2 + \frac{\mu}{L_{k - 1}}
                        + \sqrt{
                            \left(
                                \alpha_{k - 1} - \frac{\mu}{L_{k - 1}}
                            \right)^2
                            + \frac{4\alpha_{k - 1}^2L_k}{L_{k - 1}}
                        }
                    \right). 
                \end{align*}
                \item ... \todoinline{This part is not finished yet. }
            \end{enumerate}
        \end{lemma}
        The following lemma state the relationships of the iterates generated by SNAPG-V2. 
        They are needed for the convergence proof. 
        % --------------------------------------------------------------------------------------------------------------
        \begin{lemma}[properties of the iterates]\label{lemma:snapg2-itrs-props}
            Suppose that the iterates $(z_k, x_k, y_k)_{k \ge 0}$ and sequence $(\alpha_k)_{k \ge 1}$ are produced by an algorithm satisfying Definition \ref{def:snapg-v2}. 
            Let $\bar x \in \RR^n$.
            Define the sequence $z_k = \alpha_k\bar x + (1 - \alpha_k)x_{k - 1}$. 
            Then, the following are true: 
            \begin{enumerate}
                \item\label{lemma:snapg2-itrs-props-item1} For all $k \ge 1$ it has: 
                \begin{align*}
                        z_k - y_k 
                        = 
                        \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
                        + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}(\bar x - x_{k - 1}).
                \end{align*}
                \item\label{lemma:snapg2-itrs-props-item2} For all $k \ge 1$, it has: $z_k - x_k = \alpha_k(x - \bar x)$
            \end{enumerate}
        \end{lemma}
        \begin{proof}
            \begin{proof}[proof of \ref{lemma:snapg2-itrs-props-item1}]
                From Definition \ref{def:snapg-v2}, it has
                \begin{align*}
                    (1 + \tau_k)^{-1}
                    &=
                    \left(
                        1 + \frac{L_k(1 - \alpha_k)}{L_k\alpha_k - \mu^{(i)}}
                    \right)^{-1} = \left(
                        \frac{L_k\alpha_k - \mu^{(i)} + L_k(1 - \alpha_k)}{L_k\alpha_k - \mu^{(i)}}
                    \right)^{-1}
                    = \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}. 
                \end{align*}
                Therefore, for all $k \ge 0$, $y_k$ has 
                \begin{align*}
                    0 &= (1 + \tau_k)^{-1} v_{k - 1} + \tau_k (1 + \tau_k)^{-1} x_{k - 1} - y_k
                    \\
                    &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} 
                    \left(
                        v_{k - 1} + \frac{L_k(1 - \alpha_k)}{L_k\alpha_k - \mu^{(i)}} x_{k - 1}
                    \right) - y_k
                    \\
                    &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1}
                    + \frac{L_k(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1} - y_k
                    \\
                    &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                    + 
                    \left(
                        \frac{L_k(1 - \alpha_k)}{L_k - \mu^{(i)}} - (1 - \alpha_k)
                    \right) x_{k - 1} - y_k
                    \\
                    &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                    + 
                    (1 - \alpha_k)\left(
                        \frac{L_k - L_k + \mu^{(i)}}{L_k - \mu^{(i)}}
                    \right) x_{k - 1} - y_k
                    \\
                    &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                    + 
                    \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}x_{k - 1} - y_k. 
                \end{align*}
                Therefore, we establish the equality 
                \begin{align*}
                    (1 - \alpha_k)x_{k - 1} - y_k &= 
                    - \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} 
                    - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}. 
                \end{align*}
                On the second equality below, we will the above equality, it goes: 
                \begin{align*}
                    z_k - y_k &= 
                    \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - y_k
                    \\
                    &= \alpha_k \bar x 
                    - \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} 
                    - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}
                    \\
                    &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
                    + \left(
                        \alpha_k - \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}
                    \right)\bar x
                    - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}
                    \\
                    &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
                    + \left(
                        \frac{\alpha_kL_k - \alpha_k \mu^{(i)} - L_k\alpha_k + \mu^{(i)}}{L_k - \mu^{(i)}}
                    \right)\bar x
                    - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}
                    \\
                    &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
                    + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}\bar x
                    - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}
                    \\
                    &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
                    + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}(\bar x - x_{k - 1}).
                \end{align*}
            \end{proof}
            \begin{proof}[proof of \ref{lemma:snapg2-itrs-props-item2}]
                From Definition \ref{def:snapg-v2} it has directly: 
                \begin{align*}
                    z_k - x_k &= \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - x_k
                    \\
                    &= \alpha_k \bar x + x_{k - 1} - x_k - \alpha_k x_{k - 1}
                    \\
                    &= \alpha_k(\bar x - \alpha_k^{-1}(x_k - x_{k - 1}) - x_{k - 1})
                    \\
                    &= \alpha_k (\bar x - v_k).
                \end{align*}
            \end{proof}
        \end{proof}
        % --------------------------------------------------------------------------------------------------------------
        \begin{theorem}[SNAPG-V2 one step convergence]\label{thm:snapg2-one-step}
            Let $F$ satisfies assumption \ref{ass:interp-hypothesis}. 
            Suppose that an algorithm satisfying Definition \ref{def:snapg-v2} uses this $F$. 
            Let $\mathbb E_k$ denotes the expectation conditioned on $I_0, I_1, \ldots, I_{k - 1}$. 
            Then, for all $k \ge 1$, it has the following inequality 
            \begin{align*}
                & \mathbb E_k\left[F_{I_k}(x_{k})\right] 
                - F(\bar x) 
                + \mathbb E_k\left[
                    \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                \right]
                \\
                &\le 
                (1 - \alpha_k)\left(
                        \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
                        - F(\bar x)
                        + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
                \right)
                    \\ &\quad 
                    + \mathbb E_k\left[
                        \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
                    \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
            \end{align*}
            And for $k = 0$, it has 
            \begin{align*}
                \mathbb E \left[ F_{I_0}\right] - F(\bar x) 
                + \frac{L_0}{2}\mathbb E \left[\Vert \bar x - x_0\Vert^2\right]
                &\le \frac{L_0 - \mu}{2}\Vert \bar x - v_{-1}\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Let's suppose that $I_k = i$ and, for all $k \ge 0$. 
            Let $z_k = \alpha_k \bar x + (1 - \alpha_k)x_{k - 1}$ where $\bar x$ is a minimizer of $F$. 
            The proof is long so, we use letters and subscript under relations such as $\underset{(\cdot)}{=}, \underset{(\cdot)}{\ge}$ to indicate which result is used going from the previous expression to the next. 
            We list the following intermediate results, (d)-(g) are proved at the end of the proof. 
            \begin{itemize}
                \item[(a)] We can use proximal gradient inequality from Theorem \ref{thm:pg-ineq} with $z = z_k$ because each $F_i$ is $K_i$ Lipschitz smooth and, $\mu^{(i)}$ strongly convex with $K_i \ge \mu^{(i)}$. 
                \item[(b)] We can use Jensen's inequality of Theorem \ref{thm:jesen} with $z = z_k$ on $F_i$. 
                \item[(c)] The sequence $(\alpha_k)_{k \ge 0}$ has $(L_{k - 1}/L_k)(1 - \alpha_{k})\alpha_{k - 1}^2 = \alpha_{k}\left(\alpha_{k} - \mu/L_k\right)$. It is a special case of Definition \ref{def:rwapg-seq} with $\rho_{k - 1} = L_{k - 1}/L_k$. 
                \item[(d)] Prove in Lemma \ref{lemma:snapg2-itrs-props} \ref{lemma:snapg2-itrs-props-item1} we use the equality:
                \begin{align*}
                    (\forall k \ge 1)\; 
                    z_k - y_k 
                    = 
                    \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
                    + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}(\bar x - x_{k - 1}).
                \end{align*}
                \item [(e)] From Lemma \ref{lemma:snapg2-itrs-props} \ref{lemma:snapg2-itrs-props-item2}, we use: $(\forall k \ge 1)\; z_k - x_k = \alpha_k (\bar x - v_k)$. 
                \item [(f)] Using direct algebra, we have for all $k \ge 1$: 
                \begin{align*}
                    \frac{\left(\mu^{(i)}\right)^2(1 - \alpha_k)^2}{2(L_k - \mu^{(i)})} 
                    - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}
                    = \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}
                    {2\left(L_k - \mu^{(i)}\right)}. 
                \end{align*}
                \item [(g)] Using (c), we have for all $k \ge 1$: 
                \begin{align*}
                    \frac{\left(
                        L_k\alpha_k - \mu^{(i)}
                    \right)^2}{2(L_k - \mu^{(i)})} 
                    -
                    \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} 
                    = 
                    \frac{
                        \left(L_k \alpha_k - \mu^{(i)}\right)\mu^{(i)}
                        \left(\alpha_k - 1\right)
                    }
                    {2(L_k - \mu^{(i)})}
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2}. 
                \end{align*}
                \item[(h)] Because we assumed interpolation hypothesis in Assumption \ref{ass:interp-hypothesis}, it has $\mathbb E[F_{I_k}(\bar x)] = F(\bar x)$ for all $\bar x$ that is a minimizer of $F$. 
            \end{itemize}
            For all $k \ge 1$, starting with (a) we have: 
            \begin{align}\label{ineq:snapg2-one-step-chain1}
                \begin{split}
                    0 &\le F_i(z_k) - F_i(x_k) - \frac{L_k}{2}\Vert z_k - x_k\Vert^2 + \frac{L_k - \mu^{(i)}}{2}\Vert z_k - y_k\Vert^2
                    \\
                    &\underset{\text{(b)}}{\le}
                    \alpha_k F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k) \\
                        &\quad 
                        - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                        - \frac{L_k}{2}\Vert z_k - x_k\Vert^2 
                        + \frac{L_k - \mu^{(i)}}{2}\Vert z_k - y_k\Vert^2. 
                \end{split}
            \end{align}
            And we have the following chain of equalities:
            {\allowdisplaybreaks
            \begin{align*}
                & - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                + \frac{L_k - \mu^{(i)}}{2}\Vert z_k - y_k\Vert^2
                \\
                &\underset{\text{(d)}}{=}
                - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2 
                    \\&\quad
                    + \frac{L_k - \mu^{(i)}}{2}
                    \left\Vert
                        \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
                        + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}(\bar x - x_{k - 1})
                    \right\Vert^2
                \\
                &= 
                - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2 
                    \\&\quad
                    + \frac{(L_k\alpha_k - \mu^{(i)})^2}{2(L_k - \mu^{(i)})} \Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(\mu^{(i)})^2(1 - \alpha_k)^2}{2(L_k - \mu^{(i)})}\Vert \bar x - x_{k - 1}\Vert^2 
                    \\&\quad 
                    + \frac{(L_k\alpha_k  - \mu^{(i)})\mu^{(i)}(1 - \alpha_k)}{(L_k - \mu^{(i)})}\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
                \\
                &= 
                \left(
                    \frac{(\mu^{(i)})^2(1 - \alpha_k)^2}{2(L_k - \mu^{(i)})} - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}
                \right)\Vert \bar x - x_{k - 1}\Vert^2
                    \\ &\quad 
                    + \left(
                        \frac{(L_k\alpha_k - \mu^{(i)})^2}{2(L_k - \mu^{(i)})} - \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2}
                    \right) \Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
                    \\&\quad 
                    + \frac{(L_k\alpha_k  - \mu^{(i)})\mu^{(i)}(1 - \alpha_k)}{(L_k - \mu^{(i)})}\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
                \\
                &\underset{\text{(f)}}{=}
                \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}
                {2\left(L_k - \mu^{(i)}\right)} \Vert \bar x - x_{k - 1}\Vert^2
                    \\ &\quad 
                    + \left(
                        \frac{(L_k\alpha_k - \mu^{(i)})^2}{2(L_k - \mu^{(i)})} - \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2}
                    \right) \Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
                    \\&\quad 
                    + \frac{(L_k\alpha_k  - \mu^{(i)})\mu^{(i)}(1 - \alpha_k)}{(L_k - \mu^{(i)})}\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
                \\
                & \underset{\text{(g)}}{=} 
                \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}
                {2\left(L_k - \mu^{(i)}\right)}\Vert \bar x - x_{k - 1}\Vert^2
                    \\ &\quad 
                    + \left(
                        \frac{
                            \left(L_k \alpha_k - \mu^{(i)}\right)\mu^{(i)}
                            \left(\alpha_k - 1\right)
                        }
                        {2(L_k - \mu^{(i)})}
                        + \frac{\alpha_k(\mu - \mu^{(i)})}{2}
                    \right) \Vert \bar x - v_{k - 1}\Vert^2
                    \\ &\quad 
                    + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(L_k\alpha_k  - \mu^{(i)})\mu^{(i)}(1 - \alpha_k)}{(L_k - \mu^{(i)})}\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
                \\
                &= 
                \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}\left(
                    \Vert \bar x - x_{k - 1}\Vert^2 + \Vert \bar x - v_{k - 1}\Vert^2 - 2\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
                \right) 
                    \\ &\quad 
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
                \\
                &= \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}
                    \Vert x_{k - 1} - v_{k - 1} \Vert^2
                    \\ &\quad 
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2.
            \end{align*}
            }
            Substituting the above back to the tail of Inequality \eqref{ineq:snapg2-one-step-chain1} it gives: 
            {\allowdisplaybreaks
            \begin{align*}
                0 &\le 
                \alpha_k F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k) 
                    \\&\quad 
                    - \frac{L_k}{2}\Vert z_k - x_k\Vert^2 
                    + \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}
                    \Vert x_{k - 1} - v_{k - 1} \Vert^2
                    \\ &\quad 
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
                \\
                &\underset{\text{(e)}}{=} 
                \alpha_k F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k) 
                    \\&\quad 
                    - \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                    + \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}
                    \Vert x_{k - 1} - v_{k - 1} \Vert^2
                    \\ &\quad 
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
                \\
                &= (\alpha_k - 1)F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k) + F_i(\bar x)
                    \\&\quad 
                    - \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                    + \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2
                    \\ &\quad 
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
                \\
                &= (1 - \alpha_k)\left(
                    F_i(x_{k - 1}) - F_i(\bar x) + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert v_{k - 1} - \bar x\Vert^2
                \right) 
                    \\ & \quad
                    - \left(
                        F_i(x_{k}) - F_i(\bar x) + \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                    \right)
                    \\ &\quad 
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
            \end{align*}
            }
            Recall that $i = I_k$ is the random variable from Definition \ref{def:snapg-v2}. 
            Rearranging the last expression in the above equality chain can be conveniently written as
            \begin{align}
                \begin{split}
                    & F_{I_k}(x_{k}) - F_{I_k}(\bar x) + \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                    \\ &\le 
                    (1 - \alpha_k)\left(
                        F_{I_k}(x_{k - 1}) - F_{I_k}(\bar x) + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert v_{k - 1} - \bar x\Vert^2
                    \right) 
                        \\ &\quad 
                        + \frac{\alpha_k(\mu - \mu^{(I_k)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
                        + \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
                \end{split}
            \label{ineq:snapg2-one-step-presult1}\end{align}
            Recall $\mathbb E_k$ denotes the conditional expectation on $I_0, I_1, \ldots, I_{k - 1}$. 
            Taking the conditional expectation on the LHS of the \eqref{ineq:snapg2-one-step-presult1} yields: 
            \begin{align*}
                & \mathbb E_k\left[
                    F_{I_k}(x_{k}) - F_{I_k}(\bar x) + \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                \right]
                \\
                &\underset{\text{(h)}}{=}
                \mathbb E_k\left[F_{I_k}(x_{k})\right] 
                - F(\bar x) 
                + \mathbb E_k\left[
                    \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                \right]. 
            \end{align*}
            On the RHS of \eqref{ineq:snapg2-one-step-presult1}, using the linearity property while taking the conditional expectation yields: 
            {\allowdisplaybreaks
            \begin{align*}
                & \mathbb E_k\left[
                    (1 - \alpha_k)\left(
                        F_{I_k}(x_{k - 1}) - F_{I_k}(\bar x) + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert v_{k - 1} - \bar x\Vert^2
                    \right)
                \right]
                    \\ &\quad 
                    + \mathbb E_k \left[
                        \frac{\alpha_k(\mu - \mu^{(I_k)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
                    \right]
                    + \mathbb E_k\left[
                        \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2
                    \right]
                \\
                &\underset{\text{(1)}}{=} 
                (1 - \alpha_k)\left(
                        \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
                        -\mathbb E_k [F_{I_k}(\bar x)] 
                        + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
                \right)
                    \\ &\quad 
                    + \mathbb E_k \left[
                        \frac{\alpha_k(\mu - \mu^{(I_k)})}{2} 
                    \right]\Vert \bar x - v_{k - 1}\Vert^2
                    + \mathbb E_k\left[
                        \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
                    \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
                \\
                &\underset{\text{(h)}}{=} 
                (1 - \alpha_k)\left(
                        \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
                        - F(\bar x)
                        + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
                \right)
                    \\ &\quad 
                    + \mathbb E_k \left[
                        \frac{\alpha_k(\mu - \mu^{(I_k)})}{2} 
                    \right]\Vert \bar x - v_{k - 1}\Vert^2
                    + \mathbb E_k\left[
                        \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
                    \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2
                \\
                &\underset{\text{(2)}}{=}
                (1 - \alpha_k)\left(
                        \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
                        - F(\bar x)
                        + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
                \right)
                    \\ &\quad 
                    + \mathbb E_k\left[
                        \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
                    \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2
            \end{align*}
            }
            We note that at label (1), we used the fact that $\alpha_k$ is a constant and, $x_{k - 1}, v_{k - 1}$ only depends on random variable $I_0, I_1, \ldots, I_{k - 1}$ hence it falls out of the conditional expectation $\mathbb E_k$. 
            At label (2), we used assumption (Assumption \ref{ass:sum-of-many}) that the averages of all the $\mu^{(I_k)}$ on each $F_{I_k}$ equals to $\mu$ hence, the expectation evaluates to zero by linearity of the expected value operator. 
            \par
            Combining the above results on the expectation of RHS, and LHS of \eqref{ineq:snapg2-one-step-presult1}, we have the one-step inequality in expectation: 
            \begin{align*}
                & \mathbb E_k\left[F_{I_k}(x_{k})\right] 
                - F(\bar x) 
                + \mathbb E_k\left[
                    \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                \right]
                \\
                &\le 
                (1 - \alpha_k)\left(
                        \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
                        - F(\bar x)
                        + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
                \right)
                    \\ &\quad 
                    + \mathbb E_k\left[
                        \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
                    \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
            \end{align*}
            Finally, we show the base case. 
            When $k = 0$, by assumption it had $\alpha_0 = 1$ hence $\tau_0$ in Definition \ref{def:snapg-v2} has $\tau_0 = 0$ which makes $y_0 = v_{- 1} = x_{-1}$. 
            Therefore, it makes $x_0 = T_{L_0}(y_0 | F_{I_0}) = T_{L_0}(v_{-1} | F_{I_0})$. 
            Similarly, it has also $z_0 = \bar x$.
            Applying Theorem \ref{thm:pg-ineq} with $z = z_0$ and, assume a successful line search with $L_0$, it yields: 
            \begin{align*}
                0 &\le F_{I_0}(z_0) - F_{I_0}(x_0) - \frac{L_0}{2}\Vert z_0 - x_0\Vert^2 + \frac{L_0 - \mu^{(I_0)}}{2}\Vert z_0 - y_0\Vert^2
                \\
                &= F_{I_0}(\bar x) - F_{I_0}(x_0) - \frac{L_0}{2}\Vert \bar x - x_0\Vert^2 + \frac{L_0 - \mu^{(I_0)}}{2}\Vert \bar x - v_{-1}\Vert^2. 
            \end{align*}
            Re-arranging and taking the expectation it yields: 
            \begin{align*}
                \mathbb E \left[
                    F_{I_0}(x_0) - F_{I_0}(\bar x) + \frac{L_0}{2}\Vert \bar x - x_0\Vert^2
                \right]
                &\underset{\text{(h)}}{=}
                \mathbb E \left[ F_{I_0}\right] - F(\bar x) 
                + \frac{L_0}{2}\mathbb E \left[\Vert \bar x - x_0\Vert^2\right]
                \\
                &\le \frac{L_0 - \mathbb E \left[\mu^{(I_0)}\right]}{2}\Vert \bar x - v_{-1}\Vert^2
                \\
                &= \frac{L_0 - \mu}{2}\Vert \bar x - v_{-1}\Vert^2. 
            \end{align*}
            % INTERMEDIATE RESULTS
            \begin{proof}[Proof of (f)]
                The proof is direct algebra and, it has: 
                {\small\allowdisplaybreaks
                \begin{align*}
                    & \frac{\left(\mu^{(i)}\right)^2(1 - \alpha_k)^2}{2(L_k - \mu^{(i)})} 
                    - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}
                    \\
                    &= 
                    \frac{1}{2\left(L_k - \mu^{(i)}\right)}
                    \left(
                        \left(\mu^{(i)}\right)^2(1 - \alpha_k)^2
                        - \left(L_k - \mu^{(i)}\right)\mu^{(i)} \alpha_k(1 - \alpha_k)
                    \right)
                    \\
                    &= \frac{1 - \alpha_k}{2\left(L_k - \mu^{(i)}\right)}\left(
                        \left(\mu^{(i)}\right)^2 
                        - \left(\mu^{(i)}\right)^2\alpha_k 
                        - \left(L_k \mu^{(i)} \alpha_k - \left(\mu^{(i)}\right)^2 \alpha_k\right)
                    \right)
                    \\
                    &= 
                    \frac{1 - \alpha_k}{2(L_k - \mu)}\left(
                        \left(\mu^{(i)}\right)^2 - L_k\left(\mu^{(i)}\right)\alpha_k
                    \right)
                    \\
                    &= 
                    \frac{(1 - \alpha_k)\mu^{(i)}\left(\mu^{(i)} - L_k\alpha_k\right)}
                    {2\left(L_k - \mu^{(i)}\right)}
                    \\
                    &= \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}
                    {2\left(L_k - \mu^{(i)}\right)}. 
                \end{align*}
                }    
            \end{proof}
            % INTERMEDIATE RESULTS 
            \begin{proof}[Proof of (g)]
                From the property of the $\alpha_k$ sequence stated in item (c), we have: 
                {\allowdisplaybreaks
                \begin{align*}
                    &\frac{\left(
                        L_k\alpha_k - \mu^{(i)}
                    \right)^2}{2(L_k - \mu^{(i)})} 
                    -
                    \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} 
                    \\
                    &= 
                    \frac{\left(
                        L_k\alpha_k - \mu^{(i)}
                    \right)^2}{2(L_k - \mu^{(i)})} 
                    -
                    \frac{L_k\alpha_k(\alpha_k - \mu/L_k)}{2} 
                    \\
                    &=
                    \frac{\left(
                        L_k\alpha_k - \mu^{(i)}
                    \right)^2}{2(L_k - \mu^{(i)})} 
                    - \frac{L_k\alpha_k(\alpha_k - \mu^{(i)}/L_k)}{2}
                    + \frac{L_k\alpha_k(\alpha_k - \mu^{(i)}/L_k)}{2} 
                    - \frac{L_k\alpha_k(\alpha_k - \mu/L_k)}{2} 
                    \\
                    &= 
                    \frac{\left(
                        L_k\alpha_k - \mu^{(i)}
                    \right)^2}{2(L_k - \mu^{(i)})} 
                    - \frac{\alpha_k\left(L_k\alpha_k - \mu^{(i)}\right)}{2}
                    + \frac{L_k\alpha_k}{2}
                    \frac{\left(
                        \mu - \mu^{(i)}
                    \right)}{L_k}
                    \\
                    &=
                    \frac{L_k \alpha_k - \mu^{(i)}}{2(L_k - \mu^{(i)})}\left(
                        L_k \alpha_k - \mu^{(i)} 
                        - \left(L_k - \mu^{(i)}\right)\alpha_k
                    \right)
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2}
                    \\
                    &= \frac{L_k \alpha_k - \mu^{(i)}}{2(L_k - \mu^{(i)})}\left(
                        \mu^{(i)}\alpha_k - \mu^{(i)} 
                    \right)
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2}
                    \\
                    &= 
                    \frac{
                        \left(L_k \alpha_k - \mu^{(i)}\right)\mu^{(i)}
                        \left(\alpha_k - 1\right)
                    }
                    {2(L_k - \mu^{(i)})}
                    + \frac{\alpha_k(\mu - \mu^{(i)})}{2}. 
                \end{align*}   
                } 
            \end{proof}
        \end{proof}
    \subsection{\textcolor{purple}{So, what to do next?}}
        Hi Arron would you like to add me for the co-authorship to continue this line of work and see how Nesterov's Accelerated Technique may work out for the stochastic gradient method? 
        These results are solid results but, they are still partial results and, below are the potential I foresee for this these ideas. 
        \begin{enumerate}
            \item Narrow down the sequence $\alpha_k$ and make sure that it can allow the quantity: 
            \begin{align*}
                \mathbb E_k\left[
                        \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
                    \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2
            \end{align*}
            is negative, or at least bounded. I am not sure how this will work out, but I have some solid ideas around it. 
            \item Roll up the inequality in Theorem \ref{thm:snapg2-one-step} recursively and, determine the convergence rate through $\alpha_k$ that makes the previous item true. 
            In addition, I have the hunches that the convergence rate involves the variance of $\mu^{(I_k)}$ and, it will slower than the non-stochastic case of the algorithm. 
        \end{enumerate}
        For the future we can: 
        \begin{enumerate}
            \item Extend the definition of strong convexity to relative strong convexity with respect to a quasi-norm. This would extend interpolation hypothesis in Assumption \ref{ass:interp-hypothesis} where, even if $\mu > 0$, it doesn't mean that $F$ has a unique solution through strong convexity. This is entirely possible and appeared in the literatures before so, I can give you the words of confidence. 
            \item Show the convergence of the method for objective function based on quasi-strong convexity. This is a much weaker assumption it works well in practice for the common known problems in convex programming. 
        \end{enumerate}
    
\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}
