\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% special operators and stuff introduced for this file. 

\newcommand{\cov}{\ensuremath{\operatorname{Cov}}}
\newcommand{\var}{\ensuremath{\operatorname{Var}}}
\newcommand{\expect}{\ensuremath{\mathbb E}}
\newcommand{\prob}{\ensuremath{\mathbb P}}
\newcommand{\dist}{\ensuremath{\operatorname{dist}}}
\usepackage{tikz}\newcommand*\circled[1]{
    \text{
        \hspace{-0.5em}\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.6pt](char){#1};
        }\hspace{-0.5em}
    }
}

\begin{document}


\title{{\fontfamily{ptm}\selectfont Linear Convergence of Stochastic Nesterov's Accelerated Proximal Gradient method under Interpolation Hypothesis, Truth or Just a Dream?}}

\author{
    Author
    \thanks{
        University of British Columbia Okanagan,
        Canada. E-mail: \texttt{alto@mail.ubc.ca}.
    }
}

\date{\today}

\maketitle
\todoinline{This paper is currently in draft mode. Check source to change options. }
% \vskip 8mm

\begin{abstract} 
    This file is for communication purposes between collaborators. 
    In brief, we think that the conditions required for fast linear convergence rate, with a square root on the condition number of Stochastic Nesterov's accelerated gradient (or proximal gradient) is too precarious to hold, even with interpolation hypothesis. 
    Instead of attacking the problem head on, this file will characterize the conditions required for fast linear convergence rate. 
    We place specific constraints on the random variable representing the error made when estimating gradient via some random variables. 
\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}


% ==============================================================================
\section{Introduction}
    \cite{bauschke_convex_2017}
    Previously we got some results, but unfortunately it was incorrect and, it was impossible to recover from the mistake. 
    \par
    Does stochastic accelerated Nesterov's acceleration (SNAG) produces accelerated convergence rate (or, any type of convergence) when the Interpolation Hypothesis is true? 
    \textbf{I don't think that it's true after some mistakes from previous version of the notes and careful investigations.}
    In this file we develop some sufficient conditions for Linear convergence of (SNAG). 
    We will give explanations on why we don't think this is necessarily true. 
    \par
    When we use stochastic gradient to approximate the true graduate, it has an error. 
    Fix some $x\in \RR^n$, let $\tilde \nabla f(x)$ be an estimate of $\nabla f(x)$, the error we consider is $\expect \Vert \nabla f(x) - \nabla f(x)\Vert$. 
    To make the algebra simpler, we assume that the algorithm produced the next iterates $\tilde x$ by a step of gradient descent, and the error of the expectation satisfies a relative error conditions of the form 
    \begin{align*}
        \frac{
            \expect \left[
                \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
            \right]
            }{
            \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
            \right]
            } &= \epsilon. 
    \end{align*}
    Where the variable $z$ will be explained later. 
    We will show that, the value of $\epsilon$ must decreases at a rate convergence relative to the Nesterov's accelerated sequence, under the standard Framework of analysis similar to what is in the literature. 
    Take note that usually in the literature, people analyze the quantity $\expect\left\Vert \tilde \nabla f(x) - \tilde \nabla f(y)\right\Vert^2$ for stochastic gradient type  of method. 
    The above expression is drastically different from what we usually have in the literature. 

\section{In preparations}
    Unless specifically specified in the context, we use the following notations. 
    $\Pi_C$ denotes the projection onto a set $C$. 
    Let $A \in \RR^{m \times n}$ be a matrix. 
    $\sigma_{\min}(A)$ denotes the smallest non-zero absolute value of all singular values of $A$. 
    Let $\Vert A\Vert$ denotes the spectral norm of the matrix $A$. 
    $I$ denotes the identity operator. 
    \par
    When two expressions are connected via non-trivial results, it's expressed with $\underset{(\cdot)}{=}, \underset{(\cdot)}{\ge}$ where $(\cdot)$ is a label of some intermediate results immediately before it, or explained right after a chain of expressions. 
    If the label is letter, like: (a), (b), ..., then they are stated in advanced at the start of the proof and, they are usually non-trivial results. 
    These labels are reused in every proof. 
    If the label is circled numbers, like: \circled{1}, \circled{2},... they are explained right after the chain of relations, and they are often reused right after their explanations. 
    \subsection{Basic definitions}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Proximal gradient operator]\label{def:pg-opt}
            Suppose $F = f + g$ with $\reli(\dom f) \cap \reli(\dom g) \neq \emptyset$, and $f$ is a differentiable function. 
            Let $\beta > 0$. 
            Then, we define the proximal gradient operator $T_{\beta}$ as 
            \begin{align*}
                T_\beta (x | F) &= \argmin_{z} \left\lbrace
                    g(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
                \right\rbrace. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If the function $g \equiv 0$, then it yields the gradient descent operator $T_\beta(x) = x - \beta^{-1}\nabla f(x)$. 
            In the context where it's clear what the function $F = f + g$ is, we simply write $T_\beta(x)$ for short. 
            Note, it also has $T_\beta(x | f + g) = \hprox_{\beta^{-1}g}(x - \beta^{-1}\nabla f(x))$ in optimization literatures. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Bregman Divergence]
            Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
            Then, for all the Bregman divergence $D_f: \RR^n \times \dom\nabla f \rightarrow \RR$ is defined as: 
            \begin{align*}
                D_f(x, y) := f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If, $f$ is $\mu \ge 0$ strongly convex and $L$ Lipschitz smooth then, its Bregman Divergence has for all $x, y \in \RR^n$: $\mu/2 \Vert x - y\Vert^2 \le D_f(x, y) \le L/2 \Vert x - y\Vert^2$. 
            We note that usually the Bregman Divergence is used with a Legendre function, but in here, we do not assume that $f$ has to be Legendre. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Lipschitz smoothness and strongly convex]\label{def:lip-smooth-and-scnvx}
            A differentiable function $f: \RR^n \rightarrow \RR$ is $L$ lipschitz smooth and, $\mu$ strong convex for some $L > \mu \ge 0$ if and only if for all $x, y \in \RR^n$ it satisfies the inequality 
            \begin{align*}
                \frac{\mu}{2}\Vert x - y\Vert^2 \le D_f(x, y) \le \frac{L}{2}\Vert x - y\Vert^2. 
            \end{align*}
        \end{definition}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Relative proximal gradient error ruler]\label{def:pg-err-ruler}
            Let $F$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
            Fix any $x \in \RR^n$, suppose that $\tilde x$ is an estimated of $T_B(x|F)$. 
            Then the relative proximal gradient error is a set defined as
            \begin{align*}
                S_B(\tilde x, x | F) := 
                \partial \left[
                    z \mapsto \partial g(z) + \langle \nabla f(x), z - x\rangle + 
                    \frac{B}{2}\Vert x - z\Vert^2
                \right](\tilde x). 
            \end{align*}
        \end{definition}
        \begin{remark}
            The definition exists to simplifies the notations for the discussions. 
            When $B > 0$ by strong convexity, the set $\{z : \mathbf 0 \in S_B(z, x | F)\}$ is a singleton, conveniently. 
        \end{remark}
    \subsection{Important inequalities}
        \begin{assumption}\label{ass:smooth-plus-nonsmooth}
            Suppose that $F = f + g$ where $f, g$ are both convex, proper and closed. 
            In addition, assume $f$ is $L > \mu \ge 0$ Lipschitz smooth and strongly convex satisfying Definition \ref{def:lip-smooth-and-scnvx}. 
        \end{assumption}
        % --------------------------------------------------------------------------------------------------------------
        \begin{theorem}[Jensen's inequality]\label{thm:jesen}
            Let $F: \RR^n \rightarrow \overline \RR$ be a $\mu \ge 0$ strongly convex function. 
            Then, it is equivalent to the following condition. 
            For all $x, y \in \RR^n$, $\lambda \in (0, 1)$ it satisfies the inequality 
            \begin{align*}
                (\forall \lambda \in [0, 1])\; 
                F(\lambda x + (1 - \lambda)y) \le \lambda F(x) + (1 - \lambda)F(y) -\frac{\mu\lambda(1 - \lambda)}{2} \Vert y - x\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{remark}
            If $x, y$ is out of $\dom F$, the inequality still work by convexity. 
        \end{remark}
        \begin{lemma}[inexact proximal gradient inequality prototype]\label{lemma:inex-pg-ineq-proto}
            Let $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
            Let $x \in \RR^n$, and let $\tilde x$ be an estimate of $T_B(x | F)$ such that $D_f(\tilde x, x) \le B/2\Vert x - \tilde x\Vert^2$, where $B \ge 0$. 
            Let $S_B(\tilde x, x | F)$ be given by Definition \ref{def:pg-err-ruler}. 
            Then, for all $z \in \RR^n$ and, any $w \in S_B(\tilde x, x | F)$ it satisfies: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - \langle w, z - \tilde x \rangle. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Since $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}, for all $B \ge L$, it will be an obvious choice. 
            The proof is direct algebra. 
            Let $h = z \mapsto g(z) + \langle \nabla f(x), z - x\rangle + B/2\Vert z - x\Vert^2$. 
            $h$ is a $B$ strongly convex function, using the subgradient inequality of a strongly convex function it has for all $z \in \RR^n$: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2
                &\le h(z) - h(\tilde x) - \langle w, z - \tilde x\rangle
                \\
                &= 
                \left(
                    g(z) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right)
                \\ &\quad 
                    - 
                    \left(
                        g(\tilde x) + \langle \nabla f(x), \tilde x - x\rangle + \frac{B}{2}\Vert \tilde x - x\Vert^2
                    \right)
                    - \langle w, z - \tilde x\rangle
                \\
                &= \left(
                    g(z) + f(z) - f(z) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right)
                \\ &\quad 
                    - 
                    \left(
                        g(\tilde x) + f(\tilde x) - f(\tilde x) 
                        + \langle \nabla f(x), \tilde x - x\rangle + \frac{B}{2}\Vert \tilde x - x\Vert^2
                    \right)
                    - \langle w, z - \tilde x\rangle
                \\
                &= \left(
                    F(z) - D_f(z, x) + \frac{B}{2}\Vert z - x\Vert^2
                \right)
                \\ &\quad 
                    - 
                    \left(
                        F(\tilde x) - D_f(\tilde x, x) + \frac{B}{2}\Vert \tilde x - x\Vert^2
                    \right)
                    - \langle w, z - \tilde x\rangle
                \\
                &\underset{\text{\circled{1}}}{\le} F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - 0 - \langle w, z - \tilde x \rangle
            \end{align*}
            At \circled{1}, we used the fact that $f$ is $L > \mu \ge 0$ Lipschitz smooth and strongly convex therefore it has for all $y \in \RR^n$: 
            \begin{align*}
                0 
                \le \frac{L}{2}\Vert z - y\Vert^2  - D_f(z, y)
                \le \frac{L - \mu}{2}\Vert z - y\Vert^2. 
            \end{align*}
        \end{proof}
        % --------------------------------------------------------------------------------------------------------------
        \begin{lemma}[inexact proximal gradient inequality]\label{lemma:inex-pg-ineq}
            Let $F = f + g$ satisfies Definition \ref{def:lip-smooth-and-scnvx} with $L > \mu \ge 0$. 
            Fix arbitrary $x \in \RR^n$. 
            Assume the followings: 
            \begin{enumerate}[nosep]
                \item $\tilde x$ estimates $T_B(x | F)$. 
                \item $B \ge 0$ satisfies $D_f(\tilde x, x) \le B/2\Vert x - \tilde x\Vert^2$. 
                \item Let $S_B(\tilde x, x | F)$ be given by Definition \ref{def:pg-err-ruler}. 
            \end{enumerate}
            Let $\epsilon \ge 0$ be such that $\Vert x - \tilde x\Vert\epsilon \ge \dist(\mathbf 0 |S_B(\tilde x, x | F))$.
            Then, for all $z\in \RR^n$ it satisfies the inequality: 
            \begin{align*}
                0 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\Vert z - \tilde x\Vert^2
                + \frac{\epsilon}{2}\Vert x - \tilde x \Vert^2. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            The error $w$ satisfies Lemma \ref{lemma:inex-pg-ineq-proto} hence, it has for all $z \in \RR^n$ the inequality: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2
                &\le F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - 0 - \langle w, z - \tilde x \rangle
                \\
                &\underset{\text{\circled{1}}}{\le}
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \Vert w\Vert \Vert z - \tilde x\Vert. 
            \end{align*}
            At \circled{1}, we used Cauchy inequality. 
            Since this is true for all $w \in S_B(\tilde x, x | F)$, it has: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2
                &\le F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \dist(\mathbf 0 | S_B(\tilde x, x | F))\Vert z - \tilde x\Vert
                \\
                &\le
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 + \epsilon \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert. 
            \end{align*}
            Continuing it has 
            \begin{align*}
                0 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                + \epsilon \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert 
                - \frac{B}{2}\Vert z - \tilde x\Vert^2
                    \\&\quad 
                    - \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 - \frac{\epsilon}{2}\Vert z - \tilde x \Vert^2
                    + \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 + \frac{\epsilon}{2}\Vert z - \tilde x \Vert^2
                \\
                &= F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                - \frac{1}{2}\left(
                    \sqrt{\epsilon}\Vert z - \tilde x\Vert - \sqrt{\epsilon}\Vert x - \tilde x\Vert
                \right)^2
                - \frac{B}{2}\Vert z - \tilde x\Vert^2
                    \\
                    &\quad 
                    + \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 + \frac{\epsilon}{2}\Vert z - \tilde x \Vert^2
                \\
                &= F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\Vert z - \tilde x\Vert^2
                + \frac{\epsilon}{2}\Vert x - \tilde x \Vert^2. 
            \end{align*}
        \end{proof}
        \begin{remark}
            Usually in practice, the precise value of $F(\tilde x)$ is never known and, function value is also a random variable, therefore, $B$ cannot be easily determined via $D_f(\tilde x, x)$. 
            In that case we can only choose some $B \ge L$ which gives: 
            \begin{align*}
                0 &\le 
                F(z) - F(\tilde x) + \frac{L - \mu}{2}\Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\Vert z - \tilde x\Vert^2
                + \frac{\epsilon}{2}\Vert x - \tilde x \Vert^2.
            \end{align*}
            The smallest possible choice for $\epsilon$ when $x \neq \tilde x$ is $\epsilon = \Vert w\Vert/\Vert x - \tilde x\Vert$ and, if $x = \tilde x$ then $\epsilon = 0$ is the smallest. 
        \end{remark}
        Note, an inexact evaluation of the proximal gradient operator can be caused by an inexact gradient on the smooth part. 
        Suppose that one take $\tilde \nabla f(x)$ to be an estimate of $\nabla f(x)$ and use it for the proximal gradient operator to produce $\tilde x$, then: 
        \begin{align}
            \mathbf 0 
            &\in \partial g(\tilde x) + \tilde \nabla f(x) + B(\tilde x - x)
            \\
            &= 
            \partial g(\tilde x) + \tilde\nabla f(x) - \nabla f(x) 
            + \nabla f(x) + B(\tilde x - x)
            \\
            \iff &
            \nabla f(x) - \tilde \nabla f(x) \in 
            \partial g(\tilde x) 
            + \nabla f(x) + B(\tilde x - x).\label{eqn:stoch-grad-err-vec}
        \end{align}
        In this case, it adds the interpretation that $w = \nabla f(x) - \tilde \nabla f(x)$. 
        It fully characterizes the error made to estimate the true gradient $\nabla f(x)$. 
        In that case, we have the equation: 
        \begin{align*}
            \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert x - \tilde x\Vert
            = \epsilon \Vert x - \tilde x\Vert^2. 
        \end{align*}
        It's very unclear what LHS really is without additional details and assumptions. 
        \textbf{We very much would like $\epsilon$ to be a constant instead of a random variable to make the algebra possible when deriving the convergence rate of the algorithm. }
        \par
        The following lemma gives a proximal gradient inequality when $\tilde \nabla f(x)$ is an estimate by some random variable, \textbf{and it is the precursor that gives the analysis of our convergence rate.} 
        \begin{lemma}[proximal stochastic gradient inequality]\label{lemma:pg-stoch-ineq}
            Let $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
            Fix any $x, z \in \RR^n$. 
            Assume the following
            \begin{enumerate}[nosep]
                \item $\tilde \nabla f(x)$ is a random variable which estimates $\nabla f(x)$, which produces the estimate $\tilde x$. 
                \item There exists $B \ge 0$ such that $D_f(\tilde x,x)\le B/2 \Vert x - \tilde x\Vert^2$. 
            \end{enumerate}
            If in addition, there exists some $\epsilon \ge 0$: 
            \begin{align*}
                \expect \left[
                    \left\Vert \nabla f(x) - \tilde \nabla f(x)\right\Vert 
                    \Vert z - \tilde x\Vert
                \right] &\le 
                \epsilon \expect \left[
                    \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
                \right]. 
            \end{align*}
            Then it has: 
            \begin{align*}
                0 &\le 
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2} \Vert z - x\Vert^2
                - \frac{B - \epsilon}{2}\expect\left[\Vert z - \tilde x\Vert^2\right]
                + \frac{\epsilon}{2} \expect\left[\Vert x - \tilde x\Vert^2\right]. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            The can choose $w = \nabla f(x) - \tilde \nabla f(x) \in S_B(\tilde x, x | F)$ which is explained in \eqref{eqn:stoch-grad-err-vec}. 
            Using Lemma \ref{lemma:inex-pg-ineq-proto}, for any fixed $z$ it has: 
            \begin{align*}
                \frac{B}{2}\Vert z - \tilde x\Vert^2 &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 - \langle w, z - \tilde x\rangle
                \\
                &\le 
                F(z) - F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 + \Vert w\Vert\Vert z - \tilde x\Vert
                \\
                &= 
                F(z) - F(\tilde x) 
                + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \left\Vert \nabla f(x) - \tilde \nabla f(x)\right\Vert\Vert z - \tilde x\Vert. 
            \end{align*}
            Take note that, since $w = \nabla f(x) - \tilde \nabla f(x)$ is a random variable, it determines that $\tilde x$ is also a random variable related to $w$. 
            Here, $x, z$ is not a random variable. 
            We take the expectation on both sides and move things all to the RHS then it has 
            \begin{align*}
                0&\le 
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \expect\left[
                    \Vert w\Vert\Vert z - \tilde x\Vert
                \right]
                - \frac{B}{2}\expect \Vert z - \tilde x\Vert^2
                \\
                &\le F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \epsilon \expect \left[
                    \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
                \right]
                - \frac{B}{2}\expect \Vert z - \tilde x\Vert^2
                \\
                &= 
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \expect\left[
                    \epsilon\Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
                    - \frac{B}{2} \Vert z - \tilde x\Vert^2
                \right]
                \\
                &=
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                    \\ &\quad 
                    + \expect\left[
                        - \frac{1}{2}\left(
                            \sqrt{\epsilon}\Vert x - \tilde x\Vert - \sqrt{\epsilon}\Vert z - \tilde x\Vert
                        \right)^2
                        + \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2 + \frac{\epsilon}{2}\Vert z - \tilde x\Vert^2
                        - \frac{B}{2} \Vert z - \tilde x\Vert^2
                    \right]
                \\
                &\le 
                F(z) - \expect F(\tilde x) + \frac{\max(B, L) - \mu}{2}\Vert z - x\Vert^2 
                + \expect\left[
                    \frac{\epsilon}{2}\Vert x - \tilde x\Vert^2
                    - \frac{B - \epsilon}{2} \Vert z - \tilde x\Vert^2
                \right]. 
            \end{align*}
        \end{proof}
    \subsection{discussion}
        In practice, is chosen in prior to satisfies $B \ge L$. 
        In here, $z, x$ is not a random variable, $\epsilon$ just a constant, but it's determined by $z$ and $x$. 
        Assuming $z \neq \tilde x$ and, $\tilde x \neq x$, then one of the smallest choice for it in this lemma is
        \begin{align}\label{eqn:stoch-grad-epsilon}
            \frac{
                \expect \left[
                    \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
                \right]
            }{
            \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
            \right]
            } &= \epsilon. 
        \end{align}
        It depends on both $z$ and $x$. 
        Let's think about the edge case. 
        When $\expect [\Vert x - \tilde x\Vert\Vert z - \tilde x\Vert] = 0$, it must be that both $\Vert x - \tilde x\Vert,\Vert z - \tilde x\Vert$ are zero, indicating that $\nabla f(x) = \tilde \nabla f(x)$ and, $x = \tilde x = z$. 
        Otherwise, we can assume $\prob(\Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0) > 0$ and, the expectation has: 
        \begin{align*}
            & \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
            \right] 
            \\&= 
            \expect\left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert \;|\; 
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
            \right]\prob(
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
            ). 
        \end{align*}
        Let's suppose that $\tilde \nabla f(x)$ is a random variable comes from the space: $\Omega(x)$, then it has the following: 
        {\small
        \begin{align*}
            & \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert \; | \; 
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
            \right] 
            \\
            &\ge
            \min_{y \in \Omega(x)}\left\lbrace
                \Vert x - \tilde x\Vert
                :x\neq \tilde x = \argmin_{z}\left\lbrace
                    g(z) + \langle y, z\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right\rbrace
            \right\rbrace
            \prob(
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
            )
            \expect\Vert z - \tilde x\Vert, 
            \\
            & \expect \left[
                \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
            \right]
            \\
            &\le 
            \max_{y \in \Omega(x)}\left\lbrace
                \Vert \nabla f(x) - y\Vert 
            \right\rbrace\expect\Vert z - \tilde x \Vert, 
        \end{align*}
        }
        And it would mean: 
        {\small
        \begin{align*}
            \epsilon &= 
            \frac{
                \expect \left[
                    \left\Vert  \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
                \right]
            }{
            \expect \left[
                \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
            \right]
            }\\
            &\le \frac{
                \max_{y \in \Omega(x)}\Vert \nabla f(x) - y\Vert 
            }{
                \min_{y \in \Omega(x)}\left\lbrace
                    \Vert x - \tilde x\Vert:
                    x\neq \tilde x = \argmin_{z}\left\lbrace
                    g(z) + \langle y, z\rangle + \frac{B}{2}\Vert z - x\Vert^2
                \right\rbrace
                \right\rbrace
                \prob(
                    \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert > 0
                )
            }. 
        \end{align*}
        }
        \par
        Of course, look, if there is no random variable and $\tilde \nabla f(x)$ is simply not a probabilistic estimate then the expectation is gone and $x\neq \tilde x$, it has: 
        \begin{align*}
            \epsilon = \frac{\left\Vert
                \nabla f(x) - \tilde \nabla f(x)
            \right\Vert}{\Vert x - \tilde x\Vert}.
        \end{align*}
        And in this case, it has nothing to do with $z$. 
        \par
        \textbf{Ok, what about the relation between this error term \eqref{eqn:stoch-grad-epsilon} and the strong growth conditions that is usually found in the literature?}
        There is no direct equality relationship between the Strong Growth condition of the stochastic gradient which is usually used in the literature, with the condition we have here. 
        Nonetheless, an indirect inequality relation exists. 
        Suppose that the gradient of $f$ satisfies strong growth condition with constant $\rho$, so it has $\expect \left\Vert \tilde \nabla f(x)\right\Vert^2 \le \rho \Vert \nabla f(x)\Vert^2$, then $\left\Vert \nabla f(x) - \tilde \nabla f(x)\right\Vert$ squared has: 
        \begin{align*}
            & \left(
                \expect \left \Vert \nabla f(x) - \tilde \nabla f(x)\right\Vert
            \right)^2
            \\
            &\le 
            \expect \left \Vert \nabla f(x) - \tilde \nabla f(x)\right\Vert^2 
            \\
            &= \expect \left\Vert \expect \tilde \nabla f(x) - \tilde \nabla f(x) \right\Vert^2 
            \\
            &= \expect \left\Vert \tilde \nabla f(x)\right\Vert^2 
            - \Vert \nabla f(x)\Vert^2 
            \le (\rho - 1)\Vert \nabla f(x)\Vert^2. 
        \end{align*}
        The last two equalities, we used the $\expect (X - \expect X)^2 = \expect X^2 - (\expect X)^2$ with $X$ being each of the element of the vector and, the linearity of expectations to apply it individually and them sum it up. 
        The first inequality also uses that equality but $X = \left\Vert \nabla f(x) - \tilde \nabla f(x)\right\Vert$ instead, and $\expect (X - \expect X)^2 \ge 0$ always. 
        The last inequality comes from the assumed strong-growth conditions. 
        Strong growth condition does make $\epsilon$ smaller if $x$ is close to the set of minimizer and this is the result: 
        \begin{align*}
            \expect \left\Vert
                \nabla f(x) - \tilde \nabla f(x)
            \right\Vert &\le 
            \sqrt{\rho - 1} \Vert \nabla f(x)\Vert. 
        \end{align*}
        \par
        Keeping the strong growth condition, let's assume that $\tilde x$ is generated by stochastic gradient, so that the proximal operator has $g \equiv 0$, then $\tilde x = x - L^{-1}\tilde \nabla f (x)$ directly. 
        Assuming that relation $m \le \Vert z - \tilde x\Vert \le M$ exists then \eqref{eqn:stoch-grad-epsilon} has upper bound
        \begin{align*}
            & \frac{M}{m} \frac{\expect\left\Vert \nabla f(x) - \tilde \nabla f(x)\right\Vert}{\expect \Vert x - \tilde x\Vert}
            \\
            &\le \frac{M}{m}\frac{\sqrt{\rho - 1}\Vert \nabla f(x)\Vert}{L^{-1}\expect \left\Vert\tilde \nabla f(x)\right\Vert}
            \\
            &\le \frac{LM}{m}
            \frac{\sqrt{\rho - 1}\Vert \nabla f(x)\Vert}{
                \min_{y \in \Omega(x)}\left\{
                    \left\Vert y \right\Vert: y\neq \mathbf 0
                \right\}\prob\left(\left\Vert\tilde \nabla f(x)\right\Vert \neq 0\right)
            }. 
        \end{align*}
        \todoinline{NOT FINISHED YET. It seems like no pretty relaton exists. }


\section{Stochastic/Inexact accelerated proximal gradient algorithm}
    The following defines the inexact proximal gradient operator where the gradient of the smooth part of the function is estimated. 
    All algorithms satisfying the following definition will be referred to as Stochastic Nesterov's Accelerated Gradient (SNAG). 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[proximal inexact gradient operator with relative error]
        Let $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}, let $x \in \RR^n$ be fixed. 
        Suppose that $\tilde \nabla f(x)$ estimates $\nabla f(x)$. 
        We define the inexact proximal gradient operator by the relationships between: 
        \begin{enumerate}[nosep]
            \item $\tilde x = \mathbf T_B(x | F)$ is an inexact output of proximal gradient operator by evaluating on $\tilde \nabla f(x)$. 
            \item $B \ge 0$ is any constant such that it satisfies $D_f(\tilde x, x) \le B/2\Vert \tilde x - x\Vert^2$. 
            \item $\epsilon \ge 0$ is a constant that quantifies the error of inexact evaluation. 
        \end{enumerate}
        Then, we define the relative error $\epsilon$ by: 
        \begin{align*}
            \epsilon = \begin{cases}
                \frac{\left\Vert \tilde \nabla f(x) - \nabla f(x)\right\Vert}{\Vert x - \tilde x\Vert} & \text{if } x \neq \tilde x, 
                \\
                \infty & \text{if } x = \tilde x, \nabla f(x) \neq \tilde \nabla f(x), 
                \\
                0 & \text{if } x = \tilde x, \nabla f(x) = \tilde \nabla f(x). 
            \end{cases}
        \end{align*}
        And the inexact output is defined as: 
        \begin{align*}
            \tilde x = \mathbf T_B(x | F) = \argmin_{z \in \RR^n}\left\lbrace
                g(z) + \left\langle \tilde \nabla f(x), z - x\right\rangle
                + \frac{B}{2}\Vert z - x\Vert^2
            \right\rbrace. 
        \end{align*}
    \end{definition}
    The inexact evaluation can be caused by a random variable. 
    The definition that follows characterize algorithm in which the errors are related to a random variable that estimates the gradient of the objective function. 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[stochastic proximal gradient operator with relative error]\label{def:stoch-pg-opt-rel-err}
        Let $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
        Let $x \in \RR^n$ be fixed. 
        Suppose that $\tilde \nabla f(x)$ is random variable and, it estimates $\nabla f(x)$. 
        Then stochastic proximal gradient operator are the relationships between
        \begin{enumerate}[nosep]
            \item $\tilde x= \mathbf{\widetilde T}_B(x| F)$, an inexact output of proximal gradient operator by evaluating on $\tilde \nabla f(x)$. 
            \item Any $B \ge 0$ such that it satisfies $D_f(\tilde x, x) \le B/2\Vert x - \tilde x\Vert^2$. 
            \item $\epsilon$ is an relative error, determined by $z, x, \tilde\nabla f(x)$ and, defined immediately below. 
        \end{enumerate}
        The relative error $\epsilon$ is defined as: 
        \begin{align*}
            \epsilon 
            &= 
            \begin{cases}
                \frac{
                \expect \left[
                    \left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert
                \right]
                }{\expect \left[
                    \Vert x - \tilde x\Vert\Vert z - \tilde x\Vert
                \right]} 
                & \text{if } \expect\left[\Vert x - \tilde x\Vert\Vert z - \tilde x\Vert\right] \neq 0, 
                \vspace{0.5em}
                \\
                \left.\begin{cases}
                    0 & \expect \left[\left\Vert \nabla f(x) - \tilde \nabla f(x) \right\Vert \Vert z - \tilde x\Vert\right] = 0, 
                    \\
                    \infty & \text{else. } 
                \end{cases}\right\rbrace
                & \text{else. }
            \end{cases}
        \end{align*}
        Then the inexact proximal gradient operator with relative error $\epsilon$ is the random variable defined as: 
        \begin{align*}
            \tilde x = \mathbf{\widetilde T}_B(x| F) = \argmin_{z \in \RR^n}\left\lbrace
                g(z) + \left\langle \tilde \nabla f(x), z - x\right\rangle
                + \frac{B}{2}\Vert z - x\Vert^2
            \right\rbrace. 
        \end{align*}
    \end{definition}
    \begin{remark}
        For this definition of the stochastic proximal gradient operator, Lemma \ref{lemma:pg-stoch-ineq} is applicable. 
    \end{remark}
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[inexact/stochastic SNAG]\label{def:SNAG}
        Suppose that $F = f + g$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
        Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_k \in (0, 1]$. 
        Let $(\epsilon_k)_{k \ge 0}$ be a sequence of errors. 
        Given initial conditions $v_{-1}, x_{- 1}$. 
        An algorithm satisfying the SNAG definition if it generates a sequence $(y_k, x_k, v_k)_{k \ge 0}$ if for all $k \ge 0$, the following conditions are satisfied: 
        \begin{align*}
            & \tau_k = L(1 - \alpha_k)\left(L \alpha_k - \mu\right)^{-1}, \\
            & y_k = (1 + \tau_k)^{-1}v_{k - 1} + \tau_k(1 + \tau_k)^{-1}x_{k - 1}, \\
            & x_k =  \mathbf T_{L}(y_k | F) \text{ or }, \mathbf{\widetilde T}_L(y_k | F)\\
            & v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}).
        \end{align*}
    \end{definition}
    The following definition gives a momentum sequence where it makes the derivation of the convergence rate easier. 
    The sequence is powerful, instead of an equality relation, the sequence can instead satisfies $\alpha_k \in (\mu/L, 1)$. 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{definition}[relaxed momentum sequence]\label{def:relax-momen-seq}
        Let $(\alpha_k)_{k \ge 0}$ be non-negative sequence. 
        Let $L, \mu$ be some constant such that $L > \mu \ge 0$. 
        It is a relaxed momentum sequence if the following conditions are satisfied: 
        \begin{enumerate}[nosep]
            \item $\alpha_0 \in (0, 1]$ and for all $k \ge 1$, it satisfies that $\alpha_k \in (\mu/L, 1)$. 
        \end{enumerate}
    \end{definition}
    \begin{remark}
        In the context of its usage, the constants $L, \mu$ are the Lipschitz smoothness constant  and, strong convexity constant associated with a smooth and strongly convex function. 
    \end{remark}
    The following lemma defines the conditions required for the momentum sequence to be compatible with convergence claims. 
    % ------------------------------------------------------------------------------------------------------------------
    \begin{lemma}[relaxed momentum sequence conditions]\label{lemma:seq-properties}
        Let $(\alpha_k)_{k \ge 0}$, $L, \mu$ to be two constant such that it has $L > \mu \ge 0$. 
        If we assume that the sequence has for all $k \ge 0: \alpha_k \ge \mu/L$, then we can define a positive sequence 
        \begin{align*}
            (\forall k \ge 1): \rho_{k - 1} = \frac{\alpha_k(\alpha_k - \mu/L)}{(1 - \alpha_k)\alpha_{k - 1}^2}. 
        \end{align*}
        And under this relationship, the followings are true inductively: 
        \begin{enumerate}[nosep]
            \item In general, for all $\rho_{k - 1} \ge 0$, it has $0\le \alpha_k \le \min\left(1, |\rho\alpha^2 - q| + \sqrt{\rho}\alpha\right)$. 
            \item If $\alpha_{k - 1} \ge \mu/L$, then $\alpha_k \ge \mu/L$ too. 
            \item If $\alpha_{k - 1} > \mu/L > 0$, then $\alpha_k \in (\mu/L, 1)$ for all $\rho_{k - 1} > 0$. 
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        \textbf{Proof of item (i), (iii)}. 
        With $\rho_{k - 1}(1 - \alpha_k)\alpha_{k - 1}^2 = \alpha_k(\alpha_k - \mu/L)$ it gives: 
        \begin{align*}
            \alpha_k 
            &= \frac{1}{2}\left(
                \frac{\mu}{L} - \rho_{k - 1}\alpha_{k - 1}^2
                + \sqrt{
                    \left(
                        \rho_{k - 1}\alpha_{k - 1}^2 - \frac{\mu}{L}
                    \right)^2 
                    + 4\rho_{k - 1}\alpha_{k - 1}^2
                }
            \right)
            > 0. 
        \end{align*}
        Focusing exclusively on the RHS, we omit the subscript and write $\alpha_{k - 1}, \rho_{k - 1}$ as $\alpha, \rho$. 
        We also just write $q = \mu / L$. 
        By definition, we have $L > \mu \ge 0$ hence, $q \in [0, 1)$.
        We also note that the quantity $(r \alpha^2 + q)^2 + 4\rho \alpha$ is clearly $> 0$. 
        We will show that there is an upper bound for the RHS in terms of $\rho, q$. 
        Completing the square should give
        \begin{align*}
            0 &\le 
            \left(
                \rho \alpha^2 - q
            \right)^2 + 4 \rho \alpha^2
            \\
            &= \rho^2\alpha^4 + q^2 - 2\rho\alpha^2q + 4 \rho \alpha^2
            \\
            &= \rho^2\alpha^4 + 2\rho(2 - q)\alpha^2 + q^2
            \\
            &= 
            \rho^2\alpha^4 + 2\rho(2 - q)\alpha^2 
            + \rho^2(2 - q)^2
            - \rho^2(2 - q)^2
            + q^2
            \\
            &= (\rho\alpha^2 + \rho(2 - q))^2 - \rho^2 (2 - q)^2 + q^2
            \\
            &= \rho^2(\alpha^2 + 2 - q)^2 
            + q^2 
            - \rho^2 (2 - q)^2
            \\
            &\le 
            \rho^2(\alpha^2 + 2 - q)^2 
            + \max(0, q^2 - \rho^2 (2 - q)^2). 
        \end{align*}
        The above is non-negative, taking the square root it has: 
        \begin{align*}
            \sqrt{\left(\rho \alpha^2 - q\right)^2 + 4 \rho \alpha^2} 
            &\le 
            \rho|\alpha^2 + 2 - q| 
            + \sqrt{\max(0, q^2 - \rho^2 (2 - q)^2)}
            \\
            &\le 
            \rho|\alpha^2 + 2 - q| 
        \end{align*}
        But look, if we directly apply the $\sqrt{a + b} \le \sqrt{a} + \sqrt{b}$ it has another upper bound which is: 
        \begin{align}\label{ineq:seq-properties-pr3}
            \sqrt{\left(\rho \alpha^2 - q\right)^2 + 4 \rho \alpha^2} 
            &\le 
            |\rho\alpha^2 - q| + 2\sqrt{\rho}\alpha. 
        \end{align}
        Both upper bounds apply and hence we have: 
        \begin{align*}
            \alpha_k 
            &\le
            \frac{1}{2}\left(
                q - \rho\alpha^2 + \min\left(
                    \rho|\alpha^2 + 2 - q|  , 
                    |\rho\alpha^2 - q| + 2\sqrt{\rho}\alpha
                \right)
            \right)
            \\
            &= 
            \frac{1}{2}\left(
                q - \rho\alpha^2 + \min\left(
                    \rho\alpha^2 + (2 - q)  , 
                    |\rho\alpha^2 - q| + 2\sqrt{\rho}\alpha
                \right)
            \right)
            \\
            &= 
            \frac{1}{2}\left(
                \min\left(
                    2, 
                    |\rho\alpha^2 - q| + 2\sqrt{\rho}\alpha + q - \rho\alpha^2
                \right)
            \right)
            \\
            &= 
            \min\left(
                    1, 
                    (1/2)|\rho\alpha^2 - q| + \sqrt{\rho}\alpha + (1/2)(q - \rho\alpha^2)
                \right)
            \\
            &\le \min\left(
                1, 
                |\rho\alpha^2 - q| + \sqrt{\rho}\alpha
            \right). 
        \end{align*}
        \par
        \textbf{Proof of (ii)}. 
        To see the lower bound, we assume that $\alpha_k \ge \mu/L$ , denote $\mu/L$ by $q$, we have the following: 
        \begin{align}\label{ineq:seq-properties-pr5}
            \alpha_k - q 
            \underset{\circled{1}}{\ge} \alpha_k(\alpha_k - q) 
            = \rho_{k - 1}(1 - \alpha_k)\alpha_{k - 1}^2
            \underset{\circled{2}}{\ge} \rho_{k - 1}(1 - \alpha_k)q^2 \ge 0. 
        \end{align}
        At \circled{1}, we used the fact that $\alpha_k \le 1$ and $\alpha_k \ge 0$ from previous results. 
        At \circled{2}, we used the assumption that $\alpha_{k - 1} \ge q \ge 0$. 
        The last inequality is true because is $\rho_{k - 1} \ge 0, \alpha_k \le 1$. 
        \par
        \textbf{Proof of (iii)}. 
        In addition, if $\alpha > \mu/L >  0$, then the inequality in \eqref{ineq:seq-properties-pr3} is strict, then it would instead make $\alpha_k < 1$. 
        It will also make the inequality chain in \eqref{ineq:seq-properties-pr5} strict. 
        Therefore, Item (iii) is true. 
    \end{proof}
    \subsection{Building up the convergence results}
        In this section we derive a generic convergence results. 
        \par
        The following lemma states two important relationships on the iterates generated by Definition \ref{def:SNAG}. 
        Take note that it's only related to the iterates generated: $x_k, y_k, v_k$, it involves the sequence $(\alpha_k)_{k \ge 0}$, but the sequence can be anything in between $(0, 1]$ and these relations won't change. 
        % --------------------------------------------------------------------------------------------------------------
        \begin{lemma}[SNAG identities]\label{lemma:snag-identities}
            The iterates $(y_k, x_k, v_k)_{k \ge0}$ satisfying Definition \ref{def:SNAG} satisfies for all $k \ge 1$ the identities: 
            \begin{enumerate}[nosep]
                \item $z_k - y_k = (L - \mu)^{-1}((L\alpha_k - \mu)(\bar x - v_{k - 1}) + \mu(1 - \alpha_k)(\bar x - x_{k - 1})).$
                \item $z_k - x_k = \alpha_k (\bar x - v_k).$
            \end{enumerate}
        \end{lemma}
        \begin{proof}
            We prove (i) first. 
            Recall the definitions of $\tau_k$ from Definition \ref{def:SNAG}, it has: 
            \begin{align*}
                (1 + \tau_k)^{-1}
                &=
                \left(
                    1 + \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)^{-1} = \left(
                    \frac{L\alpha_k - \mu + L(1 - \alpha_k)}{L\alpha_k - \mu}
                \right)^{-1}
                = \frac{L\alpha_k - \mu}{L - \mu}. 
            \end{align*}
            Therefore, for all $k \ge 0$, $y_k$ has 
            \begin{align*}
                0 &= (1 + \tau_k)^{-1} v_{k - 1} + \tau_k (1 + \tau_k)^{-1} x_{k - 1} - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} 
                \left(
                    v_{k - 1} + \frac{L(1 - \alpha_k)}{L\alpha_k - \mu} x_{k - 1}
                \right) - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1}
                + \frac{L(1 - \alpha_k)}{L - \mu} x_{k - 1} - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                + 
                \left(
                    \frac{L(1 - \alpha_k)}{L - \mu} - (1 - \alpha_k)
                \right) x_{k - 1} - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                + 
                (1 - \alpha_k)\left(
                    \frac{L - L + \mu}{L - \mu}
                \right) x_{k - 1} - y_k
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
                + 
                \frac{\mu(1 - \alpha_k)}{L - \mu}x_{k - 1} - y_k. 
            \end{align*}
            Therefore, we establish the equality 
            \begin{align*}
                (1 - \alpha_k)x_{k - 1} - y_k &= 
                - \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} 
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}. 
            \end{align*}
            On the second equality below, we will the above equality, it goes: 
            \begin{align*}
                z_k - y_k &= 
                \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - y_k
                \\
                &= \alpha_k \bar x 
                - \frac{L\alpha_k - \mu}{L - \mu} v_{k - 1} 
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1})
                + \left(
                    \alpha_k - \frac{L\alpha_k - \mu}{L - \mu}
                \right)\bar x
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1})
                + \left(
                    \frac{\alpha_kL - \alpha_k \mu - L\alpha_k + \mu}{L - \mu}
                \right)\bar x
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1})
                + \frac{\mu(1 - \alpha_k)}{L - \mu}\bar x
                - \frac{\mu(1 - \alpha_k)}{L - \mu} x_{k - 1}
                \\
                &= \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1})
                + \frac{\mu(1 - \alpha_k)}{L - \mu}(\bar x - x_{k - 1}).
            \end{align*}
            To see item (ii), the proof is direct algebra: 
            \begin{align*}
                z_k - x_k &= \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - x_k
                \\
                &= \alpha_k \bar x + x_{k - 1} - x_k - \alpha_k x_{k - 1}
                \\
                &= \alpha_k(\bar x - \alpha_k^{-1}(x_k - x_{k - 1}) - x_{k - 1})
                \\
                &= \alpha_k (\bar x - v_k).
            \end{align*}
        \end{proof}
        Remark the following definitions
        \begin{definition}[conditional expectations]
            Given probability space $(\Omega, \mathcal F_0, \prob)$ and, $\mathcal F \subseteq \mathcal F_0$, and a random variable $X \in \mathcal F$ such that $\expect |X| < \infty$. 
            We define the conditional expectation of $X$ given $\mathcal F$, $\expect [X| \mathcal F]$ to be any random variable $Y$ that has 
            \begin{enumerate}[nosep]
                \item $Y \in \mathcal F$, i.e: $Y$ is $\mathcal F$ measurable. 
                \item For all $A \in \mathcal F$, $\int_A X d\prob = \int_A Y d\prob$. 
            \end{enumerate}
        \end{definition}
        \begin{remark}
            \todoinline{CITATIONS HERE NEEDED. }
        \end{remark}
        The conditional expectation exists and it's unique. 
        The following theorem given an inequality characterizing a descent relation for the SNAG algorithm. 
        % --------------------------------------------------------------------------------------------------------------
        \begin{theorem}[SNAG descent lemma]\label{thm:snag-descent}
            Suppose the iterates sequence $(y_k, x_k, v_k)_{k\ge 0}$ are generated by algorithms satisfying Definition \ref{def:SNAG}. 
            Assume that
            \begin{enumerate}[nosep]
                \item it uses stochastic proximal gradient operator as in Definition \ref{def:stoch-pg-opt-rel-err}, 
                \item it is initialized with $v_{-1} = x_{-1}$, $\alpha_0 = 1$ and, the momentum sequence $(\alpha_k)_{k \ge 0}$ is given as in Lemma \ref{lemma:seq-properties}. 
            \end{enumerate}
            Define $\expect_{k}$ to be the expectation conditioned on $\tilde \nabla f(y_i)$ for $i = 1, 2, \ldots, k - 1$. 
            Then for all $k \ge 1$, for all $\bar x \in \RR^n$ it satisfies the inequality: 
            \begin{align*}
                & \expect_kF(x_k) - F(\bar x) 
                + \frac{\alpha_k^2(L - \epsilon_k)}{2}\expect_k \left[\Vert v_k - \bar x\Vert^2\right]
                \\
                &\le 
                (1 - \alpha_k)\left(
                    F(x_{k - 1}) - F(\bar x)
                    + \frac{\alpha_{k - 1}^2L\rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right) 
                + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right].  
            \end{align*}
            In the edge case of $k = 0$, it has: 
            \begin{align*}
                \expect_0 F(x_0) - F(\bar x)  
                + \frac{L - \epsilon}{2}\expect_0\left[\Vert \bar x - x_0\Vert^2\right]
                &\le 
                \frac{L - \mu}{2} \Vert \bar x - v_{-1}\Vert^2
                + \frac{\epsilon_0}{2}\expect_0\left[\Vert v_{-1} - x_0\Vert^2\right]. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            The following intermediate results will clear out some algebras, they are all proved by the end of the proof. 
            \begin{enumerate}
                \item [(a)] For all $k \ge 1$, it has $\frac{\mu^2(1 - \alpha_k)^2}{2(L - \mu)} - \frac{\mu\alpha_k(1 - \alpha_k)}{2} = \frac{(\alpha_k - 1)\mu\left(L\alpha_k - \mu\right)}{2\left(L - \mu\right)}$ using some algebra. 
                \item [(b)] We assumed that the sequence $(\alpha_k)_{k \ge 0}$ satisfies\\ for all $k \ge 1$: $\rho_{k - 1}(1 - \alpha_{k})\alpha_{k - 1}^2 = \alpha_{k}(\alpha_{k} - \mu/L)$, hence Lemma \ref{lemma:seq-properties} applies. 
                \item [(c)] Using (b) and some algebra, we have for all $k \ge 1$ the identity: \\
                $\frac{(L\alpha_k - \mu)^2}{2(L - \mu)} - \frac{\alpha_{k - 1}^2 L \rho_{k - 1}(1 - \alpha_k)}{2} = \frac{(L\alpha_k - \mu)\mu(\alpha_k - 1)}{2(L - \mu)}$. 
                \item [(d)] Using (a), and (c), we can derive for all $k\ge 1$, we have the following identity: 
                \begin{align*}
                    & - \frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                    + \frac{L - \mu}{2}\Vert z_k - y_k\Vert^2
                    \\ &= 
                    \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert x_{k - 1} - v_{k - 1}\Vert^2. 
                \end{align*}
            \end{enumerate}
            Using intermediate results (a), (b), (c), (d), we can prove the claim in just a few steps. 
            For any fixed $\bar x \in \RR^n$. 
            Define $z_k = \alpha_k \bar x + (1 - \alpha_k)x_{k - 1}$ for all $k \ge 0$. 
            Consider the case for all $k \ge 1$. 
            Recall $\expect_{k}$ is the expectation conditioned on all $\tilde \nabla f(y_i)$ for $i = 0, 1, \ldots, k - 1$. 
            We note that under this conditioning the only random variable is $\tilde \nabla f(y_k)$, so iterates $x_{k - 1}, v_{k - 1}, y_k$ are not random variables, but $x_k$, and $v_k$ are. 
            The sequence $(\alpha_k)_{k \ge 0}, (\epsilon_k)_{k \ge 0}$ are also not random variables. 
            \par
            We use Lemma \ref{lemma:pg-stoch-ineq} with $x = y_k, z = z_k, \tilde x = x_{k}$ and, $B = L$ then it means: 
            {\allowdisplaybreaks
            \begin{align*}
                0 &\le 
                F(z_k) - \expect_k F(x_k) 
                + \frac{L - \mu}{2}\Vert z_k - y_k\Vert^2 
                + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    \\&\quad
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\
                &\underset{\circled{1}}{\le} 
                \alpha_k F(\bar x) + (1 - \alpha_k)F(x_{k - 1}) - \expect_k F(x_k) - \frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                    \\&\quad 
                    + \frac{L - \mu}{2}\Vert z_k - y_k\Vert^2 
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\
                &\underset{\text{(d)}}{=}
                \alpha_k F(\bar x) + (1 - \alpha_k)F(x_{k - 1}) - \expect_k F(x_k) 
                    \\&\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert x_{k - 1} - v_{k - 1}\Vert^2
                    \\&\quad 
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\&\underset{\circled{2}}{\le}
                \alpha_k F(\bar x) + (1 - \alpha_k)F(x_{k - 1}) - \expect_k F(x_k) 
                    \\&\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\&= 
                (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - \expect_kF(x_k)
                \\&\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                \\&= 
                (1 - \alpha_k)\left(
                    F(x_{k - 1}) - F(\bar x)
                    + \frac{\alpha_{k - 1}^2L\rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right)
                    \\ &\quad 
                    + F(\bar x) - \expect_kF(x_k) 
                    - \frac{L - \epsilon_k}{2}\expect_k \left[\Vert z_k - x_k\Vert^2\right]
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right] 
                \\
                &\underset{\circled{3}}{=} 
                (1 - \alpha_k)\left(
                    F(x_{k - 1}) - F(\bar x)
                    + \frac{\alpha_{k - 1}^2L\rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right)
                    \\ &\quad 
                    + F(\bar x) - \expect_kF(x_k) 
                    - \frac{\alpha_k^2(L - \epsilon_k)}{2}\expect_k \left[\Vert v_k - \bar x\Vert^2\right]
                    + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right]. 
            \end{align*}
            }
            At \circled{1}, we used Lemma \ref{thm:jesen}. 
            For \circled{2}, we used Lemma \ref{lemma:seq-properties} (i) because of what is assumed in (b) so, the sequence has $\alpha_k \le 1$, therefore $1 - \alpha_k \le 0$. 
            Next, $\alpha_0 = 1 > \mu/L$, Lemma \ref{lemma:seq-properties} (ii) applies too, hence $L\alpha_k - \mu \ge 0$. 
            Since $L > \mu$ always, the coefficient on $\Vert x_{k - 1} - v_{k - 1}\Vert^2$ is always $\le 0$, therefore the term can be removed to keep the inequality. 
            At \circled{3}, we used Lemma \ref{lemma:snag-identities} (ii). 
            Therefore, at the end the chain of inequalities from above produced the following inequality: 
            \begin{align}\begin{split}
                & \expect_kF(x_k) - F(\bar x) 
                + \frac{\alpha_k^2(L - \epsilon_k)}{2}\expect_k \left[\Vert v_k - \bar x\Vert^2\right]
                \\
                &\le 
                (1 - \alpha_k)\left(
                    F(x_{k - 1}) - F(\bar x)
                    + \frac{\alpha_{k - 1}^2L\rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right) + \frac{\epsilon_k}{2}\expect_k \left[\Vert y_k - x_k \Vert^2\right]. 
            \end{split}\end{align}
            Next, we handle the Edge case of $k = 0$. 
            The base case has $\alpha_0 = 1$, and $v_{-1} = x_{-1}$. 
            Then, the following consequences will be immediate. 
            Fom Definition \ref{def:SNAG}, $\alpha_0 = 1$ makes $\tau_0 = 0$, so $y_0 = v_{-1}$. 
            Followed by it, $x_0 = \tilde {\mathbf T}_L(y_0 | F) = \widetilde{\mathbf T}_L(v_{-1} | F)$. 
            Therefore, we can apply Lemma \ref{lemma:pg-stoch-ineq} with $z = \bar x$, $\tilde x = x_0$, $x = v_{-1}$, and $\epsilon = \epsilon_0$ it yields: 
            {\small
            \begin{align*}
                0
                &\le 
                F(z) - \expect_0 F(\tilde x)
                + \frac{L - \mu}{2} \Vert z - x\Vert^2
                - \frac{L - \epsilon}{2}\expect_0\left[\Vert z - \tilde x\Vert^2\right]
                + \frac{\epsilon}{2} \expect_0\left[\Vert x - \tilde x\Vert^2\right]
                \\
                &= 
                F(\bar x) - \expect_0 F(x_0)
                + \frac{L - \mu}{2} \Vert \bar x - v_{-1}\Vert^2
                - \frac{L - \epsilon}{2}\expect_0\left[\Vert \bar x - x_0\Vert^2\right]
                + \frac{\epsilon}{2}\expect_0\left[\Vert v_{-1} - x_0\Vert^2\right].
            \end{align*}
            }
            % INTERMEDIATE RESULTS 
            \par \textbf{Proof of (a)}. 
            Using basic algebra: 
            {\allowdisplaybreaks
            \begin{align*}
                & \frac{\mu^2(1 - \alpha_k)^2}{2(L - \mu)} 
                - \frac{\mu\alpha_k(1 - \alpha_k)}{2}
                \\
                &= 
                \frac{1}{2\left(L - \mu\right)}
                \left(
                    \mu^2(1 - \alpha_k)^2
                    - \left(L - \mu\right)\mu \alpha_k(1 - \alpha_k)
                \right)
                \\
                &= \frac{1 - \alpha_k}{2\left(L - \mu\right)}\left(
                    \mu^2 
                    - \mu^2\alpha_k 
                    - \left(L \mu \alpha_k - \mu^2 \alpha_k\right)
                \right)
                \\
                &= 
                \frac{1 - \alpha_k}{2(L - \mu)}\left(
                    \mu^2 - L\left(\mu\right)\alpha_k
                \right)
                \\
                &= 
                \frac{(1 - \alpha_k)\mu\left(\mu - L\alpha_k\right)}
                {2\left(L - \mu\right)}
                \\
                &= \frac{(\alpha_k - 1)\mu\left(L\alpha_k - \mu\right)}
                {2\left(L - \mu\right)}. 
            \end{align*}
            }
            % INTERMEDIATE RESULTS 
            \textbf{Proof of (c)}. 
            Using (b) and some algebra, we can derive: 
            {\allowdisplaybreaks
            \begin{align*}
                & \frac{(L\alpha_k - \mu)^2}{2(L - \mu)} - \frac{\alpha_{k - 1}^2 L \rho_{k - 1}(1 - \alpha_k)}{2}
                \\
                &= \frac{(L\alpha_k - \mu)^2}{2(L - \mu)} - \frac{L\alpha_k(\alpha_k - \mu/L)}{2}
                \\
                &= \frac{1}{2(L - \mu)}\left(
                    (L\alpha_k - \mu)^2 - (L - \mu)L\alpha_k(\alpha_k - \mu/L)
                \right)
                \\
                &= 
                \frac{1}{2(L - \mu)}\left(
                    (L\alpha_k - \mu)^2 - (L - \mu)\alpha_k(L\alpha_k - \mu)
                \right)
                \\
                &= \frac{L\alpha_k - \mu}{2(L - \mu)}\left(
                    L\alpha_k - \mu - (L - \mu)\alpha_k
                \right)
                \\
                &= \frac{L\alpha_k - \mu}{2(L - \mu)}\left(
                    \mu\alpha_k - \mu
                \right)
                \\
                &= \frac{(L\alpha_k - \mu)\mu(\alpha_k - 1)}{2(L - \mu)}. 
            \end{align*}
            }
            % INTERMEDIATE RESULTS 
            \textbf{Proof of (d)}. 
            {\allowdisplaybreaks
            \begin{align*}
                &- \frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                + \frac{L - \mu}{2}\Vert z_k - y_k\Vert^2
                \\
                &\underset{\text{\circled{1}}}{=} 
                -\frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                + \frac{L - \mu}{2}
                \left\Vert 
                    \frac{L\alpha_k - \mu}{L - \mu}(\bar x - v_{k - 1}) + 
                    \frac{\mu(1 - \alpha_k)}{L - \mu}(\bar x - x_{k - 1})
                \right\Vert^2
                \\
                &= 
                - \frac{\mu\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
                + \frac{(L\alpha_k - \mu)^2}{2(L - \mu)} \Vert \bar x - v_{k - 1}\Vert^2
                \\
                &\quad
                    + \frac{\mu^2(1 - \alpha_k)^2}{2(L - \mu)} \Vert \bar x - x_{k - 1}\Vert^2
                    + \frac{(L\alpha_k - \mu)\mu(1 - \alpha_k)}{L - \mu}\langle \bar x - x_{k - 1}, \bar x - v_{k - 1}\rangle
                \\
                &= \left(
                    \frac{\mu^2(1 - \alpha_k)^2}{2(L - \mu)} - \frac{\mu\alpha_k(1 - \alpha_k)}{2}
                \right)\Vert \bar x - x_{k - 1}\Vert^2
                    \\ &\quad 
                    + 
                    \left(
                        \frac{(L\alpha_k - \mu)^2}{2(L - \mu)}
                        - \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}
                    \right)\Vert \bar x - v_{k - 1}\Vert^2
                    \\ &\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(L\alpha_k - \mu)\mu(1 - \alpha_k)}{L - \mu}\langle \bar x - x_{k - 1}, \bar x - v_{k - 1}\rangle
                \\
                &\underset{\text{(a)}}{=} 
                \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert \bar x - x_{k - 1}\Vert^2
                + 
                \left(
                    \frac{(L\alpha_k - \mu)^2}{2(L - \mu)}
                    - \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}
                \right)\Vert \bar x - v_{k - 1}\Vert^2
                \\ &\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(L\alpha_k - \mu)\mu(1 - \alpha_k)}{L - \mu}\langle \bar x - x_{k - 1}, \bar x - v_{k - 1}\rangle
                \\
                &\underset{\text{(c)}}{=}
                \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert \bar x - x_{k - 1}\Vert^2
                + 
                \frac{\mu(L\alpha_k - \mu)(\alpha_k - 1)}{2(L - \mu)}\Vert \bar x - v_{k - 1}\Vert^2
                \\ &\quad 
                    + \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \frac{(L\alpha_k - \mu)\mu(1 - \alpha_k)}{L - \mu}\langle \bar x - x_{k - 1}, \bar x - v_{k - 1}\rangle
                \\
                &= 
                \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \\ & \quad
                    + \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\left(
                        \Vert \bar x - x_{k - 1}\Vert^2 + \Vert \bar x - v_{k - 1}\Vert^2 - 2\langle\bar x - x_{k - 1},\bar x - v_{k - 1} \rangle
                    \right)
                \\
                &= \frac{\alpha_{k - 1}^2L \rho_{k - 1}(1 - \alpha_k)}{2}\Vert \bar x - v_{k - 1}\Vert^2
                + \frac{(\alpha_k - 1)\mu(L\alpha_k - \mu)}{2(L - \mu)}\Vert x_{k - 1} - v_{k - 1}\Vert^2. 
            \end{align*}
            }
            At label \circled{1} we used results (i) from Lemma \ref{lemma:snag-identities}. 
        \end{proof}
        Obtaining the convergence results from the lemma requires a recursive relation in expectation. 
        The following theorem gives necessary results to derive the convergence rate of the algorithm in expectations. 
        \begin{theorem}[SNAG generic convergence rate]\label{thm:snag-generic-cnvg}
            Suppose that the sequence $(y_k, x_k, v_k)_{k \ge 0}$ is generated by algorithms satisfying Definition \ref{def:SNAG}. 
            Assume that the momentum sequence $(\alpha_k)_{k \ge 0}$, stochastic gradient $\tilde \nabla f$, and $v_{-1}, x_{-1}$ as in Theorem \ref{thm:snag-descent}. 
            For all $\bar x \in \RR^n$, for all $k \ge 1$, the convergence in expectation, can be compactly written as: 
            \begin{align*}
                \expect \Phi_k &\le
                \left(
                    \prod_{i = 1}^k \lambda_i
                \right)\expect \Phi_0 + 
                \sum_{j = 0}^{k - 1}\left(
                    \prod_{i = k - j}^k \lambda_i
                \right)\expect R_{k - j - 1}
                + \expect R_k.
            \end{align*}
            Where: 
            \begin{enumerate}[nosep]
                \item $\Phi_k := F(x_k) - F(\bar x) + \alpha_k^2(L - \epsilon_k)/2 \Vert v_k - \bar x\Vert^2$, 
                \item $R_k := \frac{\epsilon_k}{2}\Vert y_k - x_k\Vert^2$, 
                \item $\lambda_k := (1 - \alpha_k)\max(1, L\rho_{k - 1}/(L - \epsilon_{k - 1}))$. 
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            Denote $\expect_k$ to be the conditional expectation based on random variables $\tilde \nabla f(y_i)$ for $i = 1, \ldots, k - 1$. 
            The base case has $\expect_0$ which is the overall expectation. 
            Fix any $\bar x \in \RR^n$ throughout. 
            Consider any $k \ge 1$. 
            Under the new notations, the relations from Theorem \ref{thm:snag-descent} yields 
            \begin{align*}
                & \expect_k \Phi_k 
                \\
                &= \expect_k F(x_k) - F(\bar x) + \frac{\alpha_k(L - \epsilon_k)}{2}\expect_k \Vert v_k - \bar x\Vert^2
                \\
                &\underset{\circled{1}}{\le} 
                (1 - \alpha_k)\left(
                    F(x_{k - 1})- F(\bar x) + \frac{\alpha_{k - 1}^2 L \rho_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2
                \right)
                + \frac{\epsilon_k}{2}\expect_k \Vert y_k - x_k\Vert^2
                \\
                &= (1 - \alpha_k)\left(
                    F(x_{k - 1})- F(\bar x) + \frac{\alpha_{k - 1}^2 L \rho_{k - 1}}{2}
                    \left(
                        \frac{L - \epsilon_{k - 1}}{L}
                    \right)
                    \left(
                        \frac{L}{L - \epsilon_{k - 1}}
                    \right)
                    \Vert \bar x - v_{k - 1}\Vert^2
                \right)
                + \expect_k R_k
                \\
                &= (1 - \alpha_k)\left(
                    F(x_{k - 1})- F(\bar x) 
                    + \frac{\alpha_{k - 1}^2(L - \epsilon_{k - 1})}{2}
                    \left(
                        \frac{L\rho_{k - 1}}{L - \epsilon_{k - 1}}
                    \right)
                    \Vert \bar x - v_{k - 1}\Vert^2
                \right)
                + \expect_k R_k
                \\
                &= (1 - \alpha_k)\max\left(
                    1, \frac{L\rho_{k - 1}}{L - \epsilon_{k - 1}}
                \right)\left(
                    F(x_{k - 1})- F(\bar x) 
                    + \frac{\alpha_{k - 1}^2(L - \epsilon_{k - 1})}{2}
                    \Vert \bar x - v_{k - 1}\Vert^2
                \right)
                + \expect_k R_k
                \\
                &= \lambda_k \Phi_{k - 1} + \expect_k R_k.
            \end{align*}
            At \circled{1}, we used Theorem \ref{thm:snag-descent}. 
            Taking the full expectation, which is $\expect_0$ on both sides of the inequality above it yields 
            \begin{align}\label{ineq:snag-descent-expe-pitem-1}
                \expect_0 \Phi_{k} \le \lambda_{k + 1}\expect_0 \Phi_k + \expect_0 R_{k + 1}. 
            \end{align}
            Let $k \ge 1$, We are now in position to verify the following inductive hypothesis: 
            \begin{align}\tag{IH}
                \expect_0 \Phi_k &\le
                \left(
                    \prod_{i = 1}^k \lambda_i
                \right)\expect_0 \Phi_0 + 
                \sum_{j = 0}^{k - 1}\left(
                    \prod_{i = k - j}^k \lambda_i
                \right)\expect_0 R_{k - j - 1}
                + \expect_0 R_k.
            \end{align}
            Use Inequality \ref{ineq:snag-descent-expe-pitem-1} with $k$ as $k + 1$, the inductive proof follows: 
            \begin{align*}
                \expect_0 \Phi_{k + 1} &\le 
                \lambda_{k + 1}\expect_0\Phi_k 
                + \expect_0 R_{k + 1}
                \\
                &\hspace{-0.2em}\underset{\text{(IH)}}{\le} \lambda_{k + 1}\left(
                    \left(
                        \prod_{i = 1}^k \lambda_i
                    \right)\expect_0 \Phi_0 
                    + \sum_{j = 0}^{k - 1}\left(
                        \prod_{i = k - j}^k \lambda_i
                    \right)\expect_0 R_{k - j - 1}
                    + \expect_0 R_k
                \right) 
                + \expect_0 R_{k + 1}
                \\
                &= \left(
                    \prod_{i = 1}^{k + 1} \lambda_i
                \right)\expect_0 \Phi_0
                + \sum_{j = 0}^{k - 1}\left(
                    \prod_{i = k - j}^{k + 1}\lambda_i
                \right)\expect_0 R_{k - j - 1}
                + \lambda_{k + 1}\expect_0 R_k 
                + \expect_0 R_{k + 1}
                \\
                &= \left(
                    \prod_{i = 1}^{k + 1} \lambda_i
                \right)\expect_0 \Phi_0
                + \sum_{j = 1}^{k}\left(
                    \prod_{i = k- j}^{k + 1}\lambda_i
                \right)\expect_0 R_{k - j}
                + \sum_{j = 0}^0 \prod_{i = k + 1 - j}^{k + 1}\lambda_{k + 1}\expect_0 R_k 
                + \expect_0 R_{k + 1}
                \\
                &= 
                \left(
                    \prod_{i = 1}^{k + 1} \lambda_i
                \right)\expect_0 \Phi_0
                + \sum_{j = 0}^{k}\left(
                    \prod_{i = k- j}^{k + 1}\lambda_i
                \right)\expect_0 R_{k - j}
                + \expect_0 R_{k + 1}. 
            \end{align*}
            Therefore, the inductive hypothesis holds. 
        \end{proof}
    \subsection{Discussion of error schedules}
        Let's talk about Theorem \ref{thm:snag-generic-cnvg} here. 
        With the compact representations, we can make see what is happening here. 
        The inequality of the convergence consists of several quantities: 
        \begin{enumerate}[nosep]
            \item $\Phi_0$, the base case and a bounded quantity, depends on initial iterate $v_{-1}, x_{-1}$. 
            \item $\lambda_k$, a sequence controlling the convergence, involving relaxation parameters $\rho_{k - 1}$ and, the error $\epsilon_k$. 
            \item $R_k$, an error term related to $\epsilon_k$ and, $\Vert y_k - x_k\Vert^2$, it's none trivial to group with the other terms. 
        \end{enumerate} 
        Without necessarily introducing the interpolation hypothesis, we have linear ``convergence" with an extra additive term under mild assumption. 
        The following propositions simply stated the sufficient conditions on $\Phi_k, \lambda_k, R_k$ for the inequality to yield a linear convergence rate with additive error, nothing more. 
        \begin{proposition}[SNAG kinda converges linearly]\;\label{prop:snag-kinda-converge}\\
            Suppose that the sequence $(y_k, x_k, v_k)_{k \ge 0}$ is generated by algorithms satisfiying \ref{def:SNAG}. 
            Suppose that $\Phi_k, \lambda_k, R_k$ are as given in Theorem \ref{thm:snag-generic-cnvg}. 
            Define
            \begin{enumerate}[nosep]
                \item $\Lambda = \sup_{i \in \N} \lambda_i$. 
                \item An upper bound $\bar R \ge \expect R_k$ exists for all $k \ge 0$. 
                \item An upper bound $\overline \epsilon = \sup_{i \in \N \cup\{0\}} \epsilon_i$ exists. 
            \end{enumerate}
            If in addition, we have $\overline \epsilon < \sqrt{\mu L}$, then it satisfies for all $k \ge 1$ the following inequality: 
            \begin{align*}
                \expect \Phi_k \le \Lambda^k \expect \Phi_0 + \frac{\overline R}{1 - \Lambda}. 
            \end{align*}
            Where 
            \begin{align*}
            \Lambda = \left(
                1 - \sqrt{\frac{\mu}{L}}
                \right)\left(
                    \frac{L}{L - \overline \epsilon}
                \right).
            \end{align*}
        \end{proposition}
        \begin{proof}
            We can prove it with the following intermediate results: 
            \begin{enumerate}[nosep]
                \item[(a)] For all $k \ge 1$, $\alpha_k = \sqrt{\mu/L}$ satisfies recurrences $(1 - \alpha_k)\alpha_{k - 1}^2 = \alpha_k(\alpha_k - \mu/L)$ where $\rho_k = 1$ for all $k \ge 1$, and $\rho_0 = \mu/L$. 
                \item[(b)] When $(\epsilon_k)_{k \ge 0}$ satisfies $\epsilon_k \le  \bar \epsilon = \sup_{i \in \N\cup\{0\}} \epsilon_i < \sqrt{\mu L}$, it has $\lambda_k \in (0, 1)$ for all $k \ge 1$, with $\Lambda = \sup_{i\in\N} \lambda_i < 1$. 
            \end{enumerate}
            These results will be proved at the end. 
            Consider starting with the inequality proved in Theorem \ref{thm:snag-descent}: 
            \begin{align*}
                \expect \Phi_k 
                &\le \left(
                    \prod_{i = 1}^k \lambda_i
                \right) \expect \Phi_0
                + \sum_{j = 1}^{k - 1}\left(
                    \prod_{j = k - j}^{k - 1} \lambda_{i}
                \right)\expect R_{k - j - 1}
                + \expect R_k
                \\
                &\underset{\circled{1}}{\le} \Lambda^k \expect \Phi_0 + \sum_{j = 0}^{k - 1} \Lambda^{j + 1} \expect R_{k - j - 1} 
                + \expect R_k
                \\
                &= \Lambda^k \expect \Phi_0 + \sum_{j = 1}^{k} \Lambda^{j} \expect R_{k - j} 
                + \expect R_k
                \\
                &= \Lambda^k \expect \Phi_0 + \sum_{j = 0}^{k} \Lambda^{j} \expect R_{k - j} 
                \\
                &\underset{\circled{2}}{\le} \Lambda^k \expect \Phi_0 + \overline R \sum_{j = 0}^{k} \Lambda^{j} 
                \\
                &\underset{\circled{3}}{\le} \Lambda^k \expect \Phi_0 + \frac{\overline R}{1 - \Lambda}. 
            \end{align*}
            At \circled{1} we used that $\Lambda \ge \lambda_k$. 
            At \circled{2} we used that $\overline R \ge \expect R_{k}$ for all $k \ge 0$. 
            At \circled{3} we used the results from (b) that $\lambda_k \le \Lambda$ and $\Lambda < 1$, hence it has the convergence of the geometric series. 
            \par \textbf{Proof of (a)}
            When $L > \mu > 0$, so the objective $F$ is strongly convex, the usual $(1 - \alpha_k)\alpha_{k - 1}^2 = \alpha_k(\alpha_k - \mu/L)$ recurrence relations admits a constant solution $
            \alpha_k = \sqrt{\mu/L}$ for all $k \ge 1$. 
            As presented in Theorem \ref{thm:snag-descent}, the momentum sequence $(\alpha_k)_{k \ge 0}$ has a relaxation parameter $(\rho_k)_{k \ge 1}$. 
            In this case, it means that $\rho_{k - 1} = 1$ for all $k \ge 1$. 
            Since when $k = 0$, $\alpha_0 = 1$, it makes an exception on $\rho_0$, which is: 
            \begin{align*}
                \rho_0 &= \frac{\sqrt{\mu/L}\left(\sqrt{\mu/L} - \mu/L\right)}{1 - \sqrt{\mu/L}} = \frac{\mu}{L}. 
            \end{align*}
            \par \textbf{Proof of (b)}.
            If $\sup_{k \in \N \cup \{0\}}\epsilon_{k} < \sqrt{\mu L}$ for all $k \ge 0$, by the definition of $\lambda_k$, it has: 
            \begin{align*}
                & \sup_{k \in \N}\lambda_k
                \\
                &= \sup_{k \in \N} \left(
                    1 - \sqrt{\frac{\mu}{L}}
                \right)\left(
                    \frac{L\rho_{k - 1}}{L - \epsilon_{k - 1}}
                \right)
                \\
                &\underset{\circled{1}}{\le} 
                \left(
                    1 - \sqrt{\frac{\mu}{L}}
                \right)\left(
                    \frac{L}{L - \sup_{k \in \N}\epsilon_{k - 1}}
                \right)
                \\
                &< \left(
                    1 - \sqrt{\frac{\mu}{L}}
                \right)\left(
                    \frac{L}{1 - \sqrt{\mu L}}
                \right) = 1. 
            \end{align*}
            At \circled{1}, we used the fact that $\rho_k = 1$ for all $k \ge 1$ and,$\rho_k = \mu/L$ for $k = 0$, therefore, it $(\forall k \ge 0)\; \rho_k \le 1$. 
            So $\Lambda < 1$. 
            
        \end{proof}


\section{Interpolation helps but, it's very unclear if it's sufficient}
    Under the frameworks of our analysis, the interpolation assumption helps with controlling the errors, but it's unclear to what extent, it will help us to obtain a convergence rate faster than what is already in the literatures. 
    \subsection{The no error case}
    \subsection{The degenerate case}

\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}
