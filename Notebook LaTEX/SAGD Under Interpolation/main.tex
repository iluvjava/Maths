\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% special operators and stuff introduced for this file. 

\newcommand{\cov}{\ensuremath{\operatorname{Cov}}}
\newcommand{\var}{\ensuremath{\operatorname{Var}}}

\begin{document}
\title{{\fontfamily{ptm}\selectfont Linear Convergence of Stochastic Nesterov's Accelerated Proximal Gradient method under Interpolation Hypothesis}}

\author{
    Author
    \thanks{
        University of British Columbia Okanagan,
        Canada. E-mail: \texttt{alto@mail.ubc.ca}.
    }
}

\date{\today}

\maketitle
\todoinline{This paper is currently in draft mode. Check source to change options. }
% \vskip 8mm

\begin{abstract} 
    This file is for communication purposes between collaborators. 

\end{abstract}

\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}


% ==============================================================================
\section{In preparations}
    \textbf{Notations}. 
    \subsection{Basic definitions}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Proximal gradient operator]\label{def:pg-opt}
            Suppose $F = f + g$ with $\reli(\dom f) \cap \reli(\dom g) \neq \emptyset$, and $f$ is a differentiable function. 
            Let $\beta > 0$. 
            Then, we define the proximal gradient operator $T_{\beta}$ as 
            \begin{align*}
                T_\beta (x | F) &= \argmin_{z} \left\lbrace
                    g(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{\beta}{2}\Vert z - x\Vert^2
                \right\rbrace. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If the function $g \equiv 0$, then it yields the gradient descent operator $T_\beta(x) = x - \beta^{-1}\nabla f(x)$. 
            In the context where it's clear what the function $F = f + g$ is, we simply write $T_\beta(x)$ for short. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[Bregman Divergence]
            Let $f:\RR^n \rightarrow \overline \RR$ be a differentiable function. 
            Then, for all the Bregman divergence $D_f: \RR^n \times \dom\nabla f \rightarrow \RR$ is defined as: 
            \begin{align*}
                D_f(x, y) := f(x) - f(y) - \langle \nabla f(y), x - y\rangle. 
            \end{align*}
        \end{definition}
        \begin{remark}
            If, $f$ is $\mu \ge 0$ strongly convex and $L$ Lipschitz smooth then, its Bregman Divergence has for all $x, y \in \RR^n$: $\mu/2 \Vert x - y\Vert^2 \le D_f(x, y) \le L/2 \Vert x - y\Vert^2$. 
        \end{remark}
    \subsection{Properties of functions, characterizations}
        The definitions are ordered from the weakest to strongest. 
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[semi strongly convex function \textcolor{red}{NEW}]\label{def:semi-scnvx}
            A function $F: \RR^n \rightarrow \overline \RR$ is a semi strongly convex function, abbreviated as ``Semi-SCNVX'' with respective to a linear mapping $A \in \RR^{m \times n}$ if $F - \frac{1}{2}\Vert Ax\Vert^2$ is a convex function. 
        \end{definition}
        \begin{remark}
            Any $\mu \ge 0$ strongly convex function is Semi-SCNVX with $A = \sqrt{\mu}I$. 
            But the converse is not true because a seminorm is not a norm. 
            One feature of a Semi-SCNVX function is that it doesn't have a unique minimizer which differs it from strong convexity. 
            It may not have a unique minimizer because it's not necessary that $\kernel A = \{\mathbf 0\}$. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{definition}[smooth Semi-SCNVX \textcolor{red}{NEW}]\label{def:seminorm-smooth-scnvx}
            Let $m, n\in \N$ be a natural numbers. 
            Let $f:\RR^m \rightarrow \RR$ be a differentiable function and full domain. 
            If there exists $A_1: \RR^{m\times n}$ and, $A_2 \in \RR^{m \times n}$ matrices such that it satisfies:
            \begin{align*}
                (\forall x \in \RR^m)(\forall y \in \RR^m)\;
                \frac{1}{2}\Vert A_1x - A_1y\Vert^2 \le 
                D_f(x, y) \le \frac{1}{2}\Vert A_2x - A_2y\Vert^2. 
            \end{align*}
            Then, we call this function is semi-relative smooth with respect to $A_1$, and semi convex to $A_2$
        \end{definition}
        \begin{remark}
            The definition exchanged the $\Vert \cdot\Vert^2$ for a seminorm squared: $x \mapsto \Vert A_1x\Vert^2$ with some $A: \RR^m \rightarrow \RR^n$ for the definition of relative smoothness and relative strong convexity. 
            Obviously, it has $\Vert A_1x\Vert \le \Vert A_2x\Vert$ for all $x\in \RR^m$, which further implies that $\ker A_1\supseteq\ker A_2$. 
        \end{remark}
        % --------------------------------------------------------------------------------------------------------------
        \begin{theorem}[semi Jesen inequality \textcolor{red}{NEW}]\label{thm:semi-scnvx-equiv}
            A function $F$ is Semi-SNCVX with $A \in \RR^{m \times n}$ (Definition \ref{def:semi-scnvx}) if and only if, for all $x, y \in \RR^n$ and, $\lambda \in [0, 1]$ it satisfies the inequality: 
            \begin{align*}
                F(\lambda x + (1 - \lambda)y) \le \lambda F(x) + (1 - \lambda)F(y) -\frac{\lambda(1 - \lambda)}{2} \Vert Ay - Ax\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            For all $\lambda \in \RR, x \in \RR^n, y \in \RR^n$ it has $-1/2\Vert A(\lambda x + (1 - \lambda)y)\Vert^2 = (1/2)(\lambda\Vert Ax\Vert^2 + (1 - \lambda)\Vert Ay\Vert^2 - \lambda(1 - \lambda)\Vert Ax - Ay\Vert^2)$ by verifying: 
            \begin{align*}
                & -\frac{1}{2}\Vert A(\lambda x + (1 - \lambda)y)\Vert^2 + 
                \left(
                    \frac{\lambda}{2}\Vert Ax\Vert^2 + \frac{1 - \lambda}{2}\Vert Ay\Vert^2 - \frac{\lambda(1 - \lambda)}{2}\Vert Ay - Ax\Vert^2
                \right)
                \\
                &= 
                -\frac{1}{2}\left(
                    \lambda^2\Vert Ax\Vert^2 + (1 - \lambda)^2\Vert Ay\Vert^2 - 2\lambda(1 - \lambda) \langle Ax, Ay\rangle
                \right)
                    \\ &\quad 
                    + 
                    \left(
                        \frac{\lambda}{2} - \frac{\lambda(1 - \lambda)}{2}
                    \right)\Vert Ax\Vert^2 + \left(
                        \frac{1 - \lambda}{2} - \frac{\lambda(1 - \lambda)}{2}
                    \right)\Vert Ay\Vert^2 - \lambda(1 - \lambda)\langle Ay, Ax\rangle
                \\
                &= - \frac{\lambda^2}{2}\Vert Ax\Vert^2 - \frac{(1 - \lambda)^2}{2}\Vert Ay\Vert^2
                    \\ &\quad 
                    + 
                    \left(
                        \frac{\lambda}{2} - \frac{\lambda - \lambda^2}{2}
                    \right)\Vert Ax\Vert^2 + \left(
                        \frac{1 - \lambda}{2} - \frac{\lambda - \lambda^2}{2}
                    \right)\Vert Ay\Vert^2
                \\
                &= 0
            \end{align*}
            Using the above result we can prove the equivalency because 
            {\small
            \begin{align*}
                0 &\le F(\lambda x + (1 - \lambda)y) + \lambda F(x) + (1 - \lambda)F(y) -\frac{\lambda(1 - \lambda)}{2} \Vert Ay - Ax\Vert^2
                \\
                &=  F(\lambda x + (1 - \lambda)y) - \frac{1}{2}\Vert A(\lambda x + (1 - \lambda)y)\Vert^2 
                + \lambda F(x) - \frac{\lambda}{2}\Vert Ax\Vert^2 
                + (1 - \lambda)F(y) - \frac{1 - \lambda}{2} \Vert Ay\Vert^2 
                    \\ &\quad 
                    - \frac{\lambda(1 - \lambda)}{2} \Vert Ay - Ax\Vert^2 + \frac{1}{2}\Vert A(\lambda x + (1 - \lambda)y)\Vert^2
                    + \frac{1}{2}\Vert Ax\Vert^2 + \frac{1}{2}\Vert Ay\Vert^2 
                \\
                &= 
                F(\lambda x + (1 - \lambda)y) - \frac{1}{2}\Vert A(\lambda x + (1 - \lambda)y)\Vert^2 
                    \\ &\quad 
                    + \lambda \left(F(x) - \frac{1}{2}\Vert Ax\Vert^2\right) 
                    + (1 - \lambda)\left(F(y) - \frac{1}{2} \Vert Ay\Vert^2\right). 
            \end{align*}
            }
            The last line shows that the function $F(x) - \frac{1}{2}\Vert Ax\Vert^2$ is convex, the chain of equality shows the equivalence. 
        \end{proof}
        % --------------------------------------------------------------------------------------------------------------
        \begin{theorem}[Jensen's inequality]\label{thm:jesen}
            Let $F: \RR^n \rightarrow \overline \RR$ be a $\mu \ge 0$ strongly convex function. 
            Then, it is equivalent to the following condition. 
            For all $x, y \in \RR^n$, $\lambda \in (0, 1)$ it satisfies the inequality 
            \begin{align*}
                (\forall \lambda \in [0, 1])\; 
                F(\lambda x + (1 - \lambda)y) \le \lambda F(x) + (1 - \lambda)F(y) -\frac{\mu\lambda(1 - \lambda)}{2} \Vert y - x\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{remark}
            If $x, y$ is out of $\dom F$, the inequality still work by convexity. 
        \end{remark}
        The following theorem classifies a class of semi strongly convex function. 
        % --------------------------------------------------------------------------------------------------------------
        \begin{theorem}[affine composite smooth and strongly convex]\label{thm:aff-smooth-sq-scnvx}
            Let $f: \RR^n \rightarrow \RR$ be an $L$ smooth and, $\mu \ge 0$ strongly convex.
            Let $A: \RR^m \rightarrow \RR^n$, $b \in \RR^n$ be arbitrary and, define $h: \RR^m \rightarrow \RR= x \mapsto f(Ax - b)$. 
            Ddenote $\Pi = \Pi_{\ker A}$ to be the projection operator onto the kernel of $A$, denote $\Vert A\Vert$ to be the operator norm of the matrix, and $\sigma_{\min}(A)$ to be the smallest non-zero eigenvalues of $A$ in absolute value.
            Then, it satisfies for all $x \in \RR^m, y \in \RR^m$:  
            \begin{align*}
                \frac{\mu \sigma_{\min}(A)^2}{2}\Vert (I - \Pi)(x - y)\Vert^2
                \le \frac{\mu}{2}\Vert Ax - Ay\Vert^2 
                \le D_h(x, y) \le \frac{L\Vert A\Vert^2}{2}\Vert x - y\Vert^2. 
            \end{align*}
            Therefore, it satisfies Definition \ref{def:semi-scnvx} with $A_1 = \sqrt{L}A, A_2 = \sqrt{\mu}A$ so it's Semi-SCNVX. 
        \end{theorem}
        \begin{proof}
            Then the Bregman divergence of $h$ is: 
            \begin{align*}
                D_h(x, y) &= h(x) - h(y) - \langle \nabla h(y), x - y\rangle
                \\
                &= f(Ax - b) - f(Ay - b) - \langle A^T\nabla f(Ay - b), x - y\rangle
                \\
                &= f(Ax - b) - f(Ay - b) - \langle \nabla f(Ay - b), Ax - Ay\rangle
                \\
                &= f(Ax - b) - f(Ay - b) - \langle \nabla f(Ay - b), Ax - b - (Ay - b)\rangle
                \\
                &= D_f(Ax - b, Ay - b). 
            \end{align*}
            Since $f$ is $L$ smooth and $\mu \ge 0$ strongly convex, it means 
            \begin{align*}
                \frac{\mu}{2}\Vert Ax - Ay\Vert^2
                &= 
                \frac{\mu}{2}\Vert Ax - b - (Ay - b)\Vert^2 
                \\
                &\le D_f(Ax - b, Ay - b)
                \\
                &= D_h(x, y) 
                \\
                &\le \frac{L}{2} \Vert Ax - Ay\Vert^2
                \\
                &\le \frac{L\Vert A\Vert^2}{2} \Vert x - y\Vert^2. 
            \end{align*}
            Using some linear algebra, one can represent $x - y$ as their orthogonal components in $\ker A$, $\ker A^T$ so it has 
            \begin{align*}
                \frac{\mu}{2}\Vert A(x - y)\Vert^2 &= 
                \frac{\mu}{2}\Vert A(\Pi(x - y) + (1 - \Pi)(x - y))\Vert^2
                \\
                &= 
                \frac{\mu}{2}\Vert A(1 - \Pi)(x - y))\Vert^2
                \\
                &\ge \frac{\mu\sigma_{\min}(A)}{2}\Vert (1 - \Pi)(x - y)\Vert^2. 
            \end{align*}
            These results makes for the final inequalities: 
            \begin{align*}
                \frac{\mu\sigma_{\min}(A)}{2}\Vert (I - \Pi)(x - y)\Vert^2
                \le 
                D_h(x, y) \le 
                \frac{L\Vert A\Vert^2}{2}\Vert x - y\Vert^2. 
            \end{align*}
        \end{proof}
        \begin{theorem}[affine minimizer set for aff comp smooth strongly convex]
            \todoinline{NOT FINISHED YET.}
        \end{theorem}
    \subsection{Important inequalities}
            \begin{assumption}[smooth add nonsmooth]\label{ass:smooth-plus-nonsmooth}
                The function $F = f + g$ where $f:\RR^n \rightarrow \RR$ is an $L$ Lipschitz smooth and $\mu \ge 0$ strongly convex function. 
                The function $g:\RR^n \rightarrow \overline \RR$ is a closed convex proper function. 
            \end{assumption}
            \begin{assumption}[admitting minimizers]\label{ass:smooth-plus-nonsmooth-x}
                Let $F = f + g$ satisfies \ref{ass:smooth-plus-nonsmooth} and in addition assume that the set of minimizers $X^+ := \argmin_{x}F(x)$ is non-empty. 
            \end{assumption}
            \begin{assumption}[affine composite Semi-CNVX remative smooth plus non-smooth]\label{ass:snorm-smth-p-nsmth}
                Let $A \in \RR^{m\times n}, b \in \RR^n$
                Let $F(x): \RR^{m} \mapsto \overline\RR := x \mapsto f(x) + g(x)$. 
                Assume that: 
                \begin{enumerate}[noitemsep]
                    \item $f:\RR^m \rightarrow \RR := x \mapsto h(Ax - b)$ where $h:\RR^n\rightarrow \RR$ is $L$ Lipschitz smooth and, $\mu\ge 0$ strongly convex, then it satisfies Theorem \ref{thm:aff-smooth-sq-scnvx} with $L > \mu \ge 0$ and $A \in \RR^{m \times n}$. 
                    \item $g:\RR^m \mapsto \overline \RR$ is a convex, proper and closed function. 
                \end{enumerate}
            \end{assumption}
            % --------------------------------------------------------------------------------------------------------------
            \begin{theorem}[proximal gradient inequality]\label{thm:pg-ineq}
                Let function $F$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth}, so it's $\mu \ge 0$ strongly convex. 
                For any $x\in \RR^n$, define $x^+ = T_L(x)$. 
                Then, there exists a $B \ge 0$ such that $D_f(x^+, x) \le B/2 \Vert x^+ - x\Vert^2$ and, for all $z \in \RR^n$ it satisfies the inequality: 
                \begin{align*}
                    0&\le F(z) - F(x^+) - \frac{B}{2}\Vert z - x^+\Vert^2  + \frac{B - \mu}{2}\Vert z - x\Vert^2
                    \\
                    &=  F(z) - F(x^+) - \langle B(x - x^+), z - x\rangle
                    - \frac{\mu}{2}\Vert z - x\Vert^2
                    - \frac{B}{2}\Vert x - x^+\Vert^2. 
                \end{align*}
                Since $f$ is assumed to be $L$ Lipschitz smooth, the above condition is true for all $x, y \in \RR^n$ for all $B \ge L$. 
            \end{theorem}
            \begin{remark}
                The theorem is the same as in Nesterov's book \cite[Theorem 2.2.13]{nesterov_lectures_2018}, but with the use of proximal gradient mapping and proximal gradient instead of project gradient hence making it equivalent to the theorem in Beck's book \cite[Theorem 10.16]{beck_first-order_2017}. 
                The only generalization here is parameter $B$ which made to accommodate algorithm that implements Definition \ref{def:snapg-v2} with line search routine to determine $L_k$. 
                Each of the reference books gives a proof of the theorem. 
                But for the best consistency in notations, see Theorem 2.3 in Li and Wang \cite{li_relaxed_2025}. 
            \end{remark}
            The following theorem attempts to generalize Theorem \ref{thm:pg-ineq}. 
            % --------------------------------------------------------------------------------------------------------------
            \begin{theorem}[proximal gradient inequality with semi-scnvx \textcolor{red}{NEW}]\label{thm:pg-ineq-semi-scnvx}
                Suppose that $F:\RR^m \rightarrow \overline \RR := x \mapsto f(x) + g(x)$ satisfies Assumption \ref{ass:snorm-smth-p-nsmth} with $L > \mu \ge 0$ and $A \in \RR^{m\times n}$. 
                For any $x \in \RR^n$, let $x^+ = T_B(x | F)$. 
                Let $\sigma_{\min}(A)$ denotes the minimum non-zero singular value of $A$ in absolute value. 
                Let $\Pi = \Pi_{\ker A}$ be the linear operator that project onto the kernel of $A$. 
                Then, there exists some $B \ge 0$ such that $D_f(x^+, x) \le \frac{B}{2}\Vert x - x^+\Vert^2$ and, for all $z \in \RR^m$ it satisfies the inequality: 
                \begin{align*}
                    0 &\le F(z) - F(x^+) 
                    - \frac{\mu}{2}\Vert Az - Ax\Vert^2 
                    + \frac{B}{2}\Vert z - x\Vert^2
                    - \frac{B}{2}\Vert z - x^+\Vert^2
                    \\
                    &\le F(z) - F(x^+)
                    - \frac{B}{2}\Vert z - x^+\Vert^2 
                    + \frac{B - \sigma_{\min}(A)^2\mu}{2}\Vert (I - \Pi)(z - x) \Vert^2
                        \\ &\quad 
                        + \frac{B}{2}\Vert \Pi(z - x)\Vert^2. 
                \end{align*}
            \end{theorem}
            \begin{proof}
                Firstly, such a $B > 0$ exists, for example $B = L\Vert A\Vert^2$ would be an option because from Definition \ref{def:seminorm-smooth-scnvx}, it for all $x, y$, $D_f(x, y) \le L/2\Vert Ax - Ay \Vert^2 \le L/2\Vert A\Vert^2\Vert x - y\Vert^2$. 
                But it can be much smaller. 
                \par
                The function $z \mapsto g(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2$ inside the proximal gradient operator has the minimizer $x^+$.  
                This function is also the sum of a convex, proper closed function $g$ and, a simple quadratic and, it's $B > 0$ strongly convex hence, it satisfies the quadratic growth conditions over its minimizer $x^+ = T_B(x|F)$ so, it follows that for all $z \in \RR^m$: 
                \begin{align*}
                    0 &\le 
                    -\frac{B}{2}\Vert z - x^+\Vert^2 
                    + g(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2
                        \\&\quad 
                        - g(x^+) - f(x) - \langle \nabla f(x), x^+ - x\rangle - \frac{B}{2}\Vert x^+ - x\Vert^2
                    \\
                    &= 
                    -\frac{B}{2}\Vert z - x^+\Vert^2 
                    + \left(g(z) + f(z) - f(z) + f(x) + \langle \nabla f(x), z - x\rangle + \frac{B}{2}\Vert z - x\Vert^2\right)
                        \\&\quad 
                        +\left(- g(x^+) - f(x^+) + f(x^+) - f(x) - \langle \nabla f(x), x^+ - x\rangle - \frac{B}{2}\Vert x^+ - x\Vert^2\right)
                    \\
                    &=
                    -\frac{B}{2}\Vert z - x^+\Vert^2 
                    + \left(F(z) - D_f(z, x) + \frac{B}{2}\Vert z - x\Vert^2\right)
                        \\&\quad
                        + \left(- F(x^+) + D_f(x^+, x) - \frac{B}{2}\Vert x^+ - x\Vert^2\right)
                    \\
                    &\underset{\text{(a)}}{\le }
                    -\frac{B}{2}\Vert z - x^+\Vert^2 
                    + \left(F(z) - D_f(z, x) + \frac{B}{2}\Vert z - x\Vert^2\right)
                    - F(x^+)
                    \\
                    &\underset{\text{(b)}}{\le} 
                    - \frac{B}{2}\Vert z - x^+\Vert^2 
                    + F(z) - \frac{\mu}{2}\Vert Az - Ax\Vert^2 
                    + \frac{B}{2}\Vert z - x\Vert^2
                    - F(x^+)
                    \\
                    &= F(z) - F(x^+) 
                    - \frac{\mu}{2}\Vert Az - Ax\Vert^2 
                    + \frac{B}{2}\Vert z - x\Vert^2
                    - \frac{B}{2}\Vert z - x^+\Vert^2. 
                \end{align*}
                At (a), we used the fact that line search asserted the condition $D_f(x^+, x) \le \frac{B}{2}\Vert x^+ - x\Vert^2$. 
                At (b) we applied results from Theorem \ref{thm:aff-smooth-sq-scnvx}. 
                Continuing it further with the results from \ref{thm:aff-smooth-sq-scnvx} it adds another inequality: 
                \begin{align*}
                    0&\le  F(z) - F(x^+) 
                    - \frac{\mu}{2}\Vert Az - Ax\Vert^2 
                    + \frac{B}{2}\Vert z - x\Vert^2
                    - \frac{B}{2}\Vert z - x^+\Vert^2
                    \\
                    &\le F(z) - F(x^+) 
                    - \frac{\sigma_{\min}(A)^2\mu}{2}\Vert (I - \Pi)(z - x)\Vert^2
                    + \frac{B}{2}\Vert z - x\Vert^2
                    - \frac{B}{2}\Vert z - x^+\Vert^2
                    \\
                    &= F(z) - F(x^+) 
                    - \frac{\sigma_{\min}(A)^2\mu}{2}\Vert (I - \Pi)(z - x)\Vert^2
                        \\ &\quad
                        + \frac{B}{2}\Vert \Pi(z - x)\Vert^2
                        + \frac{B}{2}\Vert (I - \Pi)(z - x)\Vert^2
                        - \frac{B}{2}\Vert z - x^+\Vert^2
                    \\
                    &= F(z) - F(x^+) 
                    - \frac{B}{2}\Vert z - x^+\Vert^2
                    + \frac{B - \sigma_{\min}(A)^2\mu}{2}\Vert (I - \Pi)(z - x)\Vert^2
                    + \frac{B}{2}\Vert \Pi(z - x)\Vert^2. 
                \end{align*}
            \end{proof}
            \begin{remark}
                When $\ker A = \{\mathbf 0\}$, this theorem is equivalent to Theorem \ref{thm:pg-ineq} but with $\mu$ being $\sigma_{\min}(A)$ instead. 
            \end{remark}
            The following theorem attempts to generalize Theorem \ref{thm:semi-scnvx-equiv} for relatively smooth plus non-smooth function. 
            % --------------------------------------------------------------------------------------------------------------
            \begin{theorem}[seminorm smooth plus non-smooth Jensen \textcolor{red}{NEW}]\label{thm:smnrm-jnsn-smth-nsmth}
                Suppose that $F:\RR^m \rightarrow \overline \RR := x \mapsto f(x) + g(x)$ satisfies Assumption \ref{ass:snorm-smth-p-nsmth} with $L > \mu \ge 0$ and $A \in \RR^{m\times n}$.
                Let $\Pi = \Pi_{\ker A}$ be the projection onto the kernel of $A$. 
                Let $\sigma_{\min}(A)$ denote the smallest non-zero singular value of $A$ in absolute value. 
                Then, for all $x, y \in \RR^m$ and, $\lambda \in [0, 1]$ it satisfies the inequality: 
                \begin{align*}
                    F(\lambda z + (1 - \lambda)y) &\le \lambda F(z) + (1 - \lambda)F(x) - \frac{\sigma_{\min}(A)^2\lambda(1 - \lambda)\mu}{2}\Vert (I - \Pi)(x - y)\Vert^2. 
                \end{align*}
            \end{theorem}
            \begin{proof}
                $f$ satisfies Definition \ref{def:seminorm-smooth-scnvx} with $L > \mu, A$, so for all $x, y \in \RR^m$ it has 
                \begin{align*}
                    0 &\le D_f(x, y) - \frac{\mu}{2}\Vert Ax - Ay\Vert^2 
                    \le \frac{L - \mu}{2}\Vert Ax - Ay\Vert^2. 
                \end{align*}
                Using some algebra (or equivalent some properties of Bregman divergence), it shows that the function $f - \mu/2 \Vert A(\cdot)\Vert^2$ is a convex function, therefore, $f + g - \mu/2\Vert A(\cdot)\Vert^2 = F - \frac{\mu}{2}\Vert A(\cdot)\Vert^2 = F - \frac{1}{2}\Vert \sqrt{\mu}A(\cdot)\Vert^2$ is also a convex function. 
                Applying Theorem \ref{thm:semi-scnvx-equiv} it has for all $z, x \in \RR^m$ and, $\lambda \in [0, 1]$ the inequality: 
                \begin{align*}
                    F(\lambda z + (1 - \lambda)y) &\le \lambda F(z) + (1 - \lambda)F(x) - \frac{\lambda(1 - \lambda)\mu}{2}\Vert Ax - Ay\Vert^2
                    \\
                    &\le 
                    \lambda F(z) + (1 - \lambda)F(x) - \frac{\lambda(1 - \lambda)\mu\sigma_{\min}(A)^2}{2}\Vert (I - \Pi) x - y\Vert^2. 
                \end{align*}
                The second inequality uses Theorem \ref{thm:aff-smooth-sq-scnvx}. 
            \end{proof}
        

\section{Stochastic accelerated proximal gradient}
    First, this is an overview of this section. 

    \begin{assumption}[sum of many affine composite]\label{ass:sum-of-many-aff-comp}
        Let $A^{(1)}, A^{(2)}, \ldots A^{(n)}$ be a list of $\RR^{m\times n}$ matrices and, $b^{(1)}, b^{(2)}, \ldots, b^{(n)}$ be a list of vectors in $\RR^n$. 
        Suppose $F:\RR^m \rightarrow \overline \RR:= \frac{1}{n}\sum_{i = 1}^{n} F_i(x)$, so it admits representations 
        \begin{align*}
            F(x) = f(x) + g(x) = \frac{1}{n}\sum_{i = 1}^{n} f_i(x) + g_i(x) = \frac{1}{n}\sum_{i = 1}^{n} h_i(A^{(i)}x - b^{(i)}) + g_i(x)
        \end{align*}
        Where, each $F_i = f_i + g_i$ satisfies Assumption \ref{ass:snorm-smth-p-nsmth} with $K^{(i)} > \nu^{(i)} \ge 0, b^{(i)} \in \RR^n$ and $A^{(i)} \in \RR^{m \times n}$. 
        So, each $h_i$ are $\nu^{(i)}$ strongly convex and $K^{(i)}$ Lipschitz smooth. 
        \par
        Take note that the function $f(x) = \frac{1}{n}\sum_{i = 1}^{n} h_i(A^{(i)} - b^{(i)})$ is the composition of the strongly convex function $h = \frac{1}{n}\sum_{i = 1}^{n} h_i$ that is a $\RR^{mn}\rightarrow \RR$ mapping with strong convexity $\frac{1}{n}\sum_{i = 1}^{n} \nu^{(i)}$ and, the affine $\RR^m \rightarrow \RR^{mn}$ mapping $\mathcal A := x \mapsto (A^{(1)}x - b^{(1)},\ldots,  A^{(n)}x - b^{(n)})$. 
        $f$ satisifes Theorem \ref{thm:aff-smooth-sq-scnvx} and it can be written as: 
        \begin{align*}
            f(x) = \sum_{i = 1}^{n} h_i(A^{(i)} - b^{(i)}) = h(\mathcal A x). 
        \end{align*}
    \end{assumption}

    \begin{assumption}[sum of many]\label{ass:sum-of-many}
        Define $F := (1/n)\sum_{i = 1}^{n} F_i$ where each $F_i = f_i + g_i$, so it can be written as: 
        \begin{align*}
            F(x) = f(x) + g(x) = \frac{1}{n}\sum_{i = 1}^{n} f_i(x) + g_i(x)
        \end{align*}
        Assume that for all $i = 1, \ldots, n$, each $f_i:\RR^n \rightarrow \RR$ are $K^{(i)}$ smooth and $\mu^{(i)} \ge 0$ strongly convex function such that $K^{(i)} > \mu^{(i)}$ and, $g_i:\RR^n \rightarrow \overline \RR$ is a closed convex proper function satisfies Assumption \ref{ass:smooth-plus-nonsmooth}. 
        \par 
        Take note that, the function $f$ can be written as $F = g + f$ with $f = (1/n)\sum_{i = 1}^{n} f_i, g = (1/n)\sum_{i = 1}^{n}g_i$ therefore, it also satisfies Assumption \ref{ass:smooth-plus-nonsmooth} with $L = (1/n)\sum_{i = 1}^n K^{(i)}$ and $\mu = (1/n)\sum_{i = 1}^{n}\mu^{(i)}$. 
    \end{assumption}
    Both assumptions are equivalent, functions satisfies one can be translated in form so it satisfies the other with some extra/fewer parameters. 
    However, it's important to know that for a $F := \frac{1}{n}\sum_{i = 1}^{n} h_i(A^{(i)} - b^{(i)}) + g_i(x)$ satisfying Assumption \ref{ass:sum-of-many-aff-comp}, Theorem \ref{thm:pg-ineq-semi-scnvx}, \ref{thm:smnrm-jnsn-smth-nsmth} applies with parameter $\nu^{(i)}\sigma_{\min}(A^{(i)})^2$ for each summees. 
    This can't be known if it only satisfies Assumption \ref{ass:sum-of-many}. 
    \par
    The interpolation hypothesis from Machine Learning stated that the model has the capacity to perfect fit all the observed data. 
    The following assumption state the interpolation hypothesis in our context. 
    \begin{assumption}[interpolation hypothesis]\label{ass:interp-hypothesis}
        Suppose that $F := (1/n)\sum_{i = 1}^{n} F_i$ satisfying Assumption \ref{ass:sum-of-many}. 
        In addition, assuming that it has $0 = \inf_{x}F(x)$ and, there exists some $\bar x \in \RR^n$ such that for all $i = 1, \ldots, n$ it satisfies $0 = f_i(\bar x)$. 
        \par
        Consequently, each of the $F_i$ satisfies Assumption \ref{ass:smooth-plus-nonsmooth-x} with $X_i$ being the set of minimizers and, under interpolation hypothesis this equates to non-empty intersections between all $X_i$, i.e: $\bigcap_{i = 1}^n X_i \neq \emptyset$. 
    \end{assumption}
    
    Due to the fact the algorithms have to be very different support convergence claim for different assumptions on the functions, the following definition gives a generic ideas for all algorithms that specializes on top of it for different assumptions placed on the objective functions. 
    These definitions are not actual algorithms, they are conditions an algorithm must adhere to for the theorems based on it to be valid. 
    Read then as specifications. 
    % --------------------------------------------------------------------------------------------------------------
    \begin{definition}[SNAPG-V2 prototype \textcolor{red}{NEW}]\;\label{def:snapg-v2-proto}\\
        Let 
        \begin{enumerate}[nosep]
            \item $F$ satisfies Assumption \ref{ass:sum-of-many}, 
            \item $(I_k)_{k \ge 0}$ be a list of i.i.d random variables uniformly sampled from set $\{0, 1, 2, \cdots, n\}$, 
            \item $\sigma^{(k)}$ be another list of i.i.d random variable. 
            \item $\tilde \mu \ge 0$ be a constant that is fixed.
            \item Let $(L_k)_{k \ge 0}$ a sequence of strictly positive numbers. 
        \end{enumerate}
        Initialize $v_{-1} = x_{-1}, \alpha_0 = 1$. 
        The SNAPG prototype specifies algorithm that generates the sequence $(y_k, x_k, v_k)_{k \ge 0}$ such that for all $k \ge 0$ they satisfy: 
        \begin{align*}
            & \alpha_k \in (0, 1): (L_{k - 1}/L_k)(1 - \alpha_{k})\alpha_{k - 1}^2 = \alpha_{k}\left(\alpha_{k} - \tilde \mu/L_k\right), \\
            & \tau_k = L_k(1 - \alpha_k)\left(L_k \alpha_k - \sigma^{(k)}\right)^{-1}, \\
            & y_k = (1 + \tau_k)^{-1}v_{k - 1} + \tau_k(1 + \tau_k)^{-1}x_{k - 1}, \\
            & L_k > 0: D_f(x_k, y_k) \le L_k/2\Vert y_k - x_k\Vert^2, \\
            & x_k =  T_{L_k}(y_k | F_{I_k}), \\
            & v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}). 
        \end{align*}
    \end{definition}
    \begin{remark}
        $\tilde \mu_k, L_k$ are not necessary a random variable because they are determined by a line-search like conditions, consequently $(\alpha_k)_{k\ge 0}$, whether they are a random variable depends on the line search procedures. 
        Otherwise, all the iterates $(x_k, y_k, z_k)$ are random variable determined by $I_k$ when conditioned on all previous $I_{k - 1}, I_{k - 2}, \ldots, I_{0}$. 
        \par
        \textcolor{red}{NEW}. One may notice that $\alpha_k$ requires $L_k$ which comes before $L_k, x_k$ which are needed in advanced for $\alpha_k$. 
        This may seem off since no algorithm can know what $L_k$ to choose in advanced to determine the line search. 
        But, it is important to note that in here, we defined a sequence of conditions on the iterates $x_k, y_k, z_k$, and auxiliary sequences $\alpha_k, L_k$ which is not a definition of any algorithm. 
        It is quantifying the conditions needed for an algorithm that actually implements it.
        \par
        For the trivial case where we don't need to worry about it is when $L_k = \max_{i = 1, \ldots, n} K^{(i)}$. 
        See Chambolle, Calatroni \cite{calatroni_backtracking_2019} for an implementation of linear search with backtracking for the FISTA algorithm, it is how one would implement it in the deterministic case. 
    \end{remark}
    % --------------------------------------------------------------------------------------------------------------
    \begin{definition}[SNAPG-V2 affine \textcolor{red}{NEW}]\label{def:snapg-v2-aff}
        Let $F, (I_k)_{k \ge 0}, \tilde \mu$ and, $(L_k)_{k \ge 0}$ be given as in Definition \ref{def:snapg-v2-proto}. 
        But in addition, assume that: 
        \begin{enumerate}[nosep]
            \item $F$ also satisfies Assumption \ref{ass:sum-of-many-aff-comp} with $\RR^{m \times n}$ matrices: $A^{(1)}, A^{(2)}, \ldots, A^{(n)}$, vectors in $\RR^n$: $(b^{(1)}, \ldots, b^{(n)})$. 
            \item Choose $\sigma^{(k)} = \nu^{(I_k)}\sigma_{\min}(A^{(I_k)})^2$. 
        \end{enumerate}
        Then, SNAPG-V2 affine specifies algorithms that generate the sequence $(y_k, x_k, v_k)_{k \ge 0}$ satisfying conditions as specified in Definition \ref{def:snapg-v2-proto} with the above parameters. 
    \end{definition}
    % --------------------------------------------------------------------------------------------------------------
    \begin{definition}[SNAPG-V2 vanilla]\label{def:snapg-v2}
        Let $F, (I_k)_{k \ge 0}, \tilde \mu$ and, $(L_k)_{k \ge 0}$ be given as in Definition \ref{def:snapg-v2-proto}. 
        But in addition, assume that: 
        \begin{enumerate}[nosep]
            \item Choose $\sigma^{(k)} = \mu^{(I_k)}$. 
        \end{enumerate}
        Then, SNAPG-V2 vanilla affine specifies algorithms that generate the sequence $(y_k, x_k, v_k)_{k \ge 0}$ satisfying conditions as specified in Definition \ref{def:snapg-v2-proto} with the above parameters. 
    \end{definition}
    % --------------------------------------------------------------------------------------------------------------
    What is the weakest possible sequence one can use for the accelerated proximal gradient based algorithm that utilizes a strong convexity constant? 
    If we were to use the developed convergence framework for Nesterov's accelerated proximal gradient, negative momentum and, negative convergence (lower bound instead of upper bound) should be prohibited, and it means that the sequence $(\alpha_k)_{k \ge 0}$ which is going to appear in the proposed algorithm (See Definition \ref{def:snapg-v2}) must satisfy the condition $\alpha_k \in (0, 1]$ for all $k \ge 0$. 
    The following lemma with a blunt name should clarify the sufficient conditions required for the sequence to make sense. 
    \begin{lemma}[weakest possible momentum sequence that makes sense \textcolor{red}{NEW}]\;\label{lemma:snapg-v2-seq-range}\\
        Suppose that $(L_k)_{k \ge 0}$ is a sequence such that $L_k > 0$ for all $k \ge 0$. 
        Suppose that $(\tilde\mu_k)_{k\ge 0}$ is another non-negative sequence. 
        Let $(\alpha_k)_{k \ge 0}$ be a sequence such that $\alpha_0 \in (0, 1]$ and, for all $k \ge 1$, it satisfies recursively the equality: 
        \begin{align*}
            (L_{k - 1}/L_k)(1 - \alpha_{k})\alpha_{k - 1}^2 
            &= \alpha_{k}\left(\alpha_{k} - \tilde \mu_k/L_k\right). 
        \end{align*}
        And, the following items are true: 
        \begin{enumerate}
            \item The expression of $\alpha_k$ based on previous $\alpha_{k - 1}$ is given by: 
            \begin{align*}
                \alpha_k = \frac{L_{k - 1}}{2L_k} \left(
                    - \alpha_{k - 1}^2 + \frac{\tilde\mu_k}{L_{k - 1}}
                    + \sqrt{
                        \left(
                            \alpha_{k - 1} - \frac{\tilde\mu_k}{L_{k - 1}}
                        \right)^2
                        + \frac{4\alpha_{k - 1}^2L_k}{L_{k - 1}}
                    }
                \right) &\ge 0. 
            \end{align*}
            \item If, in addition, the sequence $\tilde \mu_k$ satisfies for all $k \ge 1$, $\frac{\tilde \mu_k}{L_{k - 1}} < L_{k - 1}/ L_k$, then the sequence strictly less than one and, for all $k \ge 1$: $\alpha_k \in (0, 1)$. 
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        For all $k \ge 1$, re-arranging the equality it comes to solving the following equality: 
        \begin{align*}
            0 &= L_k\alpha_k^2 - \tilde\mu_k\alpha_k + L_{k - 1}\alpha_{k - 1}^2\alpha_k - L_{k - 1}\alpha_{k - 1}^2
            \\
            &= L_k\alpha_k^2 + (L_{k - 1}\alpha_{k - 1}^2 - \tilde \mu_k)\alpha_k - L_{k - 1}\alpha_{k - 1}^2
            \\
            \iff 0 &=
            \alpha_k^2 + L_k^{-1}(L_{k - 1}\alpha_{k - 1}^2 - \tilde \mu_k)\alpha_k - L_k^{-1}L_{k - 1}\alpha_{k - 1}^2
            \\
            \iff 
            \alpha_k &= 
            \frac{1}{2}\left(
                -L^{-1}_k(L_{k - 1} \alpha_{k - 1}^2 - \tilde\mu_k)
                + \sqrt{
                    L_k^{-2}(L_{k - 1}\alpha_{k - 1}^2 - \tilde \mu_k)^2
                    + 4L_k^{-1}L_{k - 1} \alpha_{k - 1}^2
                }
            \right)
            \\
            &= \frac{L_{k-1}}{2L_k}\left(
                - \alpha_{k - 1}^2 + \frac{\tilde \mu_k}{L_{k - 1}}
                + \sqrt{
                    \left(
                        \alpha_{k - 1}^2 - \frac{\tilde \mu_k}{L_{k - 1}}
                    \right)^2 + \frac{4L_k}{L_{k - 1}}\alpha_{k - 1}^2
                }
            \right)
        \end{align*}
        Here, we take the positive root of the quadratic so that it ensures $\alpha_k \ge 0$. 
        This is true by induction. 
        If $\alpha_{k - 1} \ge 0$ then the $\frac{4L_k}{L_{k - 1}}\alpha_{k - 1}^2 \ge 0$ hence, the square root is greater than the term outside it so, $\alpha_k \ge 0$ too. 
        \par
        Assume inductively that $\alpha_{k - 1} \ge 0$. 
        Next, we want to find the conditions needed such that $\alpha_k < 1$. 
        To start, we complete the square root inside the square root: 
        \begin{align*}
            0&\le 
            \left(
                \alpha_{k - 1}^2 - \frac{\tilde \mu_k}{L_{k - 1}}
            \right)^2 + \frac{4L_k}{L_{k - 1}}\alpha_{k - 1}^2
            \\
            &= \alpha_{k - 1}^4 + \left(\frac{\tilde \mu_k}{L_{k - 1}}\right)^2
            - 2\alpha_{k - 1}^2 \frac{\tilde \mu_k}{L_{k - 1}} 
            + \frac{4L_k}{L_{k - 1}}\alpha_{k - 1}^2
            \\
            &= \alpha_{k - 1}^4 + \left(\frac{\tilde \mu_k}{L_{k - 1}}\right)^2
            + \alpha_{k - 1}^2 \left(
                \frac{-2\tilde \mu_k}{L_{k - 1}} + \frac{4L_k}{L_{k - 1}}. 
            \right)
            \\
            &= \alpha_{k - 1}^4 + \left(\frac{\tilde \mu_k}{L_{k - 1}}\right)^2
            + \alpha_{k - 1}^2 \left(
                \frac{4L_k - 2\tilde \mu_k}{L_{k - 1}}
            \right)
            \\
            &= \alpha_{k - 1}^4
            + \alpha_{k - 1}^2 \left(
                \frac{4L_k - 2\tilde \mu_k}{L_{k - 1}}
            \right) 
            + \left(
                \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
            \right)^2
            - \left(
                \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
            \right)^2
            + \left(\frac{\tilde \mu_k}{L_{k - 1}}\right)^2
            \\
            &= \left(
                \alpha_{k - 1}^2 + 
                \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
            \right)^2
            - \left(
                \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
            \right)^2
            + \left(\frac{\tilde \mu_k}{L_{k - 1}}\right)^2
            \\
            &= 
            \left(
                \alpha_{k - 1}^2 + 
                \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
            \right)^2
            + 
            \frac{
                \tilde \mu_k^2 - 4L_k^2 - \tilde \mu_k^2 + 4L_k\tilde \mu_k
            }{L_{k - 1}^2}
            \\
            &= 
            \left(
                \alpha_{k - 1}^2 + 
                \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
            \right)^2
            + 
            \frac{
                4L_k \tilde \mu_k - 4L_k^2
            }{L_{k - 1}^2}
            \\
            &= 
            \left(
                \alpha_{k - 1}^2 + 
                \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
            \right)^2
            + 
            4\left(
                \frac{L_k}{L_{k - 1}}\cdot \frac{\tilde \mu_k}{L_{k - 1}} - 1
            \right)
            \\
            &< 
            \left(
                \alpha_{k - 1}^2 + 
                \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
            \right)^2. 
        \end{align*}
        On the last inequality, we used our assumption that the sequence $\tilde\mu_k, L_k$ satisfies $\frac{\tilde \mu_k}{L_{k - 1}} < \frac{L_{k - 1}}{L_k}$. 
        Substitute it back into the expression previous obtained for $\alpha_k$, using the monotone property of the function $\sqrt{\cdot}$, it gives the inequality 
        \begin{align*}
            \alpha_k & < 
            \frac{L_{k-1}}{2L_k}\left(
                - \alpha_{k - 1}^2 + \frac{\tilde \mu_k}{L_{k - 1}}
                + \sqrt{
                    \left(
                        \alpha_{k - 1}^2 + 
                        \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
                    \right)^2
                }
            \right)
            \\
            &= 
            \frac{L_{k-1}}{2L_k}\left(
                - \alpha_{k - 1}^2 + \frac{\tilde \mu_k}{L_{k - 1}}
                + \alpha_{k - 1}^2 + \frac{2L_k - \tilde \mu_k}{L_{k - 1}}
            \right) = 1. 
        \end{align*}
    \end{proof}
    \begin{remark}
        Let's do some sanity check for the lemma we just derived. 
        The sequence $L_k$ will be from the Lipschitz line search routine of the accelerated proximal gradient method. 
        \begin{enumerate}
            \item Let's assume the obvious choice of $L_k = \max_{i = 1, \ldots,n} K^{(i)}$ for all $k = 1, 2, \ldots$ given an objective function $F$ satisfying Assumption \ref{ass:sum-of-many}. 
            Then, the sufficient condition for the second item translates to $\tilde\mu_i/L_k < 1$. 
            Hence, if we choose $\tilde \mu_i$ to be a constant sequence of $0$ then it works out to have $\alpha_k \in (0, 1)$ for all $k = 1, 2, \ldots$. 
            \par
            If $F$ has $L \ge \mu$ so, the function is non-trivial, then choose $\tilde \mu_i = \mu$, the true strong convexity parameter then it also works out. 
            \item Let's assume that some type of monotone line search routine is used for the algorithm making $L_0 \le L_1 \le \ldots \le L_k \le \ldots$ to be a non-decreasing sequence, then it requires $\tilde \mu_k / L_{k - 1} \le L_{k - 1}/L_k$. 
            \par
            Well, it will still make sense because one such choice could be $\tilde \mu_k = \rho\min_{i = 1, \ldots, k} L_{i - 1}/L_i$ for some $\rho \in (0, 1)$. 
        \end{enumerate}
    \end{remark}
    % --------------------------------------------------------------------------------------------------------------
    \begin{lemma}[properties of the iterates on SNAPG-V2 prototype \textcolor{red}{NEW}]\label{lemma:iters-snapg2-proto}
        Suppose that the iterates $(z_k, x_k, y_k)_{k \ge 0}$ and sequence $(\alpha_k)_{k \ge 1}$ satisfies Definition \ref{def:snapg-v2-proto}. 
        Let $\bar x \in \RR^n$.
        Define the sequence $z_k = \alpha_k\bar x + (1 - \alpha_k)x_{k - 1}$. 
        Then, the following are true: 
        \begin{enumerate}
            \item\label{lemma:iters-snapg2-proto-item1} For all $k \ge 1$ it has: 
            \begin{align*}
                    z_k - y_k 
                    = 
                    \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}}(\bar x - v_{k - 1})
                    + \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}}(\bar x - x_{k - 1}).
            \end{align*}
            \item\label{lemma:iters-snapg2-proto-item2} For all $k \ge 1$, it has: $z_k - x_k = \alpha_k(x - \bar x)$
        \end{enumerate}
    \end{lemma}
    \begin{proof}
        \textbf{Proof of \ref{lemma:snapg2-itrs-props-item1}}. 
        From Definition \ref{def:snapg-v2-proto}:
        \begin{align*}
            (1 + \tau_k)^{-1}
            &=
            \left(
                1 + \frac{L_k(1 - \alpha_k)}{L_k\alpha_k - \sigma^{(i)}}
            \right)^{-1} = \left(
                \frac{L_k\alpha_k - \sigma^{(i)} + L_k(1 - \alpha_k)}{L_k\alpha_k - \sigma^{(i)}}
            \right)^{-1}
            = \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}}. 
        \end{align*}
        Therefore, for all $k \ge 0$, $y_k$ has 
        \begin{align*}
            0 &= (1 + \tau_k)^{-1} v_{k - 1} + \tau_k (1 + \tau_k)^{-1} x_{k - 1} - y_k
            \\
            &= \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}} 
            \left(
                v_{k - 1} + \frac{L_k(1 - \alpha_k)}{L_k\alpha_k - \sigma^{(i)}} x_{k - 1}
            \right) - y_k
            \\
            &= \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}} v_{k - 1}
            + \frac{L_k(1 - \alpha_k)}{L_k - \sigma^{(i)}} x_{k - 1} - y_k
            \\
            &= \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
            + 
            \left(
                \frac{L_k(1 - \alpha_k)}{L_k - \sigma^{(i)}} - (1 - \alpha_k)
            \right) x_{k - 1} - y_k
            \\
            &= \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
            + 
            (1 - \alpha_k)\left(
                \frac{L_k - L_k + \sigma^{(i)}}{L_k - \sigma^{(i)}}
            \right) x_{k - 1} - y_k
            \\
            &= \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
            + 
            \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}}x_{k - 1} - y_k. 
        \end{align*}
        Therefore, we establish the equality 
        \begin{align*}
            (1 - \alpha_k)x_{k - 1} - y_k &= 
            - \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}} v_{k - 1} 
            - \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}} x_{k - 1}. 
        \end{align*}
        On the second equality below, we will the above equality, it goes: 
        \begin{align*}
            z_k - y_k &= 
            \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - y_k
            \\
            &= \alpha_k \bar x 
            - \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}} v_{k - 1} 
            - \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}} x_{k - 1}
            \\
            &= \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}}(\bar x - v_{k - 1})
            + \left(
                \alpha_k - \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}}
            \right)\bar x
            - \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}} x_{k - 1}
            \\
            &= \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}}(\bar x - v_{k - 1})
            + \left(
                \frac{\alpha_kL_k - \alpha_k \sigma^{(i)} - L_k\alpha_k + \sigma^{(i)}}{L_k - \sigma^{(i)}}
            \right)\bar x
            - \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}} x_{k - 1}
            \\
            &= \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}}(\bar x - v_{k - 1})
            + \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}}\bar x
            - \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}} x_{k - 1}
            \\
            &= \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}}(\bar x - v_{k - 1})
            + \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}}(\bar x - x_{k - 1}).
        \end{align*}
        \textbf{proof of \ref{lemma:snapg2-itrs-props-item2}.}
        From Definition \ref{def:snapg-v2} it has directly: 
        \begin{align*}
            z_k - x_k &= \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - x_k
            \\
            &= \alpha_k \bar x + x_{k - 1} - x_k - \alpha_k x_{k - 1}
            \\
            &= \alpha_k(\bar x - \alpha_k^{-1}(x_k - x_{k - 1}) - x_{k - 1})
            \\
            &= \alpha_k (\bar x - v_k).
        \end{align*}

    \end{proof}
    % --------------------------------------------------------------------------------------------------------------
    \begin{lemma}[SNAPG-V2 one step convergence stage I \textcolor{red}{NEW}]\label{lemma:snagp2-one-step-s1-proto}
        Suppose that the sequence $(y_k, x_k, v_k)$ satisfies Definition \ref{def:snapg-v2-aff}. 
        Fix some $k \in \N, k \ge 1$, suppose that $I_k = i$.
        Denote $\Pi^{(i)} = \Pi_{\ker A^{(i)}}$ to be the projection matrix onto the kernel of $A^{(i)}$. 
        Then for all $\bar x \in \RR^m$, the iterates satisfies the inequality: 
        \begin{align*}
            &F_i(x_{k}) - F_i(\bar x) 
            + \frac{L_k\alpha_k^2}{2} \left\Vert \left(I - \Pi^{(i)}\right)(\bar x - v_k)\right\Vert^2
            \\
            &\le 
            (1 - \alpha_k)\left(
                F_i(x_{k - 1}) - F_i(\bar x) + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}
                \left\Vert \left(I - \Pi^{(i)}\right)(v_{k - 1} - \bar x)\right\Vert^2
            \right)     
            + \frac{L_k}{2}\left\Vert \Pi^{(i)}(z_k - y_k)\right\Vert^2
                \\ &\quad 
                + \frac{\alpha_k(\tilde\mu - \sigma^{(i)})}{2}\Vert \bar x - v_{k - 1}\Vert^2
                + \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}{2\left(L_k - \sigma^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2.
        \end{align*}
        And when $k = 0$, we have: 
        \todoinline{HERE}
    \end{lemma}
    \begin{proof}
        Let's suppose that $I_k = i$ and, for all $k \ge 0$. 
        Let $z_k = \alpha_k \bar x + (1 - \alpha_k)x_{k - 1}$ where $\bar x$ is a minimizer of $F$. 
        The proof is long so, we use letters and subscript under relations such as $\underset{(\cdot)}{=}, \underset{(\cdot)}{\ge}$ to indicate which result is used going from the previous expression to the next. 
        We list the following intermediate results, (d)-(g) are proved at the end of the proof. 
        \begin{itemize}
            \item[(a)] We can use proximal gradient inequality from Theorem \ref{thm:pg-ineq-semi-scnvx} with $z = z_k, B = L_k, \Pi^{i} = \Pi_{\ker A^{(i)}}$ because Assumption \ref{ass:sum-of-many-aff-comp} has each $F_i = f_i + g_i$ satisfies Assumption \ref{ass:snorm-smth-p-nsmth}. 
            \item[(b)] We can use seminorm Jensen's inequality of Theorem \ref{thm:smnrm-jnsn-smth-nsmth} with $z = z_k$ on $F_i$. 
            \item[(c)] The sequence $(\alpha_k)_{k \ge 0}$ has $(L_{k - 1}/L_k)(1 - \alpha_{k})\alpha_{k - 1}^2 = \alpha_{k}\left(\alpha_{k} - \mu/L_k\right)$. 
            \item[(d)] Prove in Lemma \ref{lemma:iters-snapg2-proto} \ref{lemma:iters-snapg2-proto-item1} we will the equality:
            \begin{align*}
                (\forall k \ge 1)\; 
                z_k - y_k 
                = 
                \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}}(\bar x - v_{k - 1})
                + \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}}(\bar x - x_{k - 1}).
            \end{align*}
            \item [(e)] From Lemma \ref{lemma:iters-snapg2-proto} \ref{lemma:iters-snapg2-proto-item2}, we use: $(\forall k \ge 1)\; z_k - x_k = \alpha_k (\bar x - v_k)$. 
            \item [(f)] Using direct algebra, we have for all $k \ge 1$: 
            \begin{align*}
                \frac{\left(\mu^{(i)}\right)^2(1 - \alpha_k)^2}{2(L_k - \mu^{(i)})} 
                - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}
                = \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}
                {2\left(L_k - \mu^{(i)}\right)}. 
            \end{align*}
            \item [(g)] Using (c), we have for all $k \ge 1$: 
            \begin{align*}
                \frac{\left(
                    L_k\alpha_k - \sigma^{(i)}
                \right)^2}{2(L_k - \sigma^{(i)})} 
                -
                \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} 
                = 
                \frac{
                \left(L_k \alpha_k - \sigma^{(i)}\right)\sigma^{(i)}
                \left(\alpha_k - 1\right)
                }
                {2(L_k - \sigma^{(i)})}
                + \frac{\alpha_k(\tilde \mu_k - \sigma^{(i)})}{2}. 
            \end{align*}
        \end{itemize}
        For all $k \ge 1$, starting with (a) we have: 
        \begin{align}\label{ineq:snapg2-one-step-stage1-chain1}
            \begin{split}
                0 &\le F_i(z_k) - F_i(x_k) - 
                \frac{L_k}{2}\Vert z_k - x_k\Vert^2 
                + \frac{L_k - \nu^{(i)}\sigma_{\min}(A^{(i)})^2}{2}\left\Vert (I - \Pi^{(i)}) (z_k - y_k)\right\Vert^2
                    \\ &\quad 
                    + \frac{L_k}{2}\left\Vert \Pi^{(i)}(z_k - y_k)\right\Vert^2
                \\
                &\underset{\text{(b)}}{\le}
                \alpha_k F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k)     
                - \frac{L_k}{2}\Vert z_k - x_k\Vert^2 
                + \frac{L_k}{2}\left\Vert \Pi^{(i)}(z_k - y_k)\right\Vert^2
                    \\&\quad 
                    - \frac{\nu^{(i)}\sigma_{\min}(A^{(i)})^2\alpha_k(1 - \alpha_k)}{2}\Vert (I - \Pi^{(i)})(\bar x - x_{k - 1})\Vert^2
                    \\&\quad
                    + \frac{L_k - \nu^{(i)}\sigma_{\min}(A^{(i)})^2}{2}\Vert (I - \Pi^{(i)})(z_k - y_k)\Vert^2
            \end{split}
        \end{align}
        \textbf{For simpler notations}, we use the following notation $\Vert x\Vert_p = \Vert (I - \Pi^{(i)})x\Vert$ to denote a seminorm induced by the linear mapping $(I - \Pi^{(i)})$, and use $\langle \cdot,\cdot\rangle_p := \langle (I - \Pi^{(i)})(\cdot), \langle (I - \Pi^{(i)})(\cdot)\rangle$ for the inner product as well. 
        We also use $\sigma^{(i)} = \nu^{(i)}\sigma_{\min}(A^{(i)})^2$ which is specified in Definition \ref{def:snapg-v2-aff}. 
        And we will simplify the last two terms from the above inequality using a chain of equalities. 
        {\allowdisplaybreaks
        \begin{align*}
            & - \frac{\sigma^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2_p
            + \frac{L_k - \sigma^{(i)}}{2}\Vert z_k - y_k\Vert^2_p
            \\
            &\underset{\text{(d)}}{=}
            - \frac{\sigma^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2_p
                \\&\quad
                + \frac{L_k - \sigma^{(i)}}{2}
                \left\Vert
                    \frac{L_k\alpha_k - \sigma^{(i)}}{L_k - \sigma^{(i)}}(\bar x - v_{k - 1})
                    + \frac{\sigma^{(i)}(1 - \alpha_k)}{L_k - \sigma^{(i)}}(\bar x - x_{k - 1})
                \right\Vert^2_p
            \\
            &= 
            - \frac{\sigma^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2_p 
                \\&\quad
                + \frac{(L_k\alpha_k - \sigma^{(i)})^2}{2(L_k - \sigma^{(i)})} \Vert \bar x - v_{k - 1}\Vert^2_p
                + \frac{(\sigma^{(i)})^2(1 - \alpha_k)^2}{2(L_k - \sigma^{(i)})}\Vert \bar x - x_{k - 1}\Vert^2_p 
                \\&\quad 
                + \frac{(L_k\alpha_k  - \sigma^{(i)})\sigma^{(i)}(1 - \alpha_k)}{(L_k - \sigma^{(i)})}\left\langle (\bar x - v_{k - 1}),(\bar x - x_{k - 1})\right\rangle_p
            \\
            &= 
            \left(
                \frac{(\sigma^{(i)})^2(1 - \alpha_k)^2}{2(L_k - \sigma^{(i)})} - \frac{\sigma^{(i)}\alpha_k(1 - \alpha_k)}{2}
            \right)\Vert \bar x - x_{k - 1}\Vert^2_p
                \\ &\quad 
                + \left(
                    \frac{(L_k\alpha_k - \sigma^{(i)})^2}{2(L_k - \sigma^{(i)})} - \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2}
                \right) \Vert \bar x - v_{k - 1}\Vert^2_p
                + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
                \\&\quad 
                + \frac{(L_k\alpha_k  - \sigma^{(i)})\sigma^{(i)}(1 - \alpha_k)}{(L_k - \sigma^{(i)})}\left\langle (\bar x - v_{k - 1}),(\bar x - x_{k - 1})\right\rangle_p
            \\
            &\underset{\text{(f)}}{=}
            \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}
            {2\left(L_k - \sigma^{(i)}\right)} \Vert \bar x - x_{k - 1}\Vert^2_p
                \\ &\quad 
                + \left(
                    \frac{(L_k\alpha_k - \sigma^{(i)})^2}{2(L_k - \sigma^{(i)})} - \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2}
                \right) \Vert \bar x - v_{k - 1}\Vert^2_p
                + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
                \\&\quad 
                + \frac{(L_k\alpha_k  - \sigma^{(i)})\sigma^{(i)}(1 - \alpha_k)}{(L_k - \sigma^{(i)})}\left\langle (\bar x - v_{k - 1}),(\bar x - x_{k - 1})\right\rangle_p
            \\
            & \underset{\text{(g)}}{=} 
            \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}
            {2\left(L_k - \sigma^{(i)}\right)}\Vert \bar x - x_{k - 1}\Vert^2_p
                \\ &\quad 
                + \left(
                    \frac{
                        \left(L_k \alpha_k - \sigma^{(i)}\right)\sigma^{(i)}
                        \left(\alpha_k - 1\right)
                    }
                    {2(L_k - \sigma^{(i)})}
                    + \frac{\alpha_k(\tilde\sigma - \sigma^{(i)})}{2}
                \right) 
                \Vert \bar x - v_{k - 1}\Vert^2_p
                \\ &\quad 
                + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
                \\ &\quad
                + \frac{(L_k\alpha_k  - \sigma^{(i)})\sigma^{(i)}(1 - \alpha_k)}{(L_k - \sigma^{(i)})}\left\langle (\bar x - v_{k - 1}),(\bar x - x_{k - 1})\right\rangle_p
            \\
            &= 
            \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}{2\left(L_k - \sigma^{(i)}\right)}
            \left(
                \Vert \bar x - x_{k - 1}\Vert^2_p 
                + \Vert \bar x - v_{k - 1}\Vert^2_p 
                - 2\left\langle (\bar x - v_{k - 1}),(\bar x - x_{k - 1})\right\rangle_p
            \right) 
                \\ &\quad 
                + \frac{\alpha_k(\tilde\mu - \sigma^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
                + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
            \\
            &= \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}{2\left(L_k - \sigma^{(i)}\right)}
                \Vert x_{k - 1} - v_{k - 1} \Vert^2_p
                \\ &\quad 
                + \frac{\alpha_k(\tilde\mu - \sigma^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
                + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2_p.
        \end{align*}
        }
        Substituting the above back to the tail of Inequality \eqref{ineq:snapg2-one-step-stage1-chain1} it gives: 
        {\allowdisplaybreaks
        \begin{align*}
            0 &\le 
            \alpha_k F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k)     
            - \frac{L_k}{2}\Vert z_k - x_k\Vert^2 
                \\ &\quad 
                + \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}{2\left(L_k - \sigma^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2_p
                + \frac{L_k}{2}\left\Vert \Pi^{(i)}(z_k - y_k)\right\Vert^2
                \\ &\quad 
                + \frac{\alpha_k(\tilde\sigma - \sigma^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
                + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
            \\
            &\underset{\text{(e)}}{=} 
            \alpha_k F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k)     
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2 
                \\ &\quad 
                + \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}{2\left(L_k - \sigma^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2_p
                + \frac{L_k}{2}\left\Vert \Pi^{(i)}(z_k - y_k)\right\Vert^2
                \\ &\quad 
                + \frac{\alpha_k(\tilde\sigma - \sigma^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
                + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
            \\
            &= 
            (\alpha_k - 1) F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k) + F_i(\bar x)
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2 
                \\ &\quad 
                + \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}{2\left(L_k - \sigma^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2_p
                + \frac{L_k}{2}\left\Vert \Pi^{(i)}(z_k - y_k)\right\Vert^2
                \\ &\quad 
                + \frac{\alpha_k(\tilde\sigma - \sigma^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
                + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2_p
            \\
            &= (1 - \alpha_k)\left(
                F_i(x_{k - 1}) - F_i(\bar x) + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert v_{k - 1} - \bar x\Vert^2_p
            \right) 
                \\ & \quad
                - \left(
                    F_i(x_{k}) - F_i(\bar x) + \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
                \right) + \frac{L_k}{2}\left\Vert \Pi^{(i)}(z_k - y_k)\right\Vert^2
                \\ &\quad 
                + \frac{\alpha_k(\tilde\mu - \sigma^{(i)})}{2}\Vert \bar x - v_{k - 1}\Vert^2
                + \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}{2\left(L_k - \sigma^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2.
        \end{align*}
        }
        Re-arranging, with some linear algebra:  we get out first result: 
        \begin{align*}
            &F_i(x_{k}) - F_i(\bar x) + \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2_p 
            \\
            &\le 
            F_i(x_{k}) - F_i(\bar x) + \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
            \\
            &\le 
            (1 - \alpha_k)\left(
                F_i(x_{k - 1}) - F_i(\bar x) + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert v_{k - 1} - \bar x\Vert^2_p
            \right)     
            + \frac{L_k}{2}\left\Vert \Pi^{(i)}(z_k - y_k)\right\Vert^2
                \\ &\quad 
                + \frac{\alpha_k(\tilde\mu - \sigma^{(i)})}{2}\Vert \bar x - v_{k - 1}\Vert^2
                + \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}{2\left(L_k - \sigma^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2.
        \end{align*}
        For the first inequality, recall that $\Vert \cdot\Vert_p = \Vert (I - \Pi^{(i)})(\cdot)\Vert \le \Vert \cdot\Vert$ because $(I - \Pi^{(i)})$ is a projector and, it projects to $\ker A^{(i)}$. 
        Now, let's takle the $k = 0$ case. 
        \todoinline{HERE}
        % INTERMEDIATE RESULTS
        \par
        \textbf{Proof of (f)}. 
        The proof is direct algebra and, it has: 
        {\small\allowdisplaybreaks
        \begin{align*}
            & \frac{\left(\sigma^{(i)}\right)^2(1 - \alpha_k)^2}{2(L_k - \sigma^{(i)})} 
            - \frac{\sigma^{(i)}\alpha_k(1 - \alpha_k)}{2}
            \\
            &= 
            \frac{1}{2\left(L_k - \sigma^{(i)}\right)}
            \left(
                \left(\sigma^{(i)}\right)^2(1 - \alpha_k)^2
                - \left(L_k - \sigma^{(i)}\right)\sigma^{(i)} \alpha_k(1 - \alpha_k)
            \right)
            \\
            &= \frac{1 - \alpha_k}{2\left(L_k - \sigma^{(i)}\right)}\left(
                \left(\sigma^{(i)}\right)^2 
                - \left(\sigma^{(i)}\right)^2\alpha_k 
                - \left(L_k \sigma^{(i)} \alpha_k - \left(\sigma^{(i)}\right)^2 \alpha_k\right)
            \right)
            \\
            &= 
            \frac{1 - \alpha_k}{2(L_k - \sigma)}\left(
                \left(\sigma^{(i)}\right)^2 - L_k\left(\sigma^{(i)}\right)\alpha_k
            \right)
            \\
            &= 
            \frac{(1 - \alpha_k)\sigma^{(i)}\left(\sigma^{(i)} - L_k\alpha_k\right)}
            {2\left(L_k - \sigma^{(i)}\right)}
            \\
            &= \frac{(\alpha_k - 1)\sigma^{(i)}\left(L_k\alpha_k - \sigma^{(i)}\right)}
            {2\left(L_k - \sigma^{(i)}\right)}. 
        \end{align*}
        }    
        % INTERMEDIATE RESULTS 
        \textbf{Proof of (g)}.
        From the property of the $\alpha_k$ sequence stated in item (c), we have: 
        {\allowdisplaybreaks
        \begin{align*}
            &\frac{\left(
                L_k\alpha_k - \sigma^{(i)}
            \right)^2}{2(L_k - \sigma^{(i)})} 
            -
            \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} 
            \\
            &= 
            \frac{\left(
                L_k\alpha_k - \sigma^{(i)}
            \right)^2}{2(L_k - \sigma^{(i)})} 
            -
            \frac{L_k\alpha_k(\alpha_k - \tilde \sigma/L_k)}{2} 
            \\
            &=
            \frac{\left(
                L_k\alpha_k - \sigma^{(i)}
            \right)^2}{2(L_k - \sigma^{(i)})} 
            - \frac{L_k\alpha_k(\alpha_k - \sigma^{(i)}/L_k)}{2}
            + \frac{L_k\alpha_k(\alpha_k - \sigma^{(i)}/L_k)}{2} 
            - \frac{L_k\alpha_k(\alpha_k - \tilde \sigma/L_k)}{2} 
            \\
            &= 
            \frac{\left(
                L_k\alpha_k - \sigma^{(i)}
            \right)^2}{2(L_k - \sigma^{(i)})} 
            - \frac{\alpha_k\left(L_k\alpha_k - \sigma^{(i)}\right)}{2}
            + \frac{L_k\alpha_k}{2}
            \frac{\left(
                \tilde \sigma - \sigma^{(i)}
            \right)}{L_k}
            \\
            &=
            \frac{L_k \alpha_k - \sigma^{(i)}}{2(L_k - \sigma^{(i)})}\left(
                L_k \alpha_k - \sigma^{(i)} 
                - \left(L_k - \sigma^{(i)}\right)\alpha_k
            \right)
            + \frac{\alpha_k(\tilde \sigma - \sigma^{(i)})}{2}
            \\
            &= \frac{L_k \alpha_k - \sigma^{(i)}}{2(L_k - \sigma^{(i)})}\left(
                \sigma^{(i)}\alpha_k - \sigma^{(i)} 
            \right)
            + \frac{\alpha_k(\tilde \sigma - \sigma^{(i)})}{2}
            \\
            &= 
            \frac{
                \left(L_k \alpha_k - \sigma^{(i)}\right)\sigma^{(i)}
                \left(\alpha_k - 1\right)
            }
            {2(L_k - \sigma^{(i)})}
            + \frac{\alpha_k(\tilde \sigma - \sigma^{(i)})}{2}. 
        \end{align*}   
        } 
    \end{proof}
    % The following lemma state the relationships of the iterates generated by SNAPG-V2. 
    % They are needed for the convergence proof. 
    % % --------------------------------------------------------------------------------------------------------------
    % \begin{lemma}[properties of the iterates \textcolor{red}{DEPRECATED}]\label{lemma:snapg2-itrs-props}
    %     Suppose that the iterates $(z_k, x_k, y_k)_{k \ge 0}$ and sequence $(\alpha_k)_{k \ge 1}$ are produced by an algorithm satisfying Definition \ref{def:snapg-v2}. 
    %     Let $\bar x \in \RR^n$.
    %     Define the sequence $z_k = \alpha_k\bar x + (1 - \alpha_k)x_{k - 1}$. 
    %     Then, the following are true: 
    %     \begin{enumerate}
    %         \item\label{lemma:snapg2-itrs-props-item1} For all $k \ge 1$ it has: 
    %         \begin{align*}
    %                 z_k - y_k 
    %                 = 
    %                 \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
    %                 + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}(\bar x - x_{k - 1}).
    %         \end{align*}
    %         \item\label{lemma:snapg2-itrs-props-item2} For all $k \ge 1$, it has: $z_k - x_k = \alpha_k(x - \bar x)$
    %     \end{enumerate}
    % \end{lemma}
    % \begin{proof}
    %     \textbf{Proof of \ref{lemma:snapg2-itrs-props-item1}}. 
    %     From Definition \ref{def:snapg-v2}, it has
    %     \begin{align*}
    %         (1 + \tau_k)^{-1}
    %         &=
    %         \left(
    %             1 + \frac{L_k(1 - \alpha_k)}{L_k\alpha_k - \mu^{(i)}}
    %         \right)^{-1} = \left(
    %             \frac{L_k\alpha_k - \mu^{(i)} + L_k(1 - \alpha_k)}{L_k\alpha_k - \mu^{(i)}}
    %         \right)^{-1}
    %         = \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}. 
    %     \end{align*}
    %     Therefore, for all $k \ge 0$, $y_k$ has 
    %     \begin{align*}
    %         0 &= (1 + \tau_k)^{-1} v_{k - 1} + \tau_k (1 + \tau_k)^{-1} x_{k - 1} - y_k
    %         \\
    %         &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} 
    %         \left(
    %             v_{k - 1} + \frac{L_k(1 - \alpha_k)}{L_k\alpha_k - \mu^{(i)}} x_{k - 1}
    %         \right) - y_k
    %         \\
    %         &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1}
    %         + \frac{L_k(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1} - y_k
    %         \\
    %         &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
    %         + 
    %         \left(
    %             \frac{L_k(1 - \alpha_k)}{L_k - \mu^{(i)}} - (1 - \alpha_k)
    %         \right) x_{k - 1} - y_k
    %         \\
    %         &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
    %         + 
    %         (1 - \alpha_k)\left(
    %             \frac{L_k - L_k + \mu^{(i)}}{L_k - \mu^{(i)}}
    %         \right) x_{k - 1} - y_k
    %         \\
    %         &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} + (1 - \alpha_k)x_{k - 1}
    %         + 
    %         \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}x_{k - 1} - y_k. 
    %     \end{align*}
    %     Therefore, we establish the equality 
    %     \begin{align*}
    %         (1 - \alpha_k)x_{k - 1} - y_k &= 
    %         - \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} 
    %         - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}. 
    %     \end{align*}
    %     On the second equality below, we will the above equality, it goes: 
    %     \begin{align*}
    %         z_k - y_k &= 
    %         \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - y_k
    %         \\
    %         &= \alpha_k \bar x 
    %         - \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}} v_{k - 1} 
    %         - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}
    %         \\
    %         &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
    %         + \left(
    %             \alpha_k - \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}
    %         \right)\bar x
    %         - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}
    %         \\
    %         &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
    %         + \left(
    %             \frac{\alpha_kL_k - \alpha_k \mu^{(i)} - L_k\alpha_k + \mu^{(i)}}{L_k - \mu^{(i)}}
    %         \right)\bar x
    %         - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}
    %         \\
    %         &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
    %         + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}\bar x
    %         - \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}} x_{k - 1}
    %         \\
    %         &= \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
    %         + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}(\bar x - x_{k - 1}).
    %     \end{align*}
    %     \textbf{proof of \ref{lemma:snapg2-itrs-props-item2}.}
    %     From Definition \ref{def:snapg-v2} it has directly: 
    %     \begin{align*}
    %         z_k - x_k &= \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - x_k
    %         \\
    %         &= \alpha_k \bar x + x_{k - 1} - x_k - \alpha_k x_{k - 1}
    %         \\
    %         &= \alpha_k(\bar x - \alpha_k^{-1}(x_k - x_{k - 1}) - x_{k - 1})
    %         \\
    %         &= \alpha_k (\bar x - v_k).
    %     \end{align*}

    % \end{proof}
    % % --------------------------------------------------------------------------------------------------------------
    % \begin{theorem}[SNAPG-V2 one step convergence \textcolor{red}{DEPRECATED}]\label{thm:snapg2-one-step}
    %     Let $F$ satisfies assumption \ref{ass:interp-hypothesis}. 
    %     Suppose that an algorithm satisfying Definition \ref{def:snapg-v2} uses this $F$. 
    %     Let $\mathbb E_k$ denotes the expectation conditioned on $I_0, I_1, \ldots, I_{k - 1}$. 
    %     Then, for all $k \ge 1$, it has the following inequality 
    %     \begin{align*}
    %         & \mathbb E_k\left[F_{I_k}(x_{k})\right] 
    %         - F(\bar x) 
    %         + \mathbb E_k\left[
    %             \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
    %         \right]
    %         \\
    %         &\le 
    %         (1 - \alpha_k)\left(
    %                 \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
    %                 - F(\bar x)
    %                 + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
    %         \right)
    %             \\ &\quad 
    %             + \mathbb E_k\left[
    %                 \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
    %             \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2
    %             + \frac{\alpha_k(\tilde\mu - \mu)}{2} 
    %             \Vert \bar x - v_{k - 1}\Vert^2. 
    %     \end{align*}
    %     And for $k = 0$, it has 
    %     \begin{align*}
    %         \mathbb E \left[ F_{I_0}\right] - F(\bar x) 
    %         + \frac{L_0}{2}\mathbb E \left[\Vert \bar x - x_0\Vert^2\right]
    %         &\le 
    %         \frac{L_0 - \mu}{2}\Vert \bar x - v_{-1}\Vert^2. 
    %     \end{align*}
    % \end{theorem}
    % \begin{proof}
    %     Let's suppose that $I_k = i$ and, for all $k \ge 0$. 
    %     Let $z_k = \alpha_k \bar x + (1 - \alpha_k)x_{k - 1}$ where $\bar x$ is a minimizer of $F$. 
    %     The proof is long so, we use letters and subscript under relations such as $\underset{(\cdot)}{=}, \underset{(\cdot)}{\ge}$ to indicate which result is used going from the previous expression to the next. 
    %     We list the following intermediate results, (d)-(g) are proved at the end of the proof. 
    %     \begin{itemize}
    %         \item[(a)] We can use proximal gradient inequality from Theorem \ref{thm:pg-ineq} with $z = z_k$ because each $F_i$ is $K_i$ Lipschitz smooth and, $\mu^{(i)}$ strongly convex with $K_i \ge \mu^{(i)}$. 
    %         \item[(b)] We can use Jensen's inequality of Theorem \ref{thm:jesen} with $z = z_k$ on $F_i$. 
    %         \item[(c)] The sequence $(\alpha_k)_{k \ge 0}$ has $(L_{k - 1}/L_k)(1 - \alpha_{k})\alpha_{k - 1}^2 = \alpha_{k}\left(\alpha_{k} - \mu/L_k\right)$. 
    %         \item[(d)] Prove in Lemma \ref{lemma:snapg2-itrs-props} \ref{lemma:snapg2-itrs-props-item1} we use the equality:
    %         \begin{align*}
    %             (\forall k \ge 1)\; 
    %             z_k - y_k 
    %             = 
    %             \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
    %             + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}(\bar x - x_{k - 1}).
    %         \end{align*}
    %         \item [(e)] From Lemma \ref{lemma:snapg2-itrs-props} \ref{lemma:snapg2-itrs-props-item2}, we use: $(\forall k \ge 1)\; z_k - x_k = \alpha_k (\bar x - v_k)$. 
    %         \item [(f)] Using direct algebra, we have for all $k \ge 1$: 
    %         \begin{align*}
    %             \frac{\left(\mu^{(i)}\right)^2(1 - \alpha_k)^2}{2(L_k - \mu^{(i)})} 
    %             - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}
    %             = \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}
    %             {2\left(L_k - \mu^{(i)}\right)}. 
    %         \end{align*}
    %         \item [(g)] Using (c), we have for all $k \ge 1$: 
    %         \begin{align*}
    %             \frac{\left(
    %                 L_k\alpha_k - \mu^{(i)}
    %             \right)^2}{2(L_k - \mu^{(i)})} 
    %             -
    %             \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} 
    %             = 
    %             \frac{
    %             \left(L_k \alpha_k - \mu^{(i)}\right)\mu^{(i)}
    %             \left(\alpha_k - 1\right)
    %             }
    %             {2(L_k - \mu^{(i)})}
    %             + \frac{\alpha_k(\tilde \mu_k - \mu^{(i)})}{2}. 
    %         \end{align*}
    %         \item[(h)] Because we assumed interpolation hypothesis in Assumption \ref{ass:interp-hypothesis}, it has $\mathbb E[F_{I_k}(\bar x)] = F(\bar x)$ for all $\bar x$ that is a minimizer of $F$. 
    %     \end{itemize}
    %     For all $k \ge 1$, starting with (a) we have: 
    %     \begin{align}\label{ineq:snapg2-one-step-chain1}
    %         \begin{split}
    %             0 &\le F_i(z_k) - F_i(x_k) - \frac{L_k}{2}\Vert z_k - x_k\Vert^2 + \frac{L_k - \mu^{(i)}}{2}\Vert z_k - y_k\Vert^2
    %             \\
    %             &\underset{\text{(b)}}{\le}
    %             \alpha_k F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k) \\
    %                 &\quad 
    %                 - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
    %                 - \frac{L_k}{2}\Vert z_k - x_k\Vert^2 
    %                 + \frac{L_k - \mu^{(i)}}{2}\Vert z_k - y_k\Vert^2. 
    %         \end{split}
    %     \end{align}
    %     And we have the following chain of equalities:
    %     {\allowdisplaybreaks
    %     \begin{align*}
    %         & - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2
    %         + \frac{L_k - \mu^{(i)}}{2}\Vert z_k - y_k\Vert^2
    %         \\
    %         &\underset{\text{(d)}}{=}
    %         - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2 
    %             \\&\quad
    %             + \frac{L_k - \mu^{(i)}}{2}
    %             \left\Vert
    %                 \frac{L_k\alpha_k - \mu^{(i)}}{L_k - \mu^{(i)}}(\bar x - v_{k - 1})
    %                 + \frac{\mu^{(i)}(1 - \alpha_k)}{L_k - \mu^{(i)}}(\bar x - x_{k - 1})
    %             \right\Vert^2
    %         \\
    %         &= 
    %         - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}\Vert \bar x - x_{k - 1}\Vert^2 
    %             \\&\quad
    %             + \frac{(L_k\alpha_k - \mu^{(i)})^2}{2(L_k - \mu^{(i)})} \Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{(\mu^{(i)})^2(1 - \alpha_k)^2}{2(L_k - \mu^{(i)})}\Vert \bar x - x_{k - 1}\Vert^2 
    %             \\&\quad 
    %             + \frac{(L_k\alpha_k  - \mu^{(i)})\mu^{(i)}(1 - \alpha_k)}{(L_k - \mu^{(i)})}\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
    %         \\
    %         &= 
    %         \left(
    %             \frac{(\mu^{(i)})^2(1 - \alpha_k)^2}{2(L_k - \mu^{(i)})} - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}
    %         \right)\Vert \bar x - x_{k - 1}\Vert^2
    %             \\ &\quad 
    %             + \left(
    %                 \frac{(L_k\alpha_k - \mu^{(i)})^2}{2(L_k - \mu^{(i)})} - \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2}
    %             \right) \Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %             \\&\quad 
    %             + \frac{(L_k\alpha_k  - \mu^{(i)})\mu^{(i)}(1 - \alpha_k)}{(L_k - \mu^{(i)})}\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
    %         \\
    %         &\underset{\text{(f)}}{=}
    %         \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}
    %         {2\left(L_k - \mu^{(i)}\right)} \Vert \bar x - x_{k - 1}\Vert^2
    %             \\ &\quad 
    %             + \left(
    %                 \frac{(L_k\alpha_k - \mu^{(i)})^2}{2(L_k - \mu^{(i)})} - \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2}
    %             \right) \Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %             \\&\quad 
    %             + \frac{(L_k\alpha_k  - \mu^{(i)})\mu^{(i)}(1 - \alpha_k)}{(L_k - \mu^{(i)})}\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
    %         \\
    %         & \underset{\text{(g)}}{=} 
    %         \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}
    %         {2\left(L_k - \mu^{(i)}\right)}\Vert \bar x - x_{k - 1}\Vert^2
    %             \\ &\quad 
    %             + \left(
    %                 \frac{
    %                     \left(L_k \alpha_k - \mu^{(i)}\right)\mu^{(i)}
    %                     \left(\alpha_k - 1\right)
    %                 }
    %                 {2(L_k - \mu^{(i)})}
    %                 + \frac{\alpha_k(\tilde\mu - \mu^{(i)})}{2}
    %             \right) 
    %             \Vert \bar x - v_{k - 1}\Vert^2
    %             \\ &\quad 
    %             + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{(L_k\alpha_k  - \mu^{(i)})\mu^{(i)}(1 - \alpha_k)}{(L_k - \mu^{(i)})}\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
    %         \\
    %         &= 
    %         \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}\left(
    %             \Vert \bar x - x_{k - 1}\Vert^2 + \Vert \bar x - v_{k - 1}\Vert^2 - 2\langle \bar x - v_{k - 1}, \bar x - x_{k - 1}\rangle
    %         \right) 
    %             \\ &\quad 
    %             + \frac{\alpha_k(\tilde\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %         \\
    %         &= \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}
    %             \Vert x_{k - 1} - v_{k - 1} \Vert^2
    %             \\ &\quad 
    %             + \frac{\alpha_k(\tilde\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2.
    %     \end{align*}
    %     }
    %     Substituting the above back to the tail of Inequality \eqref{ineq:snapg2-one-step-chain1} it gives: 
    %     {\allowdisplaybreaks
    %     \begin{align*}
    %         0 &\le 
    %         \alpha_k F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k) 
    %             \\&\quad 
    %             - \frac{L_k}{2}\Vert z_k - x_k\Vert^2 
    %             + \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}
    %             \Vert x_{k - 1} - v_{k - 1} \Vert^2
    %             \\ &\quad 
    %             + \frac{\alpha_k(\tilde\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %         \\
    %         &\underset{\text{(e)}}{=} 
    %         \alpha_k F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k) 
    %             \\&\quad 
    %             - \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
    %             + \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}
    %             \Vert x_{k - 1} - v_{k - 1} \Vert^2
    %             \\ &\quad 
    %             + \frac{\alpha_k(\tilde\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %         \\
    %         &= (\alpha_k - 1)F_i(\bar x) + (1 - \alpha_k)F_i(x_{k - 1}) - F_i(x_k) + F_i(\bar x)
    %             \\&\quad 
    %             - \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
    %             + \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2
    %             \\ &\quad 
    %             + \frac{\alpha_k(\tilde\mu - \mu^{(i)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %         \\
    %         &= (1 - \alpha_k)\left(
    %             F_i(x_{k - 1}) - F_i(\bar x) + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert v_{k - 1} - \bar x\Vert^2
    %         \right) 
    %             \\ & \quad
    %             - \left(
    %                 F_i(x_{k}) - F_i(\bar x) + \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
    %             \right)
    %             \\ &\quad 
    %             + \frac{\alpha_k(\tilde\mu - \mu^{(i)})}{2}\Vert \bar x - v_{k - 1}\Vert^2
    %             + \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}{2\left(L_k - \mu^{(i)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
    %     \end{align*}
    %     }
    %     Recall that $i = I_k$ is the random variable from Definition \ref{def:snapg-v2}. 
    %     Rearranging the last expression in the above equality chain can be conveniently written as
    %     \begin{align}
    %         \begin{split}
    %             & F_{I_k}(x_{k}) - F_{I_k}(\bar x) + \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
    %             \\ &\le 
    %             (1 - \alpha_k)\left(
    %                 F_{I_k}(x_{k - 1}) - F_{I_k}(\bar x) + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert v_{k - 1} - \bar x\Vert^2
    %             \right) 
    %                 \\ &\quad 
    %                 + \frac{\alpha_k(\tilde \mu - \mu^{(I_k)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %                 + \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
    %         \end{split}
    %     \label{ineq:snapg2-one-step-presult1}\end{align}
    %     Recall $\mathbb E_k$ denotes the conditional expectation on $I_0, I_1, \ldots, I_{k - 1}$. 
    %     Taking the conditional expectation on the LHS of the \eqref{ineq:snapg2-one-step-presult1} yields: 
    %     \begin{align*}
    %         & \mathbb E_k\left[
    %             F_{I_k}(x_{k}) - F_{I_k}(\bar x) + \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
    %         \right]
    %         \\
    %         &\underset{\text{(h)}}{=}
    %         \mathbb E_k\left[F_{I_k}(x_{k})\right] 
    %         - F(\bar x) 
    %         + \mathbb E_k\left[
    %             \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
    %         \right]. 
    %     \end{align*}
    %     On the RHS of \eqref{ineq:snapg2-one-step-presult1}, using the linearity property while taking the conditional expectation yields: 
    %     {\allowdisplaybreaks
    %     \begin{align*}
    %         & \mathbb E_k\left[
    %             (1 - \alpha_k)\left(
    %                 F_{I_k}(x_{k - 1}) - F_{I_k}(\bar x) + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert v_{k - 1} - \bar x\Vert^2
    %             \right)
    %         \right]
    %             \\ &\quad 
    %             + \mathbb E_k \left[
    %                 \frac{\alpha_k(\tilde\mu - \mu^{(I_k)})}{2} \Vert \bar x - v_{k - 1}\Vert^2
    %             \right]
    %             + \mathbb E_k\left[
    %                 \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}\Vert x_{k - 1} - v_{k - 1} \Vert^2
    %             \right]
    %         \\
    %         &\underset{\text{(1)}}{=} 
    %         (1 - \alpha_k)\left(
    %                 \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
    %                 -\mathbb E_k [F_{I_k}(\bar x)] 
    %                 + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
    %         \right)
    %             \\ &\quad 
    %             + \mathbb E_k \left[
    %                 \frac{\alpha_k(\tilde\mu - \mu^{(I_k)})}{2} 
    %             \right]\Vert \bar x - v_{k - 1}\Vert^2
    %             + \mathbb E_k\left[
    %                 \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
    %             \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
    %         \\
    %         &\underset{\text{(h)}}{=} 
    %         (1 - \alpha_k)\left(
    %                 \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
    %                 - F(\bar x)
    %                 + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
    %         \right)
    %             \\ &\quad 
    %             + \mathbb E_k \left[
    %                 \frac{\alpha_k(\tilde\mu - \mu^{(I_k)})}{2} 
    %             \right]\Vert \bar x - v_{k - 1}\Vert^2
    %             + \mathbb E_k\left[
    %                 \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
    %             \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2
    %         \\
    %         &\underset{\text{(2)}}{=}
    %         (1 - \alpha_k)\left(
    %                 \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
    %                 - F(\bar x)
    %                 + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
    %         \right)
    %             \\ &\quad 
    %             \frac{\alpha_k(\tilde\mu - \mu)}{2} 
    %             \Vert \bar x - v_{k - 1}\Vert^2
    %             + \mathbb E_k\left[
    %                 \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
    %             \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
    %     \end{align*}
    %     }
    %     We note that at label (1), we used the fact that $\alpha_k$ is a constant and, $x_{k - 1}, v_{k - 1}$ only depends on random variable $I_0, I_1, \ldots, I_{k - 1}$ hence it falls out of the conditional expectation $\mathbb E_k$. 
    %     At label (2), we used assumption (Assumption \ref{ass:sum-of-many}) that the averages of all the $\mu^{(I_k)}$ on each $F_{I_k}$ equals to $\mu$ hence, the expectation evaluates to zero by linearity of the expected value operator. 
    %     \par
    %     Combining the above results on the expectation of RHS, and LHS of \eqref{ineq:snapg2-one-step-presult1}, we have the one-step inequality in expectation: 
    %     \begin{align*}
    %         & \mathbb E_k\left[F_{I_k}(x_{k})\right] 
    %         - F(\bar x) 
    %         + \mathbb E_k\left[
    %             \frac{L_k\alpha_k^2}{2}\Vert \bar x - v_k\Vert^2 
    %         \right]
    %         \\
    %         &\le 
    %         (1 - \alpha_k)\left(
    %                 \mathbb E_k \left[F_{I_k}(x_{k - 1})\right] 
    %                 - F(\bar x)
    %                 + \mathbb E_k \left[\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\right]\Vert v_{k - 1} - \bar x\Vert^2
    %         \right)
    %             \\ &\quad 
    %             + \mathbb E_k\left[
    %                 \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
    %             \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2. 
    %     \end{align*}
    %     Finally, we show the base case. 
    %     When $k = 0$, by assumption it had $\alpha_0 = 1$ hence $\tau_0$ in Definition \ref{def:snapg-v2} has $\tau_0 = 0$ which makes $y_0 = v_{- 1} = x_{-1}$. 
    %     Therefore, it makes $x_0 = T_{L_0}(y_0 | F_{I_0}) = T_{L_0}(v_{-1} | F_{I_0})$. 
    %     Similarly, it has also $z_0 = \bar x$.
    %     Applying Theorem \ref{thm:pg-ineq} with $z = z_0$ and, assume a successful line search with $L_0$, it yields: 
    %     \begin{align*}
    %         0 &\le F_{I_0}(z_0) - F_{I_0}(x_0) - \frac{L_0}{2}\Vert z_0 - x_0\Vert^2 + \frac{L_0 - \mu^{(I_0)}}{2}\Vert z_0 - y_0\Vert^2
    %         \\
    %         &= F_{I_0}(\bar x) - F_{I_0}(x_0) - \frac{L_0}{2}\Vert \bar x - x_0\Vert^2 + \frac{L_0 - \mu^{(I_0)}}{2}\Vert \bar x - v_{-1}\Vert^2. 
    %     \end{align*}
    %     Re-arranging and taking the expectation it yields: 
    %     \begin{align*}
    %         \mathbb E \left[
    %             F_{I_0}(x_0) - F_{I_0}(\bar x) + \frac{L_0}{2}\Vert \bar x - x_0\Vert^2
    %         \right]
    %         &\underset{\text{(h)}}{=}
    %         \mathbb E \left[ F_{I_0}\right] - F(\bar x) 
    %         + \frac{L_0}{2}\mathbb E \left[\Vert \bar x - x_0\Vert^2\right]
    %         \\
    %         &\le \frac{L_0 - \mathbb E \left[\mu^{(I_0)}\right]}{2}\Vert \bar x - v_{-1}\Vert^2
    %         \\
    %         &= \frac{L_0 - \mu}{2}\Vert \bar x - v_{-1}\Vert^2. 
    %     \end{align*}
    %     % INTERMEDIATE RESULTS
    %     \textbf{Proof of (f)}. 
    %     The proof is direct algebra and, it has: 
    %     {\small\allowdisplaybreaks
    %     \begin{align*}
    %         & \frac{\left(\mu^{(i)}\right)^2(1 - \alpha_k)^2}{2(L_k - \mu^{(i)})} 
    %         - \frac{\mu^{(i)}\alpha_k(1 - \alpha_k)}{2}
    %         \\
    %         &= 
    %         \frac{1}{2\left(L_k - \mu^{(i)}\right)}
    %         \left(
    %             \left(\mu^{(i)}\right)^2(1 - \alpha_k)^2
    %             - \left(L_k - \mu^{(i)}\right)\mu^{(i)} \alpha_k(1 - \alpha_k)
    %         \right)
    %         \\
    %         &= \frac{1 - \alpha_k}{2\left(L_k - \mu^{(i)}\right)}\left(
    %             \left(\mu^{(i)}\right)^2 
    %             - \left(\mu^{(i)}\right)^2\alpha_k 
    %             - \left(L_k \mu^{(i)} \alpha_k - \left(\mu^{(i)}\right)^2 \alpha_k\right)
    %         \right)
    %         \\
    %         &= 
    %         \frac{1 - \alpha_k}{2(L_k - \mu)}\left(
    %             \left(\mu^{(i)}\right)^2 - L_k\left(\mu^{(i)}\right)\alpha_k
    %         \right)
    %         \\
    %         &= 
    %         \frac{(1 - \alpha_k)\mu^{(i)}\left(\mu^{(i)} - L_k\alpha_k\right)}
    %         {2\left(L_k - \mu^{(i)}\right)}
    %         \\
    %         &= \frac{(\alpha_k - 1)\mu^{(i)}\left(L_k\alpha_k - \mu^{(i)}\right)}
    %         {2\left(L_k - \mu^{(i)}\right)}. 
    %     \end{align*}
    %     }    
    %     % INTERMEDIATE RESULTS 
    %     \textbf{Proof of (g)}.
    %     From the property of the $\alpha_k$ sequence stated in item (c), we have: 
    %     {\allowdisplaybreaks
    %     \begin{align*}
    %         &\frac{\left(
    %             L_k\alpha_k - \mu^{(i)}
    %         \right)^2}{2(L_k - \mu^{(i)})} 
    %         -
    %         \frac{\alpha_{k - 1}^2L_{k - 1}(1 - \alpha_k)}{2} 
    %         \\
    %         &= 
    %         \frac{\left(
    %             L_k\alpha_k - \mu^{(i)}
    %         \right)^2}{2(L_k - \mu^{(i)})} 
    %         -
    %         \frac{L_k\alpha_k(\alpha_k - \tilde \mu/L_k)}{2} 
    %         \\
    %         &=
    %         \frac{\left(
    %             L_k\alpha_k - \mu^{(i)}
    %         \right)^2}{2(L_k - \mu^{(i)})} 
    %         - \frac{L_k\alpha_k(\alpha_k - \mu^{(i)}/L_k)}{2}
    %         + \frac{L_k\alpha_k(\alpha_k - \mu^{(i)}/L_k)}{2} 
    %         - \frac{L_k\alpha_k(\alpha_k - \tilde \mu/L_k)}{2} 
    %         \\
    %         &= 
    %         \frac{\left(
    %             L_k\alpha_k - \mu^{(i)}
    %         \right)^2}{2(L_k - \mu^{(i)})} 
    %         - \frac{\alpha_k\left(L_k\alpha_k - \mu^{(i)}\right)}{2}
    %         + \frac{L_k\alpha_k}{2}
    %         \frac{\left(
    %             \tilde \mu - \mu^{(i)}
    %         \right)}{L_k}
    %         \\
    %         &=
    %         \frac{L_k \alpha_k - \mu^{(i)}}{2(L_k - \mu^{(i)})}\left(
    %             L_k \alpha_k - \mu^{(i)} 
    %             - \left(L_k - \mu^{(i)}\right)\alpha_k
    %         \right)
    %         + \frac{\alpha_k(\tilde \mu - \mu^{(i)})}{2}
    %         \\
    %         &= \frac{L_k \alpha_k - \mu^{(i)}}{2(L_k - \mu^{(i)})}\left(
    %             \mu^{(i)}\alpha_k - \mu^{(i)} 
    %         \right)
    %         + \frac{\alpha_k(\tilde \mu - \mu^{(i)})}{2}
    %         \\
    %         &= 
    %         \frac{
    %             \left(L_k \alpha_k - \mu^{(i)}\right)\mu^{(i)}
    %             \left(\alpha_k - 1\right)
    %         }
    %         {2(L_k - \mu^{(i)})}
    %         + \frac{\alpha_k(\tilde \mu - \mu^{(i)})}{2}. 
    %     \end{align*}   
    %     } 
    % \end{proof}
\section{Convergence rate of the algorithm under various circumstances}
    The previous section highlighted a generic convergence results from one iteration of the algorithm, however, there are a lot of loose ends. 
    This section will deal with those. 


\section{\textcolor{purple}{So, what to do next?}}
    Hi Arron would you like to add me for the co-authorship to continue this line of work and see how Nesterov's Accelerated Technique may work out for the stochastic gradient method? 
    These results are solid results but, they are still partial results and, below are the potential I foresee for this these ideas. 
    \begin{enumerate}
        \item Narrow down the sequence $\alpha_k$ and make sure that it can allow the quantity: 
        \begin{align*}
            \mathbb E_k\left[
                    \frac{(\alpha_k - 1)\mu^{(I_k)}\left(L_k\alpha_k - \mu^{(I_k)}\right)}{2\left(L_k - \mu^{(I_k)}\right)}
                \right]\Vert x_{k - 1} - v_{k - 1} \Vert^2
        \end{align*}
        is negative, or at least bounded. I am not sure how this will work out, but I have some solid ideas around it. 
        \item Roll up the inequality in Theorem \ref{thm:snapg2-one-step} recursively and, determine the convergence rate through $\alpha_k$ that makes the previous item true. 
        In addition, I have the hunches that the convergence rate involves the variance of $\mu^{(I_k)}$ and, it will slower than the non-stochastic case of the algorithm. 
    \end{enumerate}
    For the future we can: 
    \begin{enumerate}
        \item Extend the definition of strong convexity to relative strong convexity with respect to a quasi-norm. This would extend interpolation hypothesis in Assumption \ref{ass:interp-hypothesis} where, even if $\mu > 0$, it doesn't mean that $F$ has a unique solution through strong convexity. This is entirely possible and appeared in the literatures before so, I can give you the words of confidence. 
        \item Show the convergence of the method for objective function based on quasi-strong convexity. This is a much weaker assumption it works well in practice for the common known problems in convex programming. 
    \end{enumerate}

\bibliographystyle{siam}

\bibliography{references/refs.bib}


\end{document}
