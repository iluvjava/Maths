\documentclass[11pt]{beamer}
\usetheme{Madrid}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{wrapfig}
\usepackage{subcaption}
\graphicspath{{.}}

\author{Hongda Li}
\title[Catalyst Acceleration]{Catalyst Meta Acceleration Framework: The history and the gist of it}
% Informe o seu email de contato no comando a seguir
% Por exemplo, alcebiades.col@ufes.br
\newcommand{\email}{alto@mail.ubc.ca}
% \setbeamercovered{transparent}
\setbeamertemplate{navigation symbols}{}
%\logo{}
\institute[]{UBC Okanagan}
\date{\today}
\subject{MATH 590 2024 Fall Winter}

% ---------------------------------------------------------
% Selecione um estilo de referÃªncia
\bibliographystyle{IEEEtran}

%\bibliographystyle{abbrv}
%\setbeamertemplate{bibliography item}{\insertbiblabel}
% ---------------------------------------------------------

% ---------------------------------------------------------
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\usepackage{multicol}
\input{presets/wang/custom_commands.tex}




\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{ToC}
    \begin{multicols}{2}
        \tableofcontents
    \end{multicols}
\end{frame}

%     \subsection{Taxonomy of Proximal type of Methods}
%         \begin{frame}{Frame Title}
%             \begin{block}{Formula Presented in Block}
%                 \begin{align}
%                     \min_{x} g(x) + h(x)
%                 \end{align}    
%             \end{block}
%             \begin{itemize}
%                 \item [1.]Throughout this presentation, we assume the objective of a function $f$ is the sum of 2 functions.
%                 \item [2.]We are interested in the paper: FISTA (Fast Iterative-Shrinkage Algorithm) by Beck and Teboulle \cite{paper:FISTA}. 
%                 \pause 
%                 \item [1.] When $h = \delta_Q$ with $Q$ closed and convex with $Q\subseteq \text{ri}\circ \text{dom}(g)$, we use projected subgradient. 
%                 \item [2.] When $g$ is \textbf{\emph{strongly smooth}} and $h$ is \textbf{closed convex proper} whose proximal oracle is easy to compute, we consider the use of FISTA. 
%             \end{itemize}           
%         \end{frame}    
%     \subsection{The Proximal Operator}
%         \begin{frame}{Frame Title}
%             \begin{definition}[Definition of Something]
%                 Let $f$ be convex closed and proper, then the proximal operator parameterized by $\alpha > 0$ is a non-expansive mapping defined as: 
%                 \begin{align*}
%                     \text{prox}_{f, \alpha}(x) := 
%                     \arg\min_{y}\left\lbrace
%                         f(y) + \frac{1}{2\alpha} \Vert y - x\Vert^2
%                     \right\rbrace. 
%                 \end{align*}
%             \end{definition}  
%             \begin{remark}
%                 When $f$ is convex, closed, and proper, 
%             \end{remark}
%         \end{frame}
%         \begin{frame}{Prox is the Resolvant of Subgradient}
%             \begin{lemma}[The Lemma]\label{lemma:prox_alternative_form}
%                 When the function $f$ is convex closed and proper, the $\text{prox}_{\alpha, f}$ can be viewed as the following operator $(I + \alpha \partial f)^{-1}$. 
%             \end{lemma}
%             \begin{proof}
%                 Minimizer satisfies zero subgradient condition, 
%                 {\scriptsize
%                 \begin{align*}
%                     \mathbf 0 &\in \partial
%                     \left[
%                         \left.
%                             f(y) + \frac{1}{2\alpha} \Vert y - x\Vert^2 
%                         \right| y
%                     \right](y^+)
%                     \\
%                     \mathbf 0 &\in \partial f(y^+) + \frac{1}{\alpha}(y^+ - x)
%                     \\
%                     \frac{x}{\alpha} &\in 
%                     (\partial f + \alpha^{-1}I)(y^+)
%                     \\
%                     x &\in 
%                     (\alpha \partial f + I)(y^+)
%                     \\
%                     y &\in (\alpha\partial f+ I)^{-1}(x).
%                 \end{align*}
%                 }
%             \end{proof}
%         \end{frame}
        
%     \subsection{Strong Smoothness}
%         \begin{frame}{Equivalence of Strong Smoothness and Lipschitz Gradient}
%             \begin{theorem}[Lipschitz Gradient Equivalence under Convexity]
%                 Suppose $g$ is differentiable on the entire of $\mathbb E$. It is closed convex proper. It is strongly smooth with parameter $\alpha$ if and only if the gradient $\nabla g$ is globally Lipschitz continuous with a parameter of $\alpha$ and $g$ is closed and convex. 
%                 \begin{align*}
%                     \Vert \nabla g(x) -\nabla g(y)\Vert \le 
%                     \alpha 
%                     \Vert x - y \Vert\quad \forall x, y\in \mathbb E
%                 \end{align*}
%             \end{theorem}
%             \begin{proof}
%                 Using line integral, we can prove Lipschitz gradient implies strong smoothness without convexity. The converse requires convexity and applying generalized Cauchy Inequality to (iv) in Theorem 5.8 for Beck's textbook \cite{book:first_order_opt}. 
%             \end{proof}
%         \end{frame}
%     \subsection{A Major Assumption}    
%         \begin{frame}{A Major Assumption}
%             \begin{assumption}[Convex Smooth Nonsmooth with Bounded Minimizers]\label{assumption:1}
%                 We will assume that $g:\mathbb E\mapsto \mathbb R$ is \textbf{strongly smooth} with constant $L_g$ and $h:\mathbb E \mapsto \bar{\mathbb R}$ \textbf{is closed convex and proper}. We define $f := g + h$ to be the summed function and $\text{ri}\circ \text{dom}(g) \cap \text{ri}\circ \text{dom}(h) \neq \emptyset$. We also assume that a set of minimizers exists for the function $f$ and that the set is bounded. Denote the minimizer using $\bar x$. 
%             \end{assumption}
%         \end{frame}
        
    
% \section{A New Fancy Section}
%     \subsection{A Fancy Subsetction for Algorithm}
%         \begin{frame}{The Accelerated Proximal Gradient Method}
%             \begin{block}{Momentum Template Method}
%                 \begin{algorithm}[H]
%                     \begin{algorithmic}[1]
%                         \STATE{\textbf{Input:} $x^{(0)}, x^{(-1)}, L, h, g$; 2 initial guesses and stepsize L}
%                         \STATE{$y^{(0)} = x^{(0)} + \theta_k (x^{(0)} - x^{(-1)})$}
%                         \FOR{$k = 1, \cdots, N$}
%                             \STATE{$x^{(k)} = \text{prox}_{h, L^{-1}}(y^{(k)} + L^{-1}\nabla g(y^{(k)})) =: \mathcal P_{L^{-1}}^{g, h}(y^{(k)})$}
%                             \STATE{$y^{(k + 1)} = x^{(k)} + \theta_k(x^{(k)} - x^{(k - 1)})$}
%                         \ENDFOR
%                     \end{algorithmic}
%                     \caption{Template Proximal Gradient Method With Momentum}\label{alg:fista_template}
%                 \end{algorithm}
%             \end{block}
%         \end{frame}

\section{Introduction}
    \subsection{The History and a Series of Papers}
        \begin{frame}{Nesterov's Book}
            \begin{figure}
                \centering
                \includegraphics[width=10em]{assets/Nesterov Book.png}    
            \end{figure}
            \begin{itemize}
                \item Yurri Nesterov's book: ``Lectures on Convex Optimization'' 2018, Springer \cite{nesterov_lectures_2018}. 
            \end{itemize}
        \end{frame}
        \begin{frame}{Accelerated Proximal Point Method}
            \begin{figure}
                \centering
                \includegraphics[width=25em]{assets/Acc ppm}
            \end{figure}
            \begin{itemize}
                \item Osman Guler's, ``New proximal point algorithm for convex optimization'', SIAM J.Optimization 1992. \cite{guler_new_1992}
            \end{itemize}
        \end{frame}
        \begin{frame}{Catalyst Acceleration}
            \begin{figure}
                \centering
                \subfloat[Lin 2015\label{fig:a}]{\includegraphics[width=15em]{assets/lin2015.png}}
                \subfloat[Paquette 2018\label{fig:b}]{\includegraphics[width=15em]{assets/paquette 2018.png}}
                % \caption{Lin's }
                % \label{fig:1}
            \end{figure}
            \begin{itemize}
                \item Honzhou Lin et al. ``Universal Catalyst for first order optimization'' 2015 JMLR \cite{lin_universal_2015}.
                \item Paquette et al. ``Catalyst for gradient-based non-convex optimization'' 2018 JMLR \cite{paquette_catalyst_2018}. 
            \end{itemize}
        \end{frame}
        \begin{frame}{Objectives of the Talk}
            \begin{block}{List of objectives}
                \begin{enumerate}
                    \item Introduce the technique of Nesterov's estimating sequence for convergence proof of algorithms. 
                    \item Understand the historical context for the inspirations of the Catalyst algorithm.  
                    \item Understand the theories behind the Catalyst meta acceleration. 
                    \item Understand key innovations for controlling the errors in Catalyst accelerations. 
                    \item Introduce the Non-convex extension of the method. 
                \end{enumerate}
            \end{block}
            \pause
            \begin{block}{A note on the scope}
                Specific applications and algorithms are outside the scope because variance reduced stochastic method is itself a big topic.     
            \end{block}
        \end{frame}
    \subsection{Nesterov's Estimating Sequence}
        \begin{frame}{Nesterov's Estimating Sequence}            
            \begin{definition}[Nesterov's estimating sequence]\label{def:nes-est-seq}
                Let $(\phi_k : \RR^n \mapsto \RR)_{k \ge 0}$ be a sequence of functions. 
                We call this sequence of function a Nesterov's estimating sequence when it satisfies the conditions: 
                \begin{enumerate}
                    \item There exists another sequence $(x_k)_{k \ge 0}$ such that for all $k \ge 0$ it has $F(x_k) \le \phi_k^*: =\min_{x}\phi_k^*(x)$. 
                    \item There exists a sequence of $(\alpha_k)_{k \ge 0}$ where $\alpha_k \in (0, 1)\; \forall k \ge0 $ such that for all $x \in \RR^n$ it has $\phi_{k + 1}(x) - \phi_k(x) \le - \alpha_k(\phi_k(x) - F(x))$. 
                \end{enumerate}
            \end{definition}
        \end{frame}
        \begin{frame}{Nesterov's Estimating Sequence and Convergence}
            \begin{block}{Observations}
                {\small
                If we define $\phi_k$, $\Delta_k(x) := \phi_k (x) - F(x)$ for all $x \in \RR^n$ and assume that $F$ has minimizer $x^*$. 
                Then observe that $\forall k \ge 0$:  
                \begin{align*}
                    \phi_{k + 1}(x) - \phi_k(x) 
                    &\le - \alpha_k (\phi_k(x) - F(x))
                    \\
                    \iff 
                    \phi_{k + 1}(x) - F(x) - (\phi_k(x) - F(x))
                    &\le 
                    -\alpha_k(\phi_k(x) - F(x))
                    \\
                    \iff
                    \Delta_{k + 1}(x) - \Delta_k(x) &\le
                    - \alpha_k\Delta_k(x)
                    \\
                    \iff 
                    \Delta_{k + 1}(x) 
                    &\le 
                    (1 - \alpha_k)\Delta_k(x). 
                \end{align*}
                Unroll the recurrence, by setting $x = x^*$, $\Delta_k(x^*)$ is non-negative and using the property of Nesterov's estimating sequence it gives: 
                \begin{align*}
                    F(x_k) - F(x^*) \le \phi_k^* - F(x^*) \le \Delta_k(x^*) 
                    &= \phi_k(x^*) - F(x^*) 
                    \\
                    &\le \left(\prod_{i = 0}^k(1 - \alpha_i)\right)\Delta_0(x^*).
                \end{align*}
                }
            \end{block}
        \end{frame}
    \subsection{Example: Accelerated proximal gradient}
        \begin{frame}{Example: accelerated proximal gradient}
            The following algorithm is proved in the report which it's similar to Nesterov's 2.2.20 in his book \cite{nesterov_lectures_2018}. 
            \begin{block}{Quick Notations}
                Assume that: $F = f + g$ where $f$ is $L$-Lipschitz smooth and $\mu \ge 0$ strongly convex and $g$ is convex. 
                Define 
                \begin{align*}
                    \mathcal M^{L^{-1}}(x; y) 
                    &:= g(x) + f(y) 
                    + 
                    \left\langle \nabla f(x), x - y\right\rangle 
                    + 
                    \frac{L}{2}\Vert x - y\Vert^2, 
                    \\
                    \widetilde{\mathcal J}_{L^{-1}}y 
                    &:= \argmin_{x} \mathcal M^{L^{-1}}(x; y), 
                    \\
                    \mathcal G_{L^{-1}}(y)
                    &:= L\left(I - \widetilde{\mathcal J}_{L^{-1}}\right)y. 
                \end{align*}
            \end{block}
        \end{frame}
        \begin{frame}{Example: accelerated proximal gradient}
            \begin{definition}[Accelerated proximal gradient estimating sequence]
                \label{def:nes-est-seq-pg}
                {\small
                    Define $(\phi_k)_{k \ge0}$ be the Nesterov's estimating sequence recursively given by: 
                    \begin{align*}
                        &\textcolor{red}{
                            l_F(x; y_k) := 
                            F\left(\widetilde{\mathcal J}_{L^{-1}} y_k \right) 
                            + \langle \mathcal G_{L^{-1}}y_k, x - y_k\rangle + 
                            \frac{1}{2L}\Vert \mathcal G_{L^{-1}}y_k\Vert^2,
                        } 
                        \\
                        & 
                        \phi_{k + 1}(x)
                        := (1 - \alpha_k)\phi_k (x) + 
                        \alpha_k 
                        \left(
                            l_F(x; y_k) + \frac{\mu}{2}\Vert x - y_k\Vert^2
                        \right). 
                    \end{align*}
                    The Algorithm generates a sequence of vectors $y_k, x_k$, and scalars $\alpha_k$ satisfies the following: 
                    \begin{align*}
                        &x_{k + 1} = \widetilde{\mathcal J}_{L^{-1}} y_k, 
                        \\
                        & \text{find } \alpha_{k + 1} \in (0, 1): 
                        \alpha_{k + 1} = (1 - \alpha_{k + 1})\alpha_k^{2} + (\mu/L) \alpha_{k + 1}
                        \\
                        &y_{k + 1} = x_{k + 1} + \frac{\alpha_k(1 - \alpha_k)}{\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
                    \end{align*}
                    One of the possible base case can be $x_0 = y_0$ and any $\alpha_0 \in (0, 1)$. 
                }
            \end{definition}
        \end{frame}

\section{Guler 1993}
    \begin{frame}{Guler 1993: Accelerated proximal point method}
        Guler in 1993 discovered the following: 
        \begin{enumerate}
            \item The method of proximal point can be accelerated via Nesterov's estimating sequence. 
            \item The accelerated convergence rate retains for certain magnitude of errors on inexact evaluation of proximal point method. 
        \end{enumerate}
        \begin{block}{Quick notations}
            We use the following list of notations: 
            \begin{align*}
                \mathcal M^{\lambda} (x; y) &:= F(x) + \frac{1}{2\lambda}\Vert x - y\Vert^2
                \\
                \mathcal J_\lambda y &:= \argmin_x \mathcal M^{\lambda} (x; y)
                \\
                \mathcal G_\lambda &:= \lambda^{-1}(I - \mathcal J_\lambda). 
            \end{align*}
            We use $\mathcal G_k, \mathcal J_k, \mathcal M_k$ as a short for $\mathcal G_{\lambda_k}, \mathcal J_{\lambda_k}, \mathcal M_{\lambda_k}$. 
            $(\lambda_k)_{k \ge0}$ is a sequence that controls proximal operator. 
        \end{block}
    \end{frame}
    \subsection{Exact Accelerated PPM}
        \begin{frame}{Estimating sequence of accelerated PPM}
            \begin{definition}[Accelerated PPM estimating sequence]
                {\small
                    \label{def:nes-est-seq-acc-ppm}
                    $(\phi_k)_{k \ge0}$ has for all $k \ge0$, any $A \ge 0$: 
                    \begin{align*}
                        \phi_0 &:= f(x_0) + \frac{A}{2}\Vert x - x_0\Vert^2, 
                        \\
                        \phi_{k + 1}(x) &:= 
                        (1 - \alpha_k)\phi_k(x)
                        + 
                        \alpha_k(
                            \textcolor{red}
                            {
                                F(\mathcal J_k y_k) + \langle \mathcal G_k y_k, x - \mathcal J_k y_k\rangle
                            }
                        ).    
                    \end{align*}
                    $(\lambda_k)_{k \ge 0}$, $x_k = \mathcal J_\lambda y_k$. 
                    Auxiliary vectors $(y_k, v_k)$, and $(\alpha_k, A_k)_{k\ge 0}$ satisfies $k\ge0$:
                    \begin{align*}
                        \alpha_k &= \frac{1}{2}\left(
                            \sqrt{(A_k\lambda_k)^2 + 4A_k \lambda_k}
                            - A_k\lambda_k
                        \right) 
                        \\
                        y_k &= (1 - \alpha_k)x_k + \alpha_k v_k
                        \\
                        v_{k + 1}
                        &= 
                        v_k - \frac{\alpha_k}{A_{k + 1}\lambda_k}(y_k - \mathcal J_k y_k)
                        \\
                        A_{k + 1} &= (1 - \alpha_k)A_k. 
                    \end{align*}
                }
            \end{definition}
        \end{frame}
        \begin{frame}{Convergence of accelerated PPM} 
            \begin{block}{An accelerated rate}
                The accelerated PPM generate $(x_k)_{k\ge 0}$ such that $F(x_k) - F^*$ converges at a rate of: 
                {\large
                \begin{align*}
                    \mathcal O\left(
                        \frac{1}{\left(
                            \sum_{i = 1}^{k}\sqrt{\lambda_i}
                        \right)^2}
                    \right). 
                \end{align*}    
                }
            \end{block}
            Note, PPM without accelerate converges at a rate of $\mathcal O((\sum_{i = 1}^{k}\lambda_i)^{-1})$. 
        \end{frame}
    \subsection{Inexact accelerated PPM}
        \begin{frame}{Accelerated Inexact PPM}
            Guler cited Rockafellar 1976 \cite{rockafellar_monotone_1976} for condition (A'): 
            \begin{align*}
                x_{k + 1}\approx \mathcal J_{k} y_k \text{ be such that: }
                \dist\left(
                    \mathbf 0, \partial \mathcal M^{k}(x_{k + 1}; y_k)
                \right) &\le \frac{\epsilon_k}{k}
                \\
                \implies 
                \Vert x_{k + 1} - \mathcal J_{k}y_k\Vert 
                &\le \epsilon_k. 
            \end{align*}
            Putting things into the context of accelerated PPM, the theorem follows is pivotal: 
            \begin{theorem}[\small Guler's inexact proximal point error bound (Lemma 3.1)]
                Define the minimum of the Moreau Envelope: $\mathcal M_k^* := \min_z \mathcal {M}^{\lambda_k}(z; y_k)$. 
                If $x_{k +1}$ is an inexact evaluation under condition (A'), then the estimating sequence admits the conditions that: 
                \begin{align*}
                    \frac{1}{2\lambda_k} \Vert x_{k + 1} - \mathcal J_k y_k\Vert^2
                    &= 
                    \mathcal M_k(x_{k + 1}, y_k) - \mathcal M^*_k
                    \le \frac{\epsilon_k^2}{2\lambda_k}. 
                \end{align*}
            \end{theorem}
        \end{frame}
        \begin{frame}{Guler's Major Results}
            \begin{theorem}[\small Guler's accelerated inexact PPM convergence (Theorem 3.3)]
                If the error sequence $(\epsilon_k)_{k \ge0}$ for condition A' is bounded by $\mathcal O(1/k^\sigma)$ for some $\sigma > 1/2$, then the accelerated proximal point method has for any feasible $x \in \RR^n$: 
                \begin{align*}
                    f(x_k) - f(x) \le \mathcal O(1/k^2) + \mathcal (1 / k^{2\sigma - 1}) \rightarrow 0. 
                \end{align*}    
            \end{theorem}
            If $\sigma \ge 3/2$, the method converges at a rate of $\mathcal O(1/k^2)$. 
            \pause 
            It looks exciting, but it's not exciting for practical purposes because: 
            \begin{enumerate}
                \item Determining $(\epsilon_k)_{k\ge 0}$ requires knowledge on $\phi_k^*$. 
                \item $\phi_k^*$ is expressed with intractable quantity: $F(\mathcal J_k y_k)$. 
            \end{enumerate}
            So the algorithm contains intractable quantities: $F(\mathcal J_k y_k)$. 
            \textcolor{red}{It's not yet ready to be formulated into a concrete algorithm.}
        \end{frame}
\section{Lin 2015}
    \begin{frame}{Lin 2015}
        Hongzhou Lin 2015 \cite{lin_universal_2015} did the following: 
        \begin{enumerate}
            \item Improved the proof from Guler 1993 to include strongly convex objectives. 
            \item Showed that $(\epsilon_k)_{k\ge 0}$ can be determined algorithmically and  an accelerated rate can be achieved. 
            \item Invented his own accelerated variance reduced incremental method called: ``Accelerated MISO-Prox" to demonstrate the Catalyst Framework. 
        \end{enumerate}
        \begin{block}{Quick notations}
            Assume $F$ is a $\mu \ge 0$ strongly convex function. 
            Fix $\kappa$ and the notations are: 
            \begin{align*}
                \mathcal M^{\kappa^{-1}}(x; y) &:= F(x) + \frac{\kappa}{2}\Vert x - y\Vert^2, 
                \\
                \mathcal J_{\kappa^{-1}} y &:= \argmin_x \mathcal M^{\kappa^{-1}} (x, y). 
            \end{align*}
        \end{block}
    \end{frame}
    \subsection{The Catalyst Algorithm}
        \subsection{The algorithm}
            \begin{frame}{Lin's accelerated proximal point method}
                \begin{definition}[Lin's accelerated proximal point method]
                    Let the initial estimate be $x_0 \in \RR^n$, fix parameters $\kappa$ and $\alpha_0$. 
                    Let $(\epsilon_k)_{k \ge 0}$ be an error sequence chosen for the evaluation for inexact proximal point method. 
                    Initialize $x_0 = y_0$, then the algorithm generates $(x_k, y_k)$ satisfies for all $k \ge 1$
                    {\small
                    \begin{align*}
                        & \text{find } x_k \approx \mathcal J_{\kappa^{-1}} y_{k - 1} \text{ such that } \mathcal M^{\kappa^{-1}}(x_k, y_{k - 1}) - \mathcal M^{\kappa^{-1}}(\mathcal J_{\kappa^{-1}}y_{k - 1}, y_{k - 1}) \le \epsilon_k
                        \\
                        & \text{find } \alpha_k \in (0, 1) \text{ such that } \alpha_k^2 = (1 - \alpha_k)\alpha_{k - 1}^2 + (\mu/(\mu + \kappa)) 
                        \\
                        & 
                        y_{k} = x_k + \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})}{\alpha_{k - 1}^2 + \alpha_k}(x_k - x_{k - 1}). 
                    \end{align*}
                    }
                \end{definition}
                \pause
                \begin{enumerate}
                    \item It's very similar compared to accelerated proximal gradient!!!
                    \item Determining $(\epsilon_k)_{k \ge 1}$ depends on the specific context of the algorithm. 
                    \item Strong convexity of $\mathcal M_k(\cdot; y_{k - 1})$ to approximate $x_k$ up to $\epsilon_k$. 
                \end{enumerate}
            \end{frame}
        \subsection{Convergence and practical importance}
            \begin{frame}{Practical importance}
                A major result on page 6 of Lin's 2015 \cite{lin_universal_2015}. 
                \begin{block}{Accelerated convergence}
                    Assume strong convexity for $F$. 
                    Using full gradient, or randomized coordinate descent to evaluation $x_k \approx \mathcal J_{\kappa^{-1}}y_{k - 1}$ up to $\epsilon_k$, then the overall complexity is:
                    $$
                        \widetilde{\mathcal O}\left(n \sqrt{L/\mu} \log(1/\epsilon)\right). 
                    $$
                    It's the same as accelerated gradient method up to a log term. 
                    In absent of strong convexity, acceleration for Variance reduced stochastic method such as: SAG, SAGA, Finito/MISO-Prox, SDCA, SVRG is $\widetilde {\mathcal O}(n L/\sqrt{\epsilon})$, strictly faster $\mathcal O(nL/\epsilon)$ without acceleration. 
                \end{block}
                Note: SAG, SAGA, Finito/MISO-Prox, SDCA, SVRG are examples of variance reduced incremental methods. 
            \end{frame}
        \subsection{Key theoretical innovations}
            \begin{frame}{Inexact proximal inequality in Lin 2015}
                Lemma A.7 in Lin 2015 \cite{lin_universal_2015} stated the following: 
                \begin{lemma}{Inexact proximal inequality}
                    Let $F$ be a $\mu\ge 0$ strongly convex.
                    Suppose $x_k$ is an inexact proximal point evaluation of $x_k \approx \mathcal J_{\kappa^{-1}} y_{k - 1}$ with $\kappa$ fixed. 
                    Assume the approximation error is characterized by $\mathcal M^{\kappa^{-1}}(x_k; y_{k - 1}) - \mathcal M^{\kappa^{-1}}(\mathcal J_{\kappa^{-1}} y_{k - 1}, y_{k - 1}) \le \epsilon_k$. 
                    Denote $x_k^* = \mathcal J_{\kappa^{-1}} y_{k - 1}$ to be the exact evaluation of the proximal point then for all $x$: 
                        \begin{align*}   
                            F(x) &\ge 
                            \textcolor{red}{
                                F(x_k) + \kappa \langle y_{k - 1} - x_k, x - x_k\rangle
                                + \frac{\mu}{2}\Vert x - x_k\Vert^2 
                            }
                            \\
                            & \quad 
                            + (\kappa + \mu)\langle  x_k - x_k^*, x - x_k\rangle 
                            - \epsilon_k.
                        \end{align*}
                \end{lemma}
                \begin{enumerate}
                    \item The parts in red is the regular proximal inequality but with $x_k$  instead of $x^*_k$. 
                    \item This inequality allows for definition for estimating sequence $\phi_k$ that is rid of $\mathcal J_{\kappa^{-1}}y_{k - 1}$
                \end{enumerate}
            \end{frame}

\section{Paquette 2018}
    \subsection{Major Contributions}
        \begin{frame}{Major contribution in Paquette 2018}
            In Paquette 2018, these major improvements for Lin's Universal Catalyst had been made: 
            \begin{enumerate}
                \item It supports weakly convex function with an unknown weak convexity constant through a procedures call Auto Adapt. 
                \item The convergence to stationary point under weak convexity is claimed. 
                \item The method retains accelerated rate o convergence if the function is convex. 
            \end{enumerate}
            Note: $F$ is $\rho$-weakly convex if and only if $f + \rho/2\Vert \cdot\Vert^2$ is convex. 
            \begin{block}{Quick notations}
                Fix $\kappa$ we use the following notations: 
                \begin{align*}
                & \mathcal M (x; y) := F(x) + \frac{\kappa}{2}\Vert x - y\Vert^2 
                \\
                & \mathcal J y := \argmin_x \mathcal M (x; y). 
                \end{align*}
            \end{block}
        \end{frame}

    \subsection{The Basic 4WD Catalyst}
        \begin{frame}{Basic 4WD}
            \begin{definition}[Basic 4WD Catalyst Algorithm]
            {\small
                Find any $x_0 \in \text{dom}(F)$. 
                Initialize the algorithm with $\alpha_1 = 1, v_0 = x_0$. 
                For $k \ge 1$, the iterates $(x_k, y_k, v_k)$ are generated by the procedures: 
                \vspace{-0.5em}
                \begin{align*}
                    &
                    \text{find } \bar x_k \approx \argmin_{x}\left\lbrace
                        \mathcal M(x; x_{k - 1})
                    \right\rbrace
                    \\ &\quad 
                    \text{ such that:  }
                        \left\lbrace
                            \begin{aligned}
                                & \text{dist}(\mathbf 0, \partial \mathcal M(\bar x_k; x_{k - 1})) 
                                \le 
                                \kappa\Vert \bar x_k - x_{k - 1}\Vert, 
                                \\
                                & \mathcal M(\bar x_k; x_{k - 1}) 
                                \le F(x_{k - 1}). 
                            \end{aligned}
                        \right.
                    \\
                    & y_k = \alpha_k v_{k - 1} + (1 - \alpha_k) x_{k - 1};
                    \\
                    & 
                    \text{find }\tilde x_k \approx \argmin_{x} \left\lbrace
                        \mathcal M(x; y_k) 
                        : 
                        \text{dist}\left(
                        \mathbf 0, \partial \mathcal M(\tilde x_k; y_k)
                        \right) 
                        \le \frac{\kappa}{k + 1}\Vert \tilde x_k - y_k\Vert
                    \right\rbrace;
                    \\
                    & v_{k} = x_{k - 1} + \frac{1}{\alpha_k}(\tilde x_k - x_{k - 1});
                    \\
                    & 
                    \text{find } \alpha_{k + 1} \in (0, 1): 
                    \frac{1 - \alpha_{k + 1}}{\alpha_{k + 1}^2} = \frac{1}{\alpha_k^2};
                    \\
                    & \text{choose } x_k \text{ such that:  } f(x_k) = \min(f(\bar x_k), f(\tilde x_k)). 
                \end{align*}
            }
            \end{definition}
        \end{frame}
    \subsection{Convergence claims}
        \begin{frame}{Convergence claim}
            \begin{theorem}[Basic 4WD Catalyst Convergence]
                Let $(x_k, v_k, y_k)$ be generated by the basic Catalyst algorithm. 
                If $F$ is $\kappa$ weakly convex and bounded below, then $x_k$ converges to a stationary point where
                \begin{align*}
                    \min_{j = 1, \cdots, N} \dist^2(\mathbf 0, \partial F(\bar x_j))
                    \le \frac{8 \kappa}{N}(F(x_0) - F^*). 
                \end{align*}
                And when $F$ is convex, $F(x_k) - F^*$ converges at a rate of $\mathcal O(k^{-2})$. 
            \end{theorem}
            \pause
            \textbf{Let's prove this.} 
        \end{frame}

\section{Morals of the story}
    \begin{frame}{The morals of the story}
        
    \end{frame}
    
\section{References}
    \begin{frame}{References}
        \tiny\bibliography{references/refs.bib}
    \end{frame}

\end{document}