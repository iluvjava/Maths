\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{
    {
        \fontfamily{ptm}\selectfont 
        Catalyst Meta Acceleration Framework: The history and the gist of it
    }
    }

\author{
    Hongda Li
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~ and~Xianfu Wang
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    Nesterov's accelerated gradient first appeared back in the 1983 has sparked numerous theoretical and practical advancements in Mathematics programming literatures. 
    The idea behind Nesterov's acceleration is universal in the convex case it has concrete extension in the non-convex case. 
    In this paper we survey specifically the Catalyst Acceleration that incorporated ideas from the Accelerated Proximal Point Method proposed by Guler back in 1993. 
    The paper reviews Nesterov's classical analysis of accelerated gradient in the convex case.
    The paper will describe key aspects of the theoretical innovations involved to achieve the design of the algorithm in convex, and non-convex case. 
    
\end{abstract}

% \noindent{\bfseries 2010 Mathematics Subject Classification:}
% Primary 65K10, 90c25, 90C30; Secondary 65Y20. 
% \noindent{\bfseries Keywords: } Nesterov acceleration, Proximal point method. 

\section{Introduction}
    \textcolor{red}{THIS REPORT IS CURRENTLY: UNFINISHED}

    The optimal algorithm named accelerated gradient descent method is proposed in Nesterov's seminal work back in 1983 \cite{nesterov_method_1983}. 
    The algorithm closed the upper bound and lower bound on the iteration complexity for all first order Lipschitz smooth convex function among all first order algorithms. 
    For a specific definition first order method, we refer reader to Chapter 2 of Nesterov's new book \cite{nesterov_lectures_2018} for more information. 
    Gradient descent has an upper bound of $\mathcal O(1/k)$ in iteration complexity that is slower than
    the lower iteration complexity $\mathcal O(1/k^2)$. 
    Accelerated gradient descent has an upper bound of $\mathcal O(1/k^2)$, making it optimal. 

    It's tempting to believe that the existence of an optimal algorithm sealed the ceiling for the need of theories for convex first-order smooth optimization. 
    It is correct but lacks the nuance in understanding because Nesterov's accelerated gradient is a system of analysis techniques which is not a specific design paradigm for algorithms. 
    
    Guler's accelerated Proximal Point Method(PPM) \cite{guler_new_1992} in 1993 used the technique of Nesterov's estimating sequence to accelerated PPM when the objective is convex. 
    The algorithm uses $(\lambda_k)_{k \ge 0}$ in the PPM to generate $(x_k)_{k\ge 0}$ given any initial guess $x_0$.
    Guler's prior work \cite{guler_convergence_1991} showed the convergence of PPM method in the convex without acceleration has convergence $\mathcal O\left(1/\sum^{n}_{i = 1}\lambda_i\right)$. 
    His new algorithm with acceleration has a convergence rate of $\mathcal O\left(1/(\sum_{i = 1}^{n} \sqrt{\lambda_i})^2\right)$. 
    An inexact Accelerated PPM method using conditions described in Rockafellar's works in 1976 \cite{rockafellar_monotone_1976} is also discussed in the paper. 

    It's tempting to conclude that the results has reached the ceiling for extending Nesterov's acceleration. 
    It is correct, but not from a practical point of view. 
    Let $F: \RR^n \mapsto \RR$ be our objective function, $\mathcal J_\lambda := (I + \lambda \partial F)^{-1}$ and $\mathcal M^{\lambda}(x; y):= F(x) + \frac{1}{2\lambda}\Vert x - y\Vert^2$ then the inexact proximal point considers with error $\epsilon_k$ has the following characterizations of inexactness as put forward by Guler \cite{guler_new_1992}: 
    \begin{align*}
        & \tilde x \approx \mathcal J_\lambda y
        \\
        & 
        \dist(\mathbf 0, \partial\mathcal{M}^{\lambda}(\tilde x; y))
        \le \frac{\epsilon}{\lambda}. 
    \end{align*}
    The difficulty comes from controlling the error $\epsilon$ at each iteration so that the overall convergence of accelerated PPM achieves optimal convergence rate. 
    However, in the paper $\epsilon_k \rightarrow 0$, but in a specific rate. 
    It requires knowledge about exact minimum of the Moreau envelope at each step and the optimal value of the Nesterov's estimating sequence. 
    These quantities are intractable in practice. 

    Introduced in Lin et al. \cite{lin_universal_2015}\cite{lin_catalyst_2018} is a series of papers describing a concrete meta algorithm called Catalyst (It's called 4WD Catalyst for the non-convex extension in works by Paquette, Lin et al. \cite{paquette_catalyst_2018}). 
    A meta algorithm because uses other first order algorithms to evaluate inexact proximal point method and then performs accelerated PPM based on the outputs of other algorithms. 
    The innovations here are tracking and controlling the errors made in the inexact PPM throughout the algorithm and an algorithm called Proximal MISO. 

    \subsection{Contributions}
        The writing is expository and comprehensive, it will survey the history and major results, and innovations involved in conceiving and designing the Catalyst algorithm. 
        We reviewed the literatures and faithfully reproduced important claims.
        In addition, we give insights and context to understand the claims in these papers and making connections to ideas in optimization. 
        Three papers by Guler \cite{guler_new_1992} and Lin \cite{lin_universal_2015} and Paquette et al. \cite{lin_catalyst_2018} together with Nesterov's \cite{nesterov_lectures_2018} method of estimating sequence are the targets of this report. 

        We will only cover innovations in the theoretical aspect of Catalyst Acceleration. 
        Applications and specific example algorithms are out of the scope because there are too many papers on the separate topic of variance reduced stochastic algorithms. 

    
\section{Preliminaries}\label{sec:preliminaries}
    Throughout the writing, let the ambient space is $\RR^n$. 
    The optimization problem is
    \begin{align*}
        \min_{x \in \RR^n} F(x). 
    \end{align*}
    This section introduces the Nesterov's estimating sequence technique. 
    This technique is fundamental for in Guler's accelerated PPM method and Catalyst meta acceleration in the convex/strongly convex case. 
    \subsection{Method of Nesterov's Estimating Sequence}
        \begin{definition}[Nesterov's estimating sequence]\label{def:nes-est-seq}
            Let $(\phi_k : \RR^n \mapsto \RR)_{k \ge 0}$ be a sequence of functions. 
            We call this sequence of function a Nesterov's estimating sequence when it satisfies the conditions that: 
            \begin{enumerate}
                \item There exists another sequence $(x_k)_{k \ge 0}$ such that for all $k \ge 0$ it has $F(x_k) \le \phi_k^*$. 
                \item There exists a sequence of $(\alpha_k)_{k \ge 0}$ such that for all $x \in \RR^n$, $\phi_{k + 1}(x) - \phi_k(x) \le - \alpha_k(\phi_k(x) - F(x))$. 
            \end{enumerate}
        \end{definition}
        \begin{observation} 
            If we define $\phi_k$, $\Delta_k(x) := \phi_k (x) - F(x)$ for all $x \in \RR^n$ and assume that $F$ has minimizer $x^*$. 
            Then observe that $\forall k \ge 0$:  
            \begin{align*}
                \Delta_k(x) 
                &= \phi_k(x) - f(x) \ge \phi_k^* - f(x)
                \\
                x = x_k\implies 
                \Delta_k(x_k) 
                &\ge 
                \phi_k^* - f(x_k) \ge 0
                \\
                x = x_* \implies 
                \Delta_k(x_*)
                &\ge \phi_k^* - f_* \ge f(x_k) - f_* \ge 0
            \end{align*}
            The function $\Delta_k(x)$ is non-negative at points: $x_*, x_k$.
            We can derive the convergence rate of $\Delta_k(x^*)$ because $\forall x \in \RR^n$: 
            \begin{align*}
                \phi_{k + 1}(x) - \phi_k(x) 
                &\le - \alpha_k (\phi_k(x) - F(x))
                \\
                \iff 
                \phi_{k + 1}(x) - F(x) - (\phi_k(x) - F(x))
                &\le 
                -\alpha_k(\phi_k(x) - F(x))
                \\
                \iff
                \Delta_{k + 1}(x) - \Delta_k(x) &\le
                - \alpha_k\Delta_k(x)
                \\
                \iff 
                \Delta_{k + 1}(x) 
                &\le 
                (1 - \alpha_k)\Delta_k(x). 
            \end{align*}
            Unrolling the above recursion it yields: 
            \begin{align*}
                \Delta_{k + 1}(x) &\le 
                (1 - \alpha_k)\Delta_k(x) \le \cdots \le 
                \left(
                    \prod_{i = 0}^k(1 - \alpha_i)
                \right)\Delta_0(x). 
            \end{align*}
            Finally, by setting $x = x^*$, $\Delta_k(x^*)$ is non-negative and using the property of Nesterov's estimating sequence it gives: 
            $$
                f(x_k) - f(x^*) \le \phi_k^* - f(x^*) \le \Delta_k(x^*) = \phi_k(x^*) - f(x^*) \le \left(\prod_{i = 0}^k(1 - \alpha_i)\right)\Delta_0(x^*).
            $$ 
        \end{observation}
        Creativity is important in the construction of the estimating sequence $(\phi_k)_{k \ge 0}$. 

\section{Nesterov's accelerated proximal gradient}
    This section swiftly exposes the constructions of the Nesterov's estimating sequence for the FISTA algorithm by Beck\cite{beck_fast_2009-1}, which is specific case of Algorithm (2.2.63), in Nesterov's book \cite{nesterov_lectures_2018}. 
    The accelerated proximal gradient algorithm has very similar formulation as the Catalyst Acceleration framework and accelerated PPM. 
    Additionally, the estimating sequence can derive the algorithm and verify the convergence. 
    
    Throughout this section we assume that: $F = f + g$ where $f$ is $L$-Lipschitz smooth and $\mu \ge 0$ strongly convex and $g$ is convex. 
    Define 
    \begin{align*}
        \mathcal M^{L^{-1}}(x; y) 
        &:= g(x) + f(y) 
        + 
        \left\langle \nabla f(x), x - y\right\rangle 
        + 
        \frac{L}{2}\Vert x - y\Vert^2, 
        \\
        \widetilde{\mathcal J}_{L^{-1}}y 
        &:= \argmin_{x} \mathcal M^{L^{-1}}(x; y), 
        \\
        \mathcal G_{L^{-1}}(y)
        &:= L\left(I - \widetilde{\mathcal J}_{L^{-1}}\right)y. 
    \end{align*}

    In the literature, $\mathcal G_{L^{-1}}$ is commonly known as the gradient mapping. 
    The definition follows, we define the Nesterov's estimating sequence used to derive the accelerated proximal gradient method. 
    \begin{definition}[Accelerated proximal gradient estimating sequence]
    \label{def:nes-est-seq-pg}\;\\
        Define $(\phi_k)_{k \ge0}$ be the Nesterov's estimating sequence recursively given by: 
        \begin{align*}
            & l_F(x; y_k) := 
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k \right) 
                + \langle \mathcal G_{L^{-1}}y_k, x - y_k\rangle + 
            \frac{1}{2L}\Vert \mathcal G_{L^{-1}}y_k\Vert^2, 
            \\
            & 
            \phi_{k + 1}(x)
            := (1 - \alpha_k)\phi_k (x) + 
            \alpha_k 
            \left(
                l_F(x; y_k) + \frac{\mu}{2}\Vert x - y_k\Vert^2
            \right). 
        \end{align*}
        The Algorithm generates a sequence of vectors $y_k, x_k$, and scalars $\alpha_k$ satisfies the following: 
        \begin{align*}
            &x_{k + 1} = \widetilde{\mathcal J}_{L^{-1}} y_k, 
            \\
            & \text{find } \alpha_{k + 1} \in (0, 1)
            \alpha_{k + 1} = (1 - \alpha_{k + 1})\alpha_k^{2} + (\mu/L) \alpha_{k + 1}
            \\
            &y_{k + 1} = x_{k + 1} + \frac{\alpha_k(1 - \alpha_k)}{\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        One of the possible base case can be $x_0 = y_0$ and any $\alpha_0 \in (0, 1)$. 
    \end{definition}
    \begin{observation}
        Fix any $y$, for all $x\in \RR^n$, $F(x) \ge l_F(x; y_k) + \mu/2\Vert x - y_k\Vert^2$ is the proximal gradient inequality. 
        If $f \equiv 0$ then $\widetilde{\mathcal J}_{L^{-1}}y_k$ becomes resolvent $(I + L^{-1}\partial F)^{-1}$, which makes $x_k$ being an exact evaluation of PPM: 
        \begin{align*}
            l_F(x; y_k) 
            &= F(\mathcal J_{L^{-1}}y_k) 
            + \langle L(y - \mathcal J_{L^{-1}} y), x - y_k\rangle + \frac{L}{2}\Vert y_k - \mathcal J_{L^{-1}}y_k\Vert^2
            \\
            &= F(\mathcal J_{L^{-1}}y_k) 
            + \langle L(y - \mathcal J_{L^{-1}} y), x - \mathcal J_{L^{-1}}y_k\rangle. 
        \end{align*}
        This is the proximal inequality with constant a step size: $L^{-1}$. 
    \end{observation}
    To demonstrate the usage of Nesterov's estimating sequence here, consider sequence $(x_k)_{k \ge 0}$ such that $F(x_k) \le \phi_k^*$. 
    Assume the existence of minimizer $x^*$ for $F$, by definition of $\phi_k$ let $x = x^*$ then $\forall k \ge 0$: 
    {\small
    \begin{align*}
        \phi_{k + 1}(x^*)
        &= (1 - \alpha_k)\phi_k (x^*) + 
        \alpha_k 
        \left(
            l_F(x^*; y_k) + \frac{\mu}{2}\Vert x^* - y_k\Vert^2
        \right)
        \\
        \phi_{k + 1}(x^*) - \phi_k(x^*) &= 
        -\alpha_k\phi_k(x^*) 
        +
        \alpha_k
        \left(
            l_F(x^*; y_k) + \frac{\mu}{2}\Vert x^* - y_k\Vert^2
        \right) 
        \\
        \implies
        \phi_{k + 1}(x^*) - F(x^*) + F(x^*) - \phi_k(x^*) 
        &\le -\alpha_k(\phi_k(x^*) - F(x^*))
        \\
        \implies 
        F(x_{k + 1}) - F(x^*)
        \le \phi_{k + 1}^* - F(x^*)
        &\le \phi_{k + 1}(x^*) - F(x^*)
        \le 
        (1 - \alpha_k)(\phi_k(x^*) - F(x^*)). 
    \end{align*}
    }
    On the first inequality we used the fact that $l_F(x; y_k) + \mu/2\Vert x - y_k\Vert^2\le F(x)$. 
    Unrolling the recurrence, we can get the convergence rate of $F(x_k) - F(x^*)$ to be on Big O of $\prod_{i = 1}^k(1 - \alpha_i)$. 
    \begin{remark}
        The definition is a generalization of Nesterov's estimating sequence comes from (2.2.63) from Nesterov's book \cite{nesterov_lectures_2018}. 
        Compare to Nesterov's work, we used proximal gradient operator instead of projected gradient. 
        The same inequality is called ``Fundamental Proximal Gradient Inequality'' in Amir Beck's book \cite{beck_first-order_2017}, Theorem 10.16. 
    \end{remark}
    % \begin{definition}[Accelerated proixmal gradient algorithm]\label{def:acc-prox-grad-alg}
    %     The algorithm of accelerated proximal gradient generates sequence of iterates $(x_k, y_k)_{k \ge 0}$ which satisfies for all $k\ge 0$: 
    %     \begin{align*}
    %         &x_{k + 1} = \widetilde{\mathcal J}_{L^{-1}} y_k, 
    %         \\
    %         & \text{find } \alpha_{k + 1} \in (0, 1)
    %         \alpha_{k + 1} = (1 - \alpha_{k + 1})\alpha_k^{2} + (\mu/L) \alpha_{k + 1}
    %         \\
    %         &y_{k + 1} = x_{k + 1} + \frac{\alpha_k(1 - \alpha_k)}{\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
    %     \end{align*}

    % \end{definition}
    % \begin{remark}
    %     The simple case of accelerated gradient descent is stated as (2.2.63) in Nesterov's book \cite{nesterov_lectures_2018}. 
    % \end{remark}
    For a proof for the Nesterov's estimating sequence $\phi_k$ and a derivation of the algorithm, see
    \hyperref[app:sec:thm-claim-acc-prox-grad]{Appendix \ref*{app:sec:thm-claim-acc-prox-grad}}. 
    We warn the readers that the proof is long. 

\section{Guler 1993}
    This section introduces the setup of the Nesterov's estimating sequence used in Guler's accelerated Proximal Point method. 
    In addition, this section will highlight some observations and theoretical results accordingly. 
    
    Throughout this section, we assume that $F:\RR^n \mapsto \overline \RR$ is a convex function. 
    We use the following list of notations: 
    \begin{align*}
        \mathcal M^{\lambda} (x; y) &:= F(x) + \frac{1}{2\lambda}\Vert x - y\Vert^2
        \\
        \mathcal J_\lambda y &:= \argmin_x \mathcal M^{\lambda} (x; y)
        \\
        \mathcal G_\lambda &:= \lambda^{-1}(I - \mathcal J_\lambda). 
    \end{align*}
    For notations simplicity, we use $\mathcal G_k, \mathcal J_k$ to denote the gradient mapping and the proximal point operator because under the context of the algorithm, the proximal point step is conductive iteratively with some arbitrary sequence that $(\lambda)_{k \ge 0}$ which we fixed at the start. 
    
    \begin{definition}[Accelerated PPM estimating sequence]\label{def:nes-est-seq-acc-ppm}
        The Nesterov's estimating sequence $(\phi_k)_{k \ge0}$ for the accelerated proximal point method is defined by the following recurrence for all $k \ge0$, any $A \ge 0$: 
        \begin{align*}
            \phi_0 &:= f(x_0) + \frac{A}{2}\Vert x - x_0\Vert^2, 
            \\
            \phi_{k + 1}(x) &:= 
            (1 - \alpha_k)\phi_k(x)
            + 
            \alpha_k(F(\mathcal J_k y_k) + \langle \mathcal G_k y_k, x - \mathcal J_k y_k\rangle).    
        \end{align*}
        Let $(\lambda_k)_{k \ge 0}$ be the step size which defines the descent sequence $x_k = \mathcal J_\lambda y_k$. 
        Then the descent sequence $x_k$, along with the auxiliary vector sequence $(y_k, v_k)$, scalar sequence $(\alpha_k, A_k)_{k\ge 0}$ will be made to satisfy for all $k\ge0$, the conditions: 
        \begin{align*}
            \alpha_k &= \frac{1}{2}\left(
                \sqrt{(A_k\lambda_k)^2 + 4A_k \lambda_k}
                - A_k\lambda_k
            \right) 
            \\
            y_k &= (1 - \alpha_k)x_k + \alpha_k v_k
            \\
            v_{k + 1}
            &= 
            v_k - \frac{\alpha_k}{A_{k + 1}\lambda_k}(y_k - \mathcal J_k y_k)
            \\
            A_{k + 1} &= (1 - \alpha_k)A_k, 
        \end{align*}
    \end{definition}
    \begin{remark}
        The auxiliary sequences $(A_k, v_k)$ parameterizes a canonical representation of the estimating sequence $(\phi_k)_{k \ge0}$. 
        Guler didn't simplify his results compare to what Nesterov did in his book. 
    \end{remark}
    To handle the inexact evaluation of the PPM, Guler cited Rockafellar \cite{rockafellar_monotone_1976} for condition (A') in his text which is the following: 
    \begin{align*}
        x_{k + 1}\approx \mathcal J_{\lambda_k} y_k \text{ be such that: }
        \dist\left(
            \mathbf 0, \partial \mathcal M_{\lambda_k}(x_{k + 1}; y_k)
        \right) &\le \frac{\epsilon_k}{\lambda_k}
        \\
        \implies 
        \Vert x_{k + 1} - \mathcal J_{\lambda_k}y_k\Vert 
        &\le \epsilon_k. 
    \end{align*}
    Condition A' also characterizes the property of $(\epsilon_k)_{k\ge 0}$ so inexact PPM converges. 
    Guler strengthens it in his context and proved the following: 
    \begin{theorem}[Guler's inexact proximal point error bound]
        Consider defining minimum for the envelope function given by $\mathcal M_k^* := \min_z \mathcal {M}^{\lambda_k}(z; y_k)$. 
        If $x_{k +1}$ is an inexact evaluation under condition (A'), then the estimating sequence admits the conditions that: 
        \begin{align*}
            \frac{1}{2\lambda_k} \Vert x_{k + 1} - \mathcal J_{\lambda_k} y_k\Vert^2
            &= 
            \mathcal M^{\lambda_k}(x_{k + 1}, y_k) - \mathcal M^*_k
            \le \frac{\epsilon_k^2}{2\lambda_k}. 
        \end{align*}
    \end{theorem}
    The next theorem is Theorem 3.3 of Guler's 1993 papers which is a major result for inexact accelerated PPM method. 
    \begin{theorem}[Guler's accelerated inexact PPM convergence results]
        If the error sequence $(\epsilon_k)_{k \ge0}$ for condition A' is bounded by $\mathcal O(1/k^\sigma)$ for some $\sigma > 1/2$, then the accelerated proximal point method has for any feasible $x \in \RR^n$: 
        \begin{align*}
            f(x_k) - f(x) \le \mathcal O(1/k^2) + \mathcal (1 / k^{2\sigma - 1}). 
        \end{align*}    
    \end{theorem}
    The theorem looks exciting, but Lin 2015 \cite{lin_universal_2015} page 11 pointed out that $\mathcal G_k^*, \mathcal J_{\lambda_k} y_k$ are both intractable quantities. 
    In Guler's work, these intractable quantities were built into the Nesterov's estimating sequence making it unclear how to control $\epsilon_k \rightarrow 0$. 
    If we use the inexact formulation from Guler and his estimating sequence, it will result in algorithm that contains intractable quantities $\mathcal J_{\lambda_k} y_k$. 
    


\section{Lin 2015}
    This section introduced the setup of Nesterov's estimating sequence in Lin 2015 \cite{lin_universal_2015}. 
    Right at the beginning we warn the readers about the following: 
    \begin{enumerate}
        \item The proofs in HongZhou Lin's original paper of Universal Catalyst is depressingly long and complicated. This is a result of using the constructive approach of Nesterov's estimating sequence. 
        \item It's context specific for controlling the errors of inexact proximal point evaluations are being controlled. He hinted at the way of controlling and tracking the errors from inexact proximal point evaluations, but it's context specific. He only illustrated the use of the meta acceleration on their own method called: ``Proximal MISO", but in general the problem still remains open. 
        \item We will provide proofs to clarify some of their proofs and compare with existing proofs and drawing references in the literatures in the appendix. 
    \end{enumerate}
    Let's assume $F$ is a $\mu \ge 0$ strongly convex function. 
    Throughout this section we make the following notations
    \begin{align*}
        \mathcal M^{\kappa^{-1}}(x; y) &:= F(x) + \frac{\kappa}{2}\Vert x - y\Vert^2, 
        \\
        \mathcal J_{\kappa^{-1}} y &:= \argmin_x \mathcal M^{\kappa^{-1}} (x, y). 
    \end{align*}
    Their algorithm is almost exactly the same as Nesterov's 2.2.20 \cite{nesterov_lectures_2018} which we stated in the definition below: 
    \begin{definition}[Lin's accelerated proximal point method]
        Let the initial estimate be $x_0 \in \RR^n$, fix parameters $\kappa$ and $\alpha_0$. 
        Let $(\epsilon_k)_{k \ge 0}$ be an error sequence chosen for the evaluation for inexact proximal point method. 
        Initialize $x_0 = y_0$, then the algorithm generates $(x_k, y_k)$ satisfies for all $k \ge 1$
        \begin{align*}
            & \text{find } x_k \approx \mathcal J_{\kappa^{-1}} y_{k - 1} \text{ such that } \mathcal M^{\kappa^{-1}}(x_k, y_{k - 1}) - \mathcal M^{\kappa^{-1}}(\mathcal J_{\kappa^{-1}}y_{k - 1}, y_{k - 1}) \le \epsilon_k
            \\
            & \text{find } \alpha_k \in (0, 1) \text{ such that } \alpha_k^2 = (1 - \alpha_k)\alpha_{k - 1}^2 + (\mu/(\mu + \kappa)) 
            \\
            & 
            y_{k} = x_k + \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})}{\alpha_{k - 1}^2 + \alpha_k}(x_k - x_{k - 1}). 
        \end{align*}
    \end{definition}
    \begin{remark}
        The most exciting thing about this algorithm is the similarity it has compares to 
        \hyperref[def:acc-prox-grad-alg]{Definition \ref*{def:acc-prox-grad-alg}}. 
        The only difference here is the inclusion of an inexact proximal point step, and the Lipschitz constant $L$ is absent instead it has $\kappa + \mu$. 
        Evaluating $x_k \approx \mathcal J_{\kappa^{-1}}y_{k - 1}$ is also possible because the function $\mathcal M^{\kappa^{-1}}(\cdot, y_{k - 1})$ is strongly convex, hence its optimality gap can be bounded via trackable quantity $\partial \mathcal M^{\kappa^{-1}}(x_k, y_{k - 1})$. 

        Controlling the error sequence $\epsilon_k$ however is a whole new business. 
        Lin 2015 \cite{lin_universal_2015} commented on the second last paragraph on page 4, and here we quote:
        \par
        ``The choice of the sequence $(\epsilon_k)_{k \ge 0}$ is also subjected to discussion since the quantity $F(x_0) - F^*$ is unknown beforehand. Nevertheless, an upper bound may be used instead, which will only affects the corresponding constant in (7). Such an upper bounds can typically be obtained by computing a duality gap at $x_0$, or by using additional knowledge about the objective. For instance, when $F$ is non-negative, we may simply choose $\epsilon_k = (2/9)F(x_0)(1 - \rho)^k$". 
        
        This comment has upmost practical importance because it tells us how to bound the error $\epsilon_k$ to achieve accelerated convergence rate. 
        In theory, $\epsilon_k$ decreases at a rate related to $F(x_0) - F^*$. 
        It requires some knowledge about $F^*$ in prior. 
        Therefore, controlling $\epsilon_k$ is still elusive in general in a practical context. 
        To see how the error is controlled for the inexact proximal point evaluation, we refer the readers to Lemma B.1 in Lin's 2015 paper \cite{lin_universal_2015}. 
    \end{remark}
    For theoretical interests, there is a major difference between Lin's approach and Guler's approach. 
    Lin didn't formulate any of the intractable quantities in the definitions for his Nesterov's estimating sequence $\phi_k$. 
    One major innovation is Lemma A.7 in Lin's 2015 paper \cite{lin_universal_2015}. 
    The lemma allows the analysis Nesterov's estimating sequence to be carried through without using intractable quantities: $\mathcal M^{\kappa^{-1}}(\mathcal J_{k^{-1}}y_{k - 1}, y_{k - 1}), \mathcal J_{\kappa^{-1}}y_{k - 1}$. 
    
    

\section{Non-convex Extension of Catalyst Acceleration}
    The non-convex extension of Catalyst acceleration by Lin 2018 \cite{lin_catalyst_2018} is similar to the convex case in his 2015 paper \cite{lin_universal_2015}. 
    The new algorithm handles function with unknown weak convexity constant $\rho$ using a process called Auto Adapt subroutine. 
    They only claimed convergence to stationary point is claimed for the weakly convex objective. 
    \par 
    Fixed the parameter $\kappa$ and throughout the section, we use the following notations: 
    \begin{align*}
       & \mathcal M (x; y) := F(x) + \frac{\kappa}{2}\Vert x - y\Vert^2 
       \\
       & \mathcal J y := \argmin_x \mathcal M (x; y). 
    \end{align*}
    We define the algorithm and then its convergence claim below. 
    \begin{definition}[Basic 4WD Catalyst Algorithm]
        Find any $x_0 \in \text{dom}(F)$. 
        Initialize the algorithm with $\alpha_1 = 1, v_0 = x_0$. 
        For $k \ge 1$, the iterates $(x_k, y_k, v_k)$ are generated by the procedures: 
        \begin{align*}
            &
            \text{find } \bar x_k \approx \argmin{x}\left\lbrace
                \mathcal M(x; x_{k - 1})
            \right\rbrace
            \text{ such that:  }
                \left\lbrace
                    \begin{aligned}
                        & \text{dist}(\mathbf 0, \partial \mathcal M(\bar x_k; x_{k - 1})) 
                        \le 
                        \kappa\Vert \bar x_k - x_{k - 1}\Vert, 
                        \\
                        & \mathcal M(\bar x_k; x_{k - 1}) 
                        \le F(x_{k - 1}). 
                    \end{aligned}
                \right.
            \\
            & y_k = \alpha_k v_{k - 1} + (1 - \alpha_k) x_{k - 1};
            \\
            & 
            \text{find }\tilde x_k \approx \argmin{x} \left\lbrace
                \mathcal M(x; y_k) 
                : 
                \text{dist}\left(
                \mathbf 0, \partial \mathcal M(\tilde x_k; y_k)
                \right) 
                \le \frac{\kappa}{k + 1}\Vert \tilde x_k - y_k\Vert
            \right\rbrace;
            \\
            & v_{k} = x_{k - 1} + \frac{1}{\alpha_k}(\tilde x_k - x_{k - 1});
            \\
            & 
            \text{find } \alpha_{k + 1} \in (0, 1): 
            \frac{1 - \alpha_{k + 1}}{\alpha_{k + 1}^2} = \frac{1}{\alpha_k^2};
            \\
            & \text{choose } x_k \text{ such that:  } f(x_k) = \min(f(\bar x_k), f(\tilde x_k)). 
        \end{align*}
    \end{definition}
    
    \begin{theorem}[Basic 4WD Catalyst Convergence]
        Let $(x_k, v_k, y_k)$ be generated by the basic Catalyst algorithm. 
        If $F$ is weakly convex and bounded below, then $x_k$ converges to a stationary point where
        \begin{align*}
            \min_{j = 1, \cdots, N} \dist^2(\mathbf 0, \partial F(\bar x_j))
            \le \frac{8 \kappa}{N}(F(x_0) - F^*). 
        \end{align*}
        And when $F$ is convex, $F(x_k) - F^*$ converges at a rate of $\mathcal O(k^{-2})$. 
    \end{theorem}
    \begin{remark}
        
    \end{remark}




\bibliographystyle{siam}
\bibliography{references/refs}

\appendix
\section{Postponed proofs}

\subsection{Theorems and claims for accelerated proximal gradient}\label{app:sec:thm-claim-acc-prox-grad}
    Throughout this section, $F = g + f$ is an additive composite objective function with $g$ convex, $f$ $L$-lipschitz smooth and $\mu \ge0$ strongly convex. 
    The notations here are 
    \begin{align*}
        \mathcal M^{L^{-1}}(x; y) 
        &:= 
        F(x) + \frac{L}{2}\Vert x - y\Vert^2
        \\
        \widetilde{ \mathcal M}^{L^{-1}}(x; y) 
        &:= g(x) + f(y) 
        + 
        \left\langle \nabla f(x), x - y\right\rangle 
        + 
        \frac{L}{2}\Vert x - y\Vert^2
        \\
        \widetilde{\mathcal J}_{L^{-1}}y 
        &:= \argmin_{x} \widetilde{\mathcal M}^{L^{-1}}(x; y)
        \\
        \widetilde{\mathcal G}_{L^{-1}}(y)
        &:= L\left(I - \widetilde{\mathcal J}_{L^{-1}}\right)y. 
    \end{align*}

    \begin{theorem}[Fundamental theorem of proximal gradient]
    \label{app:thm:fun-thm-prox-grad}
        Let $h = f + g$ and proximal gradient operator $T$ be given as in this section. 
        Fix any $y$, we have for all $x \in \RR^n$: 
        \begin{align*}
            h(x) - h(Ty) - 
            \left\langle 
                L(y - \widetilde{\mathcal J}_{L^{-1}} y),
                x - \widetilde {\mathcal J}_{L^{-1}}y
            \right\rangle
            &\ge  D_f(x, y) . 
        \end{align*}  
    \end{theorem}
    \begin{proof}
        By a direct observation: 
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) 
            &= 
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            g(x) + f(x) - f(x) + f(y) 
            + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            h(x) - D_f(x, y) + \frac{L}{2}\Vert x - y\Vert^2 
            \\
            &= \mathcal M^{L^{-1}}(x; y) - D_f(x, y). 
        \end{align*}
        Next, since $\widetilde{\mathcal M}^{L^{-1}}(\cdot, y)$ is strongly convex, it has quadratic growth conditions on its minimizer. 
        Denote $y^+ = \widetilde{\mathcal J}_{L^{-1}}y$ then: 
        {\small
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) - 
            \widetilde{\mathcal M}^{L^{-1}}(y^+; y)
            - 
            \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 
            0
            \\
            \implies 
            \left(
                \mathcal M^{L^{-1}}(x; y) - D_f(x, y)
            \right) - 
            \mathcal M^{L^{-1}}(y^+; y) 
            - 
            \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                \mathcal M^{L^{-1}}(x; y)
                - 
                \mathcal M^{L^{-1}}(y^+; y)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(y^+) 
                + 
                \frac{L}{2}\Vert x - y\Vert^2 - 
                \frac{L}{2}\Vert y^+ - y\Vert^2
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(y^+) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - y^+ + y^+ - y\Vert^2
                    - 
                    \Vert y - y^+\Vert^2
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(y^+) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - y^+\Vert^2 + 
                    2\langle x - y^+, y^+ - y\rangle
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff
            \left(
                F(x) - F(y^+) + \frac{L}{2}\Vert x - y^+\Vert^2 
                - L\langle  x - y^+, y - y^+\rangle
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            F(x) - F(y^+)
            - \langle L(y - y^+), x - y^+\rangle
            - D_f(x, y) 
            &\ge 0. 
        \end{align*}
        }
    \end{proof}
    \begin{remark}
        The quadratic growth with respect to minimizer of the Moreau Envelope is used to derive the inequality, please take caution that this condition is strictly weaker than strong convexity of the Moreau Envelope, which could be made weaker than the strong convexity of $F$. 
        Compare the same theorems in older literatures, this proof doesn't use the subgradient inequality, making it appealing for generalizations outside convexity context. 
    \end{remark}
    
    \begin{theorem}[Cannonical form of proximal gradient estimating sequence]
    \label{thm:canon-prox-grad-est-seq}
        \; \\
        Denote $\phi_k:\RR^n \mapsto \RR$ as a sequence of functions such that it satisfies recursively for all $k\ge 0$ the following conditions 
        \begin{align*}
            & g_k := L(y_k - \widetilde{\mathcal J}_{L^{-1}} y_k)
            \\
            & l_F(x; y_k) := 
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) 
                + \langle g_k, x - y_k\rangle 
                + \frac{1}{2L}\Vert g_k\Vert^2, 
            \\
            & \alpha_k \in (0, 1)
            \\
            & 
            \phi_{k + 1}(x)
            := (1 - \alpha_k)\phi_k (x) + 
            \alpha_k (l_h(x; y_k) + \mu/2\Vert x - y_k\Vert^2). 
        \end{align*}
        Where $(y_k)_{k\ge 0}$ is any auxiliary sequence. 
        If we define the canonical form for $\phi_k$ as convex quadratic parameterized by positive sequence $(\gamma_k), \phi_k^*$ and 
        \begin{align*}
            \phi_k^* := \min_{x} \phi_k(x)
            \\
            \phi_k(x) := \phi_k^* + \frac{\gamma_k}{2}\Vert x - v_k\Vert^2.
        \end{align*}
        Then the auxiliary sequence $y_k, v_k$, parameters for the canonical form of estimating sequence must satisfy for all $k\ge 0$ these inequalities: 
        {\small
        \begin{align*}      
            \gamma_{k + 1} &= (1 - \alpha_k) \gamma_k + \mu \alpha_k
            \\
            v_{k + 1} &= \gamma_{k + 1}^{-1}
            (\gamma_k(1 - \alpha_k)v_k - \alpha_k g_k + \mu \alpha_k y_k)
            \\
            \phi_{k + 1}^* &= 
            (1 - \alpha_k)\phi_k^*
            + \alpha_k\left(
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) 
                + \frac{1}{2L}\Vert g_k\Vert^2 
            \right) 
            \\
            &\quad 
            - \frac{\alpha_k^2}{2 \gamma_{k + 1}} \Vert g_k\Vert^2 
            + 
            \frac{\alpha_k(1 - \alpha_k)\gamma_k}{\gamma_{k + 1}} 
            \left(
                \frac{\mu}{2}\Vert v_k - y_k\Vert^2 
                + \langle v_k - y_k , g_k\rangle
            \right). 
        \end{align*}
        }
    \end{theorem}
    \begin{proof}
        By the recursive definition of $\phi_k$: 
        \begin{align*}
            \phi_{k + 1}(x) 
            &= 
            (1 - \alpha_k)\phi_k(x) + \alpha_k (l_F(x; y_k) + \mu/2\Vert x - y_k\Vert^2)
            \\
            &= 
            (1 -\alpha_k)
            \left(
                \phi_k^* + \gamma_k/2\Vert x - v_k\Vert^2
            \right) 
            + 
            \alpha_k
            \left(
                l_h(x; y_k) + \mu/2\Vert x - y_k\Vert^2
            \right)
            \rightarrow \textcolor{red}{(\text{eqn1})}; 
            \\
            \nabla \phi_{k + 1}(x) 
            &= 
            (1 - \alpha_k)\gamma_k(x - v_k) + \alpha_k(g_k + \mu (x - y_k));
            \\
            \nabla^2 \phi_{k + 1}(x) &= 
            \underbrace{((1 - \alpha_k)\gamma_k + \alpha_k \mu)}_{=\gamma_{k + 1}}I. 
        \end{align*}
        The first recurrence for is discovered as $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$. 
        Because $v_{k + 1}$ is the minimizer of $\phi_{k+ 1}$ by definition of the canonical form, solving $\nabla \phi_{k + 1}(x) = \mathbf 0 $ yields $v_{k + 1}$. 
        This is obtained by considering the following: 
        \begin{align*}
            \mathbf 0 &= 
            \gamma_k(1 - \alpha_k)(x - v_k) + \alpha_k g_k + \mu \alpha_k(x - y_k)
            \\
            &= (\gamma_k(1 - \alpha_k) + \mu \alpha_k)x - 
            \gamma_k(1 - \alpha_k)v_k + \alpha_k g_k - \mu \alpha_k y_k
            \\
            \iff 
            v_{k + 1} &:= x=
            \gamma_{k +1}^{-1} 
            \left(
                \gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k
            \right). 
        \end{align*}
        From the second and third equality we used $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$. 
        Substituting the canonical form of $\phi_{k + 1}$ back to \textcolor{red}{eqn1}, choose $x = y_k$, it gives the following: 
        \begin{align*}
            \phi_{k + 1}^* 
            &= (1 - \alpha_k)\phi_k^* + \frac{(1 - \alpha_k)\gamma_k}{2}\Vert y_k - v_k\Vert^2   
            \\
            & \quad 
            - \frac{\gamma_{k + 1}}{2}\Vert y_k - v_{k + 1}\Vert^2 
            + 
            \alpha_k\left(
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) 
                + 
                \frac{1}{2L} \Vert g_k\Vert^2
            \right)
            \rightarrow 
            \textcolor{red}{(\text{eqn2})}. 
        \end{align*}
        Next move is to simplify the term $\Vert v_{k + 1} - y_k\Vert^2$. 
        With that it produces: 
        \begin{align*}
            v_{k + 1} - y_k 
            &= 
            \gamma_{k + 1}^{-1}
            \left(
                \gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k
            \right) - y_k
            \\
            &= 
            \gamma_{k + 1}^{-1}
            \left(
                \alpha_k(1 - \alpha_k)v_k - \alpha_k g_k 
                + (-\gamma_{k + 1} + \mu\alpha_k)y_k
            \right)
            \\
            &
            \textcolor{gray}{
                \begin{aligned}
                    \gamma_{k + 1} &=    
                    (1 - \alpha_k)\gamma_k + \mu \alpha_k
                    \\
                    \gamma_{k + 1} - \mu \alpha_k &= (1 - \alpha_k)\gamma_k
                \end{aligned}
            }
            \\
            &=
            \gamma_{k + 1}^{-1}
            \left(
                \alpha_k(1 - \alpha_k)v_k - \alpha_k g_k 
                (1 - \alpha_k)\gamma_ky_k
            \right)
            \\
            &= 
            \gamma_{k + 1}^{-1}(
                \alpha_k(1 - \alpha_k)(v_k - y_k) 
                - \alpha_k g_k
            ).
        \end{align*}
        Taking the norm of that we have: 
        \begin{align*}
            \Vert v_{k + 1} - y_k\Vert^2 
            &= 
            \Vert 
                \gamma_{k + 1}^{-1}(
                    \alpha_k(1 - \alpha_k)(v_k - y_k) 
                    - \alpha_k g_k
                )
            \Vert^2
            \\
            \frac{- \gamma_{k + 1}}{2}
            \Vert v_{k + 1} - y_k\Vert^2
            &= 
            - \frac{1}{2\gamma_{k + 1}}
            \Vert 
                \gamma_k(1 - \alpha_k)(v_k - y_k) - \alpha_k g_k
            \Vert^2
            \\
            &= 
            -\frac{\gamma_k^2 (1 - \alpha_k)^2}{2 \gamma_{k + 1}} 
            \Vert v_k - y_k\Vert^2 - 
            \frac{\alpha_k^2}{2 \gamma_{k + 1}} \Vert g_k\Vert^2
            \\&\quad 
                + 
                \gamma_k(1 - \alpha_k)\gamma_{k + 1}^{-1} \langle v_k - y_k, \alpha_k g_k\rangle. 
        \end{align*}
        Substitute it back to \textcolor{red}{eqn2} we have 
        \begin{align*}
            \phi_{k + 1}^* &= 
            (1 - \alpha)\phi_k^* + 
            \alpha_k
            \left(
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) + 
                \frac{1}{2L}\Vert g_k\Vert^2
            \right)
            \\
            &\quad 
            + \frac{(1 - \alpha_k)\gamma_k}{2}\Vert y_k - v_k\Vert^2
            - \frac{\gamma_k^2(1 - \alpha_k)^2}{2\gamma_{k + 1}}\Vert v_k - y_k\Vert^2
            - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
            \\
            &\quad 
            + \alpha_k\gamma_k(1 - \alpha_k)\gamma_{k + 1}^{-1}\langle v_k -y_k, g_k\rangle
            \\
            &= 
                (1 - \alpha)\phi_k^* + 
                \alpha_k
                \left(
                    F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) + 
                    \frac{1}{2L}\Vert g_k\Vert^2
                \right)
                \\
                &\quad 
                + 
                \left(
                    \frac{(1 - \alpha_k)\gamma_k}{2}
                    - 
                    \frac{\gamma_k^2(1 - \alpha_k)^2}{2\gamma_{k + 1}}
                \right)
                \Vert v_k - y_k\Vert^2
                - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                \\
                &\quad 
                + \alpha_k\gamma_k(1 - \alpha_k)\gamma_{k + 1}^{-1}\langle v_k -y_k, g_k\rangle
            \\
            & \quad 
            \textcolor{gray}{  
                \begin{aligned}
                    \frac{(1 - \alpha_k)\gamma_k}{2}
                    - 
                    \frac{\gamma_k^2(1 - \alpha_k)^2}{2\gamma_{k + 1}}   
                    &= 
                    \frac{(1 - \alpha_k)\gamma_k}{2}
                    \left(
                        1 - \frac{\gamma_k (1 - \alpha_k)}{\gamma_{k + 1}}
                    \right)
                    \\
                    &= 
                    \frac{(1 - \alpha_k)\gamma_k}{2}
                    \left(
                        \frac{\gamma_{k + 1} - \gamma_k(1 - \alpha_k)}{\gamma_{k + 1}}
                    \right)
                    \\
                    &= 
                    \frac{(1 - \alpha_k)\gamma_k}{2}
                    \left(
                        \frac{\mu \alpha_k}{\gamma_{k + 1}}
                    \right). 
                \end{aligned}
            }
            \\
            \iff 
            &= 
                (1 - \alpha)\phi_k^* + 
                \alpha_k
                \left(
                    F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) + 
                    \frac{1}{2L}\Vert g_k\Vert^2
                \right)
                \\
                &\quad 
                + 
                \frac{(1 - \alpha_k)\gamma_k}{2}
                \left(
                    \frac{\mu \alpha_k}{\gamma_{k + 1}}
                \right)
                \Vert v_k - y_k\Vert^2
                - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                \\
                & \quad 
                + \alpha_k\gamma_k(1 - \alpha_k)\gamma_{k + 1}^{-1}\langle v_k -y_k, g_k\rangle
            \\
            &= 
                (1 - \alpha)\phi_k^* 
                + 
                \alpha_k
                \left(
                    F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) + 
                    \frac{1}{2L}\Vert g_k\Vert^2
                \right)
                \\
                &\quad 
                - 
                \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                + 
                \frac{(1 - \alpha_k)\gamma_k\alpha_k}{\gamma_{k + 1}}
                \left(
                    \frac{\mu}{2}\Vert v_k - y_k\Vert^2
                    + \langle v_k - y_k, g_k\rangle
                \right). 
        \end{align*}
        The second and third inequality used the equality $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu \alpha_k$. 
    \end{proof}
    
    \begin{theorem}[Verifying the conditions of implicit descent]\;\\
        Let estimating sequence $\phi_k$ and auxiliary sequence $y_k, v_k,\gamma_k, \alpha_k$  be given by 
        \hyperref[thm:canon-prox-grad-est-seq]{Theorem \ref*{thm:canon-prox-grad-est-seq}}. 
        If for all $k\ge 0$ they verify: 
        \begin{align*}
            \frac{1}{2L} - \frac{\alpha_k^2}{2 \gamma_{k + 1}} &\ge 0, 
            \\
            \frac{\alpha_k \gamma_k }{\gamma_{k + 1}} 
            (v_k - y_k) + (T_L y_k - y_k) &= \mathbf 0, 
        \end{align*}
        then $\phi_{k}$ is an estimating sequence that verifies $\forall x \in \RR^n, k \ge 0$: 
        \begin{align*}
            F\left(
                \widetilde{\mathcal J}_{L^{-1}} y_{k - 1}
            \right)\le \phi_k^*
            \\
            \phi_{k + 1}(x) - \phi_k(x) &\le -\alpha(\phi_k(x) - F(x)). 
        \end{align*}
    \end{theorem}
    \begin{proof}
        Inductively assume that $x_k = \widetilde{\mathcal J}_{L^{-1}}y_{k - 1}$ so $F(x_k) \le \phi_k^*$. 
        Substituting the $x_k$ into the equation for $\phi_{k + 1}$: 
        {\small
        \begin{align*}
            \phi_{k + 1}^* &= 
            (1 - \alpha_k) \phi_k^*
            + 
            \alpha_k
            \left(
                F(x_k) + \frac{1}{2L}\Vert g_k\Vert^2
            \right)
            \\&\quad 
                - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                + \frac{\alpha_k (1 - \alpha_k)\gamma_k}{\gamma_{k + 1}}
                \left(
                    \frac{\mu}{2}\Vert v_k - y_k\Vert^2 + \langle v_k - y_k, g_k\rangle
                \right)
            \\
            \implies 
            &\ge 
            (1 - \alpha_k)h(x_k)
            + 
            \alpha_k
            \left(
                h(x_k) + \frac{1}{2L}\Vert g_k\Vert^2
            \right)
            \\&\quad
                - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                + \frac{\alpha_k (1 - \alpha_k)\gamma_k}{\gamma_{k + 1}}
                \left(
                    \frac{\mu}{2}\Vert v_k - y_k\Vert^2 + \langle v_k - y_k, g_k\rangle
                \right)
            \\
            \implies
            &\ge 
            (1 - \alpha_k)h(x_k)
            + 
            \alpha_k
            \left(
                h(x_k) + \frac{1}{2L}\Vert g_k\Vert^2
            \right)
            - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
            + 
            \frac{\alpha_k (1 - \alpha_k)\gamma_k}{\gamma_{k + 1}}
            \langle v_k - y_k, g_k\rangle. 
        \end{align*}
        }
        The first inequality comes from the inductive hypothesis. 
        The second inequality comes from the non-negativity of the term $\frac{\mu}{2}\Vert v_k - y_k\Vert^2$. 
        Now, recall from the fundamental proximal gradient inequality in the convex settings, we have $\forall z\in \RR^n$: 
        \begin{align*}
            F(z) &\ge 
            F\left(\widetilde{\mathcal J}_{L^{-1}}y_k\right)
            + \left\langle 
                L(y - \widetilde{\mathcal J}_{L^{-1}}y_k), 
                z - \widetilde{\mathcal J}_{L^{-1}}y_k
            \right\rangle + D_f(z, y)
            \\
            & \text{set: }x_{k + 1} := \widetilde{\mathcal J}_{L^{-1}}y_k
            \\
            &\ge 
            F(x_{k + 1}) + \langle g_k, z - x_k\rangle + \frac{\mu}{2}\Vert z - y\Vert^2
            \\
            &=
            F(x_{k + 1}) + \langle g_k, z - y + y - x_k\rangle + \frac{\mu}{2}\Vert z - y\Vert^2
            \\
            &\ge
            F(x_{k + 1}) + \langle g_k, z - y\rangle 
            + \frac{1}{2L}\Vert g_k\Vert^2. 
        \end{align*}
        Now we set $z = x_{k}$ and substitute it back to RHS of $\phi_{k + 1}$ which yields: 
        \begin{align*}
            \phi_{k + 1}^*
            &\ge 
            (1 - \alpha_k)
            \left(
                F(x_{k + 1}) + \langle g_k, x_k - y_k\rangle + \frac{1}{2L}\Vert g_k\Vert^2
            \right)
            \\
            &\quad 
            + 
            \alpha_k
            \left(
                F(x_{k + 1}) + \frac{1}{2L}\Vert g_k\Vert^2
            \right)
            - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
            + 
            \frac{\alpha_k (1 - \alpha_k)\gamma_k}{\gamma_{k + 1}}
            \langle v_k - y_k, g_k\rangle
            \\
            &\ge 
            F(x_{k + 1})
            + 
            \left(
                \frac{1}{2L} - \frac{\alpha_k^2}{2\gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left\langle 
                g_k, \frac{\alpha_k\gamma_k}{\gamma_{k + 1}}(v_k - y_k) + (x_k - y_k)
            \right\rangle. 
        \end{align*}
        To assert $\phi_{k + 1}^* \ge F(x_k)$, one set of sufficient conditions are 
        \begin{align*}
            \left(
                \frac{1}{2L} - \frac{\alpha_k^2}{2\gamma_{k + 1}}
            \right) &\ge 0
            \\
            \frac{\alpha_k\gamma_k}{\gamma_{k + 1}}(v_k - y_k) + (x_k - y_k) 
            &= \mathbf 0. 
        \end{align*}
        Before we finish it, re-arranging should give use the equivalent representations 
        \begin{align*}
            -(\alpha_k \gamma_k\alpha_{k + 1}^{-1} + 1)y_k
            &= 
            - \alpha_k \gamma_k \gamma_{k + 1}^{-1}v_k - x_k
            \\
            y_k &= 
            \frac{
                \alpha_k \gamma_k \gamma_{k + 1}^{-1}v_k + x_k
            }{1 + \alpha_k \gamma_k \gamma_{k + 1}^{-1}}
            \\
            & 
            \textcolor{gray}{
                \gamma_{k + 1} + \alpha_k \gamma_k 
                = 
                \gamma_k + \alpha_k \mu
            }
            \\
            &=  
            \frac{\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k}{\gamma_k + \alpha_k \mu}. 
        \end{align*}
        And $\alpha_k, \gamma_k$, we have the equivalent representation of 
        \begin{align*}
            1 - \frac{L \alpha_k^2}{\gamma_{k + 1}}
            &\ge 0
            \\
            1 &\ge L \alpha_k^2 / \gamma_{k + 1}
            \\
            \gamma_{k + 1} &\ge L \alpha_k^2
            \\
            L\alpha_k^2 
            &\le
            \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu \alpha_k. 
        \end{align*}
    \end{proof}
    
    \begin{definition}[Nesterov's accelerated proximal gradient raw form]
    \label{app:def:acc-prox-grad-raw-form}
        The accelerated proximal gradient algorithm generates vector iterates $x_k, y_k, v_k$ using auxiliary sequence $\alpha_k, \gamma_k$ such that for all $k\ge0$ they satisfy conditions: 
        \begin{align*}
            L\alpha_k^2 
            &\le 
            (1 - \alpha_k)\gamma_k + \alpha_k\mu = \gamma_{k + 1}; \alpha_k \in (0, 1), 
            \\
            y_k &= (\gamma_k + \alpha_k \mu)^{-1}
            (\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k),
            \\
            x_{k + 1}&= 
            \widetilde{\mathcal J}_{L^{-1}} y_k
            \\
            v_{k + 1} &= 
            \gamma_{k + 1}^{-1}
            \left(
                (1 - \alpha_k)\gamma_k v_k + \alpha_k \mu y_k - \alpha_k g_k
            \right). 
        \end{align*}
    \end{definition}

    \begin{theorem}[Intermediate form of accelerated proximal gradient]\label{app:theorem:acc-prox-grad-intermediate}\; \\
        Let iterates $(x_k, y_k, v_k)$ be given by the raw form of Nesterov's accelerated proximal gradient. 
        If we assume that $L\alpha_k^2 = \gamma_{k + 1}$, then it simplifies into the following representation without parameter $\gamma_k$: 
        \begin{align*}
            y_k &= 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            x_{k + 1} &= 
            y_k - L^{-1}  g_k
            \\
            v_{k + 1} &= 
            \left(
                1 + \frac{\mu}{L \alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
            \right) - \frac{1}{L\alpha_{k}} g_k
            \\
            0 &= \alpha_k^2 - \left(\mu/L - \alpha_{k -1}^2\right) \alpha_k - \alpha_{k - 1}^2. 
        \end{align*}
        Here we have $g_k = \widetilde{\mathcal G}_{L^{-1}}y_k$. 
    \end{theorem}
    \begin{proof}
                
        From definition,  we have equality: $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$, so $\gamma_{k + 1} + \alpha_k \gamma_k = \gamma_k + \alpha_k \mu$, with that in mind we can simplify the expression for $y_k$ by 
        \begin{align*}
            y_{k} &= 
            (\gamma_k + \alpha_k \mu)^{-1}
            (\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k)
            \\
            &= 
            (\gamma_{k + 1} + \alpha_k \gamma_k)^{-1}
            (\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k)
            \\
            &= 
            \left(
                \frac{\gamma_{k + 1}}{\alpha_k\gamma_k} + 1
            \right)^{-1}
            \left(
                v_k + \frac{\gamma_{k + 1}}{\alpha_k \gamma_k} x_k
            \right)
            \\
            &= 
            \left(
                1 + \frac{L\alpha_k^2}{\alpha_kL\alpha_{k - 1}^2} 
            \right)^{-1}
            \left(
                v_k + \frac{L\alpha_k^2}{\alpha_k L\alpha_{k - 1}^2} x_k
            \right)
            \\
            &= 
            \left(
                1 + \frac{\alpha_k}{\alpha_{k - 1}^2}
            \right)^{-1}
            \left(
                v_k + 
                \frac{\alpha_k}{\alpha_{k - 1}^2} x_k
            \right). 
        \end{align*}
        For $v_{k + 1}$ we use $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu \alpha_k$ which gives us: 
        \begin{align*}
            v_{k + 1} &= 
            \gamma_{k + 1}^{-1}
            ((1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k)
            - \alpha_k\gamma_{k + 1}^{-1}\mathcal L y_k
            \\
            &= 
            ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
            \left(
                (1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k
            \right)
            - \alpha_k\gamma_{k + 1}^{-1}\mathcal G_L y_k
            \\
            &= 
            \left(
                1 + \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k}
            \right)^{-1}
            \left(
                v_k + 
                \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k} y_k
            \right)
            - \alpha_k\gamma_{k + 1}^{-1}\mathcal G_L y_k
            \\
            &= 
            \left(
                1 + \frac{\alpha_k\mu}{(1 -\alpha_k)L\alpha_{k - 1}^2}
            \right)^{-1}
            \left(
                v_k + 
                \frac{\alpha_k\mu}{(1 - \alpha_k)L\alpha_{k - 1}^2} y_k
            \right)
            - \frac{1}{L\alpha_{k}}\mathcal G_L y_k
        \end{align*}
        We can eliminate the $\gamma_k$ which defines the $\alpha_k$ by considering 
        \begin{align*}
            L\alpha_k^2 &= 
            (1 - \alpha_k)\gamma_k + \alpha_k \mu 
            \\
            &= 
            (1 - \alpha_k)L\alpha_{k - 1}^2 
            + \alpha_k \mu
            \\
            L\alpha_k^2 &= 
            L \alpha_{k - 1}^2 + 
            (\mu - L \alpha_{k - 1}^2)\alpha_k
            \\
            \iff     
            0
            &=  
            L \alpha_k^2 - (\mu - L \alpha_{k - 1}^2)\alpha_k 
            - L \alpha_{k -1}^2. 
        \end{align*}
        Next, we simplify the coefficients using the above relations further. 
        From the above results we have the relation $(1 - \alpha_k)L\alpha_{k - 1}^2 = L \alpha_k^2 - \alpha_k \mu$. 
        Therefore,  it gives 
        \begin{align*}
            \frac{\alpha_k\mu}{(1 - \alpha_k)L \alpha_{k - 1}^2}
            &= 
            \frac{\alpha_k\mu}{L \alpha_k^2 - \alpha_k \mu}
            = \frac{\mu}{L \alpha_k - \mu}. 
        \end{align*}
        Next we have: 
        \begin{align*}
            L\alpha_k^2 &= 
            (1 - \alpha_k)L\alpha_{k - 1}^2 + \alpha_k \mu 
            \\
            L \alpha_k^2 - \alpha_k\mu &= 
            (1 - \alpha_k)L \alpha_{k - 1}^2
            \\
            \alpha_{k - 1}^2
            &= 
            \frac{L \alpha_k^2 - \alpha_k\mu}{L (1 - \alpha_k)}
            \\
            \frac{1}{\alpha_{k - 1}^2}
            &= 
            \frac{L (1 - \alpha_k)}{L \alpha_k^2 - \alpha_k\mu}
            \\
            \frac{\alpha_k}{\alpha_{k - 1}^2}
            &= 
            \frac{L - L\alpha_k}{L\alpha_k - \mu}. 
        \end{align*}
        Substitute these results back to the expression for $y_k, v_{k + 1}$, it gives what we want. 

    \end{proof}
    \begin{remark}
        This intermediate form representation of the algorithm eliminated the sequence $(\gamma_k)_{k \ge0}$ which were used for the Nesterov's estimating sequence. 
    \end{remark}

    \begin{theorem}[Nesterov's accelerated proximal gradient momentum form]
        \; \\
        Let the sequence $\alpha_k$, and vectors $y_k, x_k, v_k$ be given by the intermediate form of the Nesterov's accelerated proximal gradient, then it can be simplified to void of $v_k$. 
        The algorithm generates $y_k, x_k, \alpha_k$ such that it satisfies for all $k \ge 0$: 
        \begin{align*}
            & \text{find } \alpha_{k + 1} \text{ such that: }L \alpha_{k + 1}^2 = (1 - \alpha_{k + 1})L \alpha_{k - 1} + \mu \alpha_{k + 1}
            \\
            & x_{k + 1} = \widetilde {\mathcal J}_{L^{-1}} y_k
            \\
            & y_{k + 1} = \frac{\alpha_k(1 - \alpha_k)}{\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        Initially we choose $x_0 = y_0, \alpha_0 \in (0, 1)$. 
    \end{theorem}
    \begin{proof}
        
    \end{proof}
        
    
    
    
\section{Proofs for accelerated PPM}


\section{Proofs for Catalyst Meta Acceleration}


\section{Proofs for 4WD Catalyst Acceleration}

\end{document}
