\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

\begin{document}

\title{
    {
        \fontfamily{ptm}\selectfont 
        Catalyst Meta Acceleration Framework: The history and the gist of it
    }
    }

\author{
    Hongda Li
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{alto@mail.ubc.ca}.}~ and~Xianfu Wang
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. E-mail:  \texttt{shawn.wang@ubc.ca}.}
}

\date{\today}

\maketitle

% \vskip 8mm

\begin{abstract} 
    \noindent
    Nesterov's accelerated gradient first appeared back in the 1983 has sparked numerous theoretical and practical advancements in Mathematics programming literatures. 
    The idea behind Nesterov's acceleration is universal for convex objective, and it has concrete extension in the non-convex case. 
    In this paper we survey specifically the Catalyst Acceleration that incorporated ideas from the Accelerated Proximal Point Method proposed by Guler back in 1993. 
    The paper reviews Nesterov's classical analysis of accelerated gradient in the convex case.
    The paper will describe key aspects of the theoretical innovations involved to achieve the design of the algorithm in convex, and non-convex case. 
    
\end{abstract}

% \noindent{\bfseries 2010 Mathematics Subject Classification:}
% Primary 65K10, 90c25, 90C30; Secondary 65Y20. 
% \noindent{\bfseries Keywords: } Nesterov acceleration, Proximal point method. 

\section{Introduction}
    \textcolor{red}{THIS REPORT IS CURRENTLY: FINISHED, BUT NOT YET POLISHED}

    The optimal algorithm named accelerated gradient descent method is proposed in Nesterov's seminal work back in 1983 \cite{nesterov_method_1983}. 
    The algorithm closed the upper bound and lower bound on the iteration complexity for all first order Lipschitz smooth convex function among all first order algorithms. 
    For a specific definition of first order method, we refer reader to Chapter 2 of Nesterov's new book \cite{nesterov_lectures_2018} for more information. 
    Gradient descent has an upper bound of $\mathcal O(1/k)$ in iteration complexity that is slower than
    the lower iteration complexity $\mathcal O(1/k^2)$. 
    Accelerated gradient descent has an upper bound of $\mathcal O(1/k^2)$, making it optimal. 

    It's tempting to believe that the existence of an optimal algorithm sealed the ceiling for the need of theories for convex first-order smooth optimization. 
    It is correct but lacks the nuance in understanding because Nesterov's accelerated gradient is a system of analysis techniques which is not a specific design paradigm for algorithms. 
    
    Guler's accelerated Proximal Point Method (PPM) \cite{guler_new_1992} in 1993 used the technique of Nesterov's estimating sequence to accelerate PPM for convex objectives. 
    Use $(\lambda_k)_{k \ge 0}$ to parameterize the proximal point evaluation to generate $(x_k)_{k\ge 0}$ given any initial guess $x_0$.
    Guler's prior work \cite{guler_convergence_1991} showed the convergence of PPM method in the convex without acceleration is $\mathcal O\left(1/\sum^{n}_{i = 1}\lambda_i\right)$. 
    His new algorithm with acceleration has a rate of $\mathcal O\left(1/(\sum_{i = 1}^{n} \sqrt{\lambda_i})^2\right)$. 
    An inexact Accelerated PPM method using conditions described in Rockafellar's works in 1976 \cite{rockafellar_monotone_1976} is also discussed in the paper. 

    It's tempting to conclude that the results has reached the ceiling for extending Nesterov's acceleration. 
    It is correct, but not from a practical point of view. 
    Let $F: \RR^n \rightarrow\RR$ be our objective function, $\mathcal J_\lambda := (I + \lambda \partial F)^{-1}$ and $\mathcal M^{\lambda}(x; y):= F(x) + \frac{1}{2\lambda}\Vert x - y\Vert^2$ then the inexact proximal point considers with error $\epsilon_k$ has the following characterizations of inexactness as put forward by Guler \cite{guler_new_1992}: 
    \begin{align*}
        & \tilde x \approx \mathcal J_\lambda y
        \\
        & 
        \dist(\mathbf 0, \partial\mathcal{M}^{\lambda}(\tilde x; y))
        \le \frac{\epsilon}{\lambda}. 
    \end{align*}
    The difficulty comes from controlling the error $\epsilon$ at each iteration to ensure the overall convergence of accelerated PPM. 
    In the paper $\epsilon_k \rightarrow 0$ at a specific rate. 
    It requires knowledge about exact minimum of the Moreau envelope at each step and the optimal value of the Nesterov's estimating sequence. 
    These quantities are intractable in practice making it impossible to formulate it to algorithms directly. 

    Introduced in Lin et al. \cite{lin_universal_2015,lin_catalyst_2018} is a series of papers on a concrete meta algorithm called Catalyst (It's called 4WD Catalyst for the non-convex extension in works by Paquette, Lin et al. \cite{paquette_catalyst_2018}). 
    The meta algorithm uses other first order algorithms to evaluate inexact proximal point method and then performs accelerated PPM, therefore the umbrella term: ``meta''. 
    Major innovations include Tracking and controlling the errors made in the inexact PPM using Nesterov's estimating sequence throughout and an algorithm called accelerated MISO-Prox.  
    Prior to Lin's paper, it was an open question on the conditions required to accelerate incremental method such as stochastic gradient descent can be accelerated. 

    \subsection{Contributions}
        The writing is expository and comprehensive, it will survey the history and major results, and innovations involved in conceiving and designing the Catalyst algorithm. 
        We reviewed the literatures and faithfully reproduced important claims.
        In addition, we give insights and context to understand the claims in these papers and making connections to ideas in optimization. 
        Three papers by Guler \cite{guler_new_1992} and Lin \cite{lin_universal_2015} and Paquette et al. \cite{lin_catalyst_2018} together with Nesterov's \cite{nesterov_lectures_2018} method of estimating sequence are the targets of this report. 

        We will only cover innovations in the theoretical aspect of Catalyst Acceleration. 
        Applications and specific example algorithms are out of the scope because there are too many papers on the separate topic of variance reduced stochastic algorithms. 

\section{Preliminaries}\label{sec:preliminaries}
    Throughout the writing, let the ambient space is $\RR^n$. 
    The optimization problem is
    \begin{align*}
        \min_{x \in \RR^n} F(x). 
    \end{align*}
    This section introduces the Nesterov's estimating sequence technique. 
    This technique is fundamental for in Guler's accelerated PPM method and Catalyst meta acceleration in the convex/strongly convex case. 
    Unlike proofs using a Lyapunov argument which requires knowing exactly the algorithm in advance to verify the convergence rate, Nesterov's estimating squence can produce an algorithm that generates iterates $(x_k)_{k \ge0}$ such that it converges at some rate. 
    \subsection{Nesterov's Estimating Sequence}
        \begin{definition}[Nesterov's estimating sequence]\label{def:nes-est-seq}
            Let $(\phi_k : \RR^n \rightarrow\RR)_{k \ge 0}$ be a sequence of functions. 
            We call this sequence of function a Nesterov's estimating sequence when it satisfies the conditions: 
            \begin{enumerate}
                \item There exists another sequence $(x_k)_{k \ge 0}$ such that for all $k \ge 0$ it has $F(x_k) \le \phi_k^*: =\min_{x}\phi_k(x)$. 
                \item There exists a sequence of $(\alpha_k)_{k \ge 0}$ where $\alpha_k \in (0, 1)\; \forall k \ge0 $ such that for all $x \in \RR^n$ it has $\phi_{k + 1}(x) - \phi_k(x) \le - \alpha_k(\phi_k(x) - F(x))$. 
            \end{enumerate}
        \end{definition}
        \begin{observation} 
            If we define $\phi_k$, $\Delta_k(x) := \phi_k (x) - F(x)$ for all $x \in \RR^n$ and assume that $F$ has minimizer $x^*$. 
            Then observe that $\forall k \ge 0$:  
            \begin{align*}
                \Delta_k(x) 
                &= \phi_k(x) - F(x) \ge \phi_k^* - F(x)
                \\
                x = x_k\implies 
                \Delta_k(x_k) 
                &\ge 
                \phi_k^* - F(x_k) \ge 0
                \\
                x = x_* \implies 
                \Delta_k(x_*)
                &\ge \phi_k^* - F_* \ge F(x_k) - F_* \ge 0
            \end{align*}
            The function $\Delta_k(x)$ is non-negative at points: $x_*, x_k$.
            We can derive the convergence rate of $\Delta_k(x^*)$ because $\forall x \in \RR^n$: 
            \begin{align*}
                \phi_{k + 1}(x) - \phi_k(x) 
                &\le - \alpha_k (\phi_k(x) - F(x))
                \\
                \iff 
                \phi_{k + 1}(x) - F(x) - (\phi_k(x) - F(x))
                &\le 
                -\alpha_k(\phi_k(x) - F(x))
                \\
                \iff
                \Delta_{k + 1}(x) - \Delta_k(x) &\le
                - \alpha_k\Delta_k(x)
                \\
                \iff 
                \Delta_{k + 1}(x) 
                &\le 
                (1 - \alpha_k)\Delta_k(x). 
            \end{align*}
            Unrolling the above recursion it yields: 
            \begin{align*}
                \Delta_{k + 1}(x) &\le 
                (1 - \alpha_k)\Delta_k(x) \le \cdots \le 
                \left(
                    \prod_{i = 0}^k(1 - \alpha_i)
                \right)\Delta_0(x). 
            \end{align*}
            Finally, by setting $x = x^*$, $\Delta_k(x^*)$ is non-negative and using the property of Nesterov's estimating sequence it gives: 
            $$
                F(x_k) - F(x^*) \le \phi_k^* - F(x^*) \le \Delta_k(x^*) = \phi_k(x^*) - F(x^*) \le \left(\prod_{i = 0}^k(1 - \alpha_i)\right)\Delta_0(x^*).
            $$ 
        \end{observation}
        Creativity is important in the construction of the estimating sequence $(\phi_k)_{k \ge 0}$. 

\section{Nesterov's accelerated proximal gradient}
    This section swiftly exposes the constructions of the Nesterov's estimating sequence for accelerated proximal gradient method. A similar accelerated projected gradient is Algorithm (2.2.63) in Nesterov's book \cite{nesterov_lectures_2018}. 
    We use accelerated proximal gradient algorithm as an example because its formulation is similar to the Catalyst Acceleration framework. 

    Throughout this section we assume that: $F = f + g$ where $f$ is $L$-Lipschitz smooth and $\mu \ge 0$ strongly convex and $g$ is convex. 
    Define 
    \begin{align*}
        \mathcal M^{L^{-1}}(x; y) 
        &:= g(x) + f(y) 
        + 
        \left\langle \nabla f(x), x - y\right\rangle 
        + 
        \frac{L}{2}\Vert x - y\Vert^2, 
        \\
        \widetilde{\mathcal J}_{L^{-1}}y 
        &:= \argmin_{x} \mathcal M^{L^{-1}}(x; y), 
        \\
        \mathcal G_{L^{-1}}(y)
        &:= L\left(I - \widetilde{\mathcal J}_{L^{-1}}\right)y. 
    \end{align*}

    In the literature, $\mathcal G_{L^{-1}}$ is commonly known as the gradient mapping. 
    The definition follows, we define the Nesterov's estimating sequence used to derive the accelerated proximal gradient method. 
    \begin{definition}[Accelerated proximal gradient estimating sequence]
    \label{def:nes-est-seq-pg}\;\\
        Define $(\phi_k)_{k \ge0}$ be the Nesterov's estimating sequence recursively given by: 
        \begin{align*}
            & l_F(x; y_k) := 
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k \right) 
                + \langle \mathcal G_{L^{-1}}y_k, x - y_k\rangle + 
            \frac{1}{2L}\Vert \mathcal G_{L^{-1}}y_k\Vert^2, 
            \\
            & 
            \phi_{k + 1}(x)
            := (1 - \alpha_k)\phi_k (x) + 
            \alpha_k 
            \left(
                l_F(x; y_k) + \frac{\mu}{2}\Vert x - y_k\Vert^2
            \right). 
        \end{align*}
        The Algorithm generates a sequence of vectors $y_k, x_k$, and scalars $\alpha_k$ satisfies the following: 
        \begin{align*}
            &x_{k + 1} = \widetilde{\mathcal J}_{L^{-1}} y_k, 
            \\
            & \text{find } \alpha_{k + 1} \in (0, 1): 
            \alpha_{k + 1} = (1 - \alpha_{k + 1})\alpha_k^{2} + (\mu/L) \alpha_{k + 1}
            \\
            &y_{k + 1} = x_{k + 1} + \frac{\alpha_k(1 - \alpha_k)}{\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        One of the possible base case can be $x_0 = y_0$ and any $\alpha_0 \in (0, 1)$. 
    \end{definition}
    \begin{observation}
        Fix any $y$, for all $x\in \RR^n$, $F(x) \ge l_F(x; y_k) + \mu/2\Vert x - y_k\Vert^2$ is the proximal gradient inequality. 
        If $f \equiv 0$ then $\widetilde{\mathcal J}_{L^{-1}}y_k$ becomes resolvent $(I + L^{-1}\partial F)^{-1}$, which makes $x_k$ being an exact evaluation of PPM: 
        \begin{align*}
            l_F(x; y_k) 
            &= F(\mathcal J_{L^{-1}}y_k) 
            + \langle L(y - \mathcal J_{L^{-1}} y), x - y_k\rangle + \frac{L}{2}\Vert y_k - \mathcal J_{L^{-1}}y_k\Vert^2
            \\
            &= F(\mathcal J_{L^{-1}}y_k) 
            + \langle L(y - \mathcal J_{L^{-1}} y), x - \mathcal J_{L^{-1}}y_k\rangle. 
        \end{align*}
        This is the proximal inequality with constant a step size: $L^{-1}$. 
    \end{observation}
    To demonstrate the usage of Nesterov's estimating sequence here, consider sequence $(x_k)_{k \ge 0}$ such that $F(x_k) \le \phi_k^*$. 
    Assume the existence of minimizer $x^*$ for $F$, by definition of $\phi_k$ let $x = x^*$ then $\forall k \ge 0$: 
    {\small
    \begin{align*}
        \phi_{k + 1}(x^*)
        &= (1 - \alpha_k)\phi_k (x^*) + 
        \alpha_k 
        \left(
            l_F(x^*; y_k) + \frac{\mu}{2}\Vert x^* - y_k\Vert^2
        \right)
        \\
        \phi_{k + 1}(x^*) - \phi_k(x^*) &= 
        -\alpha_k\phi_k(x^*) 
        +
        \alpha_k
        \left(
            l_F(x^*; y_k) + \frac{\mu}{2}\Vert x^* - y_k\Vert^2
        \right) 
        \\
        \implies
        \phi_{k + 1}(x^*) - F(x^*) + F(x^*) - \phi_k(x^*) 
        &\le -\alpha_k(\phi_k(x^*) - F(x^*))
        \\
        \implies 
        F(x_{k + 1}) - F(x^*)
        \le \phi_{k + 1}^* - F(x^*)
        &\le \phi_{k + 1}(x^*) - F(x^*)
        \le 
        (1 - \alpha_k)(\phi_k(x^*) - F(x^*)). 
    \end{align*}
    }
    On the first inequality we used the fact that $l_F(x; y_k) + \mu/2\Vert x - y_k\Vert^2\le F(x)$. 
    Unrolling the recurrence, we can get the convergence rate of $F(x_k) - F(x^*)$ to be on Big O of $\prod_{i = 1}^k(1 - \alpha_i)$. 
    \begin{remark}
        The definition is a generalization of Nesterov's estimating sequence comes from (2.2.63) from Nesterov's book \cite{nesterov_lectures_2018}. 
        Compare to Nesterov's work, we used proximal gradient operator instead of projected gradient. 
    \end{remark}
    % \begin{definition}[Accelerated proixmal gradient algorithm]\label{def:acc-prox-grad-alg}
    %     The algorithm of accelerated proximal gradient generates sequence of iterates $(x_k, y_k)_{k \ge 0}$ which satisfies for all $k\ge 0$: 
    %     \begin{align*}
    %         &x_{k + 1} = \widetilde{\mathcal J}_{L^{-1}} y_k, 
    %         \\
    %         & \text{find } \alpha_{k + 1} \in (0, 1)
    %         \alpha_{k + 1} = (1 - \alpha_{k + 1})\alpha_k^{2} + (\mu/L) \alpha_{k + 1}
    %         \\
    %         &y_{k + 1} = x_{k + 1} + \frac{\alpha_k(1 - \alpha_k)}{\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
    %     \end{align*}

    % \end{definition}
    % \begin{remark}
    %     The simple case of accelerated gradient descent is stated as (2.2.63) in Nesterov's book \cite{nesterov_lectures_2018}. 
    % \end{remark}
    For a proof for the Nesterov's estimating sequence $\phi_k$ and a derivation of the algorithm, see
    \hyperref[app:sec:thm-claim-acc-prox-grad]{Appendix \ref*{app:sec:thm-claim-acc-prox-grad}}. 
    We warn the readers that the proof is long. 

\section{Guler's estimating sequence}
    This section introduces the setup of the estimating sequence in Guler's accelerated Proximal Point method \cite{guler_new_1992}. 
    Guler showed the technique can accelerate proximal point method in the convex settings. 
    Throughout the section, assume that $F:\RR^n \rightarrow \overline \RR$ is a convex function. 
    Define:
    \begin{align*}
        \mathcal M^{\lambda} (x; y) &:= F(x) + \frac{1}{2\lambda}\Vert x - y\Vert^2, 
        \\
        \mathcal J_\lambda y &:= \argmin_x \mathcal M^{\lambda} (x; y), 
        \\
        \mathcal G_\lambda &:= \lambda^{-1}(I - \mathcal J_\lambda). 
    \end{align*}
    For simplicity, we use $\mathcal G_k, \mathcal J_k$ as short for $\mathcal G_{\lambda_k}, \mathcal J_{\lambda_k}$ where $(\lambda_k)_{k \ge 0}$ is a fixed sequence used in the proximal point on the $k$ iteration. 
    
    \begin{definition}[Accelerated PPM estimating sequence]\label{def:nes-est-seq-acc-ppm}
        \; \\
        The estimating sequence  $(\phi_k)_{k \ge0}$ for the accelerated proximal point method is defined by the following recurrence for all $k \ge0$, any $A \ge 0$: 
        \begin{align*}
            \phi_0(x) &:= f(x_0) + \frac{A}{2}\Vert x - x_0\Vert^2, 
            \\
            \phi_{k + 1}(x) &:= 
            (1 - \alpha_k)\phi_k(x)
            + 
            \alpha_k(F(\mathcal J_k y_k) + \langle \mathcal G_k y_k, x - \mathcal J_k y_k\rangle).    
        \end{align*}
        Let $(\lambda_k)_{k \ge 0}$ be the step size which defines the descent sequence $x_k = \mathcal J_\lambda y_k$. 
        Then for all $k \ge 0$, the descent sequence $x_k$, along with the auxiliary vector sequence $(y_k, v_k)$, scalar sequence $(\alpha_k, A_k)_{k\ge 0}$ are generated by: 
        \begin{align*}
            \alpha_k &= \frac{1}{2}\left(
                \sqrt{(A_k\lambda_k)^2 + 4A_k \lambda_k}
                - A_k\lambda_k
            \right), 
            \\
            y_k &= (1 - \alpha_k)x_k + \alpha_k v_k, 
            \\
            v_{k + 1}
            &= 
            v_k - \frac{\alpha_k}{A_{k + 1}\lambda_k}(y_k - \mathcal J_k y_k), 
            \\
            A_{k + 1} &= (1 - \alpha_k)A_k. 
        \end{align*}
    \end{definition}
    \begin{remark}
        The auxiliary sequences $(A_k, v_k)$ parameterizes a canonical representation of the estimating sequence $(\phi_k)_{k \ge0}$. 
        Guler didn't simplify his results compare to what Nesterov did in his book. 
        For a detailed proof of the estimating sequence with comparison to the accelerated proximal gradient method, see Appendix \ref*{sec:app:exct-acc-ppm}. 
    \end{remark}
    For the inexact evaluation of PPM, Guler cited Rockafellar \cite{rockafellar_monotone_1976} for condition (A') in his text which is the following: 
    \begin{align*}
        x_{k + 1}\approx \mathcal J_{k} y_k \text{ be such that: }
        \dist\left(
            \mathbf 0, \partial \mathcal M^{k}(x_{k + 1}; y_k)
        \right) &\le \frac{\epsilon_k}{\lambda_k}
        \\
        \implies 
        \Vert x_{k + 1} - \mathcal J_{k}y_k\Vert 
        &\le \epsilon_k. 
    \end{align*}
    Condition A' also characterizes the property of $(\epsilon_k)_{k\ge 0}$ so inexact PPM converges. 
    Guler strengthens it in his context and proved the following: 
    \begin{theorem}[Guler's inexact proximal point error bound]\label{thm:guler-inexact-ppm-bound}
        \;\\
        Define Moreau Envelope at $y_k$ as $\mathcal M_k^* := \min_z \mathcal {M}^{\lambda_k}(z; y_k)$. 
        If $x_{k +1}$ is an inexact evaluation under condition (A'), then the estimating sequence admits the conditions: 
        \begin{align*}
            \frac{1}{2\lambda_k} \Vert x_{k + 1} - \mathcal J_k y_k\Vert^2
            &\le 
            \mathcal M_k(x_{k + 1}, y_k) - \mathcal M^*_k
            \le \frac{\epsilon_k^2}{2\lambda_k}. 
        \end{align*}
    \end{theorem}
    \begin{remark}
        The inequalities are from PL-condition and strong convexity. 
        They are now standard results in convex optimizations. 
        For a proof of the theorem, see 
        \hyperref[app:sec:inxt-acc-ppm]{Appendix \ref*{app:sec:inxt-acc-ppm}}. 
    \end{remark}
    The next theorem is Theorem 3.3 of Guler's 1993 papers which is a major result for inexact accelerated PPM method. 
    \begin{theorem}[Guler's accelerated inexact PPM convergence results]
        If the error sequence $(\epsilon_k)_{k \ge0}$ for condition A' is bounded by $\mathcal O(1/k^\sigma)$ for some $\sigma > 1/2$, then the accelerated proximal point method has for any feasible $x \in \RR^n$: 
        \begin{align*}
            f(x_k) - f(x) \le \mathcal O(1/k^2) + \mathcal O (1 / k^{2\sigma - 1})\rightarrow 0. 
        \end{align*}    
        When $\sigma \ge 3/2$ then the method converges at a rate of $\mathcal O(1/k^2)$. 
    \end{theorem}
    The theorem looks exciting, but Lin 2015 \cite{lin_universal_2015} page 11 pointed out that $\mathcal G_k^*, \mathcal J_{\lambda_k} y_k$ are both intractable quantities. 
    In Guler's work, these intractable quantities were built into the Nesterov's estimating sequence making it unclear how to control $\epsilon_k \rightarrow 0$. 
    If we use the inexact formulation from Guler and his estimating sequence, it will result in algorithm that contains intractable quantities $\mathcal J_{\lambda_k} y_k$. 


\section{Lin 2015}\label{sec:lin-2015}
    The section introduces the Nesterov's estimating sequence in Lin 2015 \cite{lin_universal_2015}. 
    We warn the readers about the followings: 
    \begin{enumerate}
        \item The proofs in HongZhou Lin's original paper of Universal Catalyst is depressingly long and complicated. It is a result of using the constructive approach of Nesterov's estimating sequence. 
        \item Controlling the errors of inexact proximal point evaluations is context specific. Lin hinted at ways to track the errors such as using duality and non-negativity assumption of the objective. He illustrated the use of the meta acceleration on their own method called: ``Accelerated MISO-Prox", in general there is not a universal solution. 
        \item We will provide proofs to clarify some of their proofs and compare with existing proofs and drawing references in the literatures in the appendix. 
    \end{enumerate}
    Let's assume $F$ is a $\mu \ge 0$ strongly convex function. 
    Throughout this section we make the following notations
    \begin{align*}
        \mathcal M^{\kappa^{-1}}(x; y) &:= F(x) + \frac{\kappa}{2}\Vert x - y\Vert^2, 
        \\
        \mathcal J_{\kappa^{-1}} y &:= \argmin_x \mathcal M^{\kappa^{-1}} (x, y). 
    \end{align*}
    Their algorithm is almost exactly the same as Nesterov's 2.2.20 \cite{nesterov_lectures_2018} which we stated in the definition below: 
    \begin{definition}[Lin's accelerated proximal point method]
        Let the initial estimate be $x_0 \in \RR^n$, fix parameters $\kappa$ and $\alpha_0$. 
        Let $(\epsilon_k)_{k \ge 0}$ be an error sequence chosen for the evaluation for inexact proximal point method. 
        Initialize $x_0 = y_0$, then the algorithm generates $(x_k, y_k)$ satisfies for all $k \ge 1$
        \begin{align*}
            & \text{find } x_k \approx \mathcal J_{\kappa^{-1}} y_{k - 1} \text{ such that } \mathcal M^{\kappa^{-1}}(x_k, y_{k - 1}) - \mathcal M^{\kappa^{-1}}(\mathcal J_{\kappa^{-1}}y_{k - 1}, y_{k - 1}) \le \epsilon_k
            \\
            & \text{find } \alpha_k \in (0, 1) \text{ such that } \alpha_k^2 = (1 - \alpha_k)\alpha_{k - 1}^2 + (\mu/(\mu + \kappa)) 
            \\
            & 
            y_{k} = x_k + \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})}{\alpha_{k - 1}^2 + \alpha_k}(x_k - x_{k - 1}). 
        \end{align*}
    \end{definition}
    \begin{remark}
        The algorithm is similarity to
        \hyperref[def:nes-est-seq-pg]{Definition \ref*{def:nes-est-seq-pg}}. 
        In contrast, it has an inexact proximal point step controlled by $\epsilon_k$, and the Lipschitz constant $L$ is absent instead it has $\kappa + \mu$. 
        Evaluating $x_k \approx \mathcal J_{\kappa^{-1}}y_{k - 1}$ is also possible because the function $\mathcal M^{\kappa^{-1}}(\cdot, y_{k - 1})$ is strongly convex, hence its optimality gap can be bounded via trackable quantity $\partial \mathcal M^{\kappa^{-1}}(x_k, y_{k - 1})$. 

        Controlling the error sequence $\epsilon_k$ however is a whole new business. 
        Lin 2015 \cite{lin_universal_2015} commented on the second last paragraph on page 4, and here we quote:
        \par
        ``The choice of the sequence $(\epsilon_k)_{k \ge 0}$ is also subjected to discussion since the quantity $F(x_0) - F^*$ is unknown beforehand. Nevertheless, an upper bound may be used instead, which will only affects the corresponding constant in (7). Such an upper bounds can typically be obtained by computing a duality gap at $x_0$, or by using additional knowledge about the objective. For instance, when $F$ is non-negative, we may simply choose $\epsilon_k = (2/9)F(x_0)(1 - \rho)^k$". 
        
        This comment has upmost practical importance because it tells us how to bound the error $\epsilon_k$ to achieve accelerated convergence rate. 
        In theory, $\epsilon_k$ decreases at a rate related to $F(x_0) - F^*$. 
        It requires some knowledge about $F^*$ in prior. 
        Therefore, controlling $\epsilon_k$ is still elusive in general in a practical context. 
        To see how the error is controlled for the inexact proximal point evaluation, we refer the readers to Lemma B.1 in Lin's 2015 paper \cite{lin_universal_2015}. 
    \end{remark}

    For theoretical interests, there is a major difference between Lin's approach and Guler's approach. 
    Lin didn't formulate any of the intractable quantities in the definitions for his Nesterov's estimating sequence $\phi_k$. 
    One major innovation is Lemma A.7 in Lin's 2015 paper \cite{lin_universal_2015}. 
    The lemma allows the analysis Nesterov's estimating sequence to be carried through without using intractable quantities: $\mathcal M^{\kappa^{-1}}(\mathcal J_{k^{-1}}y_{k - 1}, y_{k - 1}), \mathcal J_{\kappa^{-1}}y_{k - 1}$. 

    \begin{lemma}[Lin's inexact proximal inequality]\label{lemma:lin-ixct-prox-ineq}
        Let $F$ be a $\mu\ge 0$ strongly convex and fix $\kappa$. 
        If $x_k$ is an inexact proximal point evaluation of $x_k \approx \mathcal J_{\kappa^{-1}} y_{k - 1}$ such that there exists $\epsilon_k$ where $\mathcal M^{\kappa^{-1}}(x_k; y_{k - 1}) - \mathcal M^{\kappa^{-1}}(\mathcal J_{\kappa^{-1}} y_{k - 1}, y_{k - 1}) \le \epsilon_k$. 
        Denote $x_k^* = \mathcal J_{\kappa^{-1}} y_{k - 1}$, then it has for all $x$: 
        \begin{align*}
            F(x) &\ge 
            F(x_k) + \kappa \langle y_{k - 1} - x_k, x - x_k\rangle
            + \frac{\mu}{2}\Vert x - x_k\Vert^2 
            + (\kappa + \mu)\langle  x_k - x_k^*, x - x_k\rangle 
            - \epsilon_k.
        \end{align*}
    \end{lemma}
    \begin{remark}
        The lemma plays a key role because it allows Lin to denote his Nesterov's estimating sequence to be for all $k \ge0$: 
        \begin{align*}
            \phi_0(x) &:= F(x_0) + \frac{\gamma_0}{2}\Vert x - v_0\Vert^2, 
            \\
            \phi_k(x) &:= 
            (1 - \alpha_{k - 1})\phi_{k - 1}(x)
            + \alpha_{k - 1}\left(
                F(x_k) + \kappa\langle y_{k - 1} - x_k, x - x_k \rangle
                + \frac{\mu}{2}\Vert x - x_k\Vert^2
            \right). 
        \end{align*}
        It is void of intractable quantities. 
    \end{remark}
    

\section{Non-convex extension of Catalyst acceleration}\label{sec:4wd-catalyst}
    The non-convex extension of Catalyst acceleration by Lin 2018 \cite{lin_catalyst_2018} is similar to the convex case in his 2015 paper \cite{lin_universal_2015}. 
    The new algorithm handles function with unknown weak convexity constant $\rho$ using a process called Auto Adapt subroutine. 
    They only claimed convergence to stationarity for a weakly convex objective. 
    \par 
    Fix $\kappa$ we use the following notations: 
    \begin{align*}
       & \mathcal M (x; y) := F(x) + \frac{\kappa}{2}\Vert x - y\Vert^2 
       \\
       & \mathcal J y := \argmin_x \mathcal M (x; y). 
    \end{align*}
    We define the algorithm and then its convergence claim below. 
    \begin{definition}[Basic 4WD Catalyst Algorithm]\label{def:basic-4wd-catalyst}
        Find any $x_0 \in \text{dom}(F)$. 
        Initialize the algorithm with $\alpha_1 = 1, v_0 = x_0$. 
        For $k \ge 1$, the iterates $(x_k, y_k, v_k)$ are generated by the procedures: 
        \vspace{-0.5em}
        \begin{align*}
            &
            \textcolor{blue}{
                \text{find } \bar x_k \approx 
            }
            \argmin_{x}\left\lbrace
                    \mathcal M(x; \textcolor{violet}{x_{k - 1}})
                \right\rbrace
            \\ &\quad 
            \text{such that: }
                \text{dist}(\mathbf 0, \partial \mathcal M(\bar x_k; \textcolor{violet}{x_{k - 1}})) 
                \le 
                \kappa\Vert \textcolor{blue}{\bar x_k}- \textcolor{violet}{x_{k - 1}}\Vert, 
                \mathcal M(\textcolor{blue}{\bar x_k}; \textcolor{violet}{x_{k - 1}}) 
                \le F(\textcolor{violet}{x_{k - 1}}); 
            \\
            & y_k = \alpha_k v_{k - 1} + (1 - \alpha_k) \textcolor{violet}{x_{k - 1}};
            \\
            & 
            \textcolor{red}{\text{find }\tilde x_k \approx}
            \argmin_{x} \left\lbrace
                \mathcal M(x; y_k) 
            \right\rbrace
            \text{ such that: }
                \dist\left(
                    \mathbf 0, \partial \mathcal M(\textcolor{red}{\tilde x_k}; y_k)
                \right) 
                \le \frac{\kappa}{k + 1}\Vert \textcolor{red}{\tilde x_k} - y_k\Vert
            ;
            \\
            & v_{k} = \textcolor{violet}{x_{k - 1}} + \frac{1}{\alpha_k}(
                    \textcolor{red}{\tilde x_k} - \textcolor{violet}{x_{k - 1}}
                );
            \\
            & 
            \text{find } \alpha_{k + 1} \in (0, 1): 
            \frac{1 - \alpha_{k + 1}}{\alpha_{k + 1}^2} = \frac{1}{\alpha_k^2};
            \\
            & \text{choose } \textcolor{violet}{x_k} \text{ such that:  } 
            f(\textcolor{violet}{x_k}) = \min(f(\textcolor{blue}{\bar x_k}), f(\textcolor{red}{\tilde x_k})). 
        \end{align*}
    \end{definition}
    
    \begin{theorem}[Basic 4WD Catalyst Convergence]\label{thm:basic-4wd-catalyst}
        Let $(x_k, v_k, y_k)$ be generated by the basic Catalyst algorithm. 
        If $F$ is $\kappa$ weakly convex and bounded below, then $x_k$ converges to stationary where
        \begin{align*}
            \min_{j = 1, \cdots, N} \dist^2(\mathbf 0, \partial F(\bar x_j))
            \le \frac{8 \kappa}{N}(F(x_0) - F^*). 
        \end{align*}
        And when $F$ is convex, $F(x_k) - F^*$ converges at a rate of $\mathcal O(k^{-2})$. 
    \end{theorem}
    \begin{remark}
        Convergence to stationary is strictly weaker than convergence to any stationary point of $F$. 
        Read 
        \hyperref[app:sec:ncnvx-catalyst]{Section \ref*{app:sec:ncnvx-catalyst}} for more information on proof of this claim. 
    \end{remark}



\bibliographystyle{siam}
\bibliography{references/refs.bib}
\newpage

\appendix
\section{Theorems and claims for accelerated proximal gradient}\label{app:sec:thm-claim-acc-prox-grad}
    Throughout this section, $F = g + f$ is an additive composite objective function with $g$ convex, $f$ $L$-lipschitz smooth and $\mu \ge0$ strongly convex. 
    The notations here are 
    \begin{align*}
        \mathcal M^{L^{-1}}(x; y),
        &:= 
        F(x) + \frac{L}{2}\Vert x - y\Vert^2,
        \\
        \widetilde{ \mathcal M}^{L^{-1}}(x; y) ,
        &:= g(x) + f(y) 
        + 
        \left\langle \nabla f(x), x - y\right\rangle 
        + 
        \frac{L}{2}\Vert x - y\Vert^2,
        \\
        \widetilde{\mathcal J}_{L^{-1}}y 
        &:= \argmin_{x} \widetilde{\mathcal M}^{L^{-1}}(x; y),
        \\
        \widetilde{\mathcal G}_{L^{-1}}(y)
        &:= L\left(I - \widetilde{\mathcal J}_{L^{-1}}\right)y. 
    \end{align*}

    \begin{theorem}[Fundamental theorem of proximal gradient]
    \label{app:thm:fun-thm-prox-grad}
        Let $F = f + g$, define the proximal gradient operator $\widetilde{\mathcal J}_{L^{-1}}$.  
        For any fixed $y$, we have for all $x \in \RR^n$: 
        \begin{align*}
            F(x) - F(Ty) - 
            \left\langle 
                L(y - \widetilde{\mathcal J}_{L^{-1}} y),
                x - \widetilde {\mathcal J}_{L^{-1}}y
            \right\rangle
            &\ge  D_f(x, y) . 
        \end{align*}  
    \end{theorem}
    \begin{proof}
        By a direct observation: 
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) 
            &= 
            g(x) + f(y) + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            g(x) + f(x) - f(x) + f(y) 
            + \langle \nabla f(y), x - y\rangle + \frac{L}{2}\Vert x - y\Vert^2
            \\
            &= 
            F(x) - D_f(x, y) + \frac{L}{2}\Vert x - y\Vert^2 
            \\
            &= \mathcal M^{L^{-1}}(x; y) - D_f(x, y). 
        \end{align*}
        Next, since $\widetilde{\mathcal M}^{L^{-1}}(\cdot, y)$ is $L$-strongly convex, it has quadratic growth conditions on its minimizer $y^+$ where $y^+ := \widetilde{\mathcal J}_{L^{-1}}y$ so it implies:
        {\small
        \begin{align*}
            \widetilde{\mathcal M}^{L^{-1}}(x; y) - 
            \widetilde{\mathcal M}^{L^{-1}}(y^+; y)
            - 
            \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 
            0
            \\
            \iff
            \left(
                \mathcal M^{L^{-1}}(x; y) - D_f(x, y)
            \right) - 
            \mathcal M^{L^{-1}}(y^+; y) 
            - 
            \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                \mathcal M^{L^{-1}}(x; y)
                - 
                \mathcal M^{L^{-1}}(y^+; y)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(y^+) 
                + 
                \frac{L}{2}\Vert x - y\Vert^2 - 
                \frac{L}{2}\Vert y^+ - y\Vert^2
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(y^+) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - y^+ + y^+ - y\Vert^2
                    - 
                    \Vert y - y^+\Vert^2
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            \left(
                F(x) - F(y^+) 
                + 
                \frac{L}{2}
                \left(
                    \Vert x - y^+\Vert^2 + 
                    2\langle x - y^+, y^+ - y\rangle
                \right)
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff
            \left(
                F(x) - F(y^+) + \frac{L}{2}\Vert x - y^+\Vert^2 
                - L\langle  x - y^+, y - y^+\rangle
            \right)
            - 
            D_f(x, y) 
            - \frac{L}{2}\Vert x - y^+\Vert^2
            &\ge 0
            \\
            \iff 
            F(x) - F(y^+)
            - \langle L(y - y^+), x - y^+\rangle
            - D_f(x, y) 
            &\ge 0. 
        \end{align*}
        }
    \end{proof}
    \begin{remark}
        The quadratic growth with respect to minimizer of the Moreau Envelope is used to derive the inequality, please take caution that this condition is strictly weaker than strong convexity of the Moreau Envelope, which could be made weaker than the strong convexity of $F$. 
        Compare the same theorem in older literatures, this proof doesn't use the subgradient inequality, making it appealing for generalizations outside convexity. 
    \end{remark}
    
    \begin{theorem}[Cannonical form of proximal gradient estimating sequence]\label{thm:canon-prox-grad-est-seq}
        \; \\
        Denote $\phi_k:\RR^n \rightarrow\RR$ as a sequence of functions such that for all $k\ge 0$ it recursively satisfies the following conditions 
        \begin{align*}
            & g_k := L(y_k - \widetilde{\mathcal J}_{L^{-1}} y_k),
            \\
            & l_F(x; y_k) := 
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) 
                + \langle g_k, x - y_k\rangle 
                + \frac{1}{2L}\Vert g_k\Vert^2, 
            \\
            & \alpha_k \in (0, 1),
            \\
            & 
            \phi_{k + 1}(x)
            := (1 - \alpha_k)\phi_k (x) + 
            \alpha_k (l_F(x; y_k) + \mu/2\Vert x - y_k\Vert^2), 
        \end{align*}
        where $(y_k)_{k\ge 0}$ any sequence. 
        If we define the canonical form for $\phi_k$ as convex quadratic parameterized by positive sequence $(\gamma_k), \phi_k^*$:  
        \begin{align*}
            \phi_k(x) &:= \phi_k^* + \frac{\gamma_k}{2}\Vert x - v_k\Vert^2, 
        \end{align*}
        where $\phi_k^* := \min_{x} \phi_k(x)$. 
        Then the auxiliary sequence $y_k, v_k$, parameters for the canonical form of estimating sequence must satisfy for all $k\ge 0$: 
        {\small
        \begin{align*}      
            \gamma_{k + 1} &= (1 - \alpha_k) \gamma_k + \mu \alpha_k,
            \\
            v_{k + 1} &= \gamma_{k + 1}^{-1}
            (\gamma_k(1 - \alpha_k)v_k - \alpha_k g_k + \mu \alpha_k y_k),
            \\
            \phi_{k + 1}^* &= 
            (1 - \alpha_k)\phi_k^*
            + \alpha_k\left(
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) 
                + \frac{1}{2L}\Vert g_k\Vert^2 
            \right) 
            \\
                &\quad 
                - \frac{\alpha_k^2}{2 \gamma_{k + 1}} \Vert g_k\Vert^2 
                + 
                \frac{\alpha_k(1 - \alpha_k)\gamma_k}{\gamma_{k + 1}} 
                \left(
                    \frac{\mu}{2}\Vert v_k - y_k\Vert^2 
                    + \langle v_k - y_k , g_k\rangle
                \right). 
        \end{align*}
        }
    \end{theorem}
    
    \begin{proof}
        By the recursive definition of $\phi_k$: 
        \begin{align*}
            \phi_{k + 1}(x) 
            &= 
            (1 - \alpha_k)\phi_k(x) + \alpha_k (l_F(x; y_k) + \mu/2\Vert x - y_k\Vert^2)
            \\
            &= 
            (1 -\alpha_k)
            \left(
                \phi_k^* + \gamma_k/2\Vert x - v_k\Vert^2
            \right) 
            + 
            \alpha_k
            \left(
                l_F(x; y_k) + \mu/2\Vert x - y_k\Vert^2
            \right)
            \rightarrow \textcolor{red}{(\text{eqn1})}; 
            \\
            \nabla \phi_{k + 1}(x) 
            &= 
            (1 - \alpha_k)\gamma_k(x - v_k) + \alpha_k(g_k + \mu (x - y_k));
            \\
            \nabla^2 \phi_{k + 1}(x) &= 
            \underbrace{((1 - \alpha_k)\gamma_k + \alpha_k \mu)}_{=\gamma_{k + 1}}I. 
        \end{align*}
        The first recurrence for is discovered as $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$. 
        Because $v_{k + 1}$ is the minimizer of $\phi_{k+ 1}$ by definition of the canonical form, solving for $x$ in $\nabla \phi_{k + 1}(x) = \mathbf 0$ yields $v_{k + 1}$ by: 
        \begin{align*}
            \mathbf 0 &= 
            \gamma_k(1 - \alpha_k)(x - v_k) + \alpha_k g_k + \mu \alpha_k(x - y_k)
            \\
            &= (\gamma_k(1 - \alpha_k) + \mu \alpha_k)x - 
            \gamma_k(1 - \alpha_k)v_k + \alpha_k g_k - \mu \alpha_k y_k
            \\
            \iff 
            v_{k + 1} := x&=
            \gamma_{k +1}^{-1} 
            \left(
                \gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k
            \right). 
        \end{align*}
        From the second and third equality we used $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$. 
        Substituting the canonical form of $\phi_{k + 1}$ back to \textcolor{red}{(eqn1)}, choose $x = y_k$, it gives: 
        \begin{align*}
            \phi_{k + 1}^* 
            &= (1 - \alpha_k)\phi_k^* + \frac{(1 - \alpha_k)\gamma_k}{2}\Vert y_k - v_k\Vert^2   
            \\
            & \quad 
            - \frac{\gamma_{k + 1}}{2}\Vert y_k - v_{k + 1}\Vert^2 
            + 
            \alpha_k\left(
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) 
                + 
                \frac{1}{2L} \Vert g_k\Vert^2
            \right)
            \rightarrow 
            \textcolor{red}{(\text{eqn2})}. 
        \end{align*}
        Next, we simplify $\Vert v_{k + 1} - y_k\Vert^2$ to get rid of $v_{k + 1}$. 
        \begin{align*}
            v_{k + 1} - y_k 
            &= 
            \gamma_{k + 1}^{-1}
            \left(
                \gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k
            \right) - y_k
            \\
            &= 
            \gamma_{k + 1}^{-1}
            \left(
                \gamma_k(1 - \alpha_k)v_k - \alpha_k g_k 
                + (-\gamma_{k + 1} + \mu\alpha_k)y_k
            \right)
            \\
            &=
            \gamma_{k + 1}^{-1}
            \left(
                \gamma_k(1 - \alpha_k)v_k - \alpha_k g_k - 
                (1 - \alpha_k)\gamma_ky_k
            \right)
            \\
            &= 
            \gamma_{k + 1}^{-1}(
                \gamma_k(1 - \alpha_k)(v_k - y_k) 
                - \alpha_k g_k
            ).
        \end{align*}
        From the second to third inequality, we used: 
        \begin{align*}
            \gamma_{k + 1} &=    
                (1 - \alpha_k)\gamma_k + \mu \alpha_k
                \\
                \iff 
                -(1 - \alpha_k)\gamma_k
                &= - \gamma_{k + 1} + \mu \alpha_k. 
        \end{align*}
        Therefore, it has: 
        {\small
        \begin{align*}
            \Vert v_{k + 1} - y_k\Vert^2 
            &= 
            \Vert 
                \gamma_{k + 1}^{-1}(
                    \gamma_k(1 - \alpha_k)(v_k - y_k) 
                    - \alpha_k g_k
                )
            \Vert^2
            \\
            \frac{- \gamma_{k + 1}}{2}
            \Vert v_{k + 1} - y_k\Vert^2
            &= 
            - \frac{1}{2\gamma_{k + 1}}
            \Vert 
                \gamma_k(1 - \alpha_k)(v_k - y_k) - \alpha_k g_k
            \Vert^2
            \\
            &= 
            -\frac{\gamma_k^2 (1 - \alpha_k)^2}{2 \gamma_{k + 1}} 
            \Vert v_k - y_k\Vert^2 - 
            \frac{\alpha_k^2}{2 \gamma_{k + 1}} \Vert g_k\Vert^2
            + 
            \gamma_k(1 - \alpha_k)\gamma_{k + 1}^{-1} \langle v_k - y_k, \alpha_k g_k\rangle. 
        \end{align*}
        }
        Substitute it back to \textcolor{red}{(eqn2)} we have 
        \begin{align*}
            \phi_{k + 1}^* &= 
            (1 - \alpha)\phi_k^* + 
            \alpha_k
            \left(
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) + 
                \frac{1}{2L}\Vert g_k\Vert^2
            \right)
            \\
            &\quad 
            + \frac{(1 - \alpha_k)\gamma_k}{2}\Vert y_k - v_k\Vert^2
            - \frac{\gamma_k^2(1 - \alpha_k)^2}{2\gamma_{k + 1}}\Vert v_k - y_k\Vert^2
            - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
            \\
            &\quad 
            + \alpha_k\gamma_k(1 - \alpha_k)\gamma_{k + 1}^{-1}\langle v_k -y_k, g_k\rangle
            \\
            &= 
                (1 - \alpha)\phi_k^* + 
                \alpha_k
                \left(
                    F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) + 
                    \frac{1}{2L}\Vert g_k\Vert^2
                \right)
                \\
                &\quad 
                + 
                \frac{(1 - \alpha_k)\gamma_k}{2}
                \left(
                    \frac{\mu \alpha_k}{\gamma_{k + 1}}
                \right)
                \Vert v_k - y_k\Vert^2
                - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                \\
                & \quad 
                + \alpha_k\gamma_k(1 - \alpha_k)\gamma_{k + 1}^{-1}\langle v_k -y_k, g_k\rangle
            \\
            &= 
                (1 - \alpha)\phi_k^* 
                + 
                \alpha_k
                \left(
                    F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) + 
                    \frac{1}{2L}\Vert g_k\Vert^2
                \right)
                \\
                &\quad 
                - 
                \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                + 
                \frac{(1 - \alpha_k)\gamma_k\alpha_k}{\gamma_{k + 1}}
                \left(
                    \frac{\mu}{2}\Vert v_k - y_k\Vert^2
                    + \langle v_k - y_k, g_k\rangle
                \right). 
        \end{align*}
        From the second to third equality we added and then transformed the coefficient of $\Vert y_k - v_k\Vert$ using: 
        \begin{align*}
            \frac{(1 - \alpha_k)\gamma_k}{2}
            - 
            \frac{\gamma_k^2(1 - \alpha_k)^2}{2\gamma_{k + 1}}   
            &= 
            \frac{(1 - \alpha_k)\gamma_k}{2}
            \left(
                1 - \frac{\gamma_k (1 - \alpha_k)}{\gamma_{k + 1}}
            \right)
            \\
            &= 
            \frac{(1 - \alpha_k)\gamma_k}{2}
            \left(
                \frac{\gamma_{k + 1} - \gamma_k(1 - \alpha_k)}{\gamma_{k + 1}}
            \right)
            \\
            &= 
            \frac{(1 - \alpha_k)\gamma_k}{2}
            \left(
                \frac{\mu \alpha_k}{\gamma_{k + 1}}
            \right). 
        \end{align*}
        From the second to the third line, $\mu \alpha_k$ is brought in by the equation $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu \alpha_k$. 

    \end{proof}
    \begin{remark}
        The goal of having $\phi_k^*$ represented by iterates $v_k, y_k, g_k$ is because it assists in the future when an inductive hypothesis $\phi_k^* \ge F(x_k) \ge l_F(x_k; y_k) + \frac{\mu}{2}\Vert x - y_k\Vert^2$ is made. 
        We did things in advance here and grouped together a set of transforms by equations, nothing more. 
    \end{remark}
    
    \begin{theorem}[Verifying the conditions of implicit descent]\;\\
        Let estimating sequence $\phi_k$ and auxiliary sequence $y_k, v_k,\gamma_k, \alpha_k$  be given by 
        \hyperref[thm:canon-prox-grad-est-seq]{Theorem \ref*{thm:canon-prox-grad-est-seq}}. 
        If for all $k\ge 0$ they verify: 
        \begin{align*}
            \frac{1}{2L} - \frac{\alpha_k^2}{2 \gamma_{k + 1}} &\ge 0, 
            \\
            \frac{\alpha_k \gamma_k }{\gamma_{k + 1}} 
            (v_k - y_k) + (\widetilde {\mathcal J}_{L^{-1}} y_k - y_k) &= \mathbf 0, 
        \end{align*}
        then $\phi_{k}$ is an estimating sequence that verifies $\forall x \in \RR^n, k \ge 0$: 
        \begin{align*}
            F\left(
                \widetilde{\mathcal J}_{L^{-1}} y_{k - 1}
            \right) 
            &\le \phi_k^*
            \\
            \phi_{k + 1}(x) - \phi_k(x) &\le -\alpha(\phi_k(x) - F(x)). 
        \end{align*}
    \end{theorem}
    \begin{proof}
        Inductively assume that $x_k = \widetilde{\mathcal J}_{L^{-1}}y_{k - 1}$ so $F(x_k) \le \phi_k^*$. 
        Substituting the $x_k$ into the equation for $\phi_{k + 1}$: 
        {\small
        \begin{align}
            \begin{split}
                \phi_{k + 1}^* &= 
                (1 - \alpha_k) \phi_k^*
                + 
                \alpha_k
                \left(
                    F(x_k) + \frac{1}{2L}\Vert g_k\Vert^2
                \right)
                \\&\quad 
                    - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                    + \frac{\alpha_k (1 - \alpha_k)\gamma_k}{\gamma_{k + 1}}
                    \left(
                        \frac{\mu}{2}\Vert v_k - y_k\Vert^2 + \langle v_k - y_k, g_k\rangle
                    \right)
                \\
                \implies 
                &\ge 
                (1 - \alpha_k)F(x_k)
                + 
                \alpha_k
                \left(
                    F(x_k) + \frac{1}{2L}\Vert g_k\Vert^2
                \right)
                \\&\quad
                    - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                    + \frac{\alpha_k (1 - \alpha_k)\gamma_k}{\gamma_{k + 1}}
                    \left(
                        \frac{\mu}{2}\Vert v_k - y_k\Vert^2 + \langle v_k - y_k, g_k\rangle
                    \right)
                \\
                \implies
                &\ge 
                (1 - \alpha_k)F(x_k)
                + 
                \alpha_k
                \left(
                    F(x_k) + \frac{1}{2L}\Vert g_k\Vert^2
                \right)
                - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
                + 
                \frac{\alpha_k (1 - \alpha_k)\gamma_k}{\gamma_{k + 1}}
                \langle v_k - y_k, g_k\rangle. 
            \end{split}
            \label{app:acc-prox-grad-ineq1}
        \end{align}
        }
        The first inequality comes from the inductive hypothesis. 
        The second inequality comes from the non-negativity of the term $\frac{\mu}{2}\Vert v_k - y_k\Vert^2$. 
        Now, recall from the fundamental proximal gradient inequality in the convex setting; define $x_{k + 1} := \widetilde{\mathcal J}_{L^{-1}}y_k$, we have $\forall z\in \RR^n$: 
        \begin{align*}
            F(z) &\ge 
            F\left(\widetilde{\mathcal J}_{L^{-1}}y_k\right)
            + \left\langle 
                L(y - \widetilde{\mathcal J}_{L^{-1}}y_k), 
                z - \widetilde{\mathcal J}_{L^{-1}}y_k
            \right\rangle + D_f(z, y)
            \\
            &\ge 
            F(x_{k + 1}) + \langle g_k, z - x_k\rangle + \frac{\mu}{2}\Vert z - y\Vert^2
            \\
            &=
            F(x_{k + 1}) + \langle g_k, z - y + y - x_k\rangle + \frac{\mu}{2}\Vert z - y\Vert^2
            \\
            &\ge
            F(x_{k + 1}) + \langle g_k, z - y\rangle 
            + \frac{1}{2L}\Vert g_k\Vert^2. 
        \end{align*}
        Next, set $z = x_{k}$ and substitute it back to RHS of $\phi_{k + 1}^*$ in 
        \hyperref[app:acc-prox-grad-ineq1]{Inequality \ref*{app:acc-prox-grad-ineq1}}: 
        \begin{align*}
            \phi_{k + 1}^*
            &\ge 
            (1 - \alpha_k)
            \left(
                F(x_{k + 1}) + \langle g_k, x_k - y_k\rangle + \frac{1}{2L}\Vert g_k\Vert^2
            \right)
            \\
            &\quad 
            + 
            \alpha_k
            \left(
                F(x_{k + 1}) + \frac{1}{2L}\Vert g_k\Vert^2
            \right)
            - \frac{\alpha_k^2}{2\gamma_{k + 1}}\Vert g_k\Vert^2
            + 
            \frac{\alpha_k (1 - \alpha_k)\gamma_k}{\gamma_{k + 1}}
            \langle v_k - y_k, g_k\rangle
            \\
            &\ge 
            F(x_{k + 1})
            + 
            \left(
                \frac{1}{2L} - \frac{\alpha_k^2}{2\gamma_{k + 1}}
            \right)\Vert g_k\Vert^2
            + 
            (1 - \alpha_k)
            \left\langle 
                g_k, \frac{\alpha_k\gamma_k}{\gamma_{k + 1}}(v_k - y_k) + (x_k - y_k)
            \right\rangle. 
        \end{align*}
        To assert $\phi_{k + 1}^* \ge F(x_{k + 1})$, one set of sufficient conditions are 
        \begin{align*}
            \left(
                \frac{1}{2L} - \frac{\alpha_k^2}{2\gamma_{k + 1}}
            \right) &\ge 0, 
            \\
            \frac{\alpha_k\gamma_k}{\gamma_{k + 1}}(v_k - y_k) + (x_k - y_k) 
            &= \mathbf 0. 
        \end{align*}
        Before finishing, re-arranging should give use the equivalent representations 
        \begin{align*}
            -(\alpha_k \gamma_k\alpha_{k + 1}^{-1} + 1)y_k
            &= 
            - \alpha_k \gamma_k \gamma_{k + 1}^{-1}v_k - x_k
            \\
            y_k &= 
            \frac{
                \alpha_k \gamma_k \gamma_{k + 1}^{-1}v_k + x_k
            }{1 + \alpha_k \gamma_k \gamma_{k + 1}^{-1}}
            \\
            &=  
            \frac{\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k}{\gamma_k + \alpha_k \mu}. 
        \end{align*}
        On the second to third equality, we multiplied the numerator and denominator by $\gamma_{k + 1}$ and then simplified the numerator using equation $\gamma_{k + 1} + \alpha_k \gamma_k = \gamma_k + \alpha_k \mu$. 
        For $\alpha_k, \gamma_k$, we have the equivalent representation of 
        \begin{align*}
            1 - \frac{L \alpha_k^2}{\gamma_{k + 1}}
            &\ge 0
            \\
            \iff 1 &\ge L \alpha_k^2 / \gamma_{k + 1}
            \\
            \iff 
            \gamma_{k + 1} &\ge L \alpha_k^2
            \\
            \iff 
            L\alpha_k^2 
            &\le
            \gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu \alpha_k. 
        \end{align*}
    \end{proof}
    
    \begin{definition}[Nesterov's accelerated proximal gradient raw form]
    \label{app:def:acc-prox-grad-raw-form}
        The accelerated proximal gradient algorithm generates vector iterates $x_k, y_k, v_k$ using auxiliary sequence $\alpha_k, \gamma_k$ such that for all $k\ge0$ they satisfy conditions: 
        \begin{align*}
            L\alpha_k^2 
            &\le 
            (1 - \alpha_k)\gamma_k + \alpha_k\mu = \gamma_{k + 1}; \alpha_k \in (0, 1), 
            \\
            y_k &= (\gamma_k + \alpha_k \mu)^{-1}
            (\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k),
            \\
            x_{k + 1}&= 
            \widetilde{\mathcal J}_{L^{-1}} y_k
            \\
            v_{k + 1} &= 
            \gamma_{k + 1}^{-1}
            \left(
                (1 - \alpha_k)\gamma_k v_k + \alpha_k \mu y_k - \alpha_k g_k
            \right). 
        \end{align*}
    \end{definition}
    \begin{theorem}[Intermediate form of accelerated proximal gradient]\label{app:theorem:acc-prox-grad-intermediate}\; \\
        Let iterates $(x_k, y_k, v_k)$ be given by the raw form of Nesterov's accelerated proximal gradient which is 
        \hyperref[app:def:acc-prox-grad-raw-form]{Definition \ref*{app:def:acc-prox-grad-raw-form}}. 
        Assume for all $k \ge 0$ it has $L\alpha_k^2 = \gamma_{k + 1}$, then Definition \ref*{app:def:acc-prox-grad-raw-form} is algebraically equivalent to the following form which doesn't have $\gamma_k$: 
        \begin{align*}
            y_k &= 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            x_{k + 1} &= 
            y_k - L^{-1}  g_k
            \\
            v_{k + 1} &= 
            \left(
                1 + \frac{\mu}{L \alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
            \right) - \frac{1}{L\alpha_{k}} g_k
            \\
            0 &= \alpha_k^2 - \left(\mu/L - \alpha_{k -1}^2\right) \alpha_k - \alpha_{k - 1}^2. 
        \end{align*}
        Here we have $g_k = \widetilde{\mathcal G}_{L^{-1}}y_k$. 
    \end{theorem}
    \begin{proof}
        From definition,  we have equality: $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \alpha_k \mu$, so $\gamma_{k + 1} + \alpha_k \gamma_k = \gamma_k + \alpha_k \mu$, with that in mind we can simplify the expression for $y_k$ by 
        \begin{align*}
            y_{k} &= 
            (\gamma_k + \alpha_k \mu)^{-1}
            (\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k)
            \\
            &= 
            (\gamma_{k + 1} + \alpha_k \gamma_k)^{-1}
            (\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k)
            \\
            &= 
            \left(
                \frac{\gamma_{k + 1}}{\alpha_k\gamma_k} + 1
            \right)^{-1}
            \left(
                v_k + \frac{\gamma_{k + 1}}{\alpha_k \gamma_k} x_k
            \right)
            \\
            &= 
            \left(
                1 + \frac{L\alpha_k^2}{\alpha_kL\alpha_{k - 1}^2} 
            \right)^{-1}
            \left(
                v_k + \frac{L\alpha_k^2}{\alpha_k L\alpha_{k - 1}^2} x_k
            \right)
            \\
            &= 
            \left(
                1 + \frac{\alpha_k}{\alpha_{k - 1}^2}
            \right)^{-1}
            \left(
                v_k + 
                \frac{\alpha_k}{\alpha_{k - 1}^2} x_k
            \right). 
        \end{align*}
        For $v_{k + 1}$ we use $\gamma_{k + 1} = (1 - \alpha_k)\gamma_k + \mu \alpha_k$ which gives us: 
        \begin{align*}
            v_{k + 1} &= 
            \gamma_{k + 1}^{-1}
            ((1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k)
            - \alpha_k\gamma_{k + 1}^{-1}g_k
            \\
            &= 
            ((1 - \alpha_k)\gamma_k + \alpha_k \mu)^{-1}
            \left(
                (1 - \alpha_k)\gamma_k v_k + \mu\alpha_k y_k
            \right)
            - \alpha_k\gamma_{k + 1}^{-1} g_k
            \\
            &= 
            \left(
                1 + \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k}
            \right)^{-1}
            \left(
                v_k + 
                \frac{\alpha_k\mu}{(1 - \alpha_k)\gamma_k} y_k
            \right)
            - \alpha_k\gamma_{k + 1}^{-1} g_k
            \\
            &= 
            \left(
                1 + \frac{\alpha_k\mu}{(1 -\alpha_k)L\alpha_{k - 1}^2}
            \right)^{-1}
            \left(
                v_k + 
                \frac{\alpha_k\mu}{(1 - \alpha_k)L\alpha_{k - 1}^2} y_k
            \right)
            - \frac{1}{L\alpha_{k}} g_k
        \end{align*}
        We can eliminate the $\gamma_k$ which defines the $\alpha_k$ by considering 
        \begin{align*}
            L\alpha_k^2 &= 
            (1 - \alpha_k)\gamma_k + \alpha_k \mu 
            \\
            &= 
            (1 - \alpha_k)L\alpha_{k - 1}^2 
            + \alpha_k \mu
            \\
            L\alpha_k^2 &= 
            L \alpha_{k - 1}^2 + 
            (\mu - L \alpha_{k - 1}^2)\alpha_k
            \\
            \iff     
            0
            &=  
            L \alpha_k^2 - (\mu - L \alpha_{k - 1}^2)\alpha_k 
            - L \alpha_{k -1}^2. 
        \end{align*}
        Next, we simplify the coefficients using the above relations further. 
        From the above results we have the relation $(1 - \alpha_k)L\alpha_{k - 1}^2 = L \alpha_k^2 - \alpha_k \mu$. 
        Therefore,  it gives 
        \begin{align*}
            \frac{\alpha_k\mu}{(1 - \alpha_k)L \alpha_{k - 1}^2}
            &= 
            \frac{\alpha_k\mu}{L \alpha_k^2 - \alpha_k \mu}
            = \frac{\mu}{L \alpha_k - \mu}. 
        \end{align*}
        Next we have: 
        \begin{align*}
            L\alpha_k^2 &= 
            (1 - \alpha_k)L\alpha_{k - 1}^2 + \alpha_k \mu 
            \\
            L \alpha_k^2 - \alpha_k\mu &= 
            (1 - \alpha_k)L \alpha_{k - 1}^2
            \\
            \alpha_{k - 1}^2
            &= 
            \frac{L \alpha_k^2 - \alpha_k\mu}{L (1 - \alpha_k)}
            \\
            \frac{1}{\alpha_{k - 1}^2}
            &= 
            \frac{L (1 - \alpha_k)}{L \alpha_k^2 - \alpha_k\mu}
            \\
            \frac{\alpha_k}{\alpha_{k - 1}^2}
            &= 
            \frac{L - L\alpha_k}{L\alpha_k - \mu}. 
        \end{align*}
        Substitute these results back to the expression for $y_k, v_{k + 1}$, it gives what we want. 

    \end{proof}
    \begin{remark}
        This intermediate form representation of the algorithm eliminated the sequence $(\gamma_k)_{k \ge0}$ which were used for the Nesterov's estimating sequence. 
    \end{remark}

    \begin{theorem}[Nesterov's accelerated proximal gradient momentum form]
        \; \\
        Let the sequence $\alpha_k$, and vectors $y_k, x_k, v_k$ be given by the intermediate form of the Nesterov's accelerated proximal gradient, then it can be simplified to void of $v_k$. 
        The algorithm generates $y_k, x_k, \alpha_k$ such that it satisfies for all $k \ge 0$: 
        \begin{align*}
            & \text{find } \alpha_{k + 1} \text{ such that: }L \alpha_{k + 1}^2 = (1 - \alpha_{k + 1})L \alpha_{k - 1} + \mu \alpha_{k + 1}, 
            \\
            & x_{k + 1} = \widetilde {\mathcal J}_{L^{-1}} y_k, 
            \\
            & y_{k + 1} = \frac{\alpha_k(1 - \alpha_k)}{\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
        \end{align*}
        Initially we choose $x_0 = y_0, \alpha_0 \in (0, 1)$. 
    \end{theorem}
    \begin{proof}
        To show that, we write down the intermediate form with new symbols to make it easier to read: 
        \begin{align*}
            y_k &= (1 + \tau_k)^{-1}(v_k + \tau_k x_k),
            \\
            v_{k + 1} &= (1 + \xi_k)^{-1}(v_k + \xi_k y_k) - (1 + \xi_k)^{-1}\delta_k g_k,
            \\
            x_{k + 1} &= y_k - L^{-1} g_k. 
        \end{align*} 
        Where for all $k \ge0$: 
        \begin{align*}
            \tau_k &= \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}, 
            \xi_k = \frac{\mu}{L \alpha_k - \mu},
            \\
            (1 + \xi_k)^{-1}\delta_k &= \frac{1}{L\alpha_k}
            \iff L \delta_k = \frac{1 + \xi_k}{\alpha_k}, 
            \\
            L\alpha_k^2 &= (1 - \alpha_k)L\alpha_{k - 1}^2 + \mu\alpha_k. 
        \end{align*}
        Next, we show that if $L\delta_k = 1 + \xi_k + \tau_k$ then $v_{k + 1} - x_{k + 1} = (1 + \xi_t)^{-1}\tau_t(x_{k + 1} - x_k)$. 
        \begin{align*}
            v_{k + 1} &= (1 + \xi_k)^{-1}(v_k + \xi_k y_k) - (1 + \xi_k)^{-1}\delta_k g_k
            \\
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k)y_k - \tau_k x_k + \xi_k y_k) - (1 + \xi_k)^{-1}\delta_k g_k
            \\
            &= 
            (1 + \xi_k)^{-1}((1 + \tau_k + \xi_k)y_k - \tau_k x_k) - (1 + \xi_k)^{-1}\delta_k g_k
            \\
            \iff 
            v_{k + 1} - x_{k + 1} &= 
            (1 + \xi_k)^{-1}((1 + \tau_k + \xi_k)y_k - \tau_k x_k - \delta_k g_k) - y_k + L^{-1}g_k
            \\
            &= (1 + \xi_k)^{-1}(\tau_k y_k - \tau_k x_k - \delta_k g_k) + L^{-1}g_k
            \\
            &= (1 + \xi_k)^{-1}(\tau_k y_k - \tau_k x_k + (L^{-1}(1 + \xi_k) - \delta_k )g_k)
            \\
            &= (1 + \xi_k)^{-1}\tau_k(y_k - x_k + \tau_k^{-1}(L^{-1} + L^{-1}\xi_k - \delta_k )g_k)
        \end{align*}
        Next, consider $x_{k + 1} - x_{k}$: 
        \begin{align*}
            x_{k + 1} - x_k &= y_k - x_k - L^{-1}g_k. 
        \end{align*}
        Observe that, if we substitute $\delta_k = L^{-1}(1 + \xi_k) + L^{-1}\tau_k$, then 
        \begin{align*}
            v_{k + 1} - x_{k + 1} &= (1 + \xi_k)^{-1}\tau_k(y_k - x_k + \tau_k^{-1}(-L^{-1}\tau_k)g_k) 
            \\
            &= 
            (1 + \xi_k)^{-1}\tau_k(y_k - x_k - L^{-1}g_k)
            \\
            &= (1 + \xi_k)^{-1}\tau_k(x_{k + 1} - x_k). 
        \end{align*}
        Next, it remains to verify that $L\delta_k = 1 + \xi_k + \tau_k$ is true. 
        This is true because by definitions the RHS: 
        \begin{align*}
            1 + \tau_k + \xi_k &= 
            1 + \frac{L(1 - \alpha_k)}{L \alpha_k - \mu} 
            + \frac{\mu}{L \alpha_k - \mu}
            \\
            &= 
            1 + \frac{L - L \alpha_k + \mu}{L\alpha_k - \mu}
            \\
            &= 
            \frac{L - L \alpha_k + \mu + L \alpha_k - \mu}{L\alpha_k - \mu}
            \\
            &= \frac{L}{L\alpha_k - \mu}. 
        \end{align*}
        And the LHS: 
        \begin{align*}
            \frac{1 + \xi_k}{\alpha_k}
            &= 
            \frac{1 + \frac{\mu}{L\alpha_k - \mu}}{\alpha_k}
            = 
            \frac{\frac{L\alpha_k - \mu + \mu}{L \alpha_k - \mu}}{\alpha_k}
            = 
            \frac{L}{L\alpha_k - \mu}. 
        \end{align*}
        They are matched. Substitute $\xi_k, \tau_k$ into $v_{k + 1} = x_{k + 1} + (1 + \xi_k)^{-1}\gamma_k(x_{k + 1} - x_k)$: 
        \begin{align*}
            v_{k + 1} &= 
            x_{k + 1} + \left(
                1 + \frac{\mu}{L\alpha_k - \mu}
            \right)^{-1}\left(
                \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
            \right)(x_{k + 1} - x_k)
            \\
            &= 
            x_{k + 1} + \left(
                \frac{L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}\left(
                \frac{L(1 - \alpha_k)}{L\alpha_k - \mu}
            \right)(x_{k + 1} - x_k)
            \\
            &= 
            x_{k + 1} + \left(
                \frac{L\alpha_k - \mu}{L\alpha_k}
            \right)\left(
                \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)(x_{k + 1} - x_k)
            \\
            &= x_{k + 1} + \left(
                \alpha_k^{-1} - 1
            \right)(x_{k + 1} - x_k).
        \end{align*}
        With $v_{k +1}$ rid of $v_k$, the next step is to make $y_{k + 1}$ only using $x_k, x_{k + 1}$. 
        By definition $y_{k + 1}$ is produced by: 
        \begin{align*}
            y_k &= 
            \left(
                1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \left(
            \frac{L - \mu}{L\alpha_k - \mu} 
            \right)^{-1}
            \left(
                v_k + 
                \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
            \right)
            \\
            &= 
            \frac{L\alpha_k - \mu}{L - \mu} v_k
            + 
            \frac{L - L \alpha_k}{L - \mu} x_k. 
        \end{align*}
        Increment $k$ into $k + 1$ then 
        {\footnotesize
        \begin{align*}
            v_{k + 1} &= 
            x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \\
            (L \alpha_{k + 1} - \mu)v_{k + 1} 
            &= 
            (L \alpha_{k + 1} - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k), 
            \\[0.5em]
            y_{k + 1} &= 
            (L - \mu)^{-1}((L\alpha_{k + 1} - \mu)v_{k + 1} + (L - L \alpha_{k + 1})x_{k + 1})
            \\
            &= (L - \mu)^{-1}
            \left(
                (L\alpha_{k + 1} - \mu)x_{k + 1} + 
                (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
                + (L - L \alpha_{k + 1})x_{k + 1}
            \right)
            \\
            &= 
            (L - \mu)^{-1}
            \left(
                (L - \mu)x_{k + 1} + (L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)(x_{k + 1} - x_k)
            \right)
            \\
            &= x_{k + 1} + \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}(x_{k + 1} - x_k). 
        \end{align*}
        }
        We are closer than ever to proving it. 
        This representation contains $L, \mu$ on the momentum coefficients, to get rid of that consider: 
        \begin{align*}
            \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^{-1} - 1)}{L - \mu}
            &= \frac{(L\alpha_{k + 1} - \mu)\alpha_k(1 - \alpha_k)}{\alpha_k^2(L - \mu)}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \left(
                \frac{\alpha_k^2(L - \mu)}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \alpha_k(1 - \alpha_k)
            \left(
                \frac{L\alpha_k^2 - \mu\alpha_k^2}{L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= 
            \alpha_k(1 - \alpha_k)
            \left(
                \frac{(L\alpha_{k + 1} - \mu)(\alpha_k^2 + \alpha_{k + 1})}
                {L\alpha_{k + 1} - \mu}
            \right)^{-1}
            \\
            &= \frac{\alpha_k(1 - \alpha_k)}{\alpha_k^2 + \alpha_{k + 1}}. 
        \end{align*}
        Here, on the third to the 4th equality, we used $L\alpha_{k + 1}^2 = (1 - \alpha_{k + 1})L\alpha_k^2 + \mu\alpha_{k + 1}$ in this way: 
        \begin{align*}
            (L\alpha_{k + 1} - \mu)(\alpha_k^2 + \alpha_{k + 1})
            &= 
            L\alpha_{k + 1}\alpha_k^2 - \mu \alpha_k^2 + L \alpha_{k + 1}^2 + \mu\alpha_{k + 1}
            \\
            &= 
            L\alpha_{k + 1}\alpha_k^2 - \mu \alpha_k^2 + (
                (1 - \alpha_{k + 1})L\alpha_k^2 - \mu\alpha_{k + 1}
            ) - \mu\alpha_{k + 1}
            \\
            &= 
            L\alpha_k^2 - \mu \alpha_k^2. 
        \end{align*}
    \end{proof}
    \begin{remark}
        This proof is very long because we took a detour to an intermediate form without the $\gamma_k$ that the writer personally prefer more than a direct path. 
    \end{remark}
        
    
\section{Proofs for accelerated PPM}
    \subsection{Exact accelerated PPM}\label{sec:app:exct-acc-ppm}
        This section contains all the proofs for the exact accelerated PPM method. 
        It follows the same notation from the accelerated PPM section that $F$ is a convex function and the notations are $\mathcal J_k, \mathcal G_k$ for the proximal point evaluation and gradient mapping. 
        Recall the definition of the estimating sequence is: 
        \begin{align*}
            \phi_0(x) &:= f(x_0) + \frac{A}{2}\Vert x - x_0\Vert^2, 
            \\
            \phi_{k + 1}(x) &:= 
            (1 - \alpha_k)\phi_k(x)
            + 
            \alpha_k(F(\mathcal J_k y_k) + \langle \mathcal G_k y_k, x - \mathcal J_k y_k\rangle). 
        \end{align*}
        Observe $\phi_k$ is a sequence of simple quadratic functions. 
        We define the canonical representation to be: 
        \begin{align*}
            (\forall k \ge 0) \quad 
            \phi_k(x) &= \phi_k^* + \frac{A_k}{2} \Vert x - v_k\Vert^2. 
        \end{align*}
        Substituting the canonical form, we obtained a recursive definition of the Hessian and gradient of the estimating sequence: 
        \begin{align*}
            &
            \phi_{k+ 1}^* + \frac{A_{k + 1}}{2}\Vert x - v_{k + 1}\Vert^2
            = 
            (1 - \alpha_k)
            \left(\phi_k^* + \frac{A_k}{2}\Vert x - v_k\Vert^2\right)
            \\ & \hspace{12em}
                + 
                \alpha_k(F(\mathcal J_k y_k) + \langle \mathcal G_k y_k, x - \mathcal J_k y_k\rangle)
            \\
            \implies 
            &
            \left\lbrace
            \begin{aligned}
                A_{k + 1} 
                &= (1 - \alpha_k)A_k, 
                \\
                \nabla \phi(x)
                &= 
                (1 - \alpha_k)A_k(x - v_k) + \alpha_k \mathcal G_ky_k. 
            \end{aligned}
            \right.
        \end{align*}
        In the canonical form, $v_{k + 1}$ is the minimizer of $\phi_{k + 1}$, it can be solved for by setting the gradient $\phi_{k + 1}(v_{k + 1})= \mathbf 0$ so for all $k\ge 0$: 
        \begin{align*}
            \mathbf 0 &= (1 - \alpha_k)A_k(v_{k + 1} - v_k) + \alpha_k \mathcal G_k y_k
            \\
            \iff 
            v_{k + 1} - v_k &= 
            \frac{\alpha_k}{\lambda_k(1 - \alpha_k)A_k}
            \left(
                y_k - \mathcal J_k y_k
            \right)
            \\
            &= \frac{\alpha_k}{\lambda_kA_{k + 1}}
            \left(
                y_k - \mathcal J_k y_k
            \right). 
        \end{align*}
        \begin{theorem}[Estimating sequence for accelerated PPM]\label{app:thm:est-seq-acc-ppm}
            The parameters for the estimating sequence: $(\phi_k^*)_{k\ge 0}, (v_k)_{k\ge 0}, (A_k)_{k\ge 0}$ satisfies for all $k \ge0 $ the following conditions:
            \begin{align*}
                A_{k + 1} &= (1 - \alpha_k)A_k, 
                \\
                v_{k + 1} - v_k
                &= 
                - \frac{\alpha_k}{A_{k + 1}\lambda_k}(y_k - \mathcal J_k y_k), 
                \\
                \phi_{k + 1}^*
                &\ge 
                F(\mathcal J_ky_k) + 
                \frac{1}{2\lambda_k}\left(
                    2 - \frac{\alpha_k^2}{A_{k + 1}\lambda_k}
                \right)
                \Vert y_k - \mathcal J_k y_k\Vert^2
                \\
                    & \quad 
                    + 
                    \lambda_k^{-1}
                    \langle 
                        y_k - \mathcal J_k y_k, (1 - \alpha_k)x_k + \alpha_k v_k - y_k
                    \rangle.
            \end{align*}
            Additionally, the sequence $\alpha_k$ has $\forall k \ge0$: 
            \begin{align*}
                \alpha_k
                &= 
                \frac{1}{2}\left(
                    \sqrt{(A_k\lambda_k)^2 + 4A_k \lambda_k}
                    - A_k\lambda_k
                \right). 
            \end{align*}
        \end{theorem}
        \begin{proof}   
            Using induction, we assume the inductive hypothesis: $\phi_k^* \ge f(x_k)$ for the sequence $(x_i)_{i \ge 0}$ up to and including $i = k$.
            Proceed with the inductive hypothesis we have 
            \begin{align*}
                \phi_k^* 
                &\ge F(x_k)
                \ge F(\mathcal J_k y_k) + \langle \mathcal G_k y_k, x_k - \mathcal J_k y_k\rangle. 
            \end{align*}
            We used the proximal inequality of $F$. 
            Inductively using the definition of the estimating sequence and substitute the canonical form we will have 
            \begin{align*}
                \phi_{k + 1}^*
                &= \phi_{k + 1}(v_{k + 1})
                \\
                &= 
                (1 - \alpha_k) \phi_k(v_{k + 1})
                + \alpha_kF(\mathcal J_k y_k)
                + \alpha_k\lambda_k^{-1}\langle 
                    y_k - \mathcal J_k y_k, v_{k + 1} - y_{k + 1}
                \rangle
                \\
                &= 
                \left(
                    1 - \alpha_k
                \right)\left(
                    \phi_k^* + \frac{A_k}{2}\Vert v_{k + 1} - v_k\Vert^2
                \right)
                + 
                \alpha_k F(\mathcal J_k y_k) 
                + 
                \alpha_k\lambda_k^{-1}\langle 
                    y_k - \mathcal J_k y_k, v_{k + 1} - y_{k + 1}
                \rangle
                \\
                & \textcolor{gray}{
                    A_{k + 1} = (1 - \alpha_k)A_k
                }
                \\
                &=
                (1 - \alpha_k)\phi_k^*
                + 
                \frac{A_{k + 1}}{2}\Vert v_{k + 1} - v_k\Vert^2
                + \alpha_k F(\mathcal J_k y_k)
                + \alpha_k\lambda_k^{-1}\langle 
                    y_k - \mathcal J_k y_k, v_{k + 1} - y_{k + 1}
                \rangle
            \end{align*}
            Next, we substitute the inequality by the inductive hypothesis which gives: 
            {\small
            \begin{align*}
                \phi_{k + 1}^*
                &\ge 
                (1 - \alpha_k)
                \left(
                    F(\mathcal J_k y_k) + \langle \mathcal G_k y_k, x_k - \mathcal J_k y_k\rangle
                \right)
                \\
                    &\quad 
                    + 
                    \frac{A_{k + 1}}{2}\Vert v_{k + 1} - v_k\Vert^2
                    + \alpha_k F(\mathcal J_k y_k)
                    + 
                    \alpha_k\lambda_k^{-1}\langle 
                        y_k - \mathcal J_k y_k, v_{k + 1} - y_{k + 1}
                    \rangle
                \\
                &= 
                F(\mathcal J_k y_k) + \frac{A_{k + 1}}{2}\Vert v_{k + 1} - v_k\Vert^2
                + 
                \lambda^{-1}_k
                \left\langle 
                    y_k - \mathcal J_k y_k, 
                    (1 - \alpha_k)(x_k - \mathcal J_k y_k)
                    + 
                    \alpha_k(v_{k + 1} - \mathcal J_k y_k)
                \right\rangle
                \\
                &= 
                F(\mathcal J_k y_k) + \frac{A_{k + 1}}{2}\Vert v_{k + 1} - v_k\Vert^2
                + 
                \lambda^{-1}_k
                \left\langle 
                    y_k - \mathcal J_k y_k, 
                    (1 - \alpha_k)x_k + \alpha_k v_{k + 1} -  \mathcal J_k y_k
                \right\rangle. 
            \end{align*}
            }
            This requires further simplifications. 
            Rearranging the elements in the inner product we have: 
            \begin{align*}
                (1 - \alpha_k)x_k + \alpha_k v_{k + 1} - \mathcal J_k y_k
                &= 
                ((1 - \alpha_k)x_k + \alpha_k v_k - y_k)
                + \alpha_k(v_{k + 1} - v_k) + (y_k - \mathcal J_k y_k). 
            \end{align*}
            We also use the equality: 
            \begin{align*}
                v_{k + 1} - v_k &= 
                -\frac{\alpha_k}{A_{k + 1}\lambda_k}(y_k - \mathcal J_k y_k)
                \\
                \implies
                \Vert v_{k + 1} - v_k\Vert^2 
                &= 
                \left\Vert
                    - \frac{\alpha_k}{A_{k + 1}\lambda_k}
                    \left(
                        y_k - \mathcal J_k y_k
                    \right)
                \right\Vert^2
                \\
                \Vert v_{k + 1} - v_k\Vert^2 
                &= 
                \left(
                    \frac{\alpha_k}{A_{k + 1}\lambda_k}
                \right)^2
                \left\Vert
                    y_k - \mathcal J_k y_k
                \right\Vert^2
                \\
                \frac{A_{k + 1}}{2}
                \Vert v_{k + 1} - v_k\Vert^2 
                &= 
                \frac{\alpha_k^2}{2A_{k + 1} \lambda_k^2}
                \left\Vert
                    y_k - \mathcal J_k y_k
                \right\Vert^2. 
            \end{align*}
            Substituting both equality we simplify the inequality into 
            \begin{align*}
                \phi_{k + 1}^* &\ge 
                F(\mathcal J_k y_k)
                + \frac{\alpha_k^2}{2A_{k + 1} \lambda_k^2}
                \Vert y_k - \mathcal J_ky_k\Vert^2
                \\
                    & \quad 
                    + 
                    \lambda_k^{-1}
                    \left\langle 
                        y_k - \mathcal J_k y_k, 
                        (1 - \alpha_k)x_k + \alpha_k v_k - y_k
                    + \alpha_k(v_{k + 1} - v_k) + (y_k - \mathcal J_k y_k)
                    \right\rangle
                \\
                &= 
                F(\mathcal J_k y_k)
                + \frac{\alpha_k^2}{2A_{k + 1} \lambda_k^2}
                \Vert y_k - \mathcal J_ky_k\Vert^2
                \\
                    & \quad 
                    + 
                    \lambda_k^{-1}
                    \left\langle 
                        y_k - \mathcal J_k y_k, 
                        (1 - \alpha_k)x_k + \alpha_k v_k - y_k
                    \right\rangle
                \\
                    &\quad 
                    + 
                    \lambda_k^{-1}
                    \langle
                        y_k - \mathcal J_k y_k,
                        \alpha_k(v_{k + 1} - v_k) + (y_k - \mathcal J_k y_k)
                    \rangle
                \tag{1}
            \end{align*}
            Simplifying the second cross term on the RHS of (1): 
            \begin{align*}
                & \lambda_k^{-1}
                    \langle
                        y_k - \mathcal J_k y_k,
                        \alpha_k(v_{k + 1} - v_k) + (y_k - \mathcal J_k y_k)
                    \rangle
                \\
                &= 
                \lambda_k^{-1}
                    \left\langle
                        y_k - \mathcal J_k y_k,
                        -\frac{\alpha_k^2}{A_{k + 1}\lambda_k}(y_k - \mathcal J_k y_k)
                        + 
                        (y_k - \mathcal J_k y_k)
                    \right\rangle
                \\
                &= 
                \lambda_k^{-1}
                    \left\langle
                        y_k - \mathcal J_k y_k,
                        -\frac{\alpha_k^2}{A_{k + 1}\lambda_k}(y_k - \mathcal J_k y_k)
                        + 
                        (y_k - \mathcal J_k y_k)
                    \right\rangle
                \\
                &=
                \lambda_k^{-1} 
                \left(
                    1 - \frac{\alpha_k^2}{A_{k + 1}\lambda_k}
                \right)
                \Vert y_k - \mathcal J_k y_k\Vert^2. 
            \end{align*}
            The above term repeats with one of the term in (1), merging their coefficient it yields
            \begin{align*}
                & 
                \lambda_k^{-1} 
                \left(
                    1 - \frac{\alpha_k^2}{A_{k + 1}\lambda_k}
                \right) + 
                \frac{\alpha_k^2}{2A_{k + 1} \lambda_k^2}
                \\
                & = 
                \lambda_k^{-1}
                - 
                \frac{\alpha_k^2}{A_{k + 1}\lambda_k^2}
                + 
                \frac{\alpha_k^2}{2A_{k + 1}\lambda_k^2} 
                \\
                &= 
                \frac{1}{2\lambda_k}
                \left(
                    2 - \frac{\alpha_k^2}{2A_{k + 1}\lambda_k}
                \right). 
            \end{align*}
            Substituting back to (1): 
            \begin{align*}
                \phi_{k + 1}^* 
                &\ge 
                F(\mathcal J_k y_k)
                + \frac{\alpha_k^2}{2A_{k + 1} \lambda_k^2}
                \Vert y_k - \mathcal J_ky_k\Vert^2
                \\
                    & \quad 
                    + 
                    \lambda_k^{-1}
                    \left\langle 
                        y_k - \mathcal J_k y_k, 
                        (1 - \alpha_k)x_k + \alpha_k v_k - y_k
                    \right\rangle
                \\
                    &\quad 
                    + 
                    \lambda_k^{-1}
                    \langle
                        y_k - \mathcal J_k y_k,
                        \alpha_k(v_{k + 1} - v_k) + (y_k - \mathcal J_k y_k)
                    \rangle
                \\
                &= 
                F(\mathcal J_k y_k)
                + 
                \frac{1}{2\lambda_k}
                \left(
                    2 - \frac{\alpha_k^2}{A_{k + 1}\lambda_k}
                \right)
                \Vert y_k - \mathcal J_ky_k\Vert^2
                \\
                    & \quad 
                    + 
                    \lambda_k^{-1}
                    \left\langle 
                        y_k - \mathcal J_k y_k, 
                        (1 - \alpha_k)x_k + \alpha_k v_k - y_k
                    \right\rangle. 
            \end{align*}
            Next, if induction hypothesis $\phi_{k + 1}\ge F(\mathcal J_k y_k) = F(x_{k + 1})$ is true, it's sufficient to take the coefficient of $\Vert y_k - \mathcal J_k y_k\Vert^2$ to be greater than zero and make the inner product term zero which produces: 
            \begin{align*}
                & 
                y_k = (1 - \alpha_k)x_k + \alpha_k v_k, 
                \\
                &
                \frac{1}{2\lambda_k}
                \left(
                    2 - \frac{\alpha_k^2}{A_{k + 1}\lambda_k}
                \right) 
                \ge 0.
            \end{align*}
            Solving, the second inequality is equivalent to
            \begin{align*}
                \frac{\alpha_k^2}{A_{k + 1}\lambda_k} 
                & 
                \le 2
                \\
                \alpha_k
                & 
                \le
                \sqrt{2A_{k + 1}\lambda_k}
                = \sqrt{2A_k(1 - \alpha_k)\lambda_k}. 
            \end{align*}
            Using $\alpha_k^2 = A_k(1 - \alpha_k)\lambda_k$ to solving the quadratic: 
            \begin{align*}
                \alpha_k
                &= 
                \frac{1}{2}\left(
                    \sqrt{(A_k\lambda_k)^2 + 4A_k \lambda_k}
                    - A_k\lambda_k
                \right). 
            \end{align*}
        \end{proof}
        \begin{remark}
            The approach by Guler is slightly different compare to Accelerated Proximal Gradient method completed in Section \ref*{app:sec:thm-claim-acc-prox-grad}. 
            For the Accelerated Proximal Gradient, we simplified $\phi_{k + 1}^*$ prior to considering $F(x_{k + 1}) \le \phi_{k + 1}^*$. 
            In here, we substitute the inductive hypothesis $\phi_k^* \ge f(x_k)$ and skipped the canonical representation of $\phi_k^*$. 

            A more critical observation here is the Nesterov's estimating sequence from 
            \hyperref[app:sec:thm-claim-acc-prox-grad]{Section \ref*{app:sec:thm-claim-acc-prox-grad}} is the same as Guler's Accelerated PPM if we choose $L^{-1} = \lambda_k$,$\mu = 0$ and $g \equiv 0$ because: 
            \begin{align*}
                l_F(x; y_k) 
                &= 
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) + 
                \left\langle 
                    L\left(y_k - \widetilde{\mathcal J}_{L^{-1}}y_k\right), 
                    x - y_k
                \right\rangle
                + 
                \frac{1}{2L} 
                \left\Vert 
                    L \left(y_k - \widetilde{\mathcal J}_{L^{-1}}y_k\right)
                \right\Vert^2
                \\
                &= 
                F\left(\widetilde{\mathcal J}_{L^{-1}} y_k\right) + 
                \left\langle 
                    L\left(y_k - \widetilde{\mathcal J}_{L^{-1}}y_k\right), 
                    x - \widetilde{\mathcal J}_{L^{-1}}y_k
                \right\rangle
            \end{align*}
            When $g\equiv 0$, $L^{-1} = \lambda_k$, the proximal gradient operator has $\widetilde{\mathcal J}_{L^{-1}} = \mathcal J_{\lambda_k}(y_k)$, making $\phi_k(x)$ here the same as accelerated proximal gradient. 
        \end{remark}
    \subsection{Inexact accelerated PPM}\label{app:sec:inxt-acc-ppm}
        This section proofs the lemma that characterizes the inexact PPM errors for Guler's accelerated inexact PPM. 
        Now, we prove 
        \hyperref[thm:guler-inexact-ppm-bound]{Theorem \ref*{thm:guler-inexact-ppm-bound}}. 
                
        Denote $\mathcal M_k(x) = \mathcal M_k(x; y_k)$ for short. 
        The proof is direct by considering the strong convexity of $\mathcal M_k(\cdot, y_k)$ together with the subgradient inequality. 
        Choose any $w_k \in \partial \mathcal M_k(\mathcal J_{k} y_k)$ it has
        \begin{align*}
            \mathcal M_k(x_{k + 1}) - \mathcal M^*_k
            &= 
            \mathcal M_k(x_{k + 1}) - \mathcal M_k(\mathcal J_{k} y_k)
            \\
            &\ge 
            \left(
                \left\langle 
                    w_k, 
                    x_{k + 1} - \mathcal J_{k} y_k 
                \right\rangle
                + 
                \mathcal M_k(\mathcal J_{k}y_k)
                + \frac{1}{2\lambda_k}\Vert x_{k + 1} - \mathcal J_{k}y_k\Vert^2
            \right)
            - 
            \mathcal M_k(\mathcal J_{\lambda_k}y_k) 
            \\
            &= \frac{1}{2\lambda_k}\Vert x_{k +1} - \mathcal J_{k}y_k\Vert^2. 
        \end{align*}
        We used $\mathbf 0 \in \partial M_k(\mathcal J_{\lambda_k}y_k)$ to get rid of the inner product. 
        Consider inexact evaluation of $x_{k + 1}$ which results in $w_k \in \partial \mathcal M_k(x_{k + 1})$ with $\Vert w_k\Vert \le \epsilon_k/\lambda_k$. 
        By $\lambda_k^{-1}$ strong convexity of $\mathcal M_k$, we have 
        \begin{align*}
            \mathcal M_k(\mathcal J_{k}y_k) - \mathcal M_k(x_{k + 1})
            &\ge 
            \langle w_k, \mathcal J_{k}y_k - x_{k + 1}\rangle
            + \frac{1}{2\lambda_k}\Vert \mathcal J_{k} y_k - x_{k + 1}\Vert^2
            \\
            &\ge 
            - \Vert w_k\Vert\Vert \mathcal J_{k}y_k - x_{k + 1}\Vert^2
            + \frac{1}{2\lambda_k}\Vert \mathcal J_{k} y_k - x_{k + 1}\Vert^2
            \\
            &\ge 
            - \frac{\epsilon_k}{\lambda_k}\Vert \mathcal J_{k}y_k - x_{k + 1}\Vert^2
            + \frac{1}{2\lambda_k}\Vert \mathcal J_{k} y_k - x_{k + 1}\Vert^2
            \\
            &\ge \frac{1}{\lambda_k}
            \min_{t \in \RR} \left\lbrace
                \frac{1}{2}t^2 - \epsilon_k t
            \right\rbrace = - \frac{\epsilon_k}{2\lambda_k}. 
        \end{align*}

        The upper bound is proved. 
        \begin{remark}
            This is nothing new. 
            First inequality is a direct consequence of strong convexity and the second inequality is the PL-inequality implied by strong convexity. 
        \end{remark}

        
\section{Proofs for Catalyst Meta Acceleration}
    In this section, we prove the inexact proximal inequality and give the Nesterov's estimating sequence void of intractable quantities. 
    The notation is the same as
    \hyperref[sec:lin-2015]{Section \ref*{sec:lin-2015}}. 
    Recall inexact evaluation $x_k \approx \mathcal J_{\kappa^{-1}}y_{k - 1}$ such that $\mathcal M^{\kappa^{-1}}(x_k; y_{k - 1}) - \mathcal M^{\kappa^{-1}}(\mathcal J_{\kappa^{-1}}y_{k - 1}; y_{k - 1}) \le \epsilon_k$; $x_k^* = \mathcal J_{\kappa^{-1}}y_{k - 1}$ to be the exact evaluation. 
    
    \subsection{Inexact proximal inequality}
        \begin{proof}
            This is the proof for 
            \hyperref[lemma:lin-ixct-prox-ineq]{Lemma \ref*{lemma:lin-ixct-prox-ineq}}.   
            Define $G_k := \mathcal M_F^{\kappa^{-1}}(\cdot, y_{k - 1}), G_k^* = \mathcal M_F^{\kappa^{-1}}(x_k^*, y_{k - 1})$. 
        $G_k$ is a $\mu + \kappa$ convex function which implies quadratic growth over minimizer $x_k^*$: 
        \begin{align*}
            (\forall x)\quad G_k(x) &\ge G_k^* + \frac{\kappa + \mu}{2}\Vert x - x_k^*\Vert^2. 
        \end{align*}
        Substituting definitions:
        {\small
        \begin{align*}
            F(x) &\ge 
            G_k(x_k) + (G_k^* - G_k(x_k)) + 
            \frac{\mu + \kappa}{2}\Vert x - x_k^*\Vert^2 
            - \frac{\kappa}{2}\Vert x - y_{k - 1}\Vert^2
            \\
            &\ge 
            G(x_k) - \epsilon_k + \frac{\kappa + \mu}{2}\Vert x - x_k^*\Vert^2
            - \frac{\kappa}{2}\Vert x - y_{k - 1}\Vert^2
            \\
            &= G_k(x_k) - \epsilon_k + \frac{\kappa + \mu}{2}
            \left(
                \Vert x - x_k - x_k + x_k^*\Vert^2
            \right)
            - \frac{\kappa}{2}\Vert x - y_{k - 1}\Vert^2
            \\
            &= G_k(x_k) - \epsilon_k + \frac{\kappa + \mu}{2}
            \left(
                \Vert x - x_k\Vert^2 + \Vert x_k - x_k^*\Vert^2
                + 
                2\langle x - x_k, x_k - x_k^*\rangle
            \right)
            - \frac{\kappa}{2}\Vert x - y_{k - 1}\Vert^2
            \\
            &= 
            \left(
                G_k(x_k) + \frac{\kappa}{2}\Vert x - x_k\Vert^2 - \frac{\kappa}{2}\Vert x - y_{k - 1}\Vert^2
            \right)
            - \epsilon_k
            \\ &\quad 
                + \frac{\mu}{2}\Vert x - x_k\Vert^2
                + \frac{\kappa + \mu}{2}\Vert x_k - x_k^*\Vert^2
                + (\kappa + \mu)\langle x - x_k, x_k - x_k^*\rangle. 
        \end{align*}
        }
        Simplify terms inside the parenthesis: 
        \begin{align*}
            &
            G_k(x_k) 
            + \frac{\kappa}{2}\Vert x - x_k\Vert^2 - \frac{\kappa}{2}\Vert x - y_{k - 1}\Vert^2
            \\
            &= 
            F(x_k) + \frac{\kappa}{2}\Vert x_k - y_{k - 1}\Vert^2
            + \frac{\kappa}{2}\Vert x - x_k\Vert^2 - \frac{\kappa}{2}\Vert x - y_{k - 1}\Vert^2
            \\
            &= 
            F(x_k) + 
            \frac{\kappa}{2}\left(
                \Vert x_k - y_{k - 1}\Vert^2 - \Vert x - y_{k - 1}\Vert^2
            \right)
            + \frac{\kappa}{2} \Vert x - x_k\Vert^2
            \\
            &= 
            F(x_k) + 
            \frac{\kappa}{2}\left(
                \Vert x_k - x\Vert^2 
                + 
                2\langle x_k - x, x - y_{k - 1} \rangle
            \right)
            + 
            \frac{\kappa}{2} \Vert x - x_k\Vert^2
            \\
            &= 
            F(x_k) + 
            \kappa\Vert x_k - x\Vert^2
            + 
            \kappa\langle x_k - x, x - y_{k - 1}\rangle
            \\
            &= 
            F(x_k) + 
            \kappa\langle x_k - x, x - y_{k - 1} + x_k - x\rangle
            \\
            &= F(x_k) + \kappa\langle x_k - x, x_k - y_{k - 1} \rangle. 
        \end{align*}
        Therefore, it has the inequality: 
        \begin{align*}
            F(x) 
            &\ge 
            F(x_k) + \kappa\langle x_k - x, x_k - y_{k - 1} \rangle
            - \epsilon_k
            \\&\quad 
                + \frac{\mu}{2}\Vert x - x_k\Vert^2
                + \frac{\kappa + \mu}{2}\Vert x_k - x_k^*\Vert^2
                + (\kappa + \mu)\langle x - x_k, x_k - x_k^*\rangle
            \\
            & \ge
            F(x_k) + \kappa\langle x_k - x, x_k - y_{k - 1} \rangle
            - \epsilon_k
            + \frac{\mu}{2}\Vert x - x_k\Vert^2
            + (\kappa + \mu)\langle x - x_k, x_k - x_k^*\rangle. 
        \end{align*}
        Re-arranging it to a form by the writer's preference:
        \begin{align*}
            F(x) - F(x_k) - \kappa\langle x_k - x, x_k - y_{k - 1} \rangle
            - \frac{\mu}{2}\Vert x - x_k\Vert^2
            &\ge 
            - \epsilon_k
            + (\kappa + \mu)\langle x - x_k, x_k - x_k^*\rangle. 
        \end{align*}
        Now we have the RHS to be exclusively about the inexact evaluation $x_k \approx \mathcal J_{\kappa^{-1}} y_{k - 1}$, and $G_k(x_k) - G^*_k \le \epsilon_k$. 
        \end{proof}
        \begin{remark}
            Two major things about this lemma: 
            \begin{enumerate}
                \item Set $\epsilon_k = 0$ so $x_k = x^*_k$, the lemma is the proximal inequality. 
                \item It's not entirely obvious on its relations to the proximal gradient inequality. 
            \end{enumerate}
            For point (ii), observe that the proximal gradient inequality given by 
            \hyperref[app:thm:fun-thm-prox-grad]{Theorem \ref*{app:thm:fun-thm-prox-grad}} 
            has RHS that is completely different. 
            We have made attempts at bridging the two inequalities. 
            There are no obvious useful results and concrete claims. 
            There are no choices of parameters that makes the two inequalities equivalent. 
            We are not aware of any analogous results to this theorem in the literatures. 

        \end{remark}

    \subsection{Approximated Nesterov's Estimating sequence}
        In this subsection, let $\phi_k$ be the Nesterov's estimating sequence for Catalyst. 
        For all $k \ge0$, $\phi_k$ satisfies 
        \begin{align*}
            \phi_0(x) &:= F(x_0) + \frac{\gamma_0}{2}\Vert x - v_0\Vert^2, 
            \\
            \phi_k(x) &:= 
            (1 - \alpha_{k - 1})\phi_{k - 1}(x) + 
            \alpha_{k - 1}\left(
                F(x_k) + \kappa\langle y_{k - 1} - x_k, x - x_k\rangle
                + \frac{\mu}{2} \Vert x - x_k\Vert^2
            \right). 
        \end{align*}
        Observe that $\phi_k$ is a simple quadratic function. 
        We prove following theorem which is Lemma A.6 in Lin's writings on Catalyst \cite{lin_universal_2015}. 
        \begin{theorem}[Canonical form of Catalyst estimating sequence]\;\\
            Define $v_k, \gamma_k$ to be the parameters for the canonical form of $\phi_k$ as given by 
            \begin{align*}
                \phi_k(x) &= \phi_k^* + \frac{\gamma_k}{2}\Vert x - v_k\Vert^2, 
            \end{align*}
            where $\phi_k^* = \min_x \phi_k(x)$. 
            Then the parameters of the canonical form $(\gamma_{k})_{k \ge0}, (v_k)_{k \ge 0}, (\phi_k^*)_{k\ge 0}$ satisfies for all $k \ge 1$: 
            \begin{align*}
                \gamma_k &= (1 - \alpha_{k - 1})\gamma_{k - 1} + \alpha_{k - 1}\mu, 
                \\
                v_k &= 
                \gamma_k^{-1}(
                    (1 - \alpha_{k - 1})\gamma_{k - 1}v_{k - 1}
                    + \alpha_{k - 1}\mu x_k - \alpha_{k - 1}\kappa(y_{k - 1} - x_k)
                ), 
                \\
                \phi_k^* &= (1 - \alpha_{k - 1})\phi_{k - 1}^*
                + \alpha_{k - 1}F(x_k)
                - \frac{\alpha_{k - 1}^2}{2\gamma_k}\Vert \kappa(y_{k - 1} - x_k)\Vert^2
                \\
                &\quad 
                + \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})\gamma_{k - 1}}{\gamma_k}
                \left(
                    \frac{\mu}{2}\Vert x_k - v_{k - 1}\Vert^2 + 
                    \langle \kappa(y_{k - 1} - x_k), v_{k - 1} - x_k\rangle
                \right). 
            \end{align*}
        \end{theorem}
        \begin{proof}
            For $v_{k + 1}$, solve for $\nabla \phi_k(v_k) = \mathbf 0$ using the recursive definition. 
            For $\gamma_k$, consider the Hessian $\nabla^2 \phi_k$ using the recursive definition. 
            The process is similar to derivations in 
            \hyperref[app:sec:thm-claim-acc-prox-grad]{Section \ref*{app:sec:thm-claim-acc-prox-grad}}. 

            Next, we work on $\phi_k^*$. 
            The goal of the canonical form is to simplify the process of solving for an implicit sequence $x_k$ such that $f(x_k) \le \phi_k^*$. 
            To start consider for all $k \ge 1$: 
            \begin{align*}
                \phi_k(x_k) &= 
                \phi_k^* + \frac{\gamma_k}{2}\Vert x_k - v_k\Vert^2
                \\
                & = (1 - \alpha_{k - 1})\left(
                    \phi_{k - 1}^* + \frac{\gamma_{k - 1}}{2}\Vert x_k - v_{k - 1}\Vert^2
                \right) + \alpha_{k - 1}F(x_k)
                \\
                \iff 
                \phi_k^* &= 
                (1 - \alpha_{k - 1})\left(
                    \phi_{k - 1}^* + \frac{\gamma_{k - 1}}{2}\Vert x_k - v_{k - 1}\Vert^2
                \right) + \alpha_{k - 1}F(x_k) - \frac{\gamma_k}{2}\Vert x_k - v_k\Vert^2
                \\
                &= 
                (1 - \alpha_{k - 1})\phi_{k - 1}+ \alpha_{k - 1}F(x_k)
                + 
                \frac{(1 - \alpha_{k - 1})\gamma_{k - 1}}{2}\Vert x_k - v_{k - 1}\Vert^2
                - \frac{\gamma_k}{2}\Vert x_k - v_k\Vert^2.
            \end{align*}
                Now, it would be great to express $\Vert x_k - v_k\Vert^2$, so it depends on the iterates from previous iteration. 
                From the definition of $v_k$ we have 
            \begin{align*}
                v_k - x_k &= 
                \gamma_k^{-1}(
                (
                    1 - \alpha_{k - 1})\gamma_{k - 1}v_{k - 1}
                    + \alpha_{k - 1}\mu x_k - \alpha_{k - 1}\kappa(y_{k - 1} - x_k)
                ) - x_k
                \\
                &= 
                \gamma_k^{-1}(
                    (1 - \alpha_{k - 1})\gamma_{k - 1}v_{k - 1}
                    + (\alpha_{k - 1}\mu - \gamma_k) x_k - \alpha_{k - 1}\kappa(y_{k - 1} - x_k)
                )
                \\
                &= 
                \gamma_k^{-1}(
                    (1 - \alpha_{k - 1})\gamma_{k - 1}v_{k - 1}
                    - (1 - \alpha_{k - 1})\gamma_{k - 1} x_k - \alpha_{k - 1}\kappa(y_{k - 1} - x_k)
                )
                \\
                &= \gamma_k^{-1}(
                    (1 - \alpha_{k - 1})\gamma_{k - 1}(v_{k - 1} - x_k)
                    - \alpha_{k - 1}\kappa(y_{k - 1} - x_k)
                ).
            \end{align*}
            Taking the norm and multiplying by $\gamma_k/2$ to match the terms in (eqn1) then: 
            \begin{align*}
                \Vert v_k - x_k\Vert^2 &= 
                \gamma_k^{-2}\Vert (1 - \alpha_{k - 1})\gamma_{k - 1}(v_{k - 1} - x_k) \Vert^2
                + 
                \gamma_k^{-2}\Vert \alpha_{k - 1}\kappa(y_{k - 1} - x_k)\Vert^2
                \\
                & \quad 
                - 2\gamma_k^{-2}\gamma_{k - 1}(1 - \alpha_{k - 1})\alpha_{k - 1}\langle v_{k - 1} - x_k, \kappa(y_{k - 1} - x_k) \rangle
                \\
                \frac{\gamma_k}{2}
                \Vert v_k - x_k\Vert^2 
                &= 
                \frac{(1 - \alpha_{k - 1})^{2}\gamma_{k - 1}^{2}}{2\gamma_k}\Vert x_k - v_{k - 1}\Vert^2
                + 
                \frac{\alpha_{k - 1}^2}{2\gamma_k} \Vert \kappa(y_{k - 1} - x_k)\Vert^2
                \\
                &\quad 
                - 
                \frac{
                    \gamma_{k - 1}(1 - \alpha_{k - 1})\alpha_{k - 1}
                }{\gamma_k}\langle v_{k - 1} - x_k, \kappa(y_{k - 1} - x_k)\rangle.  
            \end{align*}
            Substituting it back we have 
            {\small
            \begin{align*}
                \phi_k^* &= 
                (1 - \alpha_{k - 1})\phi_{k - 1}+ \alpha_{k - 1}F(x_k)
                + 
                \frac{(1 - \alpha_{k - 1})\gamma_{k - 1}}{2}\Vert x - v_{k - 1}\Vert^2
                - 
                \frac{(1 - \alpha_{k - 1})^{2}\gamma_{k - 1}^{2}}{2\gamma_k}\Vert x_k - v_{k - 1}\Vert^2
                \\
                & \quad 
                    + \left(
                        - 
                        \frac{\alpha_{k - 1}^2}{2\gamma_k} \Vert \kappa(y_{k - 1} - x_k)\Vert^2
                        + 
                        \frac{
                            \gamma_{k - 1}(1 - \alpha_{k - 1})\alpha_{k - 1}
                        }{\gamma_k}\langle v_{k - 1} - x_k, \kappa(y_{k - 1} - x_k)\rangle
                    \right)
                \\
                &= 
                (1 - \alpha_{k - 1})\phi_{k - 1}+ \alpha_{k - 1}F(x_k)
                + 
                \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})\gamma_{k - 1}\mu}{2\gamma_k}
                \Vert x_k - v_{k - 1}\Vert^2
                - 
                \frac{\alpha_{k - 1}^2}{2\gamma_k} \Vert \kappa(y_{k - 1} - x_k)\Vert^2
                \\
                &\quad + 
                \frac{\gamma_{k - 1}(1 - \alpha_{k - 1})\alpha_{k - 1}}{\gamma_k}
                \langle v_{k - 1} - x_k, \kappa(y_{k - 1} - x_k)\rangle
                \\
                &= 
                (1 - \alpha_{k - 1})\phi_{k - 1} + \alpha_{k - 1}F(x_k)
                \\ &\quad 
                + \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})\gamma_{k - 1}}{\gamma_k}
                \left(
                    \frac{\mu}{2}\Vert x_k - v_{k - 1}\Vert^2 
                    + 
                    \langle v_{k - 1} - x_k, \kappa(y_{k - 1} - x_k) \rangle. 
                \right)
            \end{align*}
            }
            On the second equality, we made use of the following to simplify the coefficients for ${\Vert x_k - v_{k - 1}\Vert^2}$: 
            \begin{align*}
                \frac{(1 - \alpha_{k - 1})\gamma_{k - 1}}{2} - 
                \frac{(1 - \alpha_{k - 1})^{2}\gamma_{k - 1}^{2}}{2\gamma_k}
                &= 
                \frac{(1 - \alpha_{k - 1})\gamma_{k - 1}}{\gamma_k}
                \left(
                    \frac{\gamma_k}{2} - \frac{(1 - \alpha_{k - 1})\gamma_{k - 1}}{2}
                \right)
                \\
                &= 
                \frac{(1 - \alpha_{k - 1})\gamma_{k - 1}}{\gamma_k}
                \left(
                    \frac{\gamma_k - (1 - \alpha_{k - 1})\gamma_{k - 1}}{2}
                \right)
                \\
                &= 
                \frac{(1 - \alpha_{k - 1})\gamma_{k - 1}}{\gamma_k}
                \left(
                    \frac{
                        \alpha_{k - 1}\mu
                    }{2}
                \right). 
            \end{align*}
        \end{proof}
        \begin{remark}
            The differences of the Canonical form is cosmetic compare results from Guler, and Acceleration proximal gradient. 
            The arrangement is different because the inexact proximal inequality from \hyperref[lemma:lin-ixct-prox-ineq]{Lemma \ref*{lemma:lin-ixct-prox-ineq}} 
            are anchored on different iterates.  
        \end{remark}
    \subsection{Controlling the errors}
        The following theorem is Theorem A.8 from Lin's writing of Catalyst \cite{lin_universal_2015}. 
        It stated the propagation of errors $\epsilon_k$ from the inexact proximal inequality to the descent condition $F(x_k) \le \phi_k^*$. 
        \begin{theorem}[Controlling the error in Nesterov's estimating sequence]\;\\
            If the auxiliary sequences $v_k, y_k, \gamma_k, \alpha_k$ satisfies the conditions: 
            \begin{align*}
                \gamma_k - (\kappa + \mu)\alpha_{k - 1}^2 
                &= 0, 
                \\
                (1 - \alpha_{k - 1})\gamma_{k - 1} + \alpha_{k - 1}\mu 
                &= (\kappa + \mu)\alpha_{k - 1}^2. 
            \end{align*}
            Then the canonical representation of estimating sequence $\phi_k^*$ and the function value 
            the inexact proximal point iterates $F(x_k)$ satisfy for all $k\ge 1$
            \begin{align*}
                F(x_k) &\le \phi_k^* + \xi_k, 
                \\
                \xi_k &= 
                (1 - \alpha_{k - 1})(
                    \xi_{k - 1} + \epsilon_k 
                    - (\kappa + \mu)\langle x_k - x_k^*, x_{k - 1} - x_k\rangle
                ). 
            \end{align*}
            Where we have the base case that $\xi_0 = 0$
        \end{theorem}
        \begin{proof}
            We prove it via induction. 
            Base case is trivially satisfied via $\phi_0^* = F(x_0)$ and $\xi_0 = 0$. 
            Inductively we assume that $F(x_{k - 1}) \le \phi_{k - 1}^* + \xi_k$. 
            By definition, it means 
            \begin{align}
                \phi_{k - 1}^* &\ge 
                F(x_{k - 1}) - \xi_{k - 1} 
                \nonumber
                \\
                &\ge 
                F(x_k) + 
                \langle \kappa(y_{k - 1} - x_k), x_{k - 1} - x_k\rangle
                + 
                (\kappa + \mu)\langle x_k - x_k^*, x_{k - 1} - x_k\rangle
                - \epsilon_k - \xi_{k - 1}
                \nonumber
                \\
                &= 
                F(x_k) + 
                \langle \kappa(y_{k - 1} - x_k), x_{k - 1} - x_k\rangle
                - (1 - \alpha_{k - 1})^{-1}\xi_k. 
            \end{align}\label{app:c3-1}
            Substituting it into the canonical form representation of the estimating sequence derived from the previous section, it has 
            {\small
            \begin{align}\begin{split}
                \phi_k^* &= 
                (1 - \alpha_{k - 1})\phi_{k - 1}^* 
                + 
                \alpha_{k - 1}F(x_k)
                - 
                \frac{\alpha_{k - 1}}{2\gamma_k}
                \Vert \kappa(y_{k - 1} - x_k)\Vert^2
                \\
                &\quad 
                + \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})\gamma_{k - 1}}{\gamma_k}
                \left(
                    \frac{\mu}{2}\Vert x_k - v_{k - 1}\Vert^2
                    + \langle \kappa(y_{k - 1} - x_k), v_{k - 1} - x_k\rangle
                \right)
                \\
                &\ge 
                (1 - \alpha_{k - 1})
                \left(
                    F(x_k) + 
                    \langle \kappa(y_{k - 1} - x_k), x_{k - 1} - x_k\rangle
                    - (1 - a_{k - 1})^{-1}\xi_k
                \right)
                + 
                \alpha_{k - 1}F(x_k)
                    \\
                    &\quad 
                    + 
                    \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})\gamma_{k - 1}}{\gamma_k}
                    \left(
                        \frac{\mu}{2}\Vert x_k - v_{k - 1}\Vert^2
                        + \langle \kappa(y_{k - 1} - x_k), v_{k - 1} - x_k\rangle
                    \right)
                    -
                    \frac{\alpha_{k - 1}}{2\gamma_k}
                    \Vert \kappa(y_{k - 1} - x_k)\Vert^2
                \\
                &= 
                (1 - \alpha_{k - 1})\langle \kappa(y_{k - 1} - x_k), x_{k - 1} - x_k \rangle
                - \frac{\alpha_{k - 1}}{2\gamma_k}\Vert \kappa(y_{k - 1} - x_k)\Vert^2
                    \\
                    & \quad 
                    + 
                    \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})\gamma_{k - 1}}{\gamma_k}
                    \left(
                        \frac{\mu}{2}\Vert x_k - v_{k - 1}\Vert^2
                        + 
                        \left\langle 
                            \kappa(y_{k - 1} - x_k), v_{k - 1} - x_k
                        \right\rangle
                    \right)+ F(x_k) - \xi_k
                \\
                &= 
                (1 - \alpha_{k - 1})\left\langle 
                    \kappa(y_{k - 1} - x_k), 
                    \frac{
                        \alpha_{k - 1}\gamma_{k - 1}
                    }{
                        \gamma_k
                    }(v_{k - 1} - x_k) + x_{k - 1} - x_k
                \right\rangle
                - \frac{\alpha_{k - 1}}{2\gamma_k}\Vert \kappa(y_{k - 1} - x_k)\Vert^2
                \\
                    &\quad 
                    + \frac{\mu\alpha_{k - 1}(1 - \alpha_{k - 1})\gamma_{k - 1}}{2\gamma_k}
                    \Vert x_k - v_{k - 1}\Vert^2 + F(x_k) - \xi_k. 
                \end{split}\label{app:c3-2}\end{align}
            }
            Next, we need to focus on the first 2 terms on the RHS
            \begin{align}
                (1 - \alpha_{k - 1})\left\langle 
                    \kappa(y_{k - 1} - x_k), 
                    \frac{
                        \alpha_{k - 1}\gamma_{k - 1}
                    }{
                        \gamma_k
                    }(v_{k - 1} - x_k) + x_{k - 1} - x_k
                \right\rangle
                - \frac{\alpha_{k - 1}}{2\gamma_k}\Vert \kappa(y_{k - 1} - x_k)\Vert^2. 
            \end{align}\label{app:c3-3}
            The first inner product term in \ref*{app:c3-3} has
            \begin{align*}
                & 
                \left\langle 
                    \kappa(y_{k - 1} - x_k),
                    x_{k - 1} - y_{k - 1} 
                    + 
                    \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    (v_{k - 1} - y_{k - 1} + y_{k - 1} - x_k) 
                \right\rangle
                \\
                &= 
                \left\langle 
                    \kappa(y_{k - 1} - x_k), 
                    x_{k - 1} - y_{k - 1}
                    + 
                    \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    (v_{k - 1} - y_{k - 1})  
                \right\rangle
                \\ 
                &\quad 
                    + 
                    \left\langle 
                        \kappa(y_{k - 1} - x_k), 
                        y_{k - 1} - x_k + 
                        \frac{\alpha_{k - 1}\gamma_{k - 1}}
                        {\gamma_k}
                        (y_{k - 1} - x_k)
                    \right\rangle
                \\
                &= 
                \left\langle 
                    \kappa(y_{k - 1} - x_k), 
                    x_{k - 1} - y_{k - 1}
                    + 
                    \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    (v_{k - 1} - y_{k - 1})  
                \right\rangle
                \\ 
                &\quad 
                    + 
                    \kappa\left(
                        1 + \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    \right)
                    \Vert v_{k - 1} - y_{k - 1}\Vert^2. 
            \end{align*}
            With the above \ref*{app:c3-3} simplifies to
            \begin{align*}
                &
                (1 - \alpha_{k - 1})\left\langle 
                    \kappa(y_{k - 1} - x_k), 
                    x_{k - 1} - y_{k - 1}
                    + 
                    \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    (v_{k - 1} - y_{k - 1})  
                \right\rangle
                \\
                    & 
                    - \frac{\alpha_{k - 1}}{2\gamma_k}
                    \Vert \kappa(y_{k - 1} - x_k)\Vert^2
                    + 
                    \kappa(1 - \alpha_{k - 1})\left(
                        1 + \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    \right)
                    \Vert v_{k - 1} - y_{k - 1}\Vert^2
                \\
                &=
                (1 - \alpha_{k - 1})
                \left\langle 
                    \kappa(y_{k - 1} - x_k), 
                    x_{k - 1} - y_{k - 1}
                    + 
                    \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    (v_{k - 1} - y_{k - 1})  
                \right\rangle
                    \\
                    &\quad 
                    + 
                    (1 - \alpha_{k - 1})\kappa\left(
                        1 + \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                        - \frac{\kappa\alpha_{k - 1}}{2\gamma_k}
                    \right)
                    \Vert y_{k - 1} - x_k\Vert^2. 
            \end{align*}
            We can simplify the coefficient of $\Vert y_{k - 1} - x_k\Vert^2$ in the above expression by the recurrence of parameter $\gamma_k$ from the canonical form of estimating sequence. 
            \begin{align*}
                &
                (1 - \alpha_k)\kappa\left(
                    1 + \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    - \frac{\kappa\alpha_{k - 1}}{2\gamma_k}
                \right)
                \\
                &=
                \kappa\left(
                    1 - \alpha_{k - 1} + 
                    \frac{(1 - \alpha_{k - 1})\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    - \frac{\alpha_{k - 1}^2\kappa}{2\gamma_k}
                \right)
                \\
                & \text{Use: }\gamma_k - \alpha_{k - 1}\mu = (1 - \alpha_{k - 1})\gamma_{k - 1}
                \\
                &= 
                \kappa\left(
                    1 - \alpha_{k - 1} 
                    + 
                    \frac{(\gamma_k - \alpha_{k - 1}\mu)\alpha_{k - 1}}{\gamma_k}
                    - 
                    \frac{\alpha_{k - 1}^2\kappa}{2\gamma_k}
                \right)
                \\
                &= \kappa
                \left(
                    1 + 
                    \frac{
                        -2\gamma_k\alpha_{k - 1}
                        + 2(\gamma_k - \alpha_{k - 1}\mu)\alpha_{k - 1}
                        - \alpha_{k - 1}^2\kappa
                    }{
                        2\gamma_k
                    }
                \right)
                \\
                &= \kappa
                \left(
                    1 + \frac{-2\alpha_{k + 1}^2\mu - \alpha_{k - 1}^2\kappa}{2\gamma_k}
                \right)
                \\
                &= 
                \kappa\left(
                    1 - \frac{(2\mu + \kappa)\alpha_{k - 1}^2}{2\gamma_k}
                \right)
                \\
                &= 
                \kappa\left(
                    1 - \frac{(\mu + \kappa/2)\alpha_{k - 1}^2}{\gamma_k}
                \right). 
            \end{align*}
            Now, substitute the above back to \ref*{app:c3-3} and then substitute \ref*{app:c3-3} back to \ref*{app:c3-2} to get:
            {\small
            \begin{align*}
                \phi_k^* 
                &\ge 
                (1 - \alpha_{k - 1})
                \left\langle 
                    \kappa(y_{k - 1} - x_k), 
                    x_{k - 1} - y_{k - 1}
                    + 
                    \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    (v_{k - 1} - y_{k - 1})  
                \right\rangle
                    \\
                    &\quad 
                    + 
                    \kappa\left(
                        1 - \frac{(\mu + \kappa/2)\alpha_{k - 1}^2}{\gamma_k}
                    \right)
                    \Vert y_{k - 1} - x_k\Vert^2
                    + \frac{\mu\alpha_{k - 1}(1 - \alpha_{k - 1})\gamma_{k - 1}}{2\gamma_k}
                    \Vert x_k - v_{k - 1}\Vert^2 + F(x_k) - \xi_k
                \\
                &\ge 
                (1 - \alpha_{k - 1})
                \left\langle 
                    \kappa(y_{k - 1} - x_k), 
                    x_{k - 1} - y_{k - 1}
                    + 
                    \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                    (v_{k - 1} - y_{k - 1})  
                \right\rangle
                    \\
                    &\quad 
                    + 
                    \kappa\left(
                        1 - \frac{(\mu + \kappa/2)\alpha_{k - 1}^2}{\gamma_k}
                    \right)
                    \Vert y_{k - 1} - x_k\Vert^2
                    + 
                    \Vert x_k - v_{k - 1}\Vert^2 + F(x_k) - \xi_k. 
            \end{align*}
            }
            To assert the inductive hypothesis $F(x_k) \le \phi_k^*$ it's sufficient to have the inner product term equals to zero, and the coefficient of $\Vert y_k - x_k\Vert$ to be non-negative. 
            Therefore, it suffices to consider 
            \begin{align*}
                x_{k - 1} - y_{k - 1}
                + 
                \frac{\alpha_{k - 1}\gamma_{k - 1}}{\gamma_k}
                (v_{k - 1} - y_{k - 1}) 
                &= 
                \mathbf 0, 
                \\
                1 - \frac{(\kappa/2 + \mu)\alpha_{k - 1}^2}{\gamma_k}
                \le 
                1 - (\kappa + \mu)\frac{\alpha_{k - 1}^2}{\gamma_k} 
                & \le 0. 
            \end{align*}
            In addition, if we assume equality holds then it gives: 
            \begin{align*}
                \gamma_k - (\kappa + \mu)\alpha_{k - 1}^2 
                &= 0, 
                \\
                (1 - \alpha_{k - 1})\gamma_{k - 1} + \alpha_{k - 1}\mu 
                &= (\kappa + \mu)\alpha_{k - 1}^2. 
            \end{align*}
        \end{proof}
        \begin{remark}
            For all $k \ge 1$, with vector $x_k, v_k$ and scalar $\gamma_k, \alpha_k$ given, the updates for $x_{k + 1}, y_{k + 1}, v_{k + 1}$ and $\gamma_{k + 1}$ are produced by: 
            \begin{align*}
                \gamma_{k + 1} &= (1 - \alpha_{k})\gamma_k + \alpha_k \mu = (\kappa + \mu)\alpha_{k - 1}^2,
                \\
                y_k &= (\gamma_k + \alpha_k \mu)^{-1}
                (\alpha_k \gamma_k v_k + \gamma_{k + 1}x_k),
                \\
                \tilde x_{k + 1} &\approx \mathcal J_{\kappa^{-1}} y_{k} 
                \text{ s.t: } 
                \mathcal M^{\kappa^{-1}}(x_{k + 1}, y_k) - 
                \mathcal M^{\kappa^{-1}}(\mathcal J_{\kappa^{-1}} y_{k} , y_k) \le \epsilon_k,
                \\
                \widetilde{\mathcal G}_{\kappa^{-1}} y_k &= \kappa(y_k - \tilde x_{k + 1}),
                \\
                x_{k + 1} &= y_k - \widetilde{\mathcal G}_{\kappa^{-1}} y_k,
                \\
                v_{k + 1} &= 
                \gamma_{k + 1}^{-1}\left(
                    (1 - \alpha_k)\gamma_k v_k + 
                    \alpha_k(\mu + \kappa)x_k 
                    - \alpha_k \kappa y_k
                \right)
                \\
                &= \gamma_{k + 1}^{-1}(
                    (1 - \alpha_k)\gamma_k v_k + \alpha_k\mu x_k + 
                    \alpha_k\kappa(x_k - y_k)
                )
                \\
                &= 
                \gamma_{k + 1}^{-1}\left(
                    (1 - \alpha_k)\gamma_k v_k + \alpha_k\mu x_k - \alpha_k 
                    \widetilde{\mathcal G}_{\kappa^{-1}}y_k
                \right). 
            \end{align*}
            In terms of just the formatting of updates on $(v_k, y_k, x_k)$, $\alpha_k, \gamma_k$ are conducted, it's exactly the same as the Accelerated proximal gradient algorithm proved back in 
            \hyperref[app:sec:thm-claim-acc-prox-grad]{Section \ref*{app:sec:thm-claim-acc-prox-grad}}, but with $L = \kappa + \mu$ and $\tilde x_{k + 1}$ produced by the inexact proximal point instead of proximal gradient. 
            The results on the equivalent representations of the accelerated proximal gradient algorithm from the previous section will apply for this the Catalyst as well. 
            Therefore, by analysis from previous section, we also have for all $k \ge 0$: 
            \begin{align*}
                y_k &= x_k + \frac{\alpha_{k - 1}(1 - \alpha_{k - 1})}{\alpha_{k - 1}^2 + \alpha_k}
                (x_k - x_{k - 1}).
            \end{align*}
            Lin's innovation of the inexact proximal inequality allows us to roll the error from the inexact proximal point into $\xi_k$ to characterize the descent of the sequence $x_k \approx \mathcal J_{\kappa^{-1}}y_{k - 1}$. 
            It undoubtedly improved the results from Lemma 3.2 of Guler's paper in 1992 \cite{guler_new_1992}. 
        \end{remark}

\section{Proofs for 4WD Catalyst Acceleration}\label{app:sec:ncnvx-catalyst}
    In this section we adopt the same notations as in 
    \hyperref[sec:4wd-catalyst]{Section \ref*{sec:4wd-catalyst}}. 
    We prove
    \hyperref[thm:basic-4wd-catalyst]{Theorem \ref*{thm:basic-4wd-catalyst}}
    in this section. 
    The lemma below characterize the lower and upper bound on the $(\alpha_k)_{k \ge 0}$ which ultimately control the convergence rate. 
    \begin{lemma}[Bounds of the inverted FISTA sequence]\label{app:lemma:momentum-sequence-bounds}
        With base case $\alpha_1 = 1$
        If the sequence $\alpha_k$ has for all $k\ge 1$: 
        \begin{align*}
            \alpha_{k + 1} = \frac{\sqrt{\alpha_k^4 + 4\alpha_k^2} - \alpha_k^2}{2}, \alpha_1 = 1
        \end{align*}
        then for all $k \ge0$: 
        \begin{align*}
            \frac{\sqrt{2}}{k + 1} \le \alpha_k \le \frac{2}{k + 1}. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        The proof of the upper bound is by inductive hypothesis, assume $\alpha_k \ge 2/(k + 1)$ then consider
        \begin{align*}
            \alpha_{k + 1} &= 
            \frac{\sqrt{\alpha_k^4 + 4 \alpha_k^2} - \alpha_k^2}{2}
            = 
            \frac{\sqrt{\alpha_k^4 + 4 \alpha_k^2} - \alpha_k^2}{2}
            \frac{\sqrt{\alpha_k^4 + 4 \alpha_k^2} + \alpha_k^2}{\sqrt{\alpha_k^4 + 4 \alpha_k^2} + \alpha_k^2}
            \\
            &= \frac{\alpha_k^4 + 4\alpha_k^2 - \alpha_k^4}{2(\sqrt{\alpha_k^4 + 4 \alpha_k^2)} + \alpha_k^2)}
            \\
            &= \frac{2}{\sqrt{1 + 4 \alpha_k^{-2}} + 1} 
            \le \frac{2}{\sqrt{1 + (k + 1)^2} + 1} \le \frac{2}{k + 2}. 
        \end{align*}
        The lower bound is given by the hidden recursive definition of $\alpha_k$: 
        \begin{align*}
            \alpha_{k + 1}^2 &= (1 - \alpha_{k + 1})\alpha_k^2 = \alpha_1^2
            \\
            &\ge \alpha_1^2 \prod_{i = 2}^{k + 1}(1 - \alpha_i)
            \\
            &\ge \prod_{i = 2}^{k + 1}\left(
                1 - \frac{2}{i + 1}
            \right) = \prod_{i = 2}^{k + 1} \left(\frac{i - 1}{i + 1}\right)
            \\
            &= \frac{2}{(k + 1)(k + 2)}  \ge \frac{2}{(k + 2)^2}. 
        \end{align*}
    \end{proof}
    \begin{remark}
        $a_k^{-1}$ would just be the FISTA sequence. 
    \end{remark}

    \begin{lemma}[Stationarity condition]\label{app:lemma:ncnvx-catalyst-stationary-cond}
        Assume $F$ is $\rho$ weakly convex. 
        Fix any $y$, suppose that $y^+$ satisfies $\dist(\mathbf 0,\partial \mathcal M^{k^{-1}}(y^+; y)) \le \epsilon$ then the following inequality holds: 
        \begin{align*}
            \dist(\mathbf 0; \partial F(y^+)) 
            \le \epsilon + \kappa\Vert y^+ - y\Vert. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Take it as a fact that the limiting subgradient of a weakly convex function is a closed set. 
        Fix any $y$, there exists $w \in \partial \mathcal M(y^+; y)$ such that $\dist(\mathbf 0; \partial {\mathcal M}(y^+; y)) = \Vert w\Vert$ because $\partial M(\cdot;y)$ is a $\rho - \kappa$ weakly convex function. 
        Next, by definition we have
        \begin{align*}
            w &\in \partial F(y^+) + \kappa(y^+ - y)
            \\
            \iff 
            \exists v &
            \in \partial  F(y^+): 
            w = v + \kappa(y^+ - y)
            \\
            \implies 
            \epsilon &= 
            \Vert w\Vert = \Vert v + \kappa(y^+ - y)\Vert 
            \ge \Vert v\Vert - \Vert \kappa(y^+ - y)\Vert
            \\
            \implies 
            \dist(\mathbf 0, \partial F(y^+)) &\le 
            \Vert v\Vert 
            \le \epsilon + \Vert \kappa(y^+ - y)\Vert. 
        \end{align*}

    \end{proof}

    \subsection{Convergence proof of the basic 4WD-Catalyst}
        This subsection proves 
        \hyperref[thm:basic-4wd-catalyst]{Theorem \ref*{thm:basic-4wd-catalyst}}
        which is about the convergence of 
        \hyperref[def:basic-4wd-catalyst]{Definition \ref*{def:basic-4wd-catalyst}}. 
        
        For any $k \ge 1$, the algorithm asserts 
        \begin{align*}
            F(x_{k - 1}) \ge \mathcal M(\bar x_k, x_{k - 1}) \ge 
            F(x_k) + \frac{\kappa}{2}\Vert \bar x_k - x_{k - 1}\Vert^2. 
        \end{align*}
        Use Lemma \ref*{app:lemma:ncnvx-catalyst-stationary-cond} with $\epsilon = \kappa \Vert \bar x_k - x_{k - 1}\Vert$, $y = x_{k - 1}$, $y^+ = \bar x_k$ then 
        \begin{align*}
            \dist(\mathbf 0, \partial F(\bar x_k)) \le 2 \kappa \Vert \bar x_k - x_{k - 1} \Vert. 
        \end{align*}
        With $F$ bounded below, denote $F^*$ to be the minimum then using the two results above: 
        \begin{align*}
            F(x_{k - 1}) - F(x_k) 
            &\ge \frac{\kappa}{2}\Vert \bar x_k - x_{k - 1}\Vert^2
            \\
            8 \kappa (F(x_{k - 1}) - F(x_k)) &\ge 4 \Vert \kappa (\bar x_k - x_{k - 1})\Vert^2 \ge 
            \dist^2(\mathbf 0, \partial F(\bar x_k))
            \\
            \implies 
            \dist^2(\mathbf 0, \partial F(\bar x_k)) 
            &\le 
            8 \kappa (F(x_{k - 1}) - F(x_k))
            \\
            \implies 
            \min_{j = 1, \cdots, N} \dist^2(\mathbf 0, \partial F(\bar x_j))
            &\le 
            \frac{8\kappa}{N}
            \sum_{j = 1}^{N} F(x_{j - 1}) - F(x_j)
            \\
            &\le 
            \frac{8 \kappa}{N} (F(x_0) - F(x_N)) 
            \le \frac{8 \kappa}{N}(F(x_0) - F^*). 
        \end{align*}
        Therefore, the set limits of $\partial F(\bar x_j)$ contains $\mathbf 0$. 
        The second part of the claim about the convergence to optimality requires additional assumptions that 
        \begin{enumerate}
            \item $F$ is convex. 
            \item $F$ is bounded below and has a minimizer $x^*$. 
        \end{enumerate}
        From the algorithm we have $\xi_k \in \partial \mathcal M(\tilde x_k, y_k)$ such that $\Vert \xi_k\Vert \le \frac{\kappa}{k + 1}\Vert \tilde x_k - y_k\Vert$. 
        Then for any $x\in \RR^n$, $\kappa$ strong convexity of $\mathcal M(\cdot, y_k)$ yields inequality: 
        {\small
        \begin{align*}
            0 &\le
            F(x) + \frac{\kappa}{2}\Vert x - y_k\Vert^2 
            - \left(
                F(\tilde x) 
                + \frac{\kappa}{2}\Vert \tilde x_k - y_k\Vert^2
            \right)
            - \frac{\kappa}{2}\Vert x - \tilde x_k\Vert^2 
            - \langle \xi_k, x - \tilde x_k\rangle
            , 
            \\
            F(x_k)
            \le F(\tilde x_k) 
            &\le 
            F(x) + \frac{\kappa}{2}\left(
                \Vert x - y_k\Vert^2 - \Vert x - \tilde x_k\Vert^2 - \Vert \tilde x_k - y_k\Vert^2
            \right)
            + \langle  \xi_k, \tilde x_k - x \rangle 
            \\
            &\le 
            F(x) + \frac{\kappa}{2}\left(
                \Vert x - y_k\Vert^2 - \Vert x - \tilde x_k\Vert^2 - \Vert \tilde x_k - y_k\Vert^2
            \right)
            + \frac{\kappa}{k + 1}\Vert \tilde x_k - y_k\Vert\Vert x - \tilde x_k\Vert. 
        \end{align*}
        }
        Sure, now observe that with the substitutions $x = \alpha_k x^* + (1 - \alpha_k) x_{k-1}$ where $x^*$ is the minimizer then
        \begin{align*}
            x - y_k
            &= 
            \alpha_k x^* + (1 - \alpha_k) x_{k - 1} - y_k 
            \\
            &= \alpha_k x^* + (1 - \alpha_k) x_{k - 1} - (\alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1})
            \\
            &= \alpha_k (x^* - v_{k - 1}), 
            \\
            x - \tilde x_k 
            &= 
            \alpha_k x^* + (1 - \alpha_k) x_{k - 1} - \tilde x_k
            \\
            &\quad 
            \textcolor{gray}{
            \begin{aligned}
                v_{k} &= x_{k - 1} + \alpha_k^{-1}(\tilde x_k - x_{k - 1})
                \\
                \tilde x_k - x_{k - 1} &= \alpha_k(v_k - x_{k - 1})
                \\
                \tilde x_k &= x_{k - 1} + \alpha_k(v_k - x_{k - 1})
            \end{aligned}
            }
            \\
            &= 
            \alpha_k x^* + (1 - \alpha_k) x_{k - 1} - (x_{k - 1} + \alpha_k(v_k - x_{k - 1}))
            \\
            &= \alpha_k x^* - \alpha_k x_{k - 1} - \alpha_k(v_k - x_{k - 1})
            \\
            &= \alpha_k (x^* - v_k). 
        \end{align*}
        Using convexity, it transforms the inequality into 
        \begin{align*}
            F(x_k) &\le 
            \alpha_k F(x^*) + (1 - \alpha_k) F(x_{k - 1}) 
            + \frac{\alpha_k^2\kappa}{2}\left(
                \Vert x^* - v_{k - 1}\Vert^2 - 
                \Vert v_k - x^*\Vert^2
            \right)
            \\
            &\quad 
                - \frac{\kappa}{2}\Vert \tilde x_k - y_k\Vert^2 
                + \frac{\kappa \alpha_k}{k + 1}\Vert \tilde x - y_k\Vert\Vert v_k - x^*\Vert
            \\
            &=
            \alpha_k F(x^*) + (1 - \alpha_k) F(x_{k - 1}) 
            + \frac{\alpha_k^2\kappa}{2}\left(
                \Vert x^* - v_{k - 1}\Vert^2 - 
                \Vert v_k - x^*\Vert^2
            \right)
                \\
                &\quad 
                - \frac{\kappa}{2}\left(
                    \Vert \tilde x_k - y_k\Vert
                    - \frac{\alpha_k}{k + 1}\Vert v_k - x^*\Vert
                \right)^2 
                + \frac{\kappa}{2}\left(\frac{\alpha_k}{k + 1}\right)^2\Vert v_k - x^*\Vert^2
            \\ 
            &\le 
            \alpha_k F(x^*) + (1 - \alpha_k) F(x_{k - 1}) 
            + \frac{\alpha_k^2 \kappa}{2}\left(
                \Vert x^* - v_{k - 1}\Vert^2 - 
                \Vert v_k - x^*\Vert^2
            \right)
                \\
                &\quad  
                + \frac{\kappa \alpha_k^2}{2}\left(\frac{1}{k + 1}\right)^2\Vert v_k - x^*\Vert^2
            \\
            \iff 
            F(x_k) - F^*
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F^*)
            \\ &\quad 
                + 
                \frac{\alpha_k^2\kappa}{2}
                \left(
                    \Vert x^* - v_{k - 1}\Vert^2
                    - \left(
                        1 - \frac{1}{(k + 1)^2}
                    \right)\Vert v_k - x^*\Vert^2
                \right)
        \end{align*}
        Denote $A_k := 1 - 1/(1 + k)^2$ to simplify the notations. 
        Continue the simplifications of the above inequality
        {\footnotesize
        \begin{align*}
            F(x_k) - F^* +
            \frac{\alpha_k^2\kappa}{2}\left(
                1 - \frac{1}{(k + 1)^2}
            \right)
            \Vert v_k - x^*\Vert^2
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F^*)
            + 
            \frac{\alpha_k^2\kappa}{2}
            \Vert x^* - v_{k - 1}\Vert^2
            \\
            \iff 
            \alpha_k^{-2}(F(x_k) - F^*)
            + 
            \frac{\kappa A_k}{2}\Vert v_k - x^*\Vert^2
            &\le 
            \alpha_k^{-2}(1 - \alpha_k)(F(x_{k - 1}) - F^*)
            + 
            \frac{\kappa}{2}
            \Vert x^* - v_{k - 1}\Vert^2
            \\
            \iff
            \alpha_k^{-2}(F(x_k) - F^*)
            + 
            \frac{\kappa A_k}{2}\Vert v_k - x^*\Vert^2
            &\le 
            \alpha_{k - 1}^{-2}(F(x_{k - 1}) - F^*)
            + 
            \frac{\kappa}{2}
            \Vert x^* - v_{k - 1}\Vert^2
            \\
            & \le 
            \frac{1}{A_{k - 1}}\left(
                \alpha_{k - 1}^{-2}(F(x_{k - 1}) - F^*)
                + 
                \frac{\kappa A_{k - 1}}{2}
                \Vert x^* - v_{k - 1}\Vert^2
            \right). 
        \end{align*}
        }
        The second last inequality uses the fact that $(1 - \alpha_k)/\alpha_k^2 = \alpha_{k - 1}^{-2}$ and $\alpha_1 = 1$. 
        The last inequality used the fact that $A_{k - 1} \in (0, 1]$. 
        Simplifying a bit the above is the same as for all $k\ge 1$: 
        \begin{align*}
            \alpha_{k + 1}^{-2}(F(x_{k + 1}) - F^*) 
            + \frac{\kappa A_k}{2}\Vert v_k - x^*\Vert^2
            &\le 
            \frac{1}{A_k}\left(
                \alpha_k^{-2}(F(x_k) - F^*) + \frac{\kappa A_k}{2}\Vert v_{k} - x^*\Vert^2
            \right)
            \\
            & \le
            \left(
                \prod_{i = 1}^k A_i^{-1}
            \right)\left(
                \underbrace{
                    \alpha_1^2 (F(x_1) - F^*) + \frac{\kappa A_1}{2}\Vert v_1 - x^*\Vert^2
                }
                _{=:C}
            \right)
            \\
            \implies 
            \alpha_{k + 1}^{-2}(F(x_{k + 1}) - F^*)
            &\le 
            \left(
                \prod_{i = 1}^k A_i^{-1}
            \right) C
            \\
            F(x_{k + 1}) - F^* 
            &\le 
            \alpha_{k + 1}^2\left(
                \prod_{i = 1}^k A_i^{-1}
            \right) C. 
        \end{align*}
        Fortunately we have the big product bounded because 
        \begin{align*}
            \prod_{i = 1}^{k} A_{j}^{-1}
            &= 
            \prod_{i = 1}^{k} \left(
                1 - \frac{1}{(i + 1)^2}
            \right)^{-1}
            \\
            &= \left(
                \prod_{i = 1}^{k} \left( \frac{(i + 1)^2 - 1}{(i + 1)^2}\right)
            \right)^{-1} = 
            \left(
                \prod_{j = 2}^{k} 
                \left(
                    \frac{j^2 - 1}{j^2}
                \right)
            \right)^{-1}
            \\
            & = \exp\left(
                \sum_{j = 2}^{k + 1}
                \log\left(
                    \frac{j + 1}{j}
                \right) - \log\left(
                    \frac{j}{j - 1}
                \right)
            \right)^{-1}
            \\
            &= \left(\exp\circ \log
                \left(\frac{k + 3}{k + 2}\frac{1}{2}\right)\right)^{-1} 
            \le \left(
                \frac{1}{2}
            \right)^{-1}= 2. 
        \end{align*}
        Therefore, 
        \begin{align*}
            F(x_{k + 1}) - F^* \le \alpha_{k + 1}^2 2C\le \frac{4C}{(k + 1)^2}. 
        \end{align*}
        Which indicates that under controlled inexact evaluations of the inexact proximal step for $\tilde x_k$ and $\bar x_k$, the basic 4WD Catalyst achieves optimal convergence when $F$ is convex, and it can minimize the limiting subgradient when the function is weakly convex. 
        \begin{remark}
            It's tempting to think whether KL conditions assists with convergence to a stationary point for Nesterov's accelerated gradient method. 
            Surveys conducted by the writer in the literatures showed that this remains a huge mystery in general. 
        \end{remark}

\end{document}
