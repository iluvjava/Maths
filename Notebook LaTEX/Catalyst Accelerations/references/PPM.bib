
@misc{ahn_understanding_2022,
	title = {Understanding nesterov's acceleration via proximal point method},
	url = {http://arxiv.org/abs/2005.08304},
	doi = {10.48550/arXiv.2005.08304},
	abstract = {The proximal point method ({PPM}) is a fundamental method in optimization that is often used as a building block for designing optimization algorithms. In this work, we use the {PPM} method to provide conceptually simple derivations along with convergence analyses of different versions of Nesterov's accelerated gradient method ({AGM}). The key observation is that {AGM} is a simple approximation of {PPM}, which results in an elementary derivation of the update equations and stepsizes of {AGM}. This view also leads to a transparent and conceptually simple analysis of {AGM}'s convergence by using the analysis of {PPM}. The derivations also naturally extend to the strongly convex case. Ultimately, the results presented in this paper are of both didactic and conceptual value; they unify and explain existing variants of {AGM} while motivating other accelerated methods for practically relevant settings.},
	number = {{arXiv}:2005.08304},
	publisher = {{arXiv}},
	author = {Ahn, Kwangjun and Sra, Suvrit},
	urldate = {2023-11-04},
	date = {2022-06-02},
	eprinttype = {arxiv},
	eprint = {2005.08304 [cs, math]},
	keywords = {Computer Science - Machine Learning, Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/PZBWUWW5/Ahn and Sra - 2022 - Understanding Nesterov's Acceleration via Proximal.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/WSSGK9Q4/2005.html:text/html},
}

@article{guler_new_1992,
	title = {New Proximal Point Algorithms for Convex Minimization},
	volume = {2},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/0802032},
	doi = {10.1137/0802032},
	abstract = {The proximal point algorithm ({PPA}) for the convex minimization problem minx∈Hf(x), where f:H→R∪\{∞\} is a proper, lower semicontinuous (lsc) function in a Hilbert space H is considered. Under this minimal assumption on f, it is proved that the {PPA}, with positive parameters \{λk\}k=1∞, converges in general if and only if σn=∑k=1nλk→∞. Global convergence rate estimates for the residual f(xn)−f(u), where xn is the nth iterate of the {PPA} and u∈H is arbitrary are given. An open question of Rockafellar is settled by giving an example of a {PPA} for which xn converges weakly but not strongly to a minimizes of f.},
	pages = {649--664},
	number = {4},
	journaltitle = {{SIAM} Journal on Optimization},
	shortjournal = {{SIAM} J. Optim.},
	author = {Güler, Osman},
	urldate = {2023-11-30},
	date = {1992-11},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Güler - 1992 - New Proximal Point Algorithms for Convex Minimization.pdf:/Users/hongdali/Zotero/storage/G9LCY67Q/Güler - 1992 - New Proximal Point Algorithms for Convex Minimization.pdf:application/pdf},
}

@misc{davis_proximally_2018,
	title = {Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex Problems},
	url = {http://arxiv.org/abs/1707.03505},
	abstract = {In this paper, we introduce a stochastic projected subgradient method for weakly convex (i.e., uniformly prox-regular) nonsmooth, nonconvex functions---a wide class of functions which includes the additive and convex composite classes. At a high-level, the method is an inexact proximal point iteration in which the strongly convex proximal subproblems are quickly solved with a specialized stochastic projected subgradient method. The primary contribution of this paper is a simple proof that the proposed algorithm converges at the same rate as the stochastic gradient method for smooth nonconvex problems. This result appears to be the first convergence rate analysis of a stochastic (or even deterministic) subgradient method for the class of weakly convex functions.},
	number = {{arXiv}:1707.03505},
	publisher = {{arXiv}},
	author = {Davis, Damek and Grimmer, Benjamin},
	urldate = {2023-11-30},
	date = {2018-09-17},
	eprinttype = {arxiv},
	eprint = {1707.03505 [cs, math]},
	keywords = {Computer Science - Machine Learning, Optimization and Control},
	file = {arXiv.org Snapshot:/Users/hongdali/Zotero/storage/QTSZS2GN/1707.html:text/html;Full Text PDF:/Users/hongdali/Zotero/storage/3MDHZ789/Davis and Grimmer - 2018 - Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex Problems.pdf:application/pdf},
}

@misc{drusvyatskiy_proximal_2017,
	title = {The proximal point method revisited},
	url = {http://arxiv.org/abs/1712.06038},
	doi = {10.48550/arXiv.1712.06038},
	abstract = {In this short survey, I revisit the role of the proximal point method in large scale optimization. I focus on three recent examples: a proximally guided subgradient method for weakly convex stochastic approximation, the prox-linear algorithm for minimizing compositions of convex functions and smooth maps, and Catalyst generic acceleration for regularized Empirical Risk Minimization.},
	number = {{arXiv}:1712.06038},
	publisher = {{arXiv}},
	author = {Drusvyatskiy, Dmitriy},
	urldate = {2023-11-30},
	date = {2017-12-16},
	eprinttype = {arxiv},
	eprint = {1712.06038 [math]},
	keywords = {Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/hongdali/Zotero/storage/854FFU77/Drusvyatskiy - 2017 - The proximal point method revisited.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/NWYP3NMX/1712.html:text/html},
}

@article{rockafellar_monotone_1976,
	title = {Monotone Operators and the Proximal Point Algorithm},
	volume = {14},
	issn = {0363-0129, 1095-7138},
	url = {http://epubs.siam.org/doi/10.1137/0314056},
	doi = {10.1137/0314056},
	pages = {877--898},
	number = {5},
	journaltitle = {{SIAM} Journal on Control and Optimization},
	shortjournal = {{SIAM} J. Control Optim.},
	author = {Rockafellar, R. Tyrrell},
	urldate = {2023-11-06},
	date = {1976-08},
	langid = {english},
	file = {Monotone Operators and the Proximal Point Algorithm.pdf:/Users/hongdali/Zotero/storage/5L82ZWY4/Monotone Operators and the Proximal Point Algorithm.pdf:application/pdf},
}

@article{guler_convergence_1991,
	title = {On the Convergence of the Proximal Point Algorithm for Convex Minimization},
	volume = {29},
	rights = {[Copyright] © 1991 Society for Industrial and Applied Mathematics},
	issn = {03630129},
	url = {https://www.proquest.com/docview/925962166/abstract/A60B4BA7798A45D1PQ/1},
	doi = {10.1137/0329022},
	abstract = {The proximal point algorithm ({PPA}) for the convex minimization problem \${\textbackslash}min \_\{x {\textbackslash}in H\} f(x)\$, where \$f:H {\textbackslash}to R {\textbackslash}cup {\textbackslash}\{ {\textbackslash}infty {\textbackslash}\} \$ is a proper, lower semicontinuous (lsc) function in a Hilbert space \$H\$ is considered. Under this minimal assumption on \$f\$, it is proved that the {PPA}, with positive parameters \${\textbackslash}\{ {\textbackslash}lambda \_k {\textbackslash}\} \_\{k = 1\}{\textasciicircum}{\textbackslash}infty \$, converges in general if and only if \${\textbackslash}sigma \_n = {\textbackslash}sum\_\{k = 1\}{\textasciicircum}n \{{\textbackslash}lambda \_k {\textbackslash}to {\textbackslash}infty \} \$. Global convergence rate estimates for the residual \$f(x\_n ) - f(u)\$, where \$x\_n \$ is the \$n\$th iterate of the {PPA} and \$ u {\textbackslash}in H \$ is arbitrary are given. An open question of Rockafellar is settled by giving an example of a {PPA} for which \$x\_n \$ converges weakly but not strongly to a minimizes of \$f\$.},
	pages = {17},
	number = {2},
	journaltitle = {{SIAM} Journal on Control and Optimization},
	author = {Guler, Osman},
	urldate = {2024-05-18},
	date = {1991-03},
	note = {Num Pages: 17
Place: Philadelphia, United States
Publisher: Society for Industrial and Applied Mathematics},
	keywords = {Algorithms, Convex analysis, Hilbert space, Mathematics},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/9AWZSLK4/Guler - 1991 - On the Convergence of the Proximal Point Algorithm.pdf:application/pdf},
}

@article{attouch_convergence_2009,
	title = {On the convergence of the proximal algorithm for nonsmooth functions involving analytic features},
	volume = {116},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-007-0133-5},
	doi = {10.1007/s10107-007-0133-5},
	abstract = {We study the convergence of the proximal algorithm applied to nonsmooth functions that satisfy the Łjasiewicz inequality around their generalized critical points. Typical examples of functions complying with these conditions are continuous semialgebraic or subanalytic functions. Following Łjasiewicz’s original idea, we prove that any bounded sequence generated by the proximal algorithm converges to some generalized critical point. We also obtain convergence rate results which are related to the flatness of the function by means of Łjasiewicz exponents. Apart from the sharp and elliptic cases which yield finite or geometric convergence, the decay estimates that are derived are of the type O(k−s), where s ∈ (0, + ∞) depends on the flatness of the function.},
	pages = {5--16},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Attouch, Hedy and Bolte, Jérôme},
	urldate = {2024-04-16},
	date = {2009-01-01},
	langid = {english},
	keywords = {47N10, 90C26, 90C30, Łjasiewicz inequality, Proximal algorithm, Subanalytic functions},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/AGIWHMYN/Attouch and Bolte - 2009 - On the convergence of the proximal algorithm for nonsmooth functions involving analytic features.pdf:application/pdf},
}

@article{eckstein_douglasrachford_1992,
	title = {On the Douglas—Rachford splitting method and the proximal point algorithm for maximal monotone operators},
	volume = {55},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/BF01581204},
	doi = {10.1007/BF01581204},
	abstract = {This paper shows, by means of an operator called asplitting operator, that the Douglas—Rachford splitting method for finding a zero of the sum of two monotone operators is a special case of the proximal point algorithm. Therefore, applications of Douglas—Rachford splitting, such as the alternating direction method of multipliers for convex programming decomposition, are also special cases of the proximal point algorithm. This observation allows the unification and generalization of a variety of convex programming algorithms. By introducing a modified version of the proximal point algorithm, we derive a new,generalized alternating direction method of multipliers for convex programming. Advances of this sort illustrate the power and generality gained by adopting monotone operator theory as a conceptual framework.},
	pages = {293--318},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Mathematical Programming},
	author = {Eckstein, Jonathan and Bertsekas, Dimitri P.},
	urldate = {2024-04-09},
	date = {1992-04-01},
	langid = {english},
	keywords = {Monotone operators, decomposition, proximal point algorithm},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/QUJX7JKM/Eckstein and Bertsekas - 1992 - On the Douglas—Rachford splitting method and the proximal point algorithm for maximal monotone opera.pdf:application/pdf},
}

@article{attouch_convergence_2013,
	title = {Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward–backward splitting, and regularized Gauss–Seidel methods},
	volume = {137},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-011-0484-9},
	doi = {10.1007/s10107-011-0484-9},
	shorttitle = {Convergence of descent methods for semi-algebraic and tame problems},
	abstract = {In view of the minimization of a nonsmooth nonconvex function f, we prove an abstract convergence result for descent methods satisfying a sufficient-decrease assumption, and allowing a relative error tolerance. Our result guarantees the convergence of bounded sequences, under the assumption that the function f satisfies the Kurdyka–Łojasiewicz inequality. This assumption allows to cover a wide range of problems, including nonsmooth semi-algebraic (or more generally tame) minimization. The specialization of our result to different kinds of structured problems provides several new convergence results for inexact versions of the gradient method, the proximal method, the forward–backward splitting algorithm, the gradient projection and some proximal regularization of the Gauss–Seidel method in a nonconvex setting. Our results are illustrated through feasibility problems, or iterative thresholding procedures for compressive sensing.},
	pages = {91--129},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Attouch, Hedy and Bolte, Jérôme and Svaiter, Benar Fux},
	urldate = {2024-04-04},
	date = {2013-02-01},
	langid = {english},
	keywords = {34G25, 47J25, 47J30, 47J35, 49M15, 49M37, 65K15, 90C25, 90C53, Alternating minimization, Block-coordinate methods, Descent methods, Forward–backward splitting, Iterative thresholding, Kurdyka–Łojasiewicz inequality, Nonconvex nonsmooth optimization, o-minimal structures, Proximal algorithms, Relative error, Semi-algebraic optimization, Sufficient decrease, Tame optimization},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/LI9KFZWV/Attouch et al. - 2013 - Convergence of descent methods for semi-algebraic and tame problems proximal algorithms, forward–ba.pdf:application/pdf},
}

@inproceedings{lin_universal_2015,
	title = {A Universal Catalyst for First-Order Optimization},
	url = {https://inria.hal.science/hal-01160728},
	abstract = {We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, {SAG}, {SAGA}, {SDCA}, {SVRG}, Finito/{MISO}, and their proximal variants. For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill conditioned problems where we measure significant improvements.},
	eventtitle = {{NIPS} - Advances in Neural Information Processing Systems},
	pages = {3384},
	publisher = {{MIT} Press},
	author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
	urldate = {2024-05-31},
	date = {2015-12-07},
	langid = {english},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/8ILQU6TZ/Lin et al. - 2015 - A Universal Catalyst for First-Order Optimization.pdf:application/pdf},
}

@article{kim_accelerated_2021,
	title = {Accelerated proximal point method for maximally monotone operators},
	volume = {190},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-021-01643-0},
	doi = {10.1007/s10107-021-01643-0},
	abstract = {This paper proposes an accelerated proximal point method for maximally monotone operators. The proof is computer-assisted via the performance estimation problem approach. The proximal point method includes various well-known convex optimization methods, such as the proximal method of multipliers and the alternating direction method of multipliers, and thus the proposed acceleration has wide applications. Numerical experiments are presented to demonstrate the accelerating behaviors.},
	pages = {57--87},
	number = {1},
	journaltitle = {Mathematical Programming},
	shortjournal = {Math. Program.},
	author = {Kim, Donghwan},
	urldate = {2024-06-01},
	date = {2021-11-01},
	langid = {english},
	keywords = {49M25, 68Q25, 90C22, 90C25, 90C30, 90C60, Acceleration, Maximally monotone operators, Proximal point method, Worst-case performance analysis},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/TN2MXVQ9/Kim - 2021 - Accelerated proximal point method for maximally monotone operators.pdf:application/pdf},
}
