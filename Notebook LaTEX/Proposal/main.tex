\documentclass[12pt]{article}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% === TEMPLATE HOW TO USE ======
% To begine here is a list of things: 
% [ ]: Change title. 
% [ ]: Fill into the author names, affiliation and contact info. 
% [ ]: Fill in the abstract. 
% [ ]: Fill/change in AMS mathematics subject classification code, and the keywords. 

\title{
    {
        \fontfamily{ptm}\selectfont 
        First Order Nonsmooth Optimization: 
        Algorithm Design, Variational analysis, and Applications
    }
}
\author{
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. 
    % E-mail:  \texttt{alto@mail.ubc.ca}.}~Hongda Li
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. 
    % E-mail:  \texttt{shawn.wang@ubc.ca}.}~ and~Author Name 2
    Hongda Li\\[3ex]\\ Department of Mathematics\\
	University of British Columbia,\\
	Okanagan Campus.
}
\date{\today}

\begin{document}
% \vspace{10ex}
% \vskip 8mm

\maketitle
\tableofcontents
\pagebreak



% \begin{abstract} 
%     \noindent
%     The research proposal focuses on the theories and practice in solving nonsmooth optimization. 
%     The theme of proposal highlight topics of interests that emphasize the computations and applications aspect of algorithms that exhibits both practical and theoretical importance. 
%     We summarize our ongoing research in unifying Nesterov type accelerated proximal gradient method and proposes our Free R-WAPG method. 
%     We survey literatures under the topic of Catalyst Meta Acceleration framework used in accelerating variance reduced methods in the settings of Data Science and Machine Learning. 
%     Furthermore, we present literatures and progress in topics such as Performance Estimation Problem, Inexact Proximal Point, acceleration without convexity. 
%     At the end there is a section summarizing a method we developed for tree species classifications using Sentinel-2 satellite remote sensing data using big data analytics by extract spectral signatures of ground vegetation covers. 


% \end{abstract}
% \noindent{\bfseries 2010 Mathematics Subject Classification:}
% Primary 65K10, 90c25, 90C30; Secondary 65Y20. 
% \noindent{\bfseries Keywords: } Non-convex Optimizations, Proximal Point. 


\section{Introduction}
    Let $\RR^n$ be the ambient space. We consider 
    \begin{align}
        \min_{x \in \RR^n} \left\lbrace
            F(x): f(x) + g(x)
        \right\rbrace.
    \end{align}\label{eqn:additive-comp-obj}
    Unless specified, assume $f:\RR^n \rightarrow \RR$ is $L$-Lipscthiz smooth $\mu \ge 0$ strongly convex and $g:Q \rightarrow \overline \RR$ is convex. 
    This type of problem is referred to as additive composite problems in the literature. 
    \par
    Our ongoing research concerns accelerated proximal gradient type method for solving (\ref{eqn:additive-comp-obj}). 
    In the expository writing by Walkington \cite{noel_nesterovs_nodate}, a variant for of accelerated gradient method for strongly convex function $f$ is discussed. 
    We had two lingering questions after reading it. 
    \begin{enumerate}
        \item Do there exist a unified description for the convergence for both variants of the algorithms?
        \item Is it possible to attain faster convergence rate without knowledge about the strong convexity of function $f$?
        \item Is it possible to describe the convergence of function value for momentum sequences that are much weaker than the Nesterov's rule? 
    \end{enumerate}
    The good news is we have definitive answers for all questions by our own efforts of research. 
    Section \ref{sec:unify-nes-acceleration}, \ref{sec:spectral-momentum} are our ongoing research which present the answers to the questions. 
    \par
    In Section \ref{sec:unify-nes-acceleration}, we proposed the method of ``Relaxed Weak Accelerated Proximal Gradient (R-WAPG)'' as the foundation to describe several variants of Accelerated proximal gradient method in the literatures. 
    The convergence theories of R-WAPG allows us to model convergence of accelerated proximal gradient method where the momentum sequence doesn't strictly follow the conditions presented in the literatures. 
    The descriptive power of R-WAPG allows convergence analysis for all the variants using one single theorem. 
    \par
    In Section \ref{sec:spectral-momentum} we propose a practical algorithm that exploits a specific term in the proof of R-WAPG to achieve faster convergence for solving (\ref{eqn:additive-comp-obj}) without knowing parameter $L, \mu$ in prior. 
    Results of numerical experiments are presented. 
    \par
    Section \ref{sec:catalyst} are results of literatures review in MATH 590. 
    \todo{$\square$ Add Lin's papers and Paquette's papers.  }{\hl{It's based on a series of papers}} in the topic of Catalyst Meta Acceleration method for First Order Variance Reduced Methods. 
    We will point out potential future direction of research of Catalyst acceleration. 
    \par
    Section \ref{sec:pep}, \ref{sec:inexact-prox}, \ref{sec:nes-acc-ncnvx} preview literatures in  nonsmooth optimization frontier research where progress and impacts can be made.  
    \subsection{Theme of the research}
        This section specifies a theme of the research in this proposal. 
        Out first objective is to explore the Goldilocks zones between these topics: theories of variational analysis, design of continuous optimization algorithm and applications in sciences, engineering, and statistics.  
        Our second objective is to identify the ``chemistry'' occurring between properties of functions and the designs of continuous optimizations algorithm and how it impacts the convergence and behaviors of the algorithms. 


    
\section{Preliminaries}
    \todoinline{
        Clarify: Notations,
        Organizations. 
    }
    This section contains the basics of contents from convex optimization, and variational analysis. 
    \par
    Notations. 
    \begin{enumerate}
        \item $\overline \RR := \RR \cup \{\infty, -\infty\}$
    \end{enumerate}
    \subsection{Fundamentals in non-convex analysis}
        \todoinline{\noindent
            We are in $\RR^n$, and the weakest assumption we are making for the objective function is Local Lipschitz continuity. 
            Definitions: 
            \begin{enumerate}
                \item[$\blacksquare$] Local Lipschitz continuity. 
                \item[$\square$]Regular subgradient. Remember to cite. 
                \item[$\square$] Limiting subgradient. Remember to cite. 
                \item[$\blacksquare$] Weakly convex function. 
                \item[$\blacksquare$]The Bregman Divergence of function. 
            \end{enumerate}
            Take Limiting, Regular subgradient definitions from Cui, Pong's book, Definition 4.3.1. 
        }
        Let the ambient space be $\RR^n$ equipped with inner product and 2-norm. 
        Let $O$ be an open subset of $\RR^n$, the weakest assumption we are making for the objective function $F: O \subseteq \RR^n \rightarrow \RR$ for optimization problem is Local Lipschitz Continuity. 
        The assumption of local Lipschitz continuity is weak enough to describe most problems in applications, and strong enough to avoid most pathologies in analysis. 

        \begin{definition}[Local Lipschitz continuity]
            Let $F: O \subseteq \RR^n \rightarrow \RR$ be Locally Lipschitz and $O$ is an open set. 
            Then for all $\bar x \in O$, there exists a Neighborhood: $\mathcal N(\bar x)$ and $K \in \RR$ such that for all $x, y \in \mathcal N(\bar x)$: $|F(x) - F(y)| \le K \Vert x - y\Vert$. 
        \end{definition}
        \begin{definition}[Regular subgradient]
            Let $F: O \subseteq \RR^n \rightarrow \RR$ be locally Lipschitz and $\bar x \in O$. 
            The regular subdifferential at $\bar x$ is defined as 
            \begin{align*}
                \widehat \partial F(\bar x) := 
                \left\lbrace
                    v \in \RR^n \left| \; 
                        \liminf_{\bar x \neq x\rightarrow \bar x}
                        \frac{F(x) - F(\bar x) - \langle v, x - \bar x\rangle}{\Vert x - \bar x\Vert} 
                        \ge 0
                    \right.
                \right\rbrace. 
            \end{align*}
        \end{definition}
        \begin{definition}[Limiting subgradient]
            Let $F: O \subseteq \RR^n \rightarrow \RR$ be locally Lipschitz and $\bar x \in O$. 
            The limiting subdifferential at $\bar x$ is defined as 
            \begin{align*}
                \partial F(\bar x) := 
                \left\lbrace
                    v \in \RR^n \left| \; 
                        \exists x_k \rightarrow \bar x, v_k \rightarrow v: 
                        v_k \in \widehat \partial F(x_k) \;\forall k \in \mathbb N
                    \right.
                \right\rbrace. 
            \end{align*}
        \end{definition}
        \begin{definition}[Weakly convex function]
            $F: \RR^n \rightarrow \overline \RR$ is $\mu$ weakly convex if and only if $F + \frac{\mu}{2}\Vert \cdot\Vert^2$ is convex. 
        \end{definition}
        \begin{definition}[Bregman divergence]
            Let $F: O \subseteq \RR^n \rightarrow \RR$ be a differentiable function. 
            Then the Bregman divergence of $F$ is defined as: 
            \begin{align*}
                D_F(x, y): O \times \dom(\partial F) \rightarrow \RR
                := F(x) - F(y) - \langle \nabla F(y), x - y\rangle. 
            \end{align*}
        \end{definition}
    \subsection{Fundamentals in convex analysis}
        \todoinline{
            Introduce 
            \begin{enumerate}
                \item[$\blacksquare$] Convexity, 
                \item[$\blacksquare$] convex subgradient, 
                \item[$\blacksquare$] Lipschitz smoothness. 
            \end{enumerate}
            Definitions: 
            \begin{enumerate}
                \item [$\blacksquare$]Strong convexity of a function. 
                \item [$\blacksquare$]The proximal gradient operator. 
                \item [$\blacksquare$]The proximal mapping operator. 
            \end{enumerate}
            Lemmas: 
            \begin{enumerate}
                \item [$\blacksquare$]Quadratic growth conditions of a strongly convex function. 
            \end{enumerate}
        }
        This section introduces the classics and basics of convex analysis. 
        Define $F$ to be closed, proper and convex in this section. 
        When $F$ is convex, the limiting subgradient and the regular subgradient reduced to the following definition:
        \begin{align*}
            \partial F(x) := \left\lbrace
                v \in \RR^n \left|\; 
                    \forall y \in \RR^n\; :  F(y) - F(x)  \ge \langle v, y - x\rangle
                \right.
            \right\rbrace. 
        \end{align*}
        A convex function is locally Lipschitz in the relative interior of its domain, denoted as $\reli(\dom(F))$. 
        So it has $\reli(\dom F)\subseteq \dom(\partial F) \subseteq \dom F$. 
        \par
        When we say $F:\RR^n \rightarrow \RR$ is $L$ Lipschitz smooth function, it means that there exists $L$ such that for all $x\in \RR^n, y \in \RR^n$, it has: 
        \begin{align*}
            \Vert \nabla F(x) - \nabla F(y)\Vert \le L \Vert x - y\Vert. 
        \end{align*}
        This condition is stronger than differentiability. 
        When $F$ convex, it has descent lemma: 
        \begin{align*}
            (\forall x \in \RR^n)(\forall y \in \RR^n): 0 \le 
            F(x) - F(y) - \langle \nabla f(y), x - y\rangle \le \frac{L}{2}\Vert x - y\Vert^2. 
        \end{align*}
        When $F$ is convex, the converse holds. 
        The definitions that follow narrow things further for future discussions. 
        \begin{definition}[Strong convexity]
            A function $F:\RR^n \rightarrow \overline \RR$ is $\mu \ge 0$ strongly convex if and only if for any fixed $y \in \dom(\partial F)$, we have for all $x\in \RR^n$: 
            \begin{align*}
                (\forall v \in \partial F(x))\quad 
                F(x) - F(y) \ge \langle v, x - y\rangle + \frac{\mu}{2}\Vert x - y\Vert^2. 
            \end{align*}
        \end{definition}
        \begin{lemma}[Quadratic growth from strong convexity]
            If $F$ is $\mu \ge 0$ strongly convex, $\bar x$ is a minimizer of $F$. 
            Then for all $x \in \RR^n$
            \begin{align*}
                F(x) - F(\bar x) \ge \frac{\mu}{2}\Vert x - \bar x\Vert^2. 
            \end{align*}
        \end{lemma}
        \begin{remark}
            The minimizer is unique whenever $\mu > 0$. 
            For contradiction, assume $x$ is another minimizer, then $F(x) \neq F(\bar x)$, which is a direct contradiction. 
            The quadratic growth condition over a set of minimizer is much weaker than convexity. 
        \end{remark}
        
        \subsubsection{Smooth, nonsmooth additive composite}\label{sssec:additive-composite}
            \todoinline{\noindent
                Introduce notations for the proximal gradient model function. 
                Lemmas: 
                \begin{enumerate}
                    \item[$\blacksquare$] Proximal gradient envelope. 
                    \item[$\blacksquare$] A property of gradient mapping. 
                \end{enumerate}
                Theorems: 
                \begin{enumerate}
                    \item[$\blacksquare$] The proximal gradient inequality. 
                \end{enumerate}
            }

            In this section, we zoom in further. 
            Suppose that $F:= f + g$ where $f:\RR^n \rightarrow \RR$ is convex, $L$ Lipschitz smooth and $\mu \ge 0$ strongly convex and $g: \RR\rightarrow \overline \RR$ is convex. 
            To make the discussion simpler, fix any $\beta \ge 0$ we define the following model functions as a $\RR^n \times \RR^n \rightarrow \overline \RR$: 
            \begin{align*}
                \widetilde{\mathcal M}^{\beta^{-1}}
                (x; y)
                &:= 
                g(x) + f(y) + \langle \nabla f(y), x - y\rangle
                + \frac{\beta}{2}\Vert x - y\Vert^2,
                \\
                \mathcal M^{\beta^{-1}}(x; y) 
                &:= F(x) + \frac{\beta}{2}\Vert x - y\Vert^2.
            \end{align*}
            Under convexity assumption in this section, both $\widetilde {\mathcal M} (\cdot; y),  {\mathcal M}(\cdot;y )$ is at least $\beta \ge 0$ strongly convex. 
            \begin{definition}[Proximal gradient operator]
                Take $F := f + g$ where $g: \RR^n \rightarrow \overline \RR$ as defined in this section. 
                Define the proximal gradient operator $T_L$ on all $y \in \RR^n$: 
                \begin{align*}
                    T_L y := \argmin_{x \in \RR^n} \left\lbrace
                        g(x) + f(y) + \langle \nabla f(y), x - y\rangle 
                        + \frac{L}{2}\Vert x- y \Vert^2
                    \right\rbrace. 
                \end{align*}
            \end{definition}
            \begin{remark}
                Under the assumption of this section, the mapping $T_L$ is a single-valued mapping, it has domain on the entire $\RR^n$, and it's a $3/2$ averaged operator. 
            \end{remark}
            \begin{definition}[Gradient mapping operator]
                Take $F := f + g$ as defined in this section. 
                Define the gradient mapping operator $\mathcal G_L$ on all $y \in \RR^n$: 
                \begin{align*}
                    \mathcal G_L y:= L(y - T_L y). 
                \end{align*}
            \end{definition}

            \begin{lemma}[Proximal gradient model function]\;\\
                Take $\widetilde{\mathcal M}^{L^{-1}}, \mathcal M^{L^{-1}}$ as defined in this section, we will have for all $x \in \RR^n$ that: 
                \begin{align*}
                    \widetilde{\mathcal M}^{L^{-1}}(x; y)
                    &= 
                    \mathcal M^{L^{-1}}(x; y)- D_f(x, y).
                \end{align*}
            \end{lemma}
            \begin{lemma}[A favorable property of gradient mapping]
                Take $F:= f + g$ as defined in this section. 
                Fix any $x \in \RR^n$. 
                Then there exists $v \in \partial g(T_L x)$ such that $\mathcal G_L (x) = v + \nabla f(x)$. 
            \end{lemma}
            \begin{remark}
                This lemma still holds for non-convex $f$ under prox-boundedness and weak convexity and differentiability of $f$. 
            \end{remark}
            \begin{lemma}[The proximal gradient inequality]\label{thm:prox-grad-ineq}
                Take $F:= f + g$ as defined in this section. 
                Fix any $y \in \RR^n$, then for all $x$, the proximal gradient inequality is true: 
                \begin{align*}
                    (\forall x \in \RR^n)\quad 
                    h(x)  - h(Ty) - \langle L(y - Ty), x - y\rangle
                    - \frac{\mu}{2}\Vert x - y\Vert^2 - \frac{L}{2}\Vert y - Ty\Vert^2 
                    &\ge 0. 
                \end{align*}
            \end{lemma}
            \begin{remark}
                This lemma is proved in our draft paper. 
            \end{remark}

    \subsection{Nesterov's estimating sequence technique}
        \todoinline{\noindent
            Do the following: 
            \begin{enumerate}
                \item What is Nesterov's estimating sequence. 
                \item How is it used to derive the algorithm and convergence rate of algorithm. 
                \item Where is it used and why is it important here. 
            \end{enumerate}
        }
        \todoinline{
            Examples
            \begin{enumerate}
                \item Example estimating sequence. 
            \end{enumerate}
        }
        The method of Nesterov's estimating sequence for accelerated gradient method, and their nonsmooth counterparts assumes a convex function $F: \RR^n \rightarrow \overline{\RR}$. 
        The estimating sequence is a technique searching for candidate algorithm with extrapolated momentum, and proving their convergence rate if possible. 
        \par
        The method is widespread in the literatures, and the ideas behind it are tremendously useful. 
        \todo{$\square$ Add Guler's 1992 new proximal point paper. \\ $\square$ Cite it. }{\hl{Güler}} 
        used the method to design an accelerated proximal point method, which inspired and served as the foundation of Catalyst Acceleration for variance reduced method in machine learning. 
        \todo{$\square$ Add the paper: ``Accelerating the cubic regularization of Newton’s method on convex
        problems''\\ $\square$ Cite it.}{\hl{Nesterov}}
        also used the techinique to design an accelerated cubic regularized Newton's method. 
        In 
        \todo{$\square$ Cite it. }\hl{Chapter 6 of Nesterov's book}, it's also used to derive a method of accelerated mirror descent. 
        And finally, 
        \todo{$\square$ Add paper: ``Accelerated regularized newton methods for
        minimizing composite convex functions'' \\ $\square$ Cite it. }{\hl{Geovani N. et al}}
        used the technique to derive a accelerated Newton's method for convex composite objective function. 
        \par
        The definition of the estimating sequence that follows is based on our own understanding of the estimating sequence. 
        \begin{definition}[Nesterov's estimating sequence]\label{def:nes-est-seq}
            Let $\phi_k : \RR^n \rightarrow\RR$ for all $k \ge 0$ be a sequence of functions. 
            We call this sequence of function a Nesterov's estimating sequence when it satisfies the conditions: 
            \begin{enumerate}
                \item There exists another sequence $(x_k)_{k \ge 0}$ such that for all $k \ge 0$ it has $F(x_k) \le \phi_k^*: =\min_{x}\phi_k(x)$. 
                \item There exists a sequence of $(\alpha_k)_{k \ge 0}$ where $\alpha_k \in (0, 1)\; \forall k \ge0 $ such that for all $x \in \RR^n$ it has $\phi_{k + 1}(x) - \phi_k(x) \le - \alpha_k(\phi_k(x) - F(x))$. 
            \end{enumerate}
        \end{definition}
        \begin{observation}
            In general, identifying the sequence $(x_k)_{k \ge 0}$ is non-trivial. 
            But in case it can be found, the method of estimating sequence gives us the convergence rate described by the sequence $(\alpha_k)_{k \ge 0}$, and a candidate algorithm that generates the sequence $(x_k)_{k \ge 0}$. 
            It's two birds with one stone. 
            \par 
            If we define $\phi_k$, $\Delta_k(x) := \phi_k (x) - F(x)$ for all $x \in \RR^n$ and assume that $F$ has minimizer $x^*$. 
            Then observe that $\forall k \ge 0$:  
            \begin{align*}
                \Delta_k(x) 
                &= \phi_k(x) - F(x) \ge \phi_k^* - F(x)
                \\
                x = x_k\implies 
                \Delta_k(x_k) 
                &\ge 
                \phi_k^* - F(x_k) \ge 0;
                \\
                x = x_* \implies 
                \Delta_k(x_*)
                &\ge \phi_k^* - F_* \ge F(x_k) - F_* \ge 0. 
            \end{align*}
            The function $\Delta_k(x)$ is non-negative at points: $x_*, x_k$.
            We can derive the convergence rate of $\Delta_k(x^*)$ because $\forall x \in \RR^n$: 
            \begin{align*}
                \phi_{k + 1}(x) - \phi_k(x) 
                &\le - \alpha_k (\phi_k(x) - F(x))
                \\
                \iff 
                \phi_{k + 1}(x) - F(x) - (\phi_k(x) - F(x))
                &\le 
                -\alpha_k(\phi_k(x) - F(x))
                \\
                \iff
                \Delta_{k + 1}(x) - \Delta_k(x) &\le
                - \alpha_k\Delta_k(x)
                \\
                \iff 
                \Delta_{k + 1}(x) 
                &\le 
                (1 - \alpha_k)\Delta_k(x). 
            \end{align*}
            Unrolling the above recursion it yields: 
            \begin{align*}
                \Delta_{k + 1}(x) &\le 
                (1 - \alpha_k)\Delta_k(x) \le \cdots \le 
                \left(
                    \prod_{i = 0}^k(1 - \alpha_i)
                \right)\Delta_0(x). 
            \end{align*}
            Finally, by setting $x = x^*$, $\Delta_k(x^*)$ is non-negative and using the property of Nesterov's estimating sequence it gives: 
            \begin{align*}
                F(x_k) - F(x^*) \le \phi_k^* - F(x^*) \le \Delta_k(x^*) = \phi_k(x^*) - F(x^*) \le \left(\prod_{i = 0}^k(1 - \alpha_i)\right)\Delta_0(x^*).
            \end{align*} 

        \end{observation}
        

\section{Unifying NAG, and weakening the sequence assumption for convergences}\label{sec:unify-nes-acceleration}
    \todoinline{\noindent
        This section is really about stating the results of the draft paper and no proofs will be done here. 
        Along with the content of the draft paper, we will also explain the origin and inspirations of the ideas. 
    }
    This section is based on the theoretical aspects of our draft paper. 
    It will introduce major results and claims achieved during our research in each of the subsections. 
    All theorems and claims stated in this section have proofs in the draft paper. 
    The proofs haven't been carefully verified by authoratative people other than the author yet. 
    We will start introducing the context and ideas for our research next. 
    \par
    Assume we want to solve a convex optimization problem: $\min_{x \in \RR^n} \{F(x)\}$ and $F: \RR^n \rightarrow \RR$ is a $L$ Lipschitz smooth function. 
    We made this assumption for now for a faster exposition. 
    One of the prime candidate for solving the optimization problem is the Nesterov's Accelerated Gradient methods (NAG) finds extensions for nonsmooth function through the proximal gradient operator.
    \todo{$\square$ Add Nesterov's original paper. \\ $\square$ Cite it.  }{\hl{Proposed back in 1983 the original Nesterov's acceleration method}} 
    which uses the previous iterates to extrapolate the next iterate to evaluate the gradient. 
    It's well known that if minimizer $x^*$ exists for $F$, the method achieves a $\mathcal O(1/k^2)$ convergence rate on the objective value $F(x_k)$. 
    \todo{$\square$ Add Nesterov's new book. \\ $\square$ Cite it.}{\hl{This convergence rate is considered optimal for all class of $L$ Lipschitz smooth convex function}}. 
    The convergence rate guarantee is faster than $\mathcal O(1/k)$ exhibited by gradient descent. 
    \par
    We cover the algorithm briefly. 
    Initialize $x_1 = y_1$ and $t_0 = 1$, the algorithm finds $(x_k)_{k \ge 1}$ for all $k \ge 1$ by: 
    \begin{align}\label{eqn:nag_example}
        & x_{k + 1} = y_k - L^{-1}\nabla F(y_k), 
        \\
        & t_{k + 1} = 1/2\left(1 + \sqrt{1 + 4t_{k}^2}\right), 
        \\
        & \theta_{k + 1} = (t_{k} - 1)/t_{k + 1}, 
        \\
        & y_{k + 1} = x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k). 
    \end{align}
    Unfortunately, the algorithm sped up the convergence rate for all convex function, it becomes slower for the subset of $\mu > 0$ strongly convex function. 
    This drawback inspired a vast amount of literatures aims at improving, extending, and analyzing NAG. 
    Restarting is a popular solution to address the issue of obtaining faster convergence rate when the objective function is strongly convex. 
    \todo{$\square$ Add Beck 2009 FISTA original paper. $\square$ Cite it.}{\hl{Beck and Toubelle}} 
    mitigated the issue by restarting and showed that it still has a $\mathcal O(1/k^2)$ convergence rate, and it performs better empirically. 
    \todo{$\square$ Add Necoara linear convergence \\ $\square$ Add Aujol 2024 Parameter free FISTA restart. \\ $\square$ Cite Section 5.2.2 for the former, cite the entirety for the later. }{\hl{See}} 
    and references within for recent advancements in restarting accelerated proximal gradient algorithm.
    \par
    Restarting the algorithm is not the entire picture. 
    Let $F:\RR^n \rightarrow \RR$ be a $L$ Lipschitz smooth and $\mu > 0$ function. 
    As introduced previously, in 
    \todo{$\square$ Add Neol J. Walkington's ``Nesterov’s Method for Convex  Optimization''. \\ $\square$ Cite it. }{\hl{Walkington's writing}}, 
    he showed that there exists a variant of the Nesterov's accelerated gradient method that achieved a linear convergence rate of $\mathcal O((1 - \sqrt{\kappa})^k)$ where $\kappa = \mu/L$. 
    This convergence rate is strictly better than $\mathcal O((1 - \mu/L)^k)$ for the method of gradient descent. 
    However, this variant has a fixed momentum parameter $\theta_{k + 1} = (\sqrt{\kappa} - 1)(\sqrt{\kappa} + 1)^{-1}$ back in Equation \ref{eqn:nag_example}. 
    \todo{$\square$ Add Beck's first order textbook\\ $\square$ Cite 10.7.7. \\ $\square$ Add Nesterov's textbook. \\ $\square$ Cite it. }{\hl{The same variant also appears in Beck's book as V-FISTA, and Nesterov's book as (2.2.22).}} 
    \par
    One final Mystery of the algorithm is the convergence of the iterates which also has much to do with the momentum sequence $(\theta_k)_{k\ge 0}$ displayed in Equation \ref{eqn:nag_example}. 
    \todo{$\square$ Add paper: ``On the convergence of the iterates of the...''\\ $\square$ Cite them. }{\hl{Chambolle, Dossal}} 
    showed that by choosing sequence $(t_k)_{k \ge 1}$ to be $t_k = (n + a - 1)/a$ where $a > 2$ instead would give $(x_k)_{k \ge 0}$ weak convergence in Hilbert space. 
    It's put as an open question on what happens to the iterates when $a = 2$. 
    \par
    All of these seemingly raises a crucial question: ``Is it possible to describe something about the NAG algorithm for a set of sequence that is non-traditional?''; rephrasing it int a more technical manner: ``What is the weakest description of the momentum sequence $(\theta_k)$ such that we can still claim something of value about the NAG algorithm?''

    \subsection{Our Contributions, organizations}
        Our contributions are two folds, theoretical and practical. 
        The results are based the assumption $F = f + g$ where $g:R^n \rightarrow \overline\RR$ is convex, and $f$ is an $L$-Lipschitz smooth and $\mu \ge 0$ strongly convex function. 
        We relax the traditional choice of the sequence $\theta_k$ in Equation \ref{eqn:nag_example} and showed an upper bound of the optimal gap. 
        Let $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ be two sequences that satisfy
        \begin{align*}
            \alpha_0 &\in (0, 1], 
            \\
            \alpha_k &\in (\mu/L, 1) \quad (\forall k \ge 1), 
            \\
            \rho_k &:= \frac{\alpha_{k + 1}^2 - (\mu/L)\alpha_{k + 1}}{(1 - \alpha_{k + 1})\alpha_k^2} \quad \forall (k \ge 0). 
        \end{align*}
        Our first main result shows that if $\theta_{k + 1} = (\rho_k\alpha_k(1 - \alpha_k)/(\rho_k\alpha_k^2 + \alpha_{k + 1}))$, using the R-WAPG we proposed in Definition \ref{def:wapg} with Proposition \ref{prop:wagp-convergence}, \ref{prop:r-wapg-momentum-repr}, we can show that the gap $F(x_k) - F(x^*)$ is bounded by:
        \begin{align*}
            \mathcal O\left(
                \left(
                    \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
                \right)
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right). 
        \end{align*}
        Our second main result shows that there exists $\rho_k > 1$ such that our R-WAPG reduces to a variant of FISTA proposed in 
        \todo{$\square$ Add ``On the convergence of the iterates of...'' \\ $\square$ Cite them. }{\hl{Chambolle, Dossal}}, 
        and we are able to show the same convergence rate in Theorem \ref{thm:r-wapg-on-cham-doss}. 
        When $\rho_k = 1, \mu = 0$, R-WAPG reduces perfectly to FISTA by in 
        \todo{$\square$ Add beck's original FISTA Paper. \\$\square$ Cite it. }
        {\hl{Beck}}. 
        If $\mu > 0, \rho_k = 1$, it reduces to the V-FISTA by Beck \cite{beck_first-order_2017}. 
        In Theorem \ref{thm:fixed-momentum-fista}, it demonstrates that R-WAPG frameworks gives a linear convergence claim for all fixed momentum method where $\alpha_k := \alpha \in (\mu/L, 1)$ and  $F$ is $\mu > 0$ strongly convex. 
        \par
        Our practical contribution is an algorithm inspired by a detail in our convergence proof which we call it ``Parameter Free R-WAPG'' (See Algorithm \ref{alg:free-rwapg}). 
        The algorithm is parameter free, meaning that it doesn't require knowing $L, \mu$ in advance, and it determines the value of $\theta_t$ by estimating the local concavity using iterates $y_{k}, y_{k + 1}$ with minimal computational cost. 
        We conducted ample amount of numerical experiments to show that it has a favorable convergence rate in practice and behaves similarly to the FISTA with monotone restart. 
        \par
        Notations, and assumptions now follows. 
        For all the subsection that follows, we let $F:= f + g$ to take the same assumptions as in Section \ref{sssec:additive-composite}. 
        Recall $T_L$, $\mathcal G_L$ denotes the proximal gradient operator and the gradient mapping operator. 
        Additional notations are defined in the assumption below: 
        \begin{assumption}
            Choose any integer $k\ge 0$. 
            Given $x_k, y_k, v_k$, we define the following quantities
            \begin{align}
                g_k &\defeq L(y_k - T_L y_k), 
                \label{eqn:grad-map}
                \\
                l_F(x; y_k) &\defeq F(T_Ly_k) + \langle g_k, x - y_k\rangle + \frac{1}{2L}\Vert g_k\Vert^2, 
                \label{eqn:lower-linearization}
                \\
                \epsilon_{k} &\defeq F(x_k) - l_F(x_k; y_k), 
                \label{eqn:regret}
            \end{align}
            Observe that by convexity of $F$, $\epsilon_k \ge 0$ for all $x_k, L > 0$. 
            To see, use Theorem \ref{thm:prox-grad-ineq} and let $y = y_k, x = x_k$ which gives: 
            \begin{align*}
                F(x_k) - F(T_Ly_k)
                - \langle L(y_k - T_Ly_k),x_k - y_k \rangle
                - \frac{L}{2}\Vert y_k - T_Ly_k\Vert^2
                - \frac{\mu}{2}\Vert x_k - y_k\Vert^2
                &\ge 0
                \\
                \iff 
                F(x_k) - F(T_Ly_k)
                - \langle g_k,x_k - y_k \rangle
                - \frac{1}{2L}\Vert g_k\Vert^2
                &\ge 0. 
            \end{align*}
        \end{assumption}
        \par
        \todoinline{$\blacksquare$ Finish the organizations here after this section is finished. }

        Organization now follows. 
        Section \ref{ssec:building-block-rwapg} provides a stepwise description of the R-WAPG iterative algorithm along with an inequality crucial to proving the convergence rate later. 
        Section \ref{ssec:rwpag-seq-and-alg} introduce the definition of an R-WAPG sequence, which constraints all possible parameters used permitted by the algorithm.
        The section also states the full R-WAPG algorithm and an upper bound on $F(x_k) - F(x^*)$. 
        Here, $x_k$ is generated by the R-WAPG algorithm and $x^*$ is the minimizer. 
        Section \ref{ssec:quiv-repr-rwapg} bring forward three equivalent forms of the R-WAPG algorithm making it more comparable with other Acceerated Proximal Gradient method appeared in the literatures. 
        Section \ref{ssec:describe-variants-with-rwapg} gives characterizations of specific R-WAPG sequence that leads to convergence of the R-WAPG algorithm in terms of the optimality gap. 
        The section also identifies specific instance of permissible R-WAPG sequences where it fits with the FISTA, V-FISTA, and the algorithm proposed by 
        \todo{$\square$ Cite the Chambolle, Dossal 2015 paper here again. }{\hl{Chambolle Dossal}}. 

        
    \subsection{Building Blocks of R-WAPG}\label{ssec:building-block-rwapg}
        \todoinline{\noindent
            Definitions: 
            \begin{enumerate}
                \item[$\blacksquare$] R-WAPG stepwise definition.
                \item[$\blacksquare$] R-WAPG stepwise convergence claim.
            \end{enumerate}
            To do: 
            \begin{enumerate}
                \item[$\blacksquare$] Explain what is what. 
            \end{enumerate}
        }
        Definition\ref{def:stepwise-wapg} describes the procedures of generating the iterates $(v_{k + 1}, x_{k + 1})$ given any $(v_k, x_k)$ and parameter $\alpha_k \in (0, 1), \gamma_k > 0$. 
        Proposition \ref{prop:stepwise-lyapunov} gives an inequality instrumental to the convergence rate analysis, with the same assumption as the definition. 

        \begin{definition}[Stepwise weak accelerated proximal gradient]\label{def:stepwise-wapg}\;\\
            Assume $0 \le \mu < L$.
            Fix any $k \in \mathbb Z$. 
            For any $(v_k, x_k), \alpha_k \in (0, 1), \gamma_k > 0$, let $\hat \gamma_{k + 1}$, and vectors $y_k, v_{k + 1}, x_{k + 1}$ be given by: 
            \begin{align*}
                \hat \gamma_{k + 1} &= (1 - \alpha_k)\gamma_k + \mu \alpha_k, 
                \\
                y_k &= 
                (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k), 
                \\
                g_k &= \mathcal G_L y_k, 
                \\
                v_{k + 1} &= \hat\gamma^{-1}_{k + 1}
                (\gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
                \\
                x_{k + 1} &= T_L y_k. 
            \end{align*}
        \end{definition}

        \begin{proposition}[Stepwise Lyapunov]\label{prop:stepwise-lyapunov}\;\\
            Fix any integer $k \in \mathbb Z$.
            Given any $v_k, x_k$ and $\gamma_k > 0$, invoke Definition \ref{def:stepwise-wapg} to obtain $v_{k + 1}, x_{k + 1}, y_k, \hat \gamma_{k + 1}$. 
            Fix any arbitrary $R_k \in \RR$.
            Define: 
            \begin{align*}
                R_{k + 1}
                \defeq
                \frac{1}{2}\left(
                    L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
                \right)\Vert g_k\Vert^2
                + 
                (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                    \Vert v_k - y_k\Vert^2
                \right). 
            \end{align*}
            Then it has for all $x^* \in \RR^n$ where $F^* = F(x^*)$, the inequality: 
            \begin{align*}
                F(x_{k + 1}) - F^* + R_{k + 1} + \frac{\hat \gamma_{k + 1}}{2}\Vert v_{k + 1} - x^*\Vert^2
                &\le 
                (1 - \alpha_k)
                \left(
                    F(x_k) - F^* + R_k + \frac{\gamma_{k}}{2}\Vert v_k - x^*\Vert^2
                \right). 
            \end{align*}
        \end{proposition}

    \subsection{R-WAPG Sequence and R-WAPG algorithm}\label{ssec:rwpag-seq-and-alg}
        \todoinline{\noindent
            The R-WAPG Algorithm and convergence claim. 
            Definitions: 
            \begin{enumerate}
                \item[$\blacksquare$] R-WAPG Sequence.
                \item[$\blacksquare$] R-WAPG algorithm
                \item[$\blacksquare$] Convergence of the R-WAPG algorithm.
            \end{enumerate}
            Todo: 
            \begin{enumerate}
                \item[$\blacksquare$] Explain what is what. 
            \end{enumerate}
        }
        The Definition \ref{def:wapg} gives the definition of an iterative algorithm we called: Relaxed Weak Accelerated Proximal Gradient (R-WAPG) algorithm which generates sequence $(x_k, v_k)_{k \ge 1}$ using the R-WAPG sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ given in Defintion \ref{def:rwapg-seq}. 
        Proposition \ref{prop:wagp-convergence} shows the upper bound of the optimality gap $F(x_k) - F(x^*)$ with $(x_k)$ generated by the R-WAPG algorithm. 

        \begin{definition}[R-WAPG sequences]\label{def:rwapg-seq}\;\\
            Assume $0 \le \mu < L$. 
            The sequences $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge1}$ are sequences parameterized by $\mu, L$. 
            They are valid for R-WAPG if all the following holds: 
            \begin{align*}
                \alpha_0 &\in (0, 1], 
                \\
                \alpha_k &\in (\mu/L, 1) \quad (\forall k \ge 1), 
                \\
                \rho_k &:= \frac{\alpha_{k + 1}^2 - (\mu/L)\alpha_{k + 1}}{(1 - \alpha_{k + 1})\alpha_k^2} \quad \forall (k \ge 0). 
            \end{align*}
            We call $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ the \textbf{R-WAPG Sequences}. 
        \end{definition}
        \begin{definition}[Relaxed weak accelerated proximal gradient (R-WAPG)]\label{def:wapg}\;\\
            Choose any $x_1 \in \RR^n, v_1 \in \RR^n$. 
            Let $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ be given by Definition \ref{def:rwapg-seq}. 
            The algorithm generates a sequence of vector $(y_k, x_{k + 1}, v_{k + 1})_{k \ge 1}$ for $k\ge 1$ by the procedures:  
            \begin{tcolorbox}
                For $k=1, 2, 3, \cdots$
                \begin{align*}
                    \gamma_k &:= \rho_{k -1}L\alpha_{k - 1}^2, 
                    \\
                    \hat \gamma_{k + 1} & := (1 - \alpha_k)\gamma_k + \mu \alpha_k = L\alpha_k^2, 
                    \\
                    y_k &= 
                    (\gamma_k + \alpha_k \mu)^{-1}(\alpha_k \gamma_k v_k + \hat\gamma_{k + 1} x_k), 
                    \\
                    g_k &= \mathcal G_L y_k, 
                    \\
                    v_{k + 1} &= 
                    \hat\gamma^{-1}_{k + 1}
                    (\gamma_k(1 - \alpha_k) v_k - \alpha_k g_k + \mu \alpha_k y_k), 
                    \\
                    x_{k + 1} &= T_L y_k. 
                \end{align*}    
            \end{tcolorbox}
        \end{definition}
        \begin{proposition}[R-WAPG convergence claim]\label{prop:wagp-convergence}\; \\
            Fix any arbitrary $x^* \in \RR^n, N \in \mathbb N$. 
            Let vector sequence $(y_k, v_{k}, x_{k})_{k \ge 1}$ and R-WAPG sequences $\alpha_k, \rho_k$ be given by Definition \ref{def:wapg}. 
            Define $R_1 = 0$ and suppose that for $k = 1, 2, \cdots, N$, we have $R_k$ recursively given by: 
            \begin{align*}
                R_{k + 1}
                := 
                \frac{1}{2}\left(
                    L^{-1} - \frac{\alpha_k^2}{\hat \gamma_{k + 1}}
                \right)\Vert g_k\Vert^2
                + 
                (1 - \alpha_k)
                \left(
                    \epsilon_k + R_k + 
                    \frac{\mu\alpha_k\gamma_k}{2\hat \gamma_{k + 1}}
                    \Vert v_k - y_k\Vert^2
                \right). 
            \end{align*} 
            Then for all $k = 1, 2, \cdots, N$: 
            \begin{align*}
                & F(x_{k + 1}) - F(x^*) + \frac{L \alpha_k^2}{2}\Vert v_{k + 1} - x^*\Vert^2
                \\
                &\le 
                \left(
                    \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
                \right)
                \left(
                    \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
                \right)
                \left(
                    F(x_1) - F(x^*) + \frac{L\alpha_0^2}{2}\Vert v_1 - x^*\Vert^2
                \right). 
            \end{align*}
        \end{proposition}
        
    \subsection{Equivalent forms of R-WAPG algorithm}\label{ssec:quiv-repr-rwapg}
        \todoinline{
            Definitions:
            \begin{enumerate}
                \item[$\blacksquare$] R-WAPG Intermediate form.
                \item[$\blacksquare$] R-WAPG Similar triangle form. 
                \item[$\blacksquare$] R-WAPG Momentum form. 
            \end{enumerate}
        }
        \todoinline{
            Theorems: 
            \begin{enumerate}
                \item[$\blacksquare$] R-WAPG First equivalent form. 
                \item[$\blacksquare$] R-WAPG Second equivalent form.
                \item[$\blacksquare$] R-WAPG Third equivalent form.
            \end{enumerate}
            Lemmas
            Todo: 
            \begin{enumerate}
                \item Explain what is what. 
            \end{enumerate}
        }
        Definitions \ref{def:r-wapg-intermediate}, \ref{def:r-wapg-st-form} and \ref{def:r-wapg-momentum-form} are three equivalent representations of the R-WAPG algorithms. 
        Propositions \ref{prop:wapg-first-equivalent-repr}, \ref{prop:wagp-st-form} and \ref{prop:r-wapg-momentum-repr} states the equivalences between the forms and the sufficient conditions for initial conditions of $x_1, v_1$ such equivalence holds. 
        \todo{$\square$ Add and explain this part. }{\hl{
        The remarks of the definitions identifies specific instances in the literatures where the Accelerated Proximal Gradient method were presented under this specific form. 
        }}

        \begin{definition}[R-WAPG intermediate form]\label{def:r-wapg-intermediate}\;\\
            Assume $\mu < L$ and let $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ given by Definition \ref{def:rwapg-seq}. 
            Initialize any $x_1, v_1$ in $\RR^n$. 
            For $k \ge 1$, the algorithm generates sequence of vector iterates $(y_{k}, v_{k + 1}, x_{k + 1})_{k \ge 1}$ by the procedures: 
            \begin{tcolorbox}
                For $k = 1, 2, \cdots$
                \begin{align*} 
                    & y_{k} = 
                    \left(
                        1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                    \right)^{-1}
                    \left(
                        v_{k + 1} + 
                        \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                    \right), 
                    \\
                    & x_{k + 1} = 
                    y_k - L^{-1} \mathcal G_L y_k, 
                    \\
                    & v_{k + 1} = 
                    \left(
                        1 + \frac{\mu}{L \alpha_k - \mu}
                    \right)^{-1}
                    \left(
                        v_k + 
                        \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                    \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
                \end{align*}
            \end{tcolorbox}
        \end{definition}
        \begin{definition}[R-WAPG similar triangle form]\label{def:r-wapg-st-form} \; \\
            Given any $(x_1, v_1)$ in $\RR^n$. 
            Assume $\mu < L$.
            Let the sequence $(\alpha_k)_{k \ge 0}, (\rho_k)_{k\ge 0}$ be given by Definition \ref{def:rwapg-seq}. 
            For $k \ge 1$, the algorithm generates sequences of vector iterates $(y_k, v_{k + 1}, x_{k + 1})_{k \ge 1}$ by the procedures: 
            \begin{tcolorbox}
                For $k=1, 2, \cdots $
                \begin{align*}
                    & y_k = 
                    \left(
                        1 + \frac{L - L\alpha_k}{L\alpha_k - \mu}
                    \right)^{-1}
                    \left(
                        v_k + 
                        \left(\frac{L - L\alpha_k}{L\alpha_k - \mu} \right) x_k
                    \right), 
                    \\
                    & x_{k + 1} = 
                    y_k - L^{-1} \mathcal G_L y_k, 
                    \\
                    & v_{k + 1} = 
                    x_{k + 1} + (\alpha_k^{-1} -1)(x_{k + 1} - x_k). 
                \end{align*}    
            \end{tcolorbox}
        \end{definition}
        \begin{definition}[R-WAPG momentum form]\label{def:r-wapg-momentum-form}
            Given any $y_1 = x_1 \in \RR^n$, and sequences $(\rho_k)_{k \ge 0}, (\alpha_k)_{k\ge 0}$ Definition \ref{def:rwapg-seq}. 
            The algorithm generates iterates $x_{k + 1}, y_{k + 1}$ For $k = 1, 2, \cdots $ by the procedures: 
            \begin{tcolorbox}
                For $k=1, 2,\cdots $
                \begin{align*}
                    & x_{k + 1} = y_k - L^{-1}\mathcal G_Ly_k, 
                    \\
                    & 
                    y_{k + 1} = 
                    x_{k + 1} + 
                    \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
                \end{align*}    
            \end{tcolorbox}
            In the special case where $\mu = 0$, the momentum term can be represented without relaxation parameter $\rho_k$: 
            $$
                (\forall k \ge 1)\quad \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}} 
                = \alpha_{k + 1}(\alpha_k^{-1} - 1).  
            $$
        \end{definition}
        \begin{proposition}[First equivalent representation of R-WAPG]\label{prop:wapg-first-equivalent-repr}\;\\
            If the sequence $(y_k, v_k, x_k)_{k \ge 1}$ is produced by R-WAPG (Definition \ref{def:wapg}), 
            then the iterates can be expressed without $(\gamma_k)_{k \ge1},(\hat \gamma_k)_{k \ge 2}$, and for all $k\ge 1$ they are algebraically equivalent to
            \begin{align*}
                & 
                y_{k} = 
                \left(
                    1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                \right)^{-1}
                \left(
                    v_{k} + 
                    \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                \right), 
                \\
                & x_{k + 1} = 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                & v_{k + 1} = 
                \left(
                    1 + \frac{\mu}{L \alpha_k - \mu}
                \right)^{-1}
                \left(
                    v_k + 
                    \left(\frac{\mu}{L \alpha_k - \mu}\right) y_k
                \right) - \frac{1}{L\alpha_{k}}\mathcal G_L y_k. 
            \end{align*}
        \end{proposition}
        \begin{proposition}[Second equivalent representation of R-WAPG]\label{prop:wagp-st-form}\;\\
            Let iterates $(y_k, x_{k}, v_{k})_{k \ge 1}$ and sequence $(\alpha_k, \rho_k)_{k \ge 0}$ be given by Definition \ref{def:r-wapg-intermediate}. 
            Then for all $k \ge 1$, iterate $y_k, x_{k + 1}, v_{k + 1}$
            satisfy: 
            \begin{align*}
                y_{k} &= 
                \left(
                    1 + \frac{L - L\alpha_{k}}{L\alpha_{k} - \mu}
                \right)^{-1}
                \left(
                    v_{k} + 
                    \left(\frac{L - L\alpha_{k}}{L\alpha_{k} - \mu} \right) x_{k}
                \right), 
                \\
                x_{k + 1} &= 
                y_k - L^{-1} \mathcal G_L y_k, 
                \\
                v_{k + 1} &= x_{k + 1} + (\alpha_k^{-1} - 1)(x_{k + 1} - x_k). 
            \end{align*}
        \end{proposition}
        \begin{proposition}[Third equivalent representation of R-WAPG]\label{prop:r-wapg-momentum-repr}
            \;\\
            Let sequence $(\alpha_k, \rho_k)_{k \ge 0}$ and iterates $(x_k, v_k, y_k)_{k\ge 1}$ given by R-WAPG intermediate form (Definition \ref{def:r-wapg-st-form}). 
            Then for all $k \ge 1$, the iterates $(x_{k + 1}, y_{k + 1})_{k \ge 1}$ are algebraically equivalent to: 
            \begin{align*}
                x_{k + 1} &= y_k - L^{-1}\mathcal G_Ly_k, 
                \\
                y_{k + 1} &= 
                x_{k + 1} + 
                \frac{\rho_k\alpha_k(1 - \alpha_k)}
                {\rho_k\alpha_k^2 + \alpha_{k + 1}}(x_{k + 1} - x_k). 
            \end{align*}
            If in addition, $v_1 = x_1$ then 
            \begin{align*}
                y_1 = \left(
                    1 + \frac{L - L \alpha_1}{L\alpha_1 - \mu}
                \right)^{-1}\left(
                    v_1 + \left(
                        \frac{L - L \alpha_1}{L \alpha_1 - \mu}
                    \right)x_1
                \right) = x_1. 
            \end{align*}
            In the special case when $\mu = 0$, the momentum term admits simpler representation 
            \begin{align*}
                (\forall k \ge 1) \quad 
                \frac{\rho_k\alpha_k(1 - \alpha_k)}{\rho_k\alpha_k^2 + \alpha_{k + 1}}
                & = \alpha_{k + 1}(\alpha_k^{-1} - 1). 
            \end{align*}
        \end{proposition}


    \subsection{The descriptive power of R-WAPG on existing variants}\label{ssec:describe-variants-with-rwapg}
        \todoinline{\noindent
            Lemmas: 
            \begin{enumerate}
                \item Inverted FISTA sequence is a R-WAPG sequence.
                \item Constant R-WAPG sequence.
            \end{enumerate}
            Theorems: 
            \begin{enumerate}
                \item Convergence with constant momentum. 
                \item Convergence with Chambolle, Dossal Sequences.
            \end{enumerate}
            Todo: 
            \begin{enumerate}
                \item Explain what is what. 
            \end{enumerate}
        }
        Lemma \ref{lemma:inverted-fista-seq} indentifies a sequence $(\alpha_k)_{k \ge 0}$ such that $\alpha_k^{-2} \ge \alpha_{k + 1}^{-2} - \alpha_{k + 1}^{-1}$ as an specific instance of R-WAPG sequence. 
        The lemma showed that sequence $(\alpha_k^{-1})_{k \ge 0}$ is the FISTA sequence which governs the momentum term and convergence claim in FISTA algorithms and variants alike. 
        The lemma also provides a simplified convergence claim using the R-WAPG sequence on Proposition \ref{prop:wagp-convergence}. 
        Theorem \ref{thm:r-wapg-on-cham-doss} stated that the sequences given in  
        \todo{$\square$ Cite this paper again here. }{\hl{Chambolle, Dossal's}}
        paper indeed is an instance of R-WAPG sequence, along with that, it indeed attains a convergence rate $\mathcal O(1/k^2)$. 
        \par 
        
        
        \begin{lemma}[R-WAPG sequences as inverted FISTA sequence]\label{lemma:inverted-fista-seq}
            Let R-WAPG sequence $(\rho_k)_{k \ge 0}, (\alpha_k)_{k \ge 0}$ given by Definition \ref{def:rwapg-seq}. 
            If $\mu = 0, \rho_k \ge 1\; \forall k \ge 0$, and $\alpha_0 = 1$, then: 
            \begin{enumerate}
                \item $\alpha_k^{-2} \ge \alpha_{k + 1}^{-2} - \alpha_{k + 1}^{-1}\; \forall k \ge 0$
                \item Let $t_k := \alpha_k^{-1}$, then $0 < t_{k + 1} \le (1/2)\left(1 + \sqrt{1 + 4t_k^2}\right)\;\forall k\ge 0$, hence the name: ``Inverted FISTA sequence''. 
                \item $\prod_{i = 1}^k\max(1, \rho_{k - 1})(1 - \alpha_k) = \alpha_k^2 \quad (\forall k \ge 1)$. 
            \end{enumerate}
        \end{lemma}
        \begin{lemma}[Constant R-WAPG sequences]\label{lemma:constant-rwapg-seq}
            Suppose $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ are R-WAPG sequences given by Definition \ref{def:rwapg-seq} and assume $L > \mu > 0$.
            Define $q := \mu/L$. 
            Then $\forall r \in \left(\sqrt{q},\sqrt{q^{-1}}\right)$, the constant sequence $\alpha_k := r \sqrt{q}$ has the following: 
            \begin{enumerate}
                \item Fix any $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$ then the constant sequence $\alpha_k := \alpha \in (q, 1)$ and\\
                $\rho_k := \rho=\left(1-r^{-1}\sqrt{q}\right)\left(1 - r \sqrt{q}\right)^{-1} > 0$, hence it's a pair of valid R-WAPG sequence. 
                \item The momentum term in Definition \ref{def:r-wapg-momentum-form}, which we denoted by $\theta$ has:\\ $\theta = (1 - r^{-1}\sqrt{q})(1 - r\sqrt{q})(1- q)^{-1}$. 
                \item When $r = 1$, $\theta = (1- \sqrt{q})(1 + \sqrt{q})^{-1}$. 
                \item For all $r \in \left(1, \sqrt{q^{-1}}\right)$, $\rho > 1$; for all $r \in \left(\sqrt{q}, 1\right]$ $\rho \le 1$. 
                \item For all $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$, $\max(\rho, 1)(1 - \alpha) = \max\left(1 - r\sqrt{q}, 1 - r^{-1}q\right)$. 
            \end{enumerate}
        \end{lemma}
        \begin{theorem}[FISTA first variant Chambolle, Dossal 2015]\label{thm:r-wapg-on-cham-doss}\;\\
            Fix arbitrary $a \ge 2$.
            Define $\forall k \ge 1$ the sequence $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ by 
            \begin{align*}
                \alpha_k &= a/(k + a), 
                \\
                \rho_k &= \frac{(k + a)^2}{(k + 1)(k + a + 1)}. 
            \end{align*}
            Consider the algorithm given by: 
            \begin{tcolorbox}
                Initialize any $y_1 = x_1$. 
                \\
                For $k = 1, 2, \cdots$, update: 
                \begin{align*}
                    & x_{k + 1} := y_k + L^{-1}\mathcal G_L(y_k), 
                    \\
                    & \theta_{t + 1} := \alpha_{k + 1}(\alpha_k^{-1} - 1),
                    \\
                    & y_{k + 1} := x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k). 
                \end{align*}    
            \end{tcolorbox}
            If $\mu = 0$, then $(\alpha_k)_{k \ge 0}, (\rho_k)_{k \ge 0}$ is a valid pair of R-WAPG sequence from Definition \ref{def:rwapg-seq} and the above algorithm is a valid form of R-WAPG. 
            \par
            Assume minimizer $x^*$ exists for function $F$. 
            Then algorithm produces $(x_k)_{k \ge0}$ such that $F(x) - F(x^*)$ convergences at a rate of $\mathcal O(\alpha_k^2)$. 
        \end{theorem}
        \begin{theorem}[Fixed momentum APG]\label{thm:fixed-momentum-fista}
            Assume $L > \mu > 0$, let a pair of constant R-WAPG sequence: $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ be given by Lemma \ref{lemma:constant-rwapg-seq}.
            Define $q := \mu/L$ and for any fixed $r \in \left(\sqrt{q}, \sqrt{q^{-1}}\right)$, let $\alpha_k := \alpha = r \sqrt{q}$ be the constant R-WAPG sequence. 
            Consider the algorithm with a constant momentum specified by the following: 
            \begin{tcolorbox}
                Define $\theta = \left(1 - r^{-1}\sqrt{q}\right)(1 - r\sqrt{q})(1 - q)^{-1}$. 
                \\
                Initialize $y_1 = x_1$; for $k = 1, 2, \cdots, N$, update: 
                \begin{align*}
                    &x_{k + 1} = y_k + L^{-1}\mathcal G_L y_k, 
                    \\
                    & y_{k + 1} = x_{k + 1} + \theta(x_{k + 1} - x_k). 
                \end{align*}
            \end{tcolorbox}
            Then the algorithm generates $(x_k)_{k \ge 1}$ such that $F(x) - F(x^*)$ convergences at a rate of $\mathcal O\left(\max(1 - r\sqrt{q}, 1 - r^{-1}\sqrt{q})^k\right)$. 
        \end{theorem}

\section{Method Free R-WAPG}\label{sec:spectral-momentum}
    \todoinline{
        Algorithm, and results of numerical experiments with their descriptions. 
    }
    This section introduces an algorithm of our creation inspired by the remark of Proposition \ref{prop:stepwise-lyapunov}. 
    Algorithm \ref{alg:free-rwapg} estimates the $\mu$ constant as the algorithm executes and pools the information using the Bregman Divergence of the smooth part function $f$. 
    \begin{algorithm}
        \begin{algorithmic}[1]
        {\footnotesize
        \STATE{\textbf{Input: } $f, g, x_0, L > \mu \ge 0, \in \RR^n, N \in \N$}
        \STATE{\textbf{Initialize: }$y_0 := x_0;L := 1; \mu := 1/2; \alpha_0 = 1;$}
        \STATE{\textbf{Compute: } $f(y_k)$; }
        \FOR{$k = 0, 1, 2, \cdots, N$}
            \STATE{\textbf{Compute: }$\nabla f(y_k); x^+:= [I + L^{-1}\partial g](y_k - L^{-1}\nabla f(y_k))$;}
            \WHILE{$L/2\Vert x^+ - y\Vert^2 < D_f(x^+, y)$}
                \STATE{$L:= 2L$;}
                \STATE{$x^+ = [I + L^{-1}\partial g](y_k - L^{-1}\nabla f(y_k))$; }
            \ENDWHILE
            \STATE{$x_{k + 1} := x^+$;}
            \STATE{$\alpha_{k + 1} := (1/2)\left(\mu/L - \alpha_{k}^2 + \sqrt{(\mu/L - \alpha_{k}^2)^2 + 4\alpha_{k}^2}\right)$;}
            \STATE{$\theta_{k + 1} := \alpha_k(1 - \alpha_k)/(\alpha_k^2 + \alpha_{k + 1})$;}
            \STATE{$y_{k + 1}:= x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k)$; }
            \STATE{\textbf{Compute: } $f(y_{k + 1})$}
            \STATE{$\mu := (1/2)(2D_f(y_{k + 1}, y_{k})/\Vert y_{k + 1} - y_k\Vert^2) + (1/2)\mu$;}
        \ENDFOR
        }
        \end{algorithmic}
        \caption{Free R-WAPG}
        \label{alg:free-rwapg}
    \end{algorithm}
    \par
    Line 5-8 estimates upper bound for the Lipschitz constant and find $x^+$, the next iterates produced by proximal gradient descent on previous $y_k$.
    Line 9 updates $x_{k + 1}$ to be $x^+$, a successful iterate identified by the Lipschitz line search routine. 
    Line 10 updates the R-WAPG sequence $\alpha_k$ for the iterates $y_{k + 1}$. 
    Line 13 updates $\mu$ using the Bregman Divergence of $f$ from iterates $y_{k + 1}, y_k$. 
    \par
    Assume $L$ given is an upper bound of the Lipschitz smoothness constant of $f$, then the algorithm calls $f(\cdot)$ two times, and $\nabla f(\cdot)$ once per iteration. 
    The algorithm computes $\nabla f(y_k)$ once for $x^+$, $f(y_{k + 1})$ once for Bregman Divergence because $f(y_{k})$ is evaluated from the previous iteration, and $f(x^+)$ once for Lipschitz constant line search condition. 
    We note that $f(y_0)$ is computed before the start of the for loop. 
    And finally, it evaluates proximal of $g$ at $y_k - L^{-1}\nabla f(y_k)$ once. 

\section{Catalyst accelerations and future works}\label{sec:catalyst}
    \todoinline{\noindent
        Literatures review of the topics in Catalyst acceleration method. 
        Here is a list of topics: 
        \begin{enumerate}
            \item The original accelerated PPM. 
            \item The Catalyst with weakly convex objectives. 
        \end{enumerate}
        After the literature reviews of the core literatures, move on and state new research directions and open problems. 
        There are several directions for open problem: 
        \begin{enumerate}
            \item APPM method for monotone operators instead of just subgradient, whether the same framework exists in a greater context. 
            \item Accelerated Proximal Bregman Method. 
            \item Removing smoothness assumption in Catalyst acceleration framework. 
        \end{enumerate}  
    }
    \todoinline{\noindent
        A list of relevant literatures: 
        \begin{enumerate}
            \item Güler's 1992 paper on Accelerated Proximal Point method. 
            \item Lin's, and Payquette's three triology paper on Catalyst acceleration for convex, non-convex Variance reduced algorithm. 
        \end{enumerate}
    }


\section{Performance estimation problems}\label{sec:pep}
    \todoinline{\noindent
        There are several foundational papers relevant. 

    }

\section{Methods of inexact proximal point}\label{sec:inexact-prox}
    

\section{Nestrov's acceleration in the non-convex case}\label{sec:nes-acc-ncnvx}
    


\section{Using PostGreSQL and big data analytic method for species classification on Sentinel-2 Satellite remote sensing imagery}


\bibliographystyle{siam}
\bibliography{references/proposal.bib}
\newpage

\appendix

\end{document}
