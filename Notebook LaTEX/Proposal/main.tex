\documentclass[12pt]{article}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% === TEMPLATE HOW TO USE ======
% To begine here is a list of things: 
% [ ]: Change title. 
% [ ]: Fill into the author names, affiliation and contact info. 
% [ ]: Fill in the abstract. 
% [ ]: Fill/change in AMS mathematics subject classification code, and the keywords. 

\title{
    {
        \fontfamily{ptm}\selectfont 
        First Order Nonsmooth Optimization: 
        Algorithm Design, Variational analysis, and Applications
    }
}
\author{
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. 
    % E-mail:  \texttt{alto@mail.ubc.ca}.}~Hongda Li
    % \thanks{Department of Mathematics, I.K. Barber Faculty of Science,
    % The University of British Columbia, Kelowna, BC Canada V1V 1V7. 
    % E-mail:  \texttt{shawn.wang@ubc.ca}.}~ and~Author Name 2
    Hongda Li\\[3ex]\\ Department of Mathematics\\
	University of British Columbia,\\
	Okanagan Campus.
}
\date{\today}

\begin{document}
% \vspace{10ex}
% \vskip 8mm

\maketitle
\tableofcontents
\pagebreak



% \begin{abstract} 
%     \noindent
%     The research proposal focuses on the theories and practice in solving nonsmooth optimization. 
%     The theme of proposal highlight topics of interests that emphasize the computations and applications aspect of algorithms that exhibits both practical and theoretical importance. 
%     We summarize our ongoing research in unifying Nesterov type accelerated proximal gradient method and proposes our Free R-WAPG method. 
%     We survey literatures under the topic of Catalyst Meta Acceleration framework used in accelerating variance reduced methods in the settings of Data Science and Machine Learning. 
%     Furthermore, we present literatures and progress in topics such as Performance Estimation Problem, Inexact Proximal Point, acceleration without convexity. 
%     At the end there is a section summarizing a method we developed for tree species classifications using Sentinel-2 satellite remote sensing data using big data analytics by extract spectral signatures of ground vegetation covers. 


% \end{abstract}
% \noindent{\bfseries 2010 Mathematics Subject Classification:}
% Primary 65K10, 90c25, 90C30; Secondary 65Y20. 
% \noindent{\bfseries Keywords: } Non-convex Optimizations, Proximal Point. 


\section{Introduction}
    Let $\RR^n$ be the ambient space. We consider 
    \begin{align}
        \min_{x \in \RR^n} \left\lbrace
            F(x): f(x) + g(x)
        \right\rbrace.
    \end{align}\label{eqn:additive-comp-obj}
    Unless specified, assume $f:\RR^n \rightarrow \RR$ is $L$-Lipscthiz smooth $\mu \ge 0$ strongly convex and $g:Q \rightarrow \overline \RR$ is convex. 
    This type of problem is referred to as additive composite problems in the literature. 
    \par
    Our ongoing research concerns accelerated proximal gradient type method for solving (\ref{eqn:additive-comp-obj}). 
    In the expository writing by Walkington \cite{noel_nesterovs_nodate}, a variant for of accelerated gradient method for strongly convex function $f$ is discussed. 
    We had two lingering questions after reading it. 
    \begin{enumerate}
        \item Do there exist a unified description for the convergence for both variants of the algorithms?
        \item Is it possible to attain faster convergence rate without knowledge about the strong convexity of function $f$?
        \item Is it possible to describe the convergence of function value for momentum sequences that are much weaker than the Nesterov's rule? 
    \end{enumerate}
    The good news is we have definitive answers for all questions by our own efforts of research. 
    Section \ref{sec:unify-nes-acceleration}, \ref{sec:spectral-momentum} are our ongoing research which present the answers to the questions. 
    \par
    In Section \ref{sec:unify-nes-acceleration}, we proposed the method of ``Relaxed Weak Accelerated Proximal Gradient (R-WAPG)'' as the foundation to describe several variants of Accelerated proximal gradient method in the literatures. 
    The convergence theories of R-WAPG allows us to model convergence of accelerated proximal gradient method where the momentum sequence doesn't strictly follow the conditions presented in the literatures. 
    The descriptive power of R-WAPG allows convergence analysis for all the variants using one single theorem. 
    \par
    In Section \ref{sec:spectral-momentum} we propose a practical algorithm that exploits a specific term in the proof of R-WAPG to achieve faster convergence for solving (\ref{eqn:additive-comp-obj}) without knowing parameter $L, \mu$ in prior. 
    Results of numerical experiments are presented. 
    \par
    Section \ref{sec:catalyst} are results of literatures review in MATH 590. 
    \todo{Add citations here. }{\hl{It's based on a series of papers}} in the topic of Catalyst Meta Acceleration method for First Order Variance Reduced Methods. 
    We will point out potential future direction of research of Catalyst acceleration. 
    \par
    Section \ref{sec:pep}, \ref{sec:inexact-prox}, \ref{sec:nes-acc-ncnvx} preview literatures in  nonsmooth optimization frontier research where progress and impacts can be made.  
    \subsection{Theme of the research}
        This section specify a theme of the research in this proposal. 
        Out first objective is to explore the Goldilocks zones between these topics: theories of variational analysis, design of continuous optimization algorithm and applications in sciences, engineering, and statistics.  
        Our second objective is to identify the ``chemistry" occuring between properties of functions and the designs of continuous optimizations algorithm and how it impacts the convergence and behaviors of the algorithms. 


    
\section{Preliminaries}
    \todoinline{
        Clarify: Notations,
        Organizations. 
    }
    This section contains the basics of contents from convex optimization, and variational analysis. 
    \begin{enumerate}
        \item $\overline \RR := \RR \cup \{\infty, -\infty\}$
    \end{enumerate}
    \subsection{Fundamentals in non-convex analysis}
        \todoinline{\noindent
            We are in $\RR^n$, and the weakest assumption we are making for the objective function is Local Lipschitz continuity. 
            Definitions: 
            \begin{enumerate}
                \item[$\blacksquare$] Local Lipschitz continuity. 
                \item[$\square$]Regular subgradient. Remember to cite. 
                \item[$\square$] Limiting subgradient. Remember to cite. 
                \item[$\blacksquare$] Weakly convex function. 
                \item[$\blacksquare$]The Bregman Divergence of function. 
            \end{enumerate}
            Take Limiting, Regular subgradient definitions from Cui, Pong's book, Definition 4.3.1. 
        }
        Let the ambient space be $\RR^n$ equipped with inner product and 2-norm. 
        Let $O$ be an open subset of $\RR^n$, the weakest assumption we are making for the objective function $F: O \subseteq \RR^n \rightarrow \RR$ for optimization problem is Local Lipschitz Continuity. 
        The assumption of local Lipschitz continuity is weak enough to describe most problems in applications, and strong enough to avoid most pathologies in analysis. 

        \begin{definition}[Local Lipschitz continuity]
            Let $F: O \subseteq \RR^n \rightarrow \RR$ be Locally Lipschitz and $O$ is an open set. 
            Then for all $\bar x \in O$, there exists a Neighborhood: $\mathcal N(\bar x)$ and $K \in \RR$ such that for all $x, y \in \mathcal N(\bar x)$: $|F(x) - F(y)| \le K \Vert x - y\Vert$. 
        \end{definition}
        \begin{definition}[Regular subgradient]
            Let $F: O \subseteq \RR^n \rightarrow \RR$ be locally Lipschitz and $\bar x \in O$. 
            The regular subdifferential at $\bar x$ is defined as 
            \begin{align*}
                \widehat \partial F(\bar x) := 
                \left\lbrace
                    v \in \RR^n \left| 
                        \liminf_{\bar x \neq x\rightarrow \bar x}
                        \frac{F(x) - F(\bar x) - \langle v, x - \bar x\rangle}{\Vert x - \bar x\Vert} 
                        \ge 0
                    \right.
                \right\rbrace
            \end{align*}
        \end{definition}
        \begin{definition}[Limiting subgradient]
            Let $F: O \subseteq \RR^n \rightarrow \RR$ be locally Lipschitz and $\bar x \in O$. 
            The limiting subdifferential at $\bar x$ is defined as 
            \begin{align*}
                \partial F(\bar x) := 
                \left\lbrace
                    v \in \RR^n \left| 
                        \exists x_k \rightarrow \bar x, v_k \rightarrow v: 
                        v_k \in \widehat \partial F(x_k) \;\forall k \in \mathbb N
                    \right.
                \right\rbrace. 
            \end{align*}
        \end{definition}
        \begin{definition}[Weakly convex function]
            $F: \RR^n \rightarrow \overline \RR$ is $\mu$ weakly convex if and only if $F + \frac{\mu}{2}\Vert \cdot\Vert^2$ is convex. 
        \end{definition}
        \begin{definition}[Bregman divergence]
            Let $F: O \subseteq \RR^n \rightarrow \RR$ be a differentiable function. 
            Then the Bregman divergence of $F$ is defined as: 
            \begin{align*}
                D_F(x, y): O \times \dom(\partial F) \rightarrow \RR
                := F(x) - F(y) - \langle \nabla F(y), x - y\rangle. 
            \end{align*}
        \end{definition}
    \subsection{Fundamentals in convex analysis}
        \todoinline{
            Introduce 
            \begin{enumerate}
                \item[$\blacksquare$] Convexity, 
                \item[$\blacksquare$] convex subgradient, 
                \item[$\blacksquare$] Lipschitz smoothness. 
            \end{enumerate}
            Definitions: 
            \begin{enumerate}
                \item [$\blacksquare$]Strong convexity of a function. 
                \item [$\blacksquare$]The proximal gradient operator. 
                \item [$\blacksquare$]The proximal mapping operator. 
            \end{enumerate}
            Lemmas: 
            \begin{enumerate}
                \item [$\blacksquare$]Quadratic growth conditions of a strongly convex function. 
            \end{enumerate}
        }
        This section introduces the classics and basics of convex analysis. 
        Define $F$ to be convex in this section. 
        When $F$ is convex, the limiting subgradient and the regular subgradient reduced to the following definition:
        \begin{align*}
            \partial F(x) := \left\lbrace
                v \in \RR^n \left|
                    (\forall y \in \RR^n)\;  F(y) - F(x)  \ge \langle v, y - x\rangle
                \right.
            \right\rbrace. 
        \end{align*}
        A convex function is locally Lipschitz in the relative interior of its domain, denoted as $\reli(\dom(F))$. 
        So it has $\reli(\dom F)\subseteq \dom(\partial F) \subseteq \dom F$. 
        \par
        When we say $F:\RR^n \rightarrow \RR$ is $L$ Lipschitz smooth function, it means that there exists $L$ such that for all $x\in \RR^n, y \in \RR^n$, it has: 
        \begin{align*}
            \Vert \nabla F(x) - \nabla F(y)\Vert \le L \Vert x - y\Vert. 
        \end{align*}
        This condition is stronger than differentiability. 
        When $F$ convex, it has descent lemma: 
        \begin{align*}
            (\forall x \in \RR^n)(\forall y \in \RR^n): 0 \le 
            F(x) - F(y) - \langle \nabla f(y), x - y\rangle \le \frac{L}{2}\Vert x - y\Vert^2. 
        \end{align*}
        When $F$ is convex, the converse holds. 
        The definitions that follow narrow things further for future discussions. 
        \begin{definition}[Strong convexity]
            A function $F:\RR^n \rightarrow \overline \RR$ is $\mu \ge 0$ strongly convex if and only if for any fixed $y \in \dom(\partial F)$, we have for all $x\in \RR^n$: 
            \begin{align*}
                (\forall v \in \partial F(x))\quad 
                F(x) - F(y) \ge \langle v, x - y\rangle + \frac{\mu}{2}\Vert x - y\Vert^2. 
            \end{align*}
        \end{definition}
        \begin{lemma}[Quadratic growth from strong convexity]
            If $F$ is $\mu \ge 0$ strongly convex, $\bar x$ is a minimizer of $F$. 
            Then for all $x \in \RR^n$
            \begin{align*}
                F(x) - F(\bar x) \ge \frac{\mu}{2}\Vert x - \bar x\Vert^2. 
            \end{align*}
        \end{lemma}
        \begin{remark}
            The minimizer is unique whenever $\mu > 0$. 
            For contradiction, assume $x$ is another minimizer, then $F(x) \neq F(\bar x)$, which is a direct contradiction. 
        \end{remark}
        
        \subsubsection{Smooth, nonsmooth additive composite}\label{sssec:additive-composite}
            \todoinline{\noindent
                Introduce notations for the proximal gradient model function. 
                Lemmas: 
                \begin{enumerate}
                    \item[$\blacksquare$] Proximal gradient envelope. 
                    \item[$\blacksquare$] A property of gradient mapping. 
                \end{enumerate}
                Theorems: 
                \begin{enumerate}
                    \item[$\blacksquare$] The proximal gradient inequality. 
                \end{enumerate}
            }

            In this section, we zoom in further. 
            Suppose that $F:= f + g$ where $f:\RR^n \rightarrow \RR$ is convex, $L$ Lipschitz smooth and $\mu \ge 0$ strongly convex and $g: \RR\rightarrow \overline \RR$ is convex. 
            To make the discussion simpler, fix any $\beta \ge 0$ we define the following model functions as a $\RR^n \times \RR^n \rightarrow \overline \RR$: 
            \begin{align*}
                \widetilde{\mathcal M}^{\beta^{-1}}
                (x; y)
                &:= 
                g(x) + f(y) + \langle \nabla f(y), x - y\rangle
                + \frac{\beta}{2}\Vert x - y\Vert^2,
                \\
                \mathcal M^{\beta^{-1}}(x; y) 
                &:= F(x) + \frac{\beta}{2}\Vert x - y\Vert^2.
            \end{align*}
            Under convexity assumption in this section, both $\widetilde {\mathcal M} (\cdot; y),  {\mathcal M}(\cdot;y )$ is at least $\beta \ge 0$ strongly convex. 
            \begin{definition}[Proximal gradient operator]
                Suppose that $F := f + g$ where $g: \RR^n \rightarrow \overline \RR$ is convex and $f: \RR^n \rightarrow \RR$ is a $L$ Lipschitz smooth function.
                Define the proximal gradient operator $T_L$ on all $y \in \RR^n$: 
                \begin{align*}
                    T_L y := \argmin_{x \in \RR^n} \left\lbrace
                        g(x) + f(y) + \langle \nabla f(y), x - y\rangle 
                        + \frac{L}{2}\Vert x- y \Vert^2
                    \right\rbrace. 
                \end{align*}
            \end{definition}
            \begin{remark}
                Under the assumption of this section, the mapping $T_L$ is a single-valued mapping, it has domain on the entire $\RR^n$, and it's a $3/2$ averaged operator. 
            \end{remark}
            \begin{definition}[Gradient mapping operator]
                Take $F := f + g$ as defined in this section. 
                Define the gradient mapping operator $\mathcal G_L$ on all $y \in \RR^n$: 
                \begin{align*}
                    \mathcal G_L y:= L(y - T_L y). 
                \end{align*}
            \end{definition}

            \begin{lemma}[Proximal gradient model function]\;\\
                Take $\widetilde{\mathcal M}^{L^{-1}}, \mathcal M^{L^{-1}}$ as defined in this section, we will have for all $x \in \RR^n$ that: 
                \begin{align*}
                    \widetilde{\mathcal M}^{L^{-1}}(x; y)
                    &= 
                    \mathcal M^{L^{-1}}(x; y)- D_f(x, y).
                \end{align*}
            \end{lemma}
            \begin{lemma}[A favorable property of gradient mapping]
                Take $F:= f + g$ as defined in this section. 
                Fix any $x \in \RR^n$. 
                Then there exists $v \in \partial g(T_L x)$ such that $\mathcal G_L (x) = v + \nabla f(x)$. 
            \end{lemma}
            \begin{remark}
                This lemma still holds for non-convex $f$. 
            \end{remark}
            \begin{lemma}[The proximal gradient inequality]\label{thm:prox-grad-ineq}
                Take $F:= f + g$ as defined in this section. 
                Fix any $y \in \RR^n$, then for all $x$, the proximal gradient inequality is true: 
                \begin{align*}
                    (\forall x \in \RR^n)\quad 
                    h(x)  - h(Ty) - \langle L(y - Ty), x - y\rangle
                    - \frac{\mu}{2}\Vert x - y\Vert^2 - \frac{L}{2}\Vert y - Ty\Vert^2 
                    &\ge 0. 
                \end{align*}
            \end{lemma}
            \begin{remark}
                This lemma is proved in our draft paper. 
            \end{remark}

   

\section{Unifying NAG, and weakening the sequence assumption for convergences}\label{sec:unify-nes-acceleration}
    \todoinline{\noindent
        This section is really about stating the results of the draft paper and no proofs will be done here. 
        Along with the content of the draft paper, we will also explain the origin and inspirations of the ideas. 
    }
    \todoinline{
        Definitions:
        \begin{enumerate}
            \item \st{Method of Nesterov's estimating sequence.}
            \item R-WAPG stepwise definition 
            \item R-WAPG stepwise convergence claim
            \item R-WAPG Sequence.
            \item R-WAPG algorithm.
            \item R-WAPG Intermediate form.
            \item R-WAPG Similar triangle form. 
            \item R-WAPG Momentum form. 
        \end{enumerate}
    }
    \todoinline{
        Theorems: 
        \begin{enumerate}
            \item Convergence of the R-WAPG algorithm.
            \item R-WAPG First equivalent form. 
            \item R-WAPG Second equivalent form.
            \item R-WAPG Third equivalent form.
            \item Convergence with constant momentum. 
            \item Convergence with Chambolle, Dossal Sequences.
        \end{enumerate}
        Lemmas
        \begin{enumerate}
            \item Inverted FISTA sequence is a R-WAPG sequence.
            \item Constant R-WAPG sequence.
        \end{enumerate}
    }
    This section is based on the theoretical aspects of our draft paper. 
    It will introduce major results and claims achieved during our research in each of the subsections. 
    All theorems and claims stated in this section have proofs in the draft paper. 
    The proofs haven't been carefully verified by people other than the author yet. 
    We will start introducing the context and ideas for our research next. 
    \par
    Assume we want to solve a convex optimization problem: $\min_{x \in \RR^n} \{F(x)\}$ and $F: \RR^n \rightarrow \RR$ is a $L$ Lipschitz smooth function. 
    We made this assumption for now for a faster exposition. 
    One of the prime candidate for solving the optimization problem is the Nesterov's Accelerated Gradient methods (NAG) finds extensions for nonsmooth function through the proximal gradient operator.
    \todo{Cite Nesterov's original paper on this. }{\hl{Proposed back in 1983 the original Nesterov's acceleration method}} which uses the previous iterates to extrapolate the next iterate to evaluate the gradient. 
    It's well known that, if minimizer $x^*$ exists for $F$, the method achieves a $\mathcal O(1/k^2)$ convergence rate on the objective value $F(x_k)$. 
    \todo{Cite Chapter 2 of Nesterov's new book. }{\hl{This convergence rate is considered optimal for all class of $L$ Lipschitz smooth convex function}}. 
    The convergence rate gurantee is faster than $\mathcal O(1/k)$ exibited by gradient descent. 
    \par
    We cover the algorithm briefly. 
    Initialize $x_1 = y_1$ and $t_0 = 1$, the algorithm finds $(x_k)_{k \ge 1}$ for all $k \ge 1$ by: 
    \begin{align}\label{eqn:nag_example}
        & x_{k + 1} = y_k - L^{-1}\nabla F(y_k), 
        \\
        & t_{k + 1} = 1/2\left(1 + \sqrt{1 + 4t_{k}^2}\right), 
        \\
        & \theta_{k + 1} = (t_{k} - 1)/t_{k + 1}, 
        \\
        & y_{k + 1} = x_{k + 1} + \theta_{k + 1}(x_{k + 1} - x_k). 
    \end{align}
    Unfortunately, the algorithm sped up the convergence rate for all convex function, it becomes slower for the subset of $\mu > 0$ strongly convex function. 
    This drawback inspired a vast amount of literatures aims at improving, extending, and analyzing NAG. 
    Restarting is a popular solution to address the issue of obtaining faster convergence rate when the objective function is strongly convex. 
    \todo{Cite Beck 2009 FISTA. }{\hl{Beck and Toubelle}} mitigate the issue by restarting and showed that it still has a $\mathcal O(1/k^2)$ convergence rate, and it performs empirically better. 
    \todo{Cite Necoara linear convergence, and Aujol 2024 Parameter free FISTA restart. }{\hl{See}} and references within for recent advancements in restarting accelerated proximal gradient algorithm.
    \par
    Restarting the algorithm is not the entire picture. 
    Let $F:\RR^n \rightarrow \RR$ be a $L$ Lipschitz smooth and $\mu > 0$ function. 
    As introduced previously, in \todo{Cite Walkington's education stuff here. }{\hl{Walkington's writing}}, he showed that there exists a variant of the Nesterov's accelerated gradient method that achieved a linear convergence rate of $\mathcal O((1 - \sqrt{\mu/L})^k)$. 
    However, this variant has a fixed momentum parameter $\theta_{k + 1} = \frac{\sqrt{\kappa} - 1}{\sqrt{k} + 1}$ back in Equation \ref{eqn:nag_example}. 
    \todo{Cite them.}{\hl{The same variant also appears in Beck's book as V-FISTA, and Nesterov's book as (2.2.22).}} 
    \par
    One final Mystery of the algorithm is the convergence of the iterates which also has much to do with the momentum sequence $(\theta_k)_{k\ge 0}$ displayed in Equation \ref{eqn:nag_example}. 
    \todo{Cite them. }{\hl{Chambolle, Dossal}} showed that by choosing sequence $(t_k)_{k \ge 1}$ to be $t_k = (n + a - 1)/a$ where $a > 2$ instead would give $(x_k)_{k \ge 0}$ weak convergence in Hilbert space. 
    It's put as an open question on what happens to the iterates when $a = 2$. 
    \par
    All of these seemingly raises a crucial question: ``Is it possible to describe something about the NAG algorithm for a set of sequence that is non-traditional?''.
    Or rephrase it more clearly: ``What is the weakest description of momentum sequence $(\theta_k)$ such that we can still claim something of value about the NAG algorithm?''

    \subsection{Our Contributions, organizations}
        Our contributions are two folds, theoretical and practical. 
        Our results are based the assumption $F = f + g$ where $g:R^n \rightarrow \overline\RR$ is convex, and $f$ is an $L$-Lipschitz smooth and $\mu \ge 0$ strongly convex function. 
        We relax the traditional choice of the sequence $\theta_k$ in Equation \ref{eqn:nag_example} and showed an upper bound of the optimal gap. 
        Let $(\alpha_k)_{k \ge0}, (\rho_k)_{k \ge 0}$ be two sequences that satisfy
        \begin{align*}
            \alpha_0 &\in (0, 1], 
            \\
            \alpha_k &\in (\mu/L, 1) \quad (\forall k \ge 1), 
            \\
            \rho_k &:= \frac{\alpha_{k + 1}^2 - (\mu/L)\alpha_{k + 1}}{(1 - \alpha_{k + 1})\alpha_k^2} \quad \forall (k \ge 0). 
        \end{align*}
        Our first main result shows that if $\theta_{k + 1} = (\rho_k\alpha_k(1 - \alpha_k)/(\rho_k\alpha_k^2 + \alpha_{k + 1}))$, using the R-WAPG we proposed in Definition \ref{def:wapg} with Proposition \ref{prop:wagp-convergence}, \ref{prop:r-wapg-momentum-repr}, we can show that the gap $F(x_k) - F(x^*)$ is bounded by:
        \begin{align*}
            \mathcal O\left(
                \left(
                    \prod_{i = 0}^{k - 1} \max(1, \rho_{k})
                \right)
                \prod_{i = 1}^{k} \left(1  - \alpha_i\right)
            \right). 
        \end{align*}
        Our second main result shows that there exists $\rho_k > 1$ such that our R-WAPG reduces to a variant of FISTA proposed in Chambolle, Dossal \cite{chambolle_convergence_2015}, and we are able to show the same convergence rate in Theorem \ref{thm:r-wapg-on-cham-doss}. 
        When $\rho_k = 1, \mu = 0$, R-WAPG reduces perfectly to FISTA by Beck \cite{beck_first-order_2017}, if $\mu > 0, \rho_k = 1$, it reduces to the V-FISTA by Beck \cite{beck_first-order_2017}. 
        In Theorem \ref{thm:fixed-momentum-fista}, it demonsrate that R-WAPG frameworks gives a linear convergence claim for all fixed momentum method where $\alpha_k := \alpha \in (\mu/L, 1)$ and  $F$ is $\mu > 0$ strongly convex. 
        \par
        Our practical contribution is an algorithm inspired by a detail in our convergence proof which we call it ``Parameter Free R-WAPG'' (See Algorithm \ref{alg:free-rwapg}). 
        The algorithm is parameter free, meaning that it doesn't require knowing $L, \mu$ in advance, and it determines the value of $\theta_t$ by estimating the local concavity using iterates $y_{k}, y_{k + 1}$ with minimal computational cost. 
        We conducted ample amount of numerical experiments to show that it has a favorable convergence rate in practice and behaves similarly to the FISTA with monotone restart. 
        \par
        Organization now follows. 
        \todoinline{Finish the organizations here after this section is finished. }
    \subsection{Building Blocks of R-WAPG}
        
    \subsection{R-WAPG Sequence and R-WAPG algorithm}
        
    \subsection{Equivalent forms of R-WAPG algorithm}
    
    \subsection{The descriptive power of R-WAPG on existing variants}

\section{Method Free R-WAPG}\label{sec:spectral-momentum}
    \todoinline{
        Algorithm, and results of numerical experiments with their descriptions. 
    }

\section{Catalyst accelerations and future works}\label{sec:catalyst}
    \todoinline{\noindent
        Literatures review of the topics in Catalyst acceleration method. 
        Here is a list of topics: 
        \begin{enumerate}
            \item The original accelerated PPM. 
            \item The Catalyst with weakly convex objectives. 
        \end{enumerate}
        After the literature reviews of the core literatures, move on and state new research directions and open problems. 
        There are several directions for open problem: 
        \begin{enumerate}
            \item APPM method for monotone operators instead of just subgradient, whether the same framework exists in a greater context. 
            \item Accelerated Proximal Bregman Method. 
        \end{enumerate}  
    }
    \todoinline{\noindent
        A list of relevant literatures: 
        \begin{enumerate}
            \item GÃ¼ler's 1992 paper on Accelerated Proximal Point method. 
            \item Lin's, and Payquette's three triology paper on Catalyst acceleration for convex, non-convex Variance reduced algorithm. 
        \end{enumerate}
    }


\section{Performance estimation problems}\label{sec:pep}

\section{Methods of inexact proximal point}\label{sec:inexact-prox}
    

\section{Nestrov's acceleration in the non-convex case}\label{sec:nes-acc-ncnvx}


\section{Using PostGreSQL and big data analytic method for species classification on Sentinel-2 Satellite remote sensing imagery}


\bibliographystyle{siam}
\bibliography{references/proposal.bib}
\newpage

\appendix

\end{document}
