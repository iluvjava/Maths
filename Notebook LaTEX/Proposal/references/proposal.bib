
@book{nesterov_lectures_2018,
	address = {Cham},
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	volume = {137},
	isbn = {978-3-319-91577-7 978-3-319-91578-4},
	url = {http://link.springer.com/10.1007/978-3-319-91578-4},
	urldate = {2023-10-11},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	keywords = {Optimization, Fast Gradient Methods, Algorithmic Complexity, Interior-Point Methods, Numerical Optimization, Optimization in Relative Scale, Self-Concordant Functions, Smoothing Technique},
	file = {Nesterov - 2018 - Lectures on Convex Optimization.pdf:/Users/hongdali/Zotero/storage/HSCCPYL9/Nesterov - 2018 - Lectures on Convex Optimization.pdf:application/pdf},
}

@book{beck_first-order_2017,
	address = {israel},
	series = {{MOS}-{SIAM} {Series} in {Optimization}},
	title = {First-order {Methods} in {Optimization}},
	isbn = {978-1-61197-498-0},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974997},
	language = {en},
	urldate = {2023-10-19},
	publisher = {SIAM},
	author = {Beck, Amir},
	year = {2017},
	keywords = {Optimization, First-order Methods, Non-smooth Optimization, Numerical Optimization},
	file = {First-Order Methods in Optimization  SIAM Publication.pdf:/Users/hongdali/Zotero/storage/P2HFAVVQ/First-Order Methods in Optimization  SIAM Publication.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/88BHKZ6Y/1.html:text/html},
}

@inproceedings{ahn_understanding_2022,
	title = {Understanding {Nesterov}'s acceleration via proximal point method},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977066.9},
	doi = {https://doi.org/10.1137/1.9781611977066},
	abstract = {The proximal point method (PPM) is a fundamental method in optimization that is often used as a building block for designing optimization algorithms. In this work, we use the PPM method to provide conceptually simple derivations along with convergence analyses of different versions of Nesterov's accelerated gradient method (AGM). The key observation is that AGM is a simple approximation of PPM, which results in an elementary derivation of the update equations and stepsizes of AGM. This view also leads to a transparent and conceptually simple analysis of AGM's convergence by using the analysis of PPM. The derivations also naturally extend to the strongly convex case. Ultimately, the results presented in this paper are of both didactic and conceptual value; they unify and explain existing variants of AGM while motivating other accelerated methods for practically relevant settings.},
	urldate = {2023-11-04},
	booktitle = {Symposium on {Simplicity} in {Algorithms}},
	publisher = {SIAM},
	author = {Ahn, Kwangjun and Sra, Suvrit},
	month = jun,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Optimization and Control},
	file = {Ahn and Sra - 2022 - Understanding nesterov's acceleration via proximal.pdf:/Users/hongdali/Zotero/storage/PZBWUWW5/Ahn and Sra - 2022 - Understanding nesterov's acceleration via proximal.pdf:application/pdf;arXiv.org Snapshot:/Users/hongdali/Zotero/storage/WSSGK9Q4/2005.html:text/html},
}

@article{noel_nesterovs_nodate,
	title = {Nesterov's method for convex optimization},
	volume = {65},
	url = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/epdf/10.1137/21M1390037},
	doi = {10.1137/21M1390037},
	abstract = {While Nesterov's algorithm for computing the minimum of a convex function is now over forty years old, it is rarely presented in texts for a first course in optimization. This is unfortunate since for many problems this algorithm is superior to the ubiquitous steepest descent algorithm, and it is equally simple to implement. This article presents an elementary analysis of Nesterov's algorithm that parallels that of steepest descent. It is envisioned that this presentation of Nesterov's algorithm could easily be covered in a few lectures following the introductory material on convex functions and steepest descent included in every course on optimization.},
	language = {en},
	number = {2},
	urldate = {2023-10-09},
	journal = {SIAM Review},
	author = {Noel, Walkington},
	pages = {539--562},
	file = {Nesterov's Method for Convex Optimization.pdf:/Users/hongdali/Zotero/storage/BZC6TFHX/Nesterov's Method for Convex Optimization.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/M8MDL6E3/21M1390037.html:text/html},
}

@article{beck_fast_2009,
	title = {A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
	volume = {2},
	issn = {1936-4954},
	url = {http://epubs.siam.org/doi/10.1137/080716542},
	doi = {10.1137/080716542},
	abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be signiﬁcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
	language = {en},
	number = {1},
	urldate = {2023-11-16},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Beck, Amir and Teboulle, Marc},
	month = jan,
	year = {2009},
	pages = {183--202},
	file = {Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:/Users/hongdali/Zotero/storage/H7CGKLL3/Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:application/pdf},
}

@article{aujol_parameter-free_2024,
	title = {Parameter-{Free} {FISTA} by adaptive restart and backtracking},
	url = {https://epubs.siam.org/doi/10.1137/23M158961X},
	abstract = {We consider a combined restarting and adaptive backtracking strategy for the popular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for accelerating the convergence speed of large-scale structured convex optimization problems. Several variants of FISTA enjoy a provable linear convergence rate for the function values \$F(x\_n)\$ of the form \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}{\textasciitilde}n\})\$ under the prior knowledge of problem conditioning, i.e. of the ratio between the ({\textbackslash}L ojasiewicz) parameter \${\textbackslash}mu\$ determining the growth of the objective function and the Lipschitz constant \$L\$ of its smooth component. These parameters are nonetheless hard to estimate in many practical cases. Recent works address the problem by estimating either parameter via suitable adaptive strategies. In our work both parameters can be estimated at the same time by means of an algorithmic restarting scheme where, at each restart, a non-monotone estimation of \$L\$ is performed. For this scheme, theoretical convergence results are proved, showing that a \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}n\})\$ convergence speed can still be achieved along with quantitative estimates of the conditioning. The resulting Free-FISTA algorithm is therefore parameter-free. Several numerical results are reported to confirm the practical interest of its use in many exemplar problems.},
	language = {en},
	urldate = {2023-10-09},
	journal = {SIAM Journal on Optimization},
	author = {Aujol, Jean-François and Calatroni, Luca and Dossal, Charles and Labarrière, Hippolyte and Rondepierre, Aude},
	month = dec,
	year = {2024},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/3YBXJH4F/Aujol et al. - 2023 - Parameter-Free FISTA by Adaptive Restart and Backt.pdf:application/pdf},
}

@article{guler_new_1992,
	title = {New proximal point algorithms for convex minimization},
	volume = {2},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/0802032},
	doi = {10.1137/0802032},
	abstract = {The proximal point algorithm (PPA) for the convex minimization problem minx∈Hf(x), where f:H→R∪\{∞\} is a proper, lower semicontinuous (lsc) function in a Hilbert space H is considered. Under this minimal assumption on f, it is proved that the PPA, with positive parameters \{λk\}k=1∞, converges in general if and only if σn=∑k=1nλk→∞. Global convergence rate estimates for the residual f(xn)−f(u), where xn is the nth iterate of the PPA and u∈H is arbitrary are given. An open question of Rockafellar is settled by giving an example of a PPA for which xn converges weakly but not strongly to a minimizes of f.},
	number = {4},
	urldate = {2023-11-30},
	journal = {SIAM Journal on Optimization},
	author = {Guler, Osman},
	month = nov,
	year = {1992},
	pages = {649--664},
	file = {Güler - 1992 - New Proximal Point Algorithms for Convex Minimization.pdf:/Users/hongdali/Zotero/storage/G9LCY67Q/Güler - 1992 - New Proximal Point Algorithms for Convex Minimization.pdf:application/pdf},
}

@article{chambolle_convergence_2015,
	title = {On the convergence of the iterates of the "{Fast} iterative shrinkage/thresholding algorithm"},
	volume = {166},
	issn = {1573-2878},
	url = {https://doi.org/10.1007/s10957-015-0746-4},
	doi = {10.1007/s10957-015-0746-4},
	abstract = {We discuss here the convergence of the iterates of the “Fast Iterative Shrinkage/Thresholding Algorithm,” which is an algorithm proposed by Beck and Teboulle for minimizing the sum of two convex, lower-semicontinuous, and proper functions (defined in a Euclidean or Hilbert space), such that one is differentiable with Lipschitz gradient, and the proximity operator of the second is easy to compute. It builds a sequence of iterates for which the objective is controlled, up to a (nearly optimal) constant, by the inverse of the square of the iteration number. However, the convergence of the iterates themselves is not known. We show here that with a small modification, we can ensure the same upper bound for the decay of the energy, as well as the convergence of the iterates to a minimizer.},
	language = {en},
	number = {3},
	urldate = {2023-11-18},
	journal = {Journal of Optimization Theory and Applications},
	author = {Chambolle, A. and Dossal, Ch.},
	month = sep,
	year = {2015},
	keywords = {First-order Methods, Forward–backward Splitting, Heavy Ball Momentum, Optimization},
	pages = {968--982},
	file = {Chambolle and Dossal - 2015 - On the Convergence of the Iterates of the “Fast It.pdf:/Users/hongdali/Zotero/storage/P7LSJUWM/Chambolle and Dossal - 2015 - On the Convergence of the Iterates of the “Fast It.pdf:application/pdf},
}

@article{rockafellar_monotone_1976,
	title = {Monotone operators and the proximal point algorithm},
	volume = {14},
	issn = {0363-0129, 1095-7138},
	url = {http://epubs.siam.org/doi/10.1137/0314056},
	doi = {10.1137/0314056},
	language = {en},
	number = {5},
	urldate = {2023-11-06},
	journal = {SIAM Journal on Control and Optimization},
	author = {Rockafellar, R. Tyrrell},
	month = aug,
	year = {1976},
	pages = {877--898},
	file = {Monotone Operators and the Proximal Point Algorithm.pdf:/Users/hongdali/Zotero/storage/5L82ZWY4/Monotone Operators and the Proximal Point Algorithm.pdf:application/pdf},
}

@article{guler_convergence_1991,
	title = {On the convergence of the proximal point algorithm for convex minimization},
	volume = {29},
	issn = {03630129},
	url = {https://www.proquest.com/docview/925962166/abstract/A60B4BA7798A45D1PQ/1},
	doi = {10.1137/0329022},
	abstract = {The proximal point algorithm (PPA) for the convex minimization problem \${\textbackslash}min \_\{x {\textbackslash}in H\} f(x)\$, where \$f:H {\textbackslash}to R {\textbackslash}cup {\textbackslash}\{ {\textbackslash}infty {\textbackslash}\} \$ is a proper, lower semicontinuous (lsc) function in a Hilbert space \$H\$ is considered. Under this minimal assumption on \$f\$, it is proved that the PPA, with positive parameters \${\textbackslash}\{ {\textbackslash}lambda \_k {\textbackslash}\} \_\{k = 1\}{\textasciicircum}{\textbackslash}infty \$, converges in general if and only if \${\textbackslash}sigma \_n = {\textbackslash}sum\_\{k = 1\}{\textasciicircum}n \{{\textbackslash}lambda \_k {\textbackslash}to {\textbackslash}infty \} \$. Global convergence rate estimates for the residual \$f(x\_n ) - f(u)\$, where \$x\_n \$ is the \$n\$th iterate of the PPA and \$ u {\textbackslash}in H \$ is arbitrary are given. An open question of Rockafellar is settled by giving an example of a PPA for which \$x\_n \$ converges weakly but not strongly to a minimizes of \$f\$.},
	language = {English},
	number = {2},
	urldate = {2024-05-18},
	journal = {SIAM Journal on Control and Optimization},
	author = {Guler, Osman},
	month = mar,
	year = {1991},
	keywords = {Mathematics, Algorithms, Convex analysis, Hilbert space},
	pages = {17},
	file = {Guler - 1991 - On the Convergence of the Proximal Point Algorithm.pdf:/Users/hongdali/Zotero/storage/9AWZSLK4/Guler - 1991 - On the Convergence of the Proximal Point Algorithm.pdf:application/pdf},
}

@article{attouch_convergence_2009,
	title = {On the convergence of the proximal algorithm for nonsmooth functions involving analytic features},
	volume = {116},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-007-0133-5},
	doi = {10.1007/s10107-007-0133-5},
	abstract = {We study the convergence of the proximal algorithm applied to nonsmooth functions that satisfy the Łjasiewicz inequality around their generalized critical points. Typical examples of functions complying with these conditions are continuous semialgebraic or subanalytic functions. Following Łjasiewicz’s original idea, we prove that any bounded sequence generated by the proximal algorithm converges to some generalized critical point. We also obtain convergence rate results which are related to the flatness of the function by means of Łjasiewicz exponents. Apart from the sharp and elliptic cases which yield finite or geometric convergence, the decay estimates that are derived are of the type O(k−s), where s ∈ (0, + ∞) depends on the flatness of the function.},
	language = {en},
	number = {1},
	urldate = {2024-04-16},
	journal = {Mathematical Programming},
	author = {Attouch, Hedy and Bolte, Jérôme},
	month = jan,
	year = {2009},
	keywords = {90C26, 47N10, 90C30, Łjasiewicz inequality, Proximal algorithm, Subanalytic functions},
	pages = {5--16},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/AGIWHMYN/Attouch and Bolte - 2009 - On the convergence of the proximal algorithm for nonsmooth functions involving analytic features.pdf:application/pdf},
}

@article{necoara_linear_2019,
	title = {Linear convergence of first order methods for non-strongly convex optimization},
	volume = {175},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-018-1232-1},
	doi = {10.1007/s10107-018-1232-1},
	abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
	language = {en},
	number = {1},
	urldate = {2023-10-11},
	journal = {Mathematical Programming},
	author = {Necoara, I. and Nesterov, Yu. and Glineur, F.},
	month = may,
	year = {2019},
	pages = {69--107},
	file = {Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:/Users/hongdali/Zotero/storage/7X79PGLC/Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:application/pdf},
}

@book{ryu_large-scale_2022,
	address = {Cambridge},
	title = {Large-scale {Convex} {Optimization}: {Algorithms} \& {Analyses} via {Monotone} {Operators}},
	isbn = {978-1-009-16085-8},
	shorttitle = {Large-{Scale} {Convex} {Optimization}},
	url = {https://large-scale-book.mathopt.com/},
	abstract = {Starting from where a first course in convex optimization leaves off, this text presents a unified analysis of first-order optimization methods – including parallel-distributed algorithms – through the abstraction of monotone operators. With the increased computational power and availability of big data over the past decade, applied disciplines have demanded that larger and larger optimization problems be solved. This text covers the first-order convex optimization methods that are uniquely effective at solving these large-scale optimization problems. Readers will have the opportunity to construct and analyze many well-known classical and modern algorithms using monotone operators, and walk away with a solid understanding of the diverse optimization algorithms. Graduate students and researchers in mathematical optimization, operations research, electrical engineering, statistics, and computer science will appreciate this concise introduction to the theory of convex optimization algorithms.},
	urldate = {2024-01-22},
	publisher = {Cambridge University Press},
	author = {Ryu, Ernest K. and Yin, Wotao},
	year = {2022},
	doi = {10.1017/9781009160865},
	file = {Ryu and Yin - 2022 - Large-Scale Convex Optimization Algorithms & Analyses via Monotone Operators.pdf:/Users/hongdali/Zotero/storage/JPZLJBL8/Ryu and Yin - 2022 - Large-Scale Convex Optimization Algorithms & Analyses via Monotone Operators.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/PNYCF4FI/2A7F8E7428BFA4EDB8AFACA11AB97E4C.html:text/html},
}

@article{nesterov_gradient_2013,
	title = {Gradient methods for minimizing composite functions},
	volume = {140},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-012-0629-5},
	doi = {10.1007/s10107-012-0629-5},
	abstract = {In this paper we analyze several new methods for solving optimization problems with the objective function formed as a sum of two terms: one is smooth and given by a black-box oracle, and another is a simple general convex function with known structure. Despite the absence of good properties of the sum, such problems, both in convex and nonconvex cases, can be solved with efficiency typical for the first part of the objective. For convex problems of the above structure, we consider primal and dual variants of the gradient method (with convergence rate \$\$O{\textbackslash}left(\{1 {\textbackslash}over k\}{\textbackslash}right)\$\$), and an accelerated multistep version with convergence rate \$\$O{\textbackslash}left(\{1 {\textbackslash}over k{\textasciicircum}2\}{\textbackslash}right)\$\$, where \$\$k\$\$is the iteration counter. For nonconvex problems with this structure, we prove convergence to a point from which there is no descent direction. In contrast, we show that for general nonsmooth, nonconvex problems, even resolving the question of whether a descent direction exists from a point is NP-hard. For all methods, we suggest some efficient “line search” procedures and show that the additional computational work necessary for estimating the unknown problem class parameters can only multiply the complexity of each iteration by a small constant factor. We present also the results of preliminary computational experiments, which confirm the superiority of the accelerated scheme.},
	language = {en},
	number = {1},
	urldate = {2024-09-17},
	journal = {Mathematical Programming},
	author = {Nesterov, Yu.},
	month = aug,
	year = {2013},
	keywords = {90C25, 68Q25, 90C47, Complexity theory, Optimal methods, Structural optimization, Black-box model, Convex Optimization, l1l\_1-Regularization, Local optimization, Nonsmooth optimization},
	pages = {125--161},
	file = {Nesterov - 2013 - Gradient methods for minimizing composite functions:/Users/hongdali/Zotero/storage/6LKMMPQJ/Nesterov - 2013 - Gradient methods for minimizing composite functions.pdf:application/pdf},
}

@article{rockafellar_augmented_1976,
	title = {Augmented lagrangians and applications of the proximal point algorithm in convex programming},
	volume = {1},
	issn = {0364-765X},
	url = {https://www.jstor.org/stable/3689277},
	abstract = {The theory of the proximal point algorithm for maximal monotone operators is applied to three algorithms for solving convex programs, one of which has not previously been formulated. Rate-of-convergence results for the "method of multipliers," of the strong sort already known, are derived in a generalized form relevant also to problems beyond the compass of the standard second-order conditions for optimality. The new algorithm, the "proximal method of multipliers," is shown to have much the same convergence properties, but with some potential advantages.},
	number = {2},
	urldate = {2024-09-22},
	journal = {Mathematics of Operations Research},
	author = {Rockafellar, R. T.},
	year = {1976},
	note = {Publisher: INFORMS},
	pages = {97--116},
	file = {Rockafellar - 1976 - Augmented Lagrangians and Applications of the Proximal Point Algorithm in Convex Programming:/Users/hongdali/Zotero/storage/295EN3CR/Rockafellar - 1976 - Augmented Lagrangians and Applications of the Proximal Point Algorithm in Convex Programming.pdf:application/pdf},
}

@inproceedings{paquette_catalyst_2018,
	title = {Catalyst for gradient-based nonconvex optimization},
	url = {https://proceedings.mlr.press/v84/paquette18a.html},
	abstract = {We introduce a generic scheme to solve nonconvex optimization problems using gradient-based algorithms originally designed for minimizing convex functions. Even though these methods may originally require convexity to operate, the proposed approach allows one to use them without assuming any knowledge about the convexity of the objective. In general, the scheme is guaranteed to produce a stationary point with a worst-case efficiency typical of first-order methods, and when the objective turns out to be convex, it automatically accelerates in the sense of Nesterov and achieves near-optimal convergence rate in function values. We conclude the paper by showing promising experimental results obtained by applying our approach to incremental algorithms such as SVRG and SAGA for sparse matrix factorization and for learning neural networks.},
	language = {en},
	urldate = {2024-10-02},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Paquette, Courtney and Lin, Hongzhou and Drusvyatskiy, Dmitriy and Mairal, Julien and Harchaoui, Zaid},
	month = mar,
	year = {2018},
	pages = {613--622},
	file = {Paquette et al. - 2018 - Catalyst Acceleration for Gradient-Based Non-Convex Optimization.pdf:/Users/hongdali/Zotero/storage/N4YDZMYK/Paquette et al. - 2018 - Catalyst Acceleration for Gradient-Based Non-Convex Optimization.pdf:application/pdf},
}

@article{nesterov_method_1983,
	title = {A method for solving the convex programming problem with convergence rate {O}(1/k{\textasciicircum}2)},
	url = {https://www.semanticscholar.org/paper/A-method-for-solving-the-convex-programming-problem-Nesterov/8d3a318b62d2e970122da35b2a2e70a5d12cc16f},
	abstract = {Semantic Scholar extracted view of "A method for solving the convex programming problem with convergence rate O(1/k{\textasciicircum}2)" by Y. Nesterov},
	urldate = {2024-10-10},
	journal = {Proceedings of the USSR Academy of Sciences},
	author = {Nesterov, Y.},
	year = {1983},
}

@article{ghadimi_accelerated_2016,
	title = {Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
	volume = {156},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-015-0871-8},
	doi = {10.1007/s10107-015-0871-8},
	abstract = {In this paper, we generalize the well-known Nesterov’s accelerated gradient (AG) method, originally designed for convex smooth optimization, to solve nonconvex and possibly stochastic optimization problems. We demonstrate that by properly specifying the stepsize policy, the AG method exhibits the best known rate of convergence for solving general nonconvex smooth optimization problems by using first-order information, similarly to the gradient descent method. We then consider an important class of composite optimization problems and show that the AG method can solve them uniformly, i.e., by using the same aggressive stepsize policy as in the convex case, even if the problem turns out to be nonconvex. We demonstrate that the AG method exhibits an optimal rate of convergence if the composite problem is convex, and improves the best known rate of convergence if the problem is nonconvex. Based on the AG method, we also present new nonconvex stochastic approximation methods and show that they can improve a few existing rates of convergence for nonconvex stochastic optimization. To the best of our knowledge, this is the first time that the convergence of the AG method has been established for solving nonconvex nonlinear programming in the literature.},
	language = {en},
	number = {1},
	urldate = {2024-10-12},
	journal = {Mathematical Programming},
	author = {Ghadimi, Saeed and Lan, Guanghui},
	month = mar,
	year = {2016},
	keywords = {90C25, 68Q25, Nonconvex optimization, 62L20, 90C15, Accelerated gradient, Complexity, Stochastic programming},
	pages = {59--99},
	file = {Ghadimi and Lan - 2016 - Accelerated gradient methods for nonconvex nonlinear and stochastic programming:/Users/hongdali/Zotero/storage/6BID3J9B/Ghadimi and Lan - 2016 - Accelerated gradient methods for nonconvex nonlinear and stochastic programming.pdf:application/pdf},
}

@article{nesterov_accelerating_2008,
	title = {Accelerating the cubic regularization of {Newton}’s method on convex problems},
	volume = {112},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-006-0089-x},
	doi = {10.1007/s10107-006-0089-x},
	abstract = {In this paper we propose an accelerated version of the cubic regularization of Newton’s method (Nesterov and Polyak, in Math Program 108(1): 177–205, 2006). The original version, used for minimizing a convex function with Lipschitz-continuous Hessian, guarantees a global rate of convergence of order \$\$O{\textbackslash}big(\{1 {\textbackslash}over k{\textasciicircum}2\}{\textbackslash}big)\$\$, where k is the iteration counter. Our modified version converges for the same problem class with order \$\$O{\textbackslash}big(\{1 {\textbackslash}over k{\textasciicircum}3\}{\textbackslash}big)\$\$, keeping the complexity of each iteration unchanged. We study the complexity of both schemes on different classes of convex problems. In particular, we argue that for the second-order schemes, the class of non-degenerate problems is different from the standard class.},
	language = {en},
	number = {1},
	urldate = {2024-11-20},
	journal = {Mathematical Programming},
	author = {Nesterov, Yu.},
	month = mar,
	year = {2008},
	keywords = {49M15, 49M37, 58C15, 90C25, 90C30, Condition number, Convex optimization, Cubic regularization, Global complexity bounds, Newton’s method, Non-degenerate problems, Unconstrained minimization, Worst-case complexity},
	pages = {159--181},
	file = {Nesterov - 2008 - Accelerating the cubic regularization of Newton’s method on convex problems.pdf:/Users/hongdali/Zotero/storage/GWZSARLR/Nesterov - 2008 - Accelerating the cubic regularization of Newton’s method on convex problems.pdf:application/pdf},
}

@book{ying_modern_2021,
	series = {{MOS}-{SIAM} {Series} on {Optimization}},
	title = {Modern {Nonconvex} {Nondifferentiable} {Optimization}},
	volume = {1},
	isbn = {978-1-61197-674-8},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611976748},
	language = {en},
	number = {29},
	urldate = {2024-11-20},
	publisher = {MOS-SIAM},
	author = {Ying, Cui and Jong-Shi, Pang},
	year = {2021},
	file = {1:/Users/hongdali/Zotero/storage/AU7MC9JA/1.9781611976748.ch1.pdf:application/pdf;2:/Users/hongdali/Zotero/storage/R3ISLTZ7/1.9781611976748.ch2 (1).pdf:application/pdf;3:/Users/hongdali/Zotero/storage/TD25IPT9/1.9781611976748.ch3.pdf:application/pdf;4:/Users/hongdali/Zotero/storage/59CJTJPL/1.9781611976748.ch4.pdf:application/pdf;5:/Users/hongdali/Zotero/storage/YD7IDPZ8/1.9781611976748.ch5.pdf:application/pdf;6:/Users/hongdali/Zotero/storage/JQ5TVRB5/1.9781611976748.ch6.pdf:application/pdf;7:/Users/hongdali/Zotero/storage/AFCH4MZY/1.9781611976748.ch7.pdf:application/pdf;8:/Users/hongdali/Zotero/storage/HF8Q4DUQ/1.9781611976748.ch8.pdf:application/pdf;9:/Users/hongdali/Zotero/storage/GW8V85A5/1.9781611976748.ch9.pdf:application/pdf;10:/Users/hongdali/Zotero/storage/M63NSY62/1.9781611976748.ch10.pdf:application/pdf;11:/Users/hongdali/Zotero/storage/DHI9VZVP/1.9781611976748.ch11.pdf:application/pdf;Bib, Idx, glossary:/Users/hongdali/Zotero/storage/NUZ5WXJ4/Ba and Pang - 2022 - Exact Penalization of Generalized Nash Equilibrium Problems.pdf:application/pdf;Modern Nonconvex Nondifferentiable Optimization.pdf:/Users/hongdali/Zotero/storage/FMCJICH7/Ying and Jong-Shi - 2021 - Modern Nonconvex Nondifferentiable Optimization.pdf:application/pdf},
}

@unpublished{duy_khanh_inexact_2023,
	title = {Inexact proximal methods for weakly convex functions},
	abstract = {This paper proposes and develops inexact proximal methods for finding stationary points of the sum of a smooth function and a nonsmooth weakly convex one, where an error is present in the calculation of the proximal mapping of the nonsmooth term. A general framework for finding zeros of a continuous mapping is derived from our previous paper on this subject to establish convergence properties of the inexact proximal point method when the smooth term is vanished and of the inexact proximal gradient method when the smooth term satisfies a descent condition. The inexact proximal point method achieves global convergence with constructive convergence rates when the Moreau envelope of the objective function satisfies the Kurdyka-Lojasiewicz (KL) property. Meanwhile, when the smooth term is twice continuously differentiable with a Lipschitz continuous gradient and a differentiable approximation of the objective function satisfies the KL property, the inexact proximal gradient method achieves the global convergence of iterates with constructive convergence rates.},
	author = {Duy Khanh, Pham and Mordukhovich, Boris and Phat, Vo and Tran, Dat},
	month = jul,
	year = {2023},
	doi = {10.48550/arXiv.2307.15596},
	file = {Duy Khanh et al. - 2023 - Inexact proximal methods for weakly convex functions.pdf:/Users/hongdali/Zotero/storage/TVZ3BV26/Duy Khanh et al. - 2023 - Inexact proximal methods for weakly convex functions.pdf:application/pdf},
}

@article{polyak_minimization_1969,
	series = {{USSR} {Computational} {Mathematics} and {Mathematical} {Physics}},
	title = {Minimization of unsmooth functionals},
	issn = {0041-5553},
	url = {https://www.researchgate.net/publication/238270077_Minimization_of_Unsmooth_Functionals},
	abstract = {PDF {\textbar} WE consider the minimization of a convex, but but necessarily differentiable, functional in a convex set of Hilbert space. The minimization method... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2024-12-04},
	author = {Polyak, B.T.},
	month = dec,
	year = {1969},
	file = {Polyak - 2024 - Minimization of unsmooth functionals.pdf:/Users/hongdali/Zotero/storage/EAAGQFFM/Polyak - 2024 - Minimization of unsmooth functionals.pdf:application/pdf},
}

@article{grapiglia_accelerated_2019,
	title = {Accelerated regularized newton methods for minimizing composite convex functions},
	volume = {29},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/17M1142077},
	doi = {10.1137/17M1142077},
	abstract = {In this paper, we study the regularized second-order methods for unconstrained minimization of a  twice-differentiable (convex or nonconvex) objective function. For the current function, these methods automatically achieve the best possible global complexity estimates among different Hölder classes containing the Hessian of the objective. We show that such methods for functional residual and for the norm of the gradient must be different. For development of the latter methods, we introduced two new line-search acceptance criteria, which can be seen as generalizations of the Armijo condition.},
	number = {1},
	urldate = {2024-12-04},
	journal = {SIAM Journal on Optimization},
	author = {Grapiglia, Geovani N. and Nesterov, Yurii},
	month = jan,
	year = {2019},
	pages = {77--99},
	file = {Grapiglia and Nesterov - 2019 - Accelerated Regularized Newton Methods for Minimizing Composite Convex Functions.pdf:/Users/hongdali/Zotero/storage/Y4W3SLNY/Grapiglia and Nesterov - 2019 - Accelerated Regularized Newton Methods for Minimizing Composite Convex Functions.pdf:application/pdf},
}

@article{lin_catalyst_2018,
	title = {Catalyst acceleration for first-order convex optimization: from theory to practice},
	volume = {18},
	url = {http://jmlr.org/papers/v18/17-748.html},
	number = {212},
	journal = {Journal of Machine Learning Research},
	author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
	year = {2018},
	pages = {1--54},
	file = {Catalyst acceleration for first-order convex optimization from theory to practice:/Users/hongdali/Zotero/storage/M5UYPQIT/Catalyst acceleration for first-order convex optimization from theory to practice.pdf:application/pdf},
}

@inproceedings{lin_universal_2015,
	title = {A universal catalyst for first-order optimization},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/c164bbc9d6c72a52c599bbb43d8db8e1-Paper.pdf},
	booktitle = {Procedings of {Advances} in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	file = {A Universal Catalyst for First-Order Optimization HAL open science:/Users/hongdali/Zotero/storage/8ILQU6TZ/Lin et al. - 2015 - A Universal Catalyst for First-Order Optimization.pdf:application/pdf;A Universal Catalyst for First-Order Optimization NISP:/Users/hongdali/Zotero/storage/DAYMXD2L/Lin et al. - 2015 - A Universal Catalyst for First-Order Optimization.pdf:application/pdf},
}

@inproceedings{lee_geometric_2021,
	title = {A geometric structure of acceleration and {Its} role in making gradients small fast},
	volume = {34},
	url = {https://papers.nips.cc/paper/2021/hash/647c722bf90a49140184672e0d3723e3-Abstract.html},
	urldate = {2024-12-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Jongmin and Park, Chanwoo and Ryu, Ernest},
	year = {2021},
	pages = {11999--12012},
	file = {A Geometric Structure of Acceleration and Its Role in Making Gradients Small Fast:/Users/hongdali/Zotero/storage/IUTTGIE3/Lee et al. - 2021 - A Geometric Structure of Acceleration and Its Role in Making Gradients Small Fast.pdf:application/pdf},
}
