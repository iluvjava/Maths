
@book{nesterov_lectures_2018,
	address = {Cham},
	series = {Springer {Optimization} and {Its} {Applications}},
	title = {Lectures on {Convex} {Optimization}},
	volume = {137},
	isbn = {978-3-319-91577-7 978-3-319-91578-4},
	url = {http://link.springer.com/10.1007/978-3-319-91578-4},
	urldate = {2023-10-11},
	publisher = {Springer International Publishing},
	author = {Nesterov, Yurii},
	year = {2018},
	keywords = {Algorithmic Complexity, Fast Gradient Methods, Interior-Point Methods, Numerical Optimization, Optimization, Optimization in Relative Scale, Self-Concordant Functions, Smoothing Technique},
	file = {Nesterov - 2018 - Lectures on Convex Optimization.pdf:/Users/hongdali/Zotero/storage/HSCCPYL9/Nesterov - 2018 - Lectures on Convex Optimization.pdf:application/pdf},
}

@book{beck_first-order_2017,
	address = {israel},
	series = {{MOS}-{SIAM} {Series} in {Optimization}},
	title = {First-order {Methods} in {Optimization}},
	isbn = {978-1-61197-498-0},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611974997},
	language = {en},
	urldate = {2023-10-19},
	publisher = {SIAM},
	author = {Beck, Amir},
	year = {2017},
	keywords = {First-order Methods, Non-smooth Optimization, Numerical Optimization, Optimization},
	file = {First-Order Methods in Optimization  SIAM Publication.pdf:/Users/hongdali/Zotero/storage/P2HFAVVQ/First-Order Methods in Optimization  SIAM Publication.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/88BHKZ6Y/1.html:text/html},
}

@article{noel_nesterovs_nodate,
	title = {Nesterov's method for convex optimization},
	volume = {65},
	url = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/epdf/10.1137/21M1390037},
	doi = {10.1137/21M1390037},
	abstract = {While Nesterov's algorithm for computing the minimum of a convex function is now over forty years old, it is rarely presented in texts for a first course in optimization. This is unfortunate since for many problems this algorithm is superior to the ubiquitous steepest descent algorithm, and it is equally simple to implement. This article presents an elementary analysis of Nesterov's algorithm that parallels that of steepest descent. It is envisioned that this presentation of Nesterov's algorithm could easily be covered in a few lectures following the introductory material on convex functions and steepest descent included in every course on optimization.},
	language = {en},
	number = {2},
	urldate = {2023-10-09},
	journal = {SIAM Review},
	author = {Noel, Walkington},
	pages = {539--562},
	file = {Nesterov's Method for Convex Optimization.pdf:/Users/hongdali/Zotero/storage/BZC6TFHX/Nesterov's Method for Convex Optimization.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/M8MDL6E3/21M1390037.html:text/html},
}

@article{rockafellar_monotone_1976,
	title = {Monotone operators and the proximal point algorithm},
	volume = {14},
	issn = {0363-0129, 1095-7138},
	url = {http://epubs.siam.org/doi/10.1137/0314056},
	doi = {10.1137/0314056},
	language = {en},
	number = {5},
	urldate = {2023-11-06},
	journal = {SIAM Journal on Control and Optimization},
	author = {Rockafellar, R. Tyrrell},
	month = aug,
	year = {1976},
	pages = {877--898},
	file = {Monotone Operators and the Proximal Point Algorithm.pdf:/Users/hongdali/Zotero/storage/5L82ZWY4/Monotone Operators and the Proximal Point Algorithm.pdf:application/pdf},
}

@article{guler_convergence_1991,
	title = {On the convergence of the proximal point algorithm for convex minimization},
	volume = {29},
	issn = {03630129},
	url = {https://www.proquest.com/docview/925962166/abstract/A60B4BA7798A45D1PQ/1},
	doi = {10.1137/0329022},
	abstract = {The proximal point algorithm (PPA) for the convex minimization problem \${\textbackslash}min \_\{x {\textbackslash}in H\} f(x)\$, where \$f:H {\textbackslash}to R {\textbackslash}cup {\textbackslash}\{ {\textbackslash}infty {\textbackslash}\} \$ is a proper, lower semicontinuous (lsc) function in a Hilbert space \$H\$ is considered. Under this minimal assumption on \$f\$, it is proved that the PPA, with positive parameters \${\textbackslash}\{ {\textbackslash}lambda \_k {\textbackslash}\} \_\{k = 1\}{\textasciicircum}{\textbackslash}infty \$, converges in general if and only if \${\textbackslash}sigma \_n = {\textbackslash}sum\_\{k = 1\}{\textasciicircum}n \{{\textbackslash}lambda \_k {\textbackslash}to {\textbackslash}infty \} \$. Global convergence rate estimates for the residual \$f(x\_n ) - f(u)\$, where \$x\_n \$ is the \$n\$th iterate of the PPA and \$ u {\textbackslash}in H \$ is arbitrary are given. An open question of Rockafellar is settled by giving an example of a PPA for which \$x\_n \$ converges weakly but not strongly to a minimizes of \$f\$.},
	language = {English},
	number = {2},
	urldate = {2024-05-18},
	journal = {SIAM Journal on Control and Optimization},
	author = {Guler, Osman},
	month = mar,
	year = {1991},
	keywords = {Algorithms, Convex analysis, Hilbert space, Mathematics},
	pages = {17},
	file = {Guler - 1991 - On the Convergence of the Proximal Point Algorithm.pdf:/Users/hongdali/Zotero/storage/9AWZSLK4/Guler - 1991 - On the Convergence of the Proximal Point Algorithm.pdf:application/pdf},
}

@article{attouch_convergence_2009,
	title = {On the convergence of the proximal algorithm for nonsmooth functions involving analytic features},
	volume = {116},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-007-0133-5},
	doi = {10.1007/s10107-007-0133-5},
	abstract = {We study the convergence of the proximal algorithm applied to nonsmooth functions that satisfy the Łjasiewicz inequality around their generalized critical points. Typical examples of functions complying with these conditions are continuous semialgebraic or subanalytic functions. Following Łjasiewicz’s original idea, we prove that any bounded sequence generated by the proximal algorithm converges to some generalized critical point. We also obtain convergence rate results which are related to the flatness of the function by means of Łjasiewicz exponents. Apart from the sharp and elliptic cases which yield finite or geometric convergence, the decay estimates that are derived are of the type O(k−s), where s ∈ (0, + ∞) depends on the flatness of the function.},
	language = {en},
	number = {1},
	urldate = {2024-04-16},
	journal = {Mathematical Programming},
	author = {Attouch, Hedy and Bolte, Jérôme},
	month = jan,
	year = {2009},
	keywords = {47N10, 90C26, 90C30, Łjasiewicz inequality, Proximal algorithm, Subanalytic functions},
	pages = {5--16},
	file = {Full Text PDF:/Users/hongdali/Zotero/storage/AGIWHMYN/Attouch and Bolte - 2009 - On the convergence of the proximal algorithm for nonsmooth functions involving analytic features.pdf:application/pdf},
}

@book{ryu_large-scale_2022,
	address = {Cambridge},
	title = {Large-scale {Convex} {Optimization}: {Algorithms} \& {Analyses} via {Monotone} {Operators}},
	isbn = {978-1-009-16085-8},
	shorttitle = {Large-{Scale} {Convex} {Optimization}},
	url = {https://large-scale-book.mathopt.com/},
	abstract = {Starting from where a first course in convex optimization leaves off, this text presents a unified analysis of first-order optimization methods – including parallel-distributed algorithms – through the abstraction of monotone operators. With the increased computational power and availability of big data over the past decade, applied disciplines have demanded that larger and larger optimization problems be solved. This text covers the first-order convex optimization methods that are uniquely effective at solving these large-scale optimization problems. Readers will have the opportunity to construct and analyze many well-known classical and modern algorithms using monotone operators, and walk away with a solid understanding of the diverse optimization algorithms. Graduate students and researchers in mathematical optimization, operations research, electrical engineering, statistics, and computer science will appreciate this concise introduction to the theory of convex optimization algorithms.},
	urldate = {2024-01-22},
	publisher = {Cambridge University Press},
	author = {Ryu, Ernest K. and Yin, Wotao},
	year = {2022},
	doi = {10.1017/9781009160865},
	file = {Ryu and Yin - 2022 - Large-Scale Convex Optimization Algorithms & Analyses via Monotone Operators.pdf:/Users/hongdali/Zotero/storage/JPZLJBL8/Ryu and Yin - 2022 - Large-Scale Convex Optimization Algorithms & Analyses via Monotone Operators.pdf:application/pdf;Snapshot:/Users/hongdali/Zotero/storage/PNYCF4FI/2A7F8E7428BFA4EDB8AFACA11AB97E4C.html:text/html},
}

@article{rockafellar_augmented_1976,
	title = {Augmented lagrangians and applications of the proximal point algorithm in convex programming},
	volume = {1},
	issn = {0364-765X},
	url = {https://www.jstor.org/stable/3689277},
	abstract = {The theory of the proximal point algorithm for maximal monotone operators is applied to three algorithms for solving convex programs, one of which has not previously been formulated. Rate-of-convergence results for the "method of multipliers," of the strong sort already known, are derived in a generalized form relevant also to problems beyond the compass of the standard second-order conditions for optimality. The new algorithm, the "proximal method of multipliers," is shown to have much the same convergence properties, but with some potential advantages.},
	number = {2},
	urldate = {2024-09-22},
	journal = {Mathematics of Operations Research},
	author = {Rockafellar, R. T.},
	year = {1976},
	note = {Publisher: INFORMS},
	pages = {97--116},
	file = {Rockafellar - 1976 - Augmented Lagrangians and Applications of the Proximal Point Algorithm in Convex Programming:/Users/hongdali/Zotero/storage/295EN3CR/Rockafellar - 1976 - Augmented Lagrangians and Applications of the Proximal Point Algorithm in Convex Programming.pdf:application/pdf},
}

@article{nesterov_method_1983,
	title = {A method for solving the convex programming problem with convergence rate {O}(1/k{\textasciicircum}2)},
	url = {https://www.semanticscholar.org/paper/A-method-for-solving-the-convex-programming-problem-Nesterov/8d3a318b62d2e970122da35b2a2e70a5d12cc16f},
	abstract = {Semantic Scholar extracted view of "A method for solving the convex programming problem with convergence rate O(1/k{\textasciicircum}2)" by Y. Nesterov},
	urldate = {2024-10-10},
	journal = {Proceedings of the USSR Academy of Sciences},
	author = {Nesterov, Y.},
	year = {1983},
}

@article{polyak_minimization_1969,
	series = {{USSR} {Computational} {Mathematics} and {Mathematical} {Physics}},
	title = {Minimization of unsmooth functionals},
	issn = {0041-5553},
	url = {https://www.researchgate.net/publication/238270077_Minimization_of_Unsmooth_Functionals},
	abstract = {PDF {\textbar} WE consider the minimization of a convex, but but necessarily differentiable, functional in a convex set of Hilbert space. The minimization method... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2024-12-04},
	author = {Polyak, B.T.},
	month = dec,
	year = {1969},
	file = {Polyak - 2024 - Minimization of unsmooth functionals.pdf:/Users/hongdali/Zotero/storage/EAAGQFFM/Polyak - 2024 - Minimization of unsmooth functionals.pdf:application/pdf},
}
