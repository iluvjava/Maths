
@inproceedings{ahn_understanding_2022,
  title     = {Understanding {Nesterov}'s acceleration via proximal point method},
  url       = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977066.9},
  doi       = {https://doi.org/10.1137/1.9781611977066},
  abstract  = {The proximal point method (PPM) is a fundamental method in optimization that is often used as a building block for designing optimization algorithms. In this work, we use the PPM method to provide conceptually simple derivations along with convergence analyses of different versions of Nesterov's accelerated gradient method (AGM). The key observation is that AGM is a simple approximation of PPM, which results in an elementary derivation of the update equations and stepsizes of AGM. This view also leads to a transparent and conceptually simple analysis of AGM's convergence by using the analysis of PPM. The derivations also naturally extend to the strongly convex case. Ultimately, the results presented in this paper are of both didactic and conceptual value; they unify and explain existing variants of AGM while motivating other accelerated methods for practically relevant settings.},
  urldate   = {2023-11-04},
  booktitle = {Symposium on {Simplicity} in {Algorithms}},
  publisher = {SIAM},
  author    = {Ahn, Kwangjun and Sra, Suvrit},
  month     = jun,
  year      = {2022},
  keywords  = {Computer Science - Machine Learning, Optimization and Control},
  pages     = {117--130},
  file      = {Ahn and Sra - 2022 - Understanding nesterov's acceleration via proximal.pdf:/Volumes/T6/Zotero Library/storage/PZBWUWW5/Ahn and Sra - 2022 - Understanding nesterov's acceleration via proximal.pdf:application/pdf;arXiv.org Snapshot:/Volumes/T6/Zotero Library/storage/WSSGK9Q4/2005.html:text/html}
}

@article{apidopoulos_convergence_2018,
  title    = {Convergence rate of inertial {Forward}-{Backward} algorithm beyond {Nesterov}'s rule},
  url      = {https://hal.science/hal-01551873},
  doi      = {10.1007/s10107-018-1350-9},
  abstract = {In this paper we study the convergence of an Inertial Forward-Backward algorithm, with a particular choice of an over-relaxation term. In particular we show that for a sequence of overrrelaxation parameters, that do not satisfy Nesterov’s rule one can still expect some relatively fast convergence properties for the objective function. In addition we complement this work by studying the convergence of the algorithm in the case where the proximal operator is inexactly computed with the presence of some errors and we give sufficient conditions over these errors in order to obtain some convergence properties for the objective function .},
  urldate  = {2024-12-20},
  journal  = {Mathematical Programming, Series A},
  author   = {Apidopoulos, Vassilis and Aujol, Jean-François and Dossal, Charles H},
  month    = nov,
  year     = {2018},
  keywords = {Convex optimization, rate of convergence, inertial FB algorithm, Nesterov's rule, proximal operator},
  pages    = {1--20},
  file     = {Convergence rate of inertial Forward-Backward algorithm beyond Nesterov rule:/Volumes/T6/Zotero Library/storage/U3PBW5UM/Apidopoulos et al. - 2018 - Convergence rate of inertial Forward-Backward algorithm beyond Nesterov's rule.pdf:application/pdf}
}

@article{attouch_convergence_2009,
  title    = {On the convergence of the proximal algorithm for nonsmooth functions involving analytic features},
  volume   = {116},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-007-0133-5},
  doi      = {10.1007/s10107-007-0133-5},
  abstract = {We study the convergence of the proximal algorithm applied to nonsmooth functions that satisfy the Łjasiewicz inequality around their generalized critical points. Typical examples of functions complying with these conditions are continuous semialgebraic or subanalytic functions. Following Łjasiewicz’s original idea, we prove that any bounded sequence generated by the proximal algorithm converges to some generalized critical point. We also obtain convergence rate results which are related to the flatness of the function by means of Łjasiewicz exponents. Apart from the sharp and elliptic cases which yield finite or geometric convergence, the decay estimates that are derived are of the type O(k−s), where s ∈ (0, + ∞) depends on the flatness of the function.},
  language = {en},
  number   = {1},
  urldate  = {2024-04-16},
  journal  = {Mathematical Programming},
  author   = {Attouch, Hedy and Bolte, Jérôme},
  month    = jan,
  year     = {2009},
  keywords = {90C30, Proximal algorithm, 47N10, 90C26, Łjasiewicz inequality, Subanalytic functions},
  pages    = {5--16},
  file     = {Full Text PDF:/Volumes/T6/Zotero Library/storage/AGIWHMYN/Attouch and Bolte - 2009 - On the convergence of the proximal algorithm for nonsmooth functions involving analytic features.pdf:application/pdf}
}

@article{attouch_first-order_2022,
  title    = {First-order optimization algorithms via inertial systems with {Hessian} driven damping},
  volume   = {193},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-020-01591-1},
  doi      = {10.1007/s10107-020-01591-1},
  abstract = {In a Hilbert space setting, for convex optimization, we analyze the convergence rate of a class of first-order algorithms involving inertial features. They can be interpreted as discrete time versions of inertial dynamics involving both viscous and Hessian-driven dampings. The geometrical damping driven by the Hessian intervenes in the dynamics in the form \$\${\textbackslash}nabla {\textasciicircum}2 f (x(t)) {\textbackslash}dot\{x\} (t)\$\$. By treating this term as the time derivative of \$\$ {\textbackslash}nabla f (x (t)) \$\$, this gives, in discretized form, first-order algorithms in time and space. In addition to the convergence properties attached to Nesterov-type accelerated gradient methods, the algorithms thus obtained are new and show a rapid convergence towards zero of the gradients. On the basis of a regularization technique using the Moreau envelope, we extend these methods to non-smooth convex functions with extended real values. The introduction of time scale factors makes it possible to further accelerate these algorithms. We also report numerical results on structured problems to support our theoretical findings.},
  language = {en},
  number   = {1},
  urldate  = {2024-12-29},
  journal  = {Mathematical Programming},
  author   = {Attouch, Hedy and Chbani, Zaki and Fadili, Jalal and Riahi, Hassan},
  month    = may,
  year     = {2022},
  keywords = {90C25, 65K05, 37N40, 46N10, 49M30, 65B99, 65K10, 90B50, Hessian driven damping, Inertial optimization algorithms, Nesterov accelerated gradient method, Ravine method, Time rescaling},
  pages    = {113--155},
  file     = {First-order optimization algorithms via inertial systems with Hessian driven damping:/Volumes/T6/Zotero Library/storage/89LNXY5T/Attouch et al. - 2022 - First-order optimization algorithms via inertial systems with Hessian driven damping.pdf:application/pdf}
}

@article{aujol_parameter-free_2024,
  title    = {Parameter-{Free} {FISTA} by adaptive restart and backtracking},
  url      = {https://epubs.siam.org/doi/10.1137/23M158961X},
  abstract = {We consider a combined restarting and adaptive backtracking strategy for the popular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for accelerating the convergence speed of large-scale structured convex optimization problems. Several variants of FISTA enjoy a provable linear convergence rate for the function values \$F(x\_n)\$ of the form \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}{\textasciitilde}n\})\$ under the prior knowledge of problem conditioning, i.e. of the ratio between the ({\textbackslash}L ojasiewicz) parameter \${\textbackslash}mu\$ determining the growth of the objective function and the Lipschitz constant \$L\$ of its smooth component. These parameters are nonetheless hard to estimate in many practical cases. Recent works address the problem by estimating either parameter via suitable adaptive strategies. In our work both parameters can be estimated at the same time by means of an algorithmic restarting scheme where, at each restart, a non-monotone estimation of \$L\$ is performed. For this scheme, theoretical convergence results are proved, showing that a \${\textbackslash}mathcal\{O\}( e{\textasciicircum}\{-K{\textbackslash}sqrt\{{\textbackslash}mu/L\}n\})\$ convergence speed can still be achieved along with quantitative estimates of the conditioning. The resulting Free-FISTA algorithm is therefore parameter-free. Several numerical results are reported to confirm the practical interest of its use in many exemplar problems.},
  language = {en},
  urldate  = {2023-10-09},
  journal  = {SIAM Journal on Optimization},
  author   = {Aujol, Jean-François and Calatroni, Luca and Dossal, Charles and Labarrière, Hippolyte and Rondepierre, Aude},
  month    = dec,
  year     = {2024},
  pages    = {3259--3285},
  file     = {Full Text PDF:/Volumes/T6/Zotero Library/storage/3YBXJH4F/Aujol et al. - 2023 - Parameter-Free FISTA by Adaptive Restart and Backt.pdf:application/pdf}
}

@article{bauschke_applying_2020,
  title    = {Applying {FISTA} to optimization problems (with or) without minimizers},
  volume   = {184},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-019-01415-x},
  doi      = {10.1007/s10107-019-01415-x},
  abstract = {Beck and Teboulle’s FISTA method for finding a minimizer of the sum of two convex functions, one of which has a Lipschitz continuous gradient whereas the other may be nonsmooth, is arguably the most important optimization algorithm of the past decade. While research activity on FISTA has exploded ever since, the mathematically challenging case when the original optimization problem has no minimizer has found only limited attention. In this work, we systematically study FISTA and its variants. We present general results that are applicable, regardless of the existence of minimizers.},
  language = {en},
  number   = {1},
  urldate  = {2024-07-13},
  journal  = {Mathematical Programming},
  author   = {Bauschke, Heinz H. and Bui, Minh N. and Wang, Xianfu},
  month    = nov,
  year     = {2020},
  keywords = {65K05, Convex function, FISTA, Forward-backward method, Nesterov acceleration, Primary 90C25, Proximal gradient algorithm, Secondary 49M27},
  pages    = {349--381},
  file     = {Applying FISTA to optimization problems (with or) without minimizers:/Volumes/T6/Zotero Library/storage/S522MKDG/Bauschke et al. - 2020 - Applying FISTA to optimization problems (with or) without minimizers.pdf:application/pdf}
}

@article{beck_fast_2009,
  title    = {A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
  volume   = {2},
  issn     = {1936-4954},
  url      = {http://epubs.siam.org/doi/10.1137/080716542},
  doi      = {10.1137/080716542},
  abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be signiﬁcantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
  language = {en},
  number   = {1},
  urldate  = {2023-11-16},
  journal  = {SIAM Journal on Imaging Sciences},
  author   = {Beck, Amir and Teboulle, Marc},
  month    = jan,
  year     = {2009},
  pages    = {183--202},
  file     = {Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:/Volumes/T6/Zotero Library/storage/H7CGKLL3/Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf:application/pdf}
}

@article{beck_fast_2009-1,
  title    = {Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems},
  volume   = {18},
  issn     = {1941-0042},
  url      = {https://ieeexplore.ieee.org/document/5173518},
  doi      = {10.1109/TIP.2009.2028250},
  abstract = {This paper studies gradient-based schemes for image denoising and deblurring problems based on the discretized total variation (TV) minimization model with constraints. We derive a fast algorithm for the constrained TV-based image deburring problem. To achieve this task, we combine an acceleration of the well known dual approach to the denoising problem with a novel monotone version of a fast iterative shrinkage/thresholding algorithm (FISTA) we have recently introduced. The resulting gradient-based algorithm shares a remarkable simplicity together with a proven global rate of convergence which is significantly better than currently known gradient projections-based methods. Our results are applicable to both the anisotropic and isotropic discretized TV functionals. Initial numerical results demonstrate the viability and efficiency of the proposed algorithms on image deblurring problems with box constraints.},
  number   = {11},
  urldate  = {2023-10-19},
  journal  = {IEEE Transactions on Image Processing},
  author   = {Beck, Amir and Teboulle, Marc},
  month    = nov,
  year     = {2009},
  pages    = {2419--2434},
  file     = {IEEE Xplore Abstract Record:/Volumes/T6/Zotero Library/storage/G4ZXMUZG/5173518.html:text/html;IEEE Xplore Full Text PDF:/Volumes/T6/Zotero Library/storage/J2T9LVVK/Beck and Teboulle - 2009 - Fast Gradient-Based Algorithms for Constrained Tot.pdf:application/pdf}
}

@book{beck_first-order_2017,
  series    = {{MOS}-{SIAM} {Series} in {Optimization}},
  title     = {First-order {Methods} in {Optimization}},
  isbn      = {978-1-61197-498-0},
  url       = {https://epubs.siam.org/doi/book/10.1137/1.9781611974997},
  language  = {en},
  urldate   = {2023-10-19},
  publisher = {SIAM},
  author    = {Beck, Amir},
  year      = {2017},
  keywords  = {Optimization, First-order Methods, Numerical Optimization, Non-smooth Optimization},
  file      = {First-Order Methods in Optimization  SIAM Publication.pdf:/Volumes/T6/Zotero Library/storage/P2HFAVVQ/First-Order Methods in Optimization  SIAM Publication.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/88BHKZ6Y/1.html:text/html}
}

@article{bertsekas_incremental_2011,
  title    = {Incremental proximal methods for large scale convex optimization},
  volume   = {129},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-011-0472-0},
  doi      = {10.1007/s10107-011-0472-0},
  abstract = {We consider the minimization of a sum \$\$\{{\textbackslash}sum\_\{i=1\}{\textasciicircum}mf\_i(x)\}\$\$consisting of a large number of convex component functions fi. For this problem, incremental methods consisting of gradient or subgradient iterations applied to single components have proved very effective. We propose new incremental methods, consisting of proximal iterations applied to single components, as well as combinations of gradient, subgradient, and proximal iterations. We provide a convergence and rate of convergence analysis of a variety of such methods, including some that involve randomization in the selection of components. We also discuss applications in a few contexts, including signal processing and inference/machine learning.},
  language = {en},
  number   = {2},
  urldate  = {2024-08-04},
  journal  = {Mathematical Programming},
  author   = {Bertsekas, Dimitri P.},
  month    = oct,
  year     = {2011},
  keywords = {90C33, 90C90, Convex, Gradient method, Incremental method, Proximal algorithm},
  pages    = {163--195},
  file     = {Bertsekas - 2011 - Incremental proximal methods for large scale conve.pdf:/Volumes/T6/Zotero Library/storage/VM3LNMGN/Bertsekas - 2011 - Incremental proximal methods for large scale conve.pdf:application/pdf}
}

@misc{bertsekas_incremental_2017,
  title      = {Incremental gradient, subgradient, and proximal methods for convex optimization: {A} survey},
  shorttitle = {Incremental {Gradient}, {Subgradient}, and {Proximal} {Methods} for {Convex} {Optimization}},
  url        = {http://arxiv.org/abs/1507.01030},
  abstract   = {We survey incremental methods for minimizing a sum \${\textbackslash}sum\_\{i=1\}{\textasciicircum}mf\_i(x)\$ consisting of a large number of convex component functions \$f\_i\$. Our methods consist of iterations applied to single components, and have proved very effective in practice. We introduce a unified algorithmic framework for a variety of such methods, some involving gradient and subgradient iterations, which are known, and some involving combinations of subgradient and proximal methods, which are new and offer greater flexibility in exploiting the special structure of \$f\_i\$. We provide an analysis of the convergence and rate of convergence properties of these methods, including the advantages offered by randomization in the selection of components. We also survey applications in inference/machine learning, signal processing, and large-scale and distributed optimization.},
  urldate    = {2024-10-12},
  publisher  = {arXiv},
  author     = {Bertsekas, Dimitri P.},
  month      = dec,
  year       = {2017},
  keywords   = {Mathematics - Optimization and Control, Computer Science - Data Structures and Algorithms, Mathematics - Numerical Analysis, Computer Science - Systems and Control},
  file       = {Preprint PDF:/Volumes/T6/Zotero Library/storage/HEAFR5YI/Bertsekas - 2017 - Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization A Survey.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/I96TMLNW/1507.html:text/html}
}

@article{bezanson_julia_2017,
  title      = {Julia: {A} fresh approach to numerical computing},
  volume     = {59},
  issn       = {0036-1445},
  shorttitle = {Julia},
  url        = {https://epubs.siam.org/doi/10.1137/141000671},
  doi        = {10.1137/141000671},
  abstract   = {This is the third in a series of papers on aspects of modern computing environments that are relevant to statistical data analysis. In this paper, we discuss programming environments. In particular, we argue that integrated programming environments (for example, Lisp and Smalltalk environments) are more appropriate as a base for data analysis than conventional operating systems (for example, Unix).},
  number     = {1},
  urldate    = {2023-11-20},
  journal    = {SIAM Review},
  author     = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  month      = jan,
  year       = {2017},
  pages      = {65--98},
  file       = {Full Text:/Volumes/T6/Zotero Library/storage/UTXT9Y9J/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:application/pdf}
}

@misc{bu_note_2020,
  title      = {A {Note} on {Nesterov}'s {Accelerated} {Method} in {Nonconvex} {Optimization}: a {Weak} {Estimate} {Sequence} {Approach}},
  shorttitle = {A {Note} on {Nesterov}'s {Accelerated} {Method} in {Nonconvex} {Optimization}},
  url        = {http://arxiv.org/abs/2006.08548},
  abstract   = {We present a variant of accelerated gradient descent algorithms, adapted from Nesterov's optimal first-order methods, for weakly-quasi-convex and weakly-quasi-strongly-convex functions. We show that by tweaking the so-called estimate sequence method, the derived algorithm achieves optimal convergence rate for weakly-quasi-convex and weakly-quasi-strongly-convex in terms of oracle complexity. In particular, for a weakly-quasi-convex function with Lipschitz continuous gradient, we require \$O({\textbackslash}frac\{1\}\{{\textbackslash}sqrt\{{\textbackslash}varepsilon\}\})\$ iterations to acquire an \${\textbackslash}varepsilon\$-solution; for weakly-quasi-strongly-convex functions, the iteration complexity is \$O{\textbackslash}left( {\textbackslash}ln{\textbackslash}left({\textbackslash}frac\{1\}\{{\textbackslash}varepsilon\}{\textbackslash}right) {\textbackslash}right)\$. Furthermore, we discuss the implications of these algorithms for linear quadratic optimal control problem.},
  urldate    = {2024-11-23},
  publisher  = {arXiv},
  author     = {Bu, Jingjing and Mesbahi, Mehran},
  month      = jun,
  year       = {2020},
  note       = {arXiv:2006.08548},
  keywords   = {Mathematics - Optimization and Control},
  file       = {A Note on Nesterov's Accelerated Method in Nonconvex Optimization a Weak Estimate Sequence Approach:/Volumes/T6/Zotero Library/storage/CYFFGSSW/Bu and Mesbahi - 2020 - A Note on Nesterov's Accelerated Method in Nonconvex Optimization a Weak Estimate Sequence Approach.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/KHR4HHWX/2006.html:text/html}
}

@misc{cai_variance_2023,
  title     = {Variance {Reduced} {Halpern} {Iteration} for {Finite}-{Sum} {Monotone} {Inclusions}},
  url       = {http://arxiv.org/abs/2310.02987},
  abstract  = {Machine learning approaches relying on such criteria as adversarial robustness or multi-agent settings have raised the need for solving game-theoretic equilibrium problems. Of particular relevance to these applications are methods targeting finite-sum structure, which generically arises in empirical variants of learning problems in these contexts. Further, methods with computable approximation errors are highly desirable, as they provide verifiable exit criteria. Motivated by these applications, we study finite-sum monotone inclusion problems, which model broad classes of equilibrium problems. Our main contributions are variants of the classical Halpern iteration that employ variance reduction to obtain improved complexity guarantees in which \$n\$ component operators in the finite sum are ``on average'' either cocoercive or Lipschitz continuous and monotone, with parameter \$L\$. The resulting oracle complexity of our methods, which provide guarantees for the last iterate and for a (computable) operator norm residual, is \${\textbackslash}widetilde\{{\textbackslash}mathcal\{O\}\}( n + {\textbackslash}sqrt\{n\}L{\textbackslash}varepsilon{\textasciicircum}\{-1\})\$, which improves upon existing methods by a factor up to \${\textbackslash}sqrt\{n\}\$. This constitutes the first variance reduction-type result for general finite-sum monotone inclusions and for more specific problems such as convex-concave optimization when operator norm residual is the optimality measure. We further argue that, up to poly-logarithmic factors, this complexity is unimprovable in the monotone Lipschitz setting; i.e., the provided result is near-optimal.},
  language  = {en},
  urldate   = {2024-07-22},
  publisher = {arXiv},
  author    = {Cai, Xufeng and Alacaoglu, Ahmet and Diakonikolas, Jelena},
  month     = oct,
  year      = {2023},
  note      = {arXiv:2310.02987 [cs, math]},
  keywords  = {Computer Science - Machine Learning, ISMP 2024, Mathematics - Optimization and Control},
  file      = {Cai et al. - 2023 - Variance Reduced Halpern Iteration for Finite-Sum .pdf:/Volumes/T6/Zotero Library/storage/K8MPYEUK/Cai et al. - 2023 - Variance Reduced Halpern Iteration for Finite-Sum .pdf:application/pdf}
}

@article{chambolle_convergence_2015,
  title    = {On the convergence of the iterates of the ``{Fast} iterative shrinkage/thresholding algorithm"},
  volume   = {166},
  issn     = {1573-2878},
  url      = {https://doi.org/10.1007/s10957-015-0746-4},
  doi      = {10.1007/s10957-015-0746-4},
  abstract = {We discuss here the convergence of the iterates of the “Fast Iterative Shrinkage/Thresholding Algorithm,” which is an algorithm proposed by Beck and Teboulle for minimizing the sum of two convex, lower-semicontinuous, and proper functions (defined in a Euclidean or Hilbert space), such that one is differentiable with Lipschitz gradient, and the proximity operator of the second is easy to compute. It builds a sequence of iterates for which the objective is controlled, up to a (nearly optimal) constant, by the inverse of the square of the iteration number. However, the convergence of the iterates themselves is not known. We show here that with a small modification, we can ensure the same upper bound for the decay of the energy, as well as the convergence of the iterates to a minimizer.},
  language = {en},
  number   = {3},
  urldate  = {2023-11-18},
  journal  = {Journal of Optimization Theory and Applications},
  author   = {Chambolle, A. and Dossal, Ch.},
  month    = sep,
  year     = {2015},
  keywords = {Forward–backward Splitting, Heavy Ball Momentum, Optimization, First-order Methods},
  pages    = {968--982},
  file     = {Chambolle and Dossal - 2015 - On the Convergence of the Iterates of the “Fast It.pdf:/Volumes/T6/Zotero Library/storage/P7LSJUWM/Chambolle and Dossal - 2015 - On the Convergence of the Iterates of the “Fast It.pdf:application/pdf}
}

@article{chambolle_introduction_2016,
  series   = {Acta {Numerica}},
  title    = {An introduction to continuous optimization for imaging},
  volume   = {25},
  url      = {https://hal.science/hal-01346507},
  doi      = {10.1017/S096249291600009X},
  abstract = {A large number of imaging problems reduce to the optimization of a cost function , with typical structural properties. The aim of this paper is to describe the state of the art in continuous optimization methods for such problems, and present the most successful approaches and their interconnections. We place particular emphasis on optimal first-order schemes that can deal with typical non-smooth and large-scale objective functions used in imaging problems. We illustrate and compare the different algorithms using classical non-smooth problems in imaging, such as denoising and deblurring. Moreover, we present applications of the algorithms to more advanced problems, such as magnetic resonance imaging, multilabel image segmentation, optical flow estimation, stereo matching, and classification.},
  urldate  = {2023-10-19},
  journal  = {Acta Numerica},
  author   = {Chambolle, Antonin and Pock, Thomas},
  year     = {2016},
  keywords = {convex analysis, Nonsmooth Optimization},
  pages    = {161--319},
  file     = {Chambolle and Pock - 2016 - An introduction to continuous optimization for imaging:/Volumes/T6/Zotero Library/storage/WYGGWYVC/Chambolle and Pock - 2016 - An introduction to continuous optimization for imaging.pdf:application/pdf}
}

@misc{davis_proximally_2018,
  title     = {Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems},
  url       = {http://arxiv.org/abs/1707.03505},
  abstract  = {In this paper, we introduce a stochastic projected subgradient method for weakly convex (i.e., uniformly prox-regular) nonsmooth, nonconvex functions---a wide class of functions which includes the additive and convex composite classes. At a high-level, the method is an inexact proximal point iteration in which the strongly convex proximal subproblems are quickly solved with a specialized stochastic projected subgradient method. The primary contribution of this paper is a simple proof that the proposed algorithm converges at the same rate as the stochastic gradient method for smooth nonconvex problems. This result appears to be the first convergence rate analysis of a stochastic (or even deterministic) subgradient method for the class of weakly convex functions.},
  urldate   = {2023-11-30},
  publisher = {arXiv},
  author    = {Davis, Damek and Grimmer, Benjamin},
  month     = sep,
  year      = {2018},
  keywords  = {Computer Science - Machine Learning, Optimization and Control},
  file      = {arXiv.org Snapshot:/Volumes/T6/Zotero Library/storage/QTSZS2GN/1707.html:text/html;Full Text PDF:/Volumes/T6/Zotero Library/storage/3MDHZ789/Davis and Grimmer - 2018 - Proximally Guided Stochastic Subgradient Method for Nonsmooth, Nonconvex Problems.pdf:application/pdf}
}

@inproceedings{defazio_finito_2014,
  title      = {Finito: {A} faster, permutable incremental gradient method for big data problems},
  shorttitle = {Finito},
  url        = {https://proceedings.mlr.press/v32/defazio14.html},
  abstract   = {Recent advances in optimization theory have shown that smooth strongly convex finite sums can be minimized faster than by treating them as a black box "batch" problem. In this work we introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical results showing state of the art performance.},
  language   = {en},
  urldate    = {2025-01-07},
  booktitle  = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
  publisher  = {PMLR},
  author     = {Defazio, Aaron and Domke, Justin and Caetano, Tiberio},
  month      = jun,
  year       = {2014},
  pages      = {1125--1133},
  file       = {appendix:/Volumes/T6/Zotero Library/storage/46KNFGZM/appendix.pdf:application/pdf;Finito A faster, permutable incremental gradient method for big data problems:/Volumes/T6/Zotero Library/storage/AVIILHUJ/Defazio et al. - 2014 - Finito A faster, permutable incremental gradient method for big data problems.pdf:application/pdf}
}

@misc{defazio_saga_2014,
  title      = {{SAGA}: {A} fast increment al gradient method with support for non-strongly convex composite objectives},
  shorttitle = {{SAGA}},
  url        = {http://arxiv.org/abs/1407.0202},
  doi        = {10.48550/arXiv.1407.0202},
  abstract   = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
  urldate    = {2025-01-07},
  publisher  = {arXiv},
  author     = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  month      = dec,
  year       = {2014},
  keywords   = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
  file       = {SAGA A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives:/Volumes/T6/Zotero Library/storage/FE8UPR5N/Defazio et al. - 2014 - SAGA A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/RYFZLILJ/1407.html:text/html}
}

@inproceedings{defazio_saga_2014-1,
  title      = {{SAGA}: {A} fast incremental gradient method with support for non-strongly convex composite objectives},
  volume     = {27},
  shorttitle = {{SAGA}},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2014/hash/ede7e2b6d13a41ddf9f4bdef84fdc737-Abstract.html},
  abstract   = {In this work we introduce a new fast incremental gradient method SAGA, in the spirit of SAG, SDCA, MISO and SVRG. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
  urldate    = {2024-11-07},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher  = {Curran Associates, Inc.},
  author     = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  year       = {2014},
  file       = {SAGA A fast incremental gradient method with support for non-strongly convex composite objectives:/Volumes/T6/Zotero Library/storage/VNDRIFS5/Defazio et al. - 2014 - SAGA A fast incremental gradient method with support for non-strongly convex composite objectives.pdf:application/pdf}
}

@article{devolder_first-order_2014,
  title    = {First-order methods of smooth convex optimization with inexact oracle},
  volume   = {146},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-013-0677-5},
  doi      = {10.1007/s10107-013-0677-5},
  abstract = {We introduce the notion of inexact first-order oracle and analyze the behavior of several first-order methods of smooth convex optimization used with such an oracle. This notion of inexact oracle naturally appears in the context of smoothing techniques, Moreau–Yosida regularization, Augmented Lagrangians and many other situations. We derive complexity estimates for primal, dual and fast gradient methods, and study in particular their dependence on the accuracy of the oracle and the desired accuracy of the objective function. We observe that the superiority of fast gradient methods over the classical ones is no longer absolute when an inexact oracle is used. We prove that, contrary to simple gradient schemes, fast gradient methods must necessarily suffer from error accumulation. Finally, we show that the notion of inexact oracle allows the application of first-order methods of smooth convex optimization to solve non-smooth or weakly smooth convex problems.},
  language = {en},
  number   = {1},
  urldate  = {2025-01-03},
  journal  = {Mathematical Programming},
  author   = {Devolder, Olivier and Glineur, François and Nesterov, Yurii},
  month    = aug,
  year     = {2014},
  keywords = {90C06, 90C25, 90C60, Complexity bounds, Fast gradient methods, First-order methods, Gradient methods, Inexact oracle, Smooth convex optimization},
  pages    = {37--75},
  file     = {First-order methods of smooth convex optimization with inexact oracle:/Volumes/T6/Zotero Library/storage/G2GC7K2J/Devolder et al. - 2014 - First-order methods of smooth convex optimization with inexact oracle.pdf:application/pdf}
}

@article{dragomir_optimal_2022,
  title    = {Optimal complexity and certification of {Bregman} first-order methods},
  volume   = {194},
  issn     = {0025-5610, 1436-4646},
  url      = {https://link.springer.com/10.1007/s10107-021-01618-1},
  doi      = {10.1007/s10107-021-01618-1},
  abstract = {We provide a lower bound showing that the O(1/k) convergence rate of the NoLips method (a.k.a. Bregman Gradient or Mirror Descent) is optimal for the class of problems satisfying the relative smoothness assumption. This assumption appeared in the recent developments around the Bregman Gradient method, where acceleration remained an open issue.},
  language = {en},
  number   = {1-2},
  urldate  = {2023-12-02},
  journal  = {Mathematical Programming},
  author   = {Dragomir, Radu-Alexandru and Taylor, Adrien B. and d’Aspremont, Alexandre and Bolte, Jérôme},
  month    = jul,
  year     = {2022},
  keywords = {68Q25, 90C06, 90C22, 90C25, 90C60},
  pages    = {41--83},
  file     = {Dragomir et al. - 2022 - Optimal complexity and certification of Bregman first-order methods.pdf:/Volumes/T6/Zotero Library/storage/4UA78G9T/Dragomir et al. - 2022 - Optimal complexity and certification of Bregman first-order methods.pdf:application/pdf;Optimal complexity and certification of Bregman first-order methods:/Volumes/T6/Zotero Library/storage/APVBB2BM/Dragomir et al. - 2022 - Optimal complexity and certification of Bregman first-order methods.pdf:application/pdf}
}

@article{drori_performance_2014,
  title      = {Performance of first-order methods for smooth convex minimization: a novel approach},
  volume     = {145},
  issn       = {1436-4646},
  shorttitle = {Performance of first-order methods for smooth convex minimization},
  url        = {https://doi.org/10.1007/s10107-013-0653-0},
  doi        = {10.1007/s10107-013-0653-0},
  abstract   = {We introduce a novel approach for analyzing the worst-case performance of first-order black-box optimization methods. We focus on smooth unconstrained convex minimization over the Euclidean space. Our approach relies on the observation that by definition, the worst-case behavior of a black-box optimization method is by itself an optimization problem, which we call the performance estimation problem (PEP). We formulate and analyze the PEP for two classes of first-order algorithms. We first apply this approach on the classical gradient method and derive a new and tight analytical bound on its performance. We then consider a broader class of first-order black-box methods, which among others, include the so-called heavy-ball method and the fast gradient schemes. We show that for this broader class, it is possible to derive new bounds on the performance of these methods by solving an adequately relaxed convex semidefinite PEP. Finally, we show an efficient procedure for finding optimal step sizes which results in a first-order black-box method that achieves best worst-case performance.},
  language   = {en},
  number     = {1},
  urldate    = {2025-02-03},
  journal    = {Mathematical Programming},
  author     = {Drori, Yoel and Teboulle, Marc},
  month      = jun,
  year       = {2014},
  keywords   = {49M25, 68Q25, 90C20, 90C22, 90C25, 90C60, Complexity, Duality, Fast gradient schemes, Heavy Ball method, Performance of first-order algorithms, Rate of convergence, Semidefinite relaxations, Smooth convex minimization},
  pages      = {451--482},
  file       = {Performance of first-order methods for smooth convex minimization a novel approach:/Volumes/T6/Zotero Library/storage/QTY7ZIHM/Drori and Teboulle - 2014 - Performance of first-order methods for smooth convex minimization a novel approach.pdf:application/pdf}
}

@article{ghadimi_accelerated_2016,
  title    = {Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
  volume   = {156},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-015-0871-8},
  doi      = {10.1007/s10107-015-0871-8},
  abstract = {In this paper, we generalize the well-known Nesterov’s accelerated gradient (AG) method, originally designed for convex smooth optimization, to solve nonconvex and possibly stochastic optimization problems. We demonstrate that by properly specifying the stepsize policy, the AG method exhibits the best known rate of convergence for solving general nonconvex smooth optimization problems by using first-order information, similarly to the gradient descent method. We then consider an important class of composite optimization problems and show that the AG method can solve them uniformly, i.e., by using the same aggressive stepsize policy as in the convex case, even if the problem turns out to be nonconvex. We demonstrate that the AG method exhibits an optimal rate of convergence if the composite problem is convex, and improves the best known rate of convergence if the problem is nonconvex. Based on the AG method, we also present new nonconvex stochastic approximation methods and show that they can improve a few existing rates of convergence for nonconvex stochastic optimization. To the best of our knowledge, this is the first time that the convergence of the AG method has been established for solving nonconvex nonlinear programming in the literature.},
  language = {en},
  number   = {1},
  urldate  = {2024-10-12},
  journal  = {Mathematical Programming},
  author   = {Ghadimi, Saeed and Lan, Guanghui},
  month    = mar,
  year     = {2016},
  keywords = {68Q25, 90C25, 62L20, 90C15, Accelerated gradient, Complexity, Nonconvex optimization, Stochastic programming},
  pages    = {59--99},
  file     = {Ghadimi and Lan - 2016 - Accelerated gradient methods for nonconvex nonlinear and stochastic programming:/Volumes/T6/Zotero Library/storage/6BID3J9B/Ghadimi and Lan - 2016 - Accelerated gradient methods for nonconvex nonlinear and stochastic programming.pdf:application/pdf}
}

@article{goujaud_pepit_2024,
  title      = {{PEPit}: computer-assisted worst-case analyses of first-order optimization methods in {Python}},
  volume     = {16},
  issn       = {1867-2957},
  shorttitle = {{PEPit}},
  url        = {https://doi.org/10.1007/s12532-024-00259-7},
  doi        = {10.1007/s12532-024-00259-7},
  abstract   = {PEPit is a python package aiming at simplifying the access to worst-case analyses of a large family of first-order optimization methods possibly involving gradient, projection, proximal, or linear optimization oracles, along with their approximate, or Bregman variants. In short, PEPit is a package enabling computer-assisted worst-case analyses of first-order optimization methods. The key underlying idea is to cast the problem of performing a worst-case analysis, often referred to as a performance estimation problem (PEP), as a semidefinite program (SDP) which can be solved numerically. To do that, the package users are only required to write first-order methods nearly as they would have implemented them. The package then takes care of the SDP modeling parts, and the worst-case analysis is performed numerically via standard solvers.},
  language   = {en},
  number     = {3},
  urldate    = {2025-02-03},
  journal    = {Mathematical Programming Computation},
  author     = {Goujaud, Baptiste and Moucer, Céline and Glineur, François and Hendrickx, Julien M. and Taylor, Adrien B. and Dieuleveut, Aymeric},
  month      = sep,
  year       = {2024},
  keywords   = {65K10, Convergence analyses, First-order methods, Optimization, Performance estimation problems, Semidefinite programming, Splitting methods, Worst-case analyses},
  pages      = {337--367},
  file       = {PEPit\: computer-assisted worst-case analyses of first-order optimization methods in Python:/Volumes/T6/Zotero Library/storage/HLD2M8BS/Goujaud et al. - 2024 - PEPit computer-assisted worst-case analyses of first-order optimization methods in Python.pdf:application/pdf}
}

@article{gower_variance-reduced_2020,
  title    = {Variance-reduced methods for machine learning},
  volume   = {108},
  issn     = {1558-2256},
  url      = {https://ieeexplore.ieee.org/document/9226504},
  doi      = {10.1109/JPROC.2020.3028013},
  abstract = {Stochastic optimization lies at the heart of machine learning, and its cornerstone is stochastic gradient descent (SGD), a method introduced over 60 years ago. The last eight years have seen an exciting new development: variance reduction for stochastic optimization methods. These variance-reduced (VR) methods excel in settings where more than one pass through the training data is allowed, achieving a faster convergence than SGD in theory and practice. These speedups underline the surge of interest in VR methods and the fast-growing body of work on this topic. This review covers the key principles and main developments behind VR methods for optimization with finite data sets and is aimed at nonexpert readers. We focus mainly on the convex setting and leave pointers to readers interested in extensions for minimizing nonconvex functions.},
  number   = {11},
  urldate  = {2024-12-04},
  journal  = {Proceedings of the IEEE},
  author   = {Gower, Robert M. and Schmidt, Mark and Bach, Francis and Richtárik, Peter},
  month    = nov,
  year     = {2020},
  keywords = {Optimization, Machine learning, optimization, Computational modeling, Data models, Logistics, Stochastic processes, Training data, variance reduction},
  pages    = {1968--1983},
  file     = {Full Text PDF:/Volumes/T6/Zotero Library/storage/V8RBKZFC/Variance-reduced methods for machine learning.pdf:application/pdf}
}

@article{grapiglia_accelerated_2019,
  title    = {Accelerated regularized newton methods for minimizing composite convex functions},
  volume   = {29},
  issn     = {1052-6234},
  url      = {https://epubs.siam.org/doi/10.1137/17M1142077},
  doi      = {10.1137/17M1142077},
  abstract = {In this paper, we study the regularized second-order methods for unconstrained minimization of a  twice-differentiable (convex or nonconvex) objective function. For the current function, these methods automatically achieve the best possible global complexity estimates among different Hölder classes containing the Hessian of the objective. We show that such methods for functional residual and for the norm of the gradient must be different. For development of the latter methods, we introduced two new line-search acceptance criteria, which can be seen as generalizations of the Armijo condition.},
  number   = {1},
  urldate  = {2024-12-04},
  journal  = {SIAM Journal on Optimization},
  author   = {Grapiglia, Geovani N. and Nesterov, Yurii},
  month    = jan,
  year     = {2019},
  pages    = {77--99},
  file     = {Grapiglia and Nesterov - 2019 - Accelerated Regularized Newton Methods for Minimizing Composite Convex Functions.pdf:/Volumes/T6/Zotero Library/storage/Y4W3SLNY/Grapiglia and Nesterov - 2019 - Accelerated Regularized Newton Methods for Minimizing Composite Convex Functions.pdf:application/pdf}
}

@article{guler_convergence_1991,
  title    = {On the convergence of the proximal point algorithm for convex minimization},
  volume   = {29},
  issn     = {03630129},
  url      = {https://www.proquest.com/docview/925962166/abstract/A60B4BA7798A45D1PQ/1},
  doi      = {10.1137/0329022},
  abstract = {The proximal point algorithm (PPA) for the convex minimization problem \${\textbackslash}min \_\{x {\textbackslash}in H\} f(x)\$, where \$f:H {\textbackslash}to R {\textbackslash}cup {\textbackslash}\{ {\textbackslash}infty {\textbackslash}\} \$ is a proper, lower semicontinuous (lsc) function in a Hilbert space \$H\$ is considered. Under this minimal assumption on \$f\$, it is proved that the PPA, with positive parameters \${\textbackslash}\{ {\textbackslash}lambda \_k {\textbackslash}\} \_\{k = 1\}{\textasciicircum}{\textbackslash}infty \$, converges in general if and only if \${\textbackslash}sigma \_n = {\textbackslash}sum\_\{k = 1\}{\textasciicircum}n \{{\textbackslash}lambda \_k {\textbackslash}to {\textbackslash}infty \} \$. Global convergence rate estimates for the residual \$f(x\_n ) - f(u)\$, where \$x\_n \$ is the \$n\$th iterate of the PPA and \$ u {\textbackslash}in H \$ is arbitrary are given. An open question of Rockafellar is settled by giving an example of a PPA for which \$x\_n \$ converges weakly but not strongly to a minimizes of \$f\$.},
  language = {English},
  number   = {2},
  urldate  = {2024-05-18},
  journal  = {SIAM Journal on Control and Optimization},
  author   = {Guler, Osman},
  month    = mar,
  year     = {1991},
  keywords = {Mathematics, Convex analysis, Hilbert space, Algorithms},
  pages    = {17},
  file     = {Guler - 1991 - On the Convergence of the Proximal Point Algorithm.pdf:/Volumes/T6/Zotero Library/storage/9AWZSLK4/Guler - 1991 - On the Convergence of the Proximal Point Algorithm.pdf:application/pdf}
}

@article{guler_new_1992,
  title    = {New proximal point algorithms for convex minimization},
  volume   = {2},
  issn     = {1052-6234},
  url      = {https://epubs.siam.org/doi/10.1137/0802032},
  doi      = {10.1137/0802032},
  abstract = {The proximal point algorithm (PPA) for the convex minimization problem minx∈Hf(x), where f:H→R∪\{∞\} is a proper, lower semicontinuous (lsc) function in a Hilbert space H is considered. Under this minimal assumption on f, it is proved that the PPA, with positive parameters \{λk\}k=1∞, converges in general if and only if σn=∑k=1nλk→∞. Global convergence rate estimates for the residual f(xn)−f(u), where xn is the nth iterate of the PPA and u∈H is arbitrary are given. An open question of Rockafellar is settled by giving an example of a PPA for which xn converges weakly but not strongly to a minimizes of f.},
  number   = {4},
  urldate  = {2023-11-30},
  journal  = {SIAM Journal on Optimization},
  author   = {Guler, Osman},
  month    = nov,
  year     = {1992},
  pages    = {649--664},
  file     = {Güler - 1992 - New Proximal Point Algorithms for Convex Minimization.pdf:/Volumes/T6/Zotero Library/storage/G9LCY67Q/Güler - 1992 - New Proximal Point Algorithms for Convex Minimization.pdf:application/pdf}
}

@misc{hazan_revisiting_2022,
  title     = {Revisiting the {Polyak} step size},
  url       = {http://arxiv.org/abs/1905.00313},
  abstract  = {This paper revisits the Polyak step size schedule for convex optimization problems, proving that a simple variant of it simultaneously attains near optimal convergence rates for the gradient descent algorithm, for all ranges of strong convexity, smoothness, and Lipschitz parameters, without a-priory knowledge of these parameters.},
  urldate   = {2024-03-15},
  publisher = {arXiv},
  author    = {Hazan, Elad and Kakade, Sham},
  month     = aug,
  year      = {2022},
  keywords  = {Mathematics - Optimization and Control},
  file      = {arXiv.org Snapshot:/Volumes/T6/Zotero Library/storage/6WTTZF6E/1905.html:text/html;Revisiting the Polyak step size:/Volumes/T6/Zotero Library/storage/KTT9DMEL/Hazan and Kakade - 2022 - Revisiting the Polyak step size.pdf:application/pdf}
}

@unpublished{hermant_study_2024,
  title      = {Study of the behaviour of {Nesterov} {Accelerated} {Gradient} in a non convex setting: the strongly quasar convex case},
  shorttitle = {Study of the behaviour of {Nesterov} {Accelerated} {Gradient} in a non convex setting},
  url        = {https://hal.science/hal-04589853},
  abstract   = {We study the convergence of Nesterov Accelerated Gradient (NAG) minimization algorithm applied to a class of non convex functions called strongly quasar convex functions, which can exhibit highly non convex behaviour.
                We show that in the case of strongly quasar convex functions, NAG can achieve an accelerated convergence speed at the cost of a lower curvature assumption. We provide a continuous analysis through high resolution ODEs, in which negative friction may appear. Finally, we investigate connections with a weaker class of non convex functions (smooth Polyak-{\textbackslash}L ojasiewicz functions) by characterizing the gap between this class and the one of smooth strongly quasar convex functions.},
  urldate    = {2024-11-21},
  author     = {Hermant, Julien and Aujol, Jean-François and Dossal, Charles and Rondepierre, Aude},
  month      = may,
  year       = {2024},
  keywords   = {convergence rates, first order algorithms, geometrical properties, Non-convex optimization, strongly quasar convex},
  file       = {Study of the behaviour of Nesterov Accelerated Gradient in a non convex setting the strongly quasar:/Volumes/T6/Zotero Library/storage/KFDP6RRE/Hermant et al. - 2024 - Study of the behaviour of Nesterov Accelerated Gradient in a non convex setting the strongly quasar.pdf:application/pdf}
}

@article{khanh_inexact_2025,
  title    = {Inexact proximal methods for weakly convex functions},
  issn     = {1573-2916},
  url      = {https://doi.org/10.1007/s10898-024-01460-7},
  doi      = {10.1007/s10898-024-01460-7},
  abstract = {This paper proposes and develops inexact proximal methods for finding stationary points of the sum of a smooth function and a nonsmooth weakly convex one, where an error is present in the calculation of the proximal mapping of the nonsmooth term. A general framework for finding zeros of a continuous mapping is derived from our previous paper on this subject to establish convergence properties of the inexact proximal point method when the smooth term is vanished and of the inexact proximal gradient method when the smooth term satisfies a descent condition. The inexact proximal point method achieves global convergence with constructive convergence rates when the Moreau envelope of the objective function satisfies the Kurdyka–Łojasiewicz (KL) property. Meanwhile, when the smooth term is twice continuously differentiable with a Lipschitz continuous gradient and a differentiable approximation of the objective function satisfies the KL property, the inexact proximal gradient method achieves the global convergence of iterates with constructive convergence rates.},
  language = {en},
  urldate  = {2025-02-03},
  journal  = {Journal of Global Optimization},
  author   = {Khanh, Pham Duy and Mordukhovich, Boris S. and Phat, Vo Thanh and Tran, Dat Ba},
  month    = jan,
  year     = {2025},
  keywords = {49M05, 90C30, 90C52, Forward-backward envelopes, Global convergence, Inexact proximal methods, Kurdyka–Łojasiewicz property, Linear convergence rates, Proximal points, Weakly convex functions},
  file     = {Inexact proximal methods for weakly convex functions:/Volumes/T6/Zotero Library/storage/4C3DD8HB/Khanh et al. - 2025 - Inexact proximal methods for weakly convex functions.pdf:application/pdf}
}

@inproceedings{lee_geometric_2021,
  title     = {A {Geometric} structure of acceleration and its role in making gradients small fast},
  volume    = {34},
  url       = {https://papers.nips.cc/paper/2021/hash/647c722bf90a49140184672e0d3723e3-Abstract.html},
  urldate   = {2024-06-25},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  author    = {Lee, Jongmin and Park, Chanwoo and Ryu, Ernest},
  year      = {2021},
  pages     = {11999--12012},
  file      = {Lee et al. - 2021 - A Geometric Structure of Acceleration and Its Role.pdf:/Volumes/T6/Zotero Library/storage/F7Z9BVUA/Lee et al. - 2021 - A Geometric Structure of Acceleration and Its Role.pdf:application/pdf}
}

@article{lin_catalyst_2018,
  title   = {Catalyst acceleration for first-order convex optimization: from theory to practice},
  volume  = {18},
  url     = {http://jmlr.org/papers/v18/17-748.html},
  number  = {212},
  journal = {Journal of Machine Learning Research},
  author  = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  year    = {2018},
  pages   = {1--54},
  file    = {Catalyst acceleration for first-order convex optimization from theory to practice:/Volumes/T6/Zotero Library/storage/M5UYPQIT/Catalyst acceleration for first-order convex optimization from theory to practice.pdf:application/pdf}
}

@inproceedings{lin_universal_2015,
  title     = {A universal catalyst for first-order optimization},
  volume    = {28},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2015/file/c164bbc9d6c72a52c599bbb43d8db8e1-Paper.pdf},
  booktitle = {Procedings of {Advances} in {Neural} {Information} {Processing} {Systems}},
  author    = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  editor    = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
  year      = {2015},
  file      = {A Universal Catalyst for First-Order Optimization HAL open science:/Volumes/T6/Zotero Library/storage/8ILQU6TZ/Lin et al. - 2015 - A Universal Catalyst for First-Order Optimization.pdf:application/pdf;A Universal Catalyst for First-Order Optimization NISP:/Volumes/T6/Zotero Library/storage/DAYMXD2L/Lin et al. - 2015 - A Universal Catalyst for First-Order Optimization.pdf:application/pdf}
}

@article{maulen_speed_2023,
  title    = {A speed restart scheme for a dynamics with hessian-driven damping},
  volume   = {199},
  issn     = {1573-2878},
  url      = {https://doi.org/10.1007/s10957-023-02290-5},
  doi      = {10.1007/s10957-023-02290-5},
  abstract = {In this paper, we analyze a speed restarting scheme for the inertial dynamics with Hessian-driven damping, introduced by Attouch et al. (J Differ Equ 261(10):5734–5783, 2016). We establish a linear convergence rate for the function values along the restarted trajectories. Numerical experiments suggest that the Hessian-driven damping and the restarting scheme together improve the performance of the dynamics and corresponding iterative algorithms in practice.},
  language = {en},
  number   = {2},
  urldate  = {2024-12-16},
  journal  = {Journal of Optimization Theory and Applications},
  author   = {Maulén, Juan José and Peypouquet, Juan},
  month    = nov,
  year     = {2023},
  keywords = {34A12 (secondary), 37N40, 65K10 (primary), 90C25, Convex optimization, Differential equations, First-order methods, Hessian-driven damping, Restarting},
  pages    = {831--855},
  file     = {peed Restart Scheme for a Dynamics with Hessian-Driven Damping:/Volumes/T6/Zotero Library/storage/G8QLYLH3/Maulén and Peypouquet - 2023 - A Speed Restart Scheme for a Dynamics with Hessian-Driven Damping.pdf:application/pdf}
}

@article{necoara_linear_2019,
  title    = {Linear convergence of first order methods for non-strongly convex optimization},
  volume   = {175},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-018-1232-1},
  doi      = {10.1007/s10107-018-1232-1},
  abstract = {The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.},
  language = {en},
  number   = {1},
  urldate  = {2023-10-11},
  journal  = {Mathematical Programming},
  author   = {Necoara, I. and Nesterov, Yu. and Glineur, F.},
  month    = may,
  year     = {2019},
  pages    = {69--107},
  file     = {Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:/Volumes/T6/Zotero Library/storage/7X79PGLC/Necoara et al. - 2019 - Linear convergence of first order methods for non-.pdf:application/pdf}
}

@article{nesterov_accelerating_2008,
  title    = {Accelerating the cubic regularization of {Newton}’s method on convex problems},
  volume   = {112},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-006-0089-x},
  doi      = {10.1007/s10107-006-0089-x},
  abstract = {In this paper we propose an accelerated version of the cubic regularization of Newton’s method (Nesterov and Polyak, in Math Program 108(1): 177–205, 2006). The original version, used for minimizing a convex function with Lipschitz-continuous Hessian, guarantees a global rate of convergence of order \$\$O{\textbackslash}big(\{1 {\textbackslash}over k{\textasciicircum}2\}{\textbackslash}big)\$\$, where k is the iteration counter. Our modified version converges for the same problem class with order \$\$O{\textbackslash}big(\{1 {\textbackslash}over k{\textasciicircum}3\}{\textbackslash}big)\$\$, keeping the complexity of each iteration unchanged. We study the complexity of both schemes on different classes of convex problems. In particular, we argue that for the second-order schemes, the class of non-degenerate problems is different from the standard class.},
  language = {en},
  number   = {1},
  urldate  = {2024-11-20},
  journal  = {Mathematical Programming},
  author   = {Nesterov, Yu.},
  month    = mar,
  year     = {2008},
  keywords = {90C25, 90C30, Convex optimization, 49M37, 49M15, 58C15, Condition number, Cubic regularization, Global complexity bounds, Newton’s method, Non-degenerate problems, Unconstrained minimization, Worst-case complexity},
  pages    = {159--181},
  file     = {Nesterov - 2008 - Accelerating the cubic regularization of Newton’s method on convex problems.pdf:/Volumes/T6/Zotero Library/storage/GWZSARLR/Nesterov - 2008 - Accelerating the cubic regularization of Newton’s method on convex problems.pdf:application/pdf}
}

@article{nesterov_gradient_2013,
  title    = {Gradient methods for minimizing composite functions},
  volume   = {140},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-012-0629-5},
  doi      = {10.1007/s10107-012-0629-5},
  abstract = {In this paper we analyze several new methods for solving optimization problems with the objective function formed as a sum of two terms: one is smooth and given by a black-box oracle, and another is a simple general convex function with known structure. Despite the absence of good properties of the sum, such problems, both in convex and nonconvex cases, can be solved with efficiency typical for the first part of the objective. For convex problems of the above structure, we consider primal and dual variants of the gradient method (with convergence rate \$\$O{\textbackslash}left(\{1 {\textbackslash}over k\}{\textbackslash}right)\$\$), and an accelerated multistep version with convergence rate \$\$O{\textbackslash}left(\{1 {\textbackslash}over k{\textasciicircum}2\}{\textbackslash}right)\$\$, where \$\$k\$\$is the iteration counter. For nonconvex problems with this structure, we prove convergence to a point from which there is no descent direction. In contrast, we show that for general nonsmooth, nonconvex problems, even resolving the question of whether a descent direction exists from a point is NP-hard. For all methods, we suggest some efficient “line search” procedures and show that the additional computational work necessary for estimating the unknown problem class parameters can only multiply the complexity of each iteration by a small constant factor. We present also the results of preliminary computational experiments, which confirm the superiority of the accelerated scheme.},
  language = {en},
  number   = {1},
  urldate  = {2024-09-17},
  journal  = {Mathematical Programming},
  author   = {Nesterov, Yu.},
  month    = aug,
  year     = {2013},
  keywords = {68Q25, 90C25, Optimal methods, 90C47, Black-box model, Complexity theory, Convex Optimization, l1l\_1-Regularization, Local optimization, Nonsmooth optimization, Structural optimization},
  pages    = {125--161},
  file     = {Nesterov - 2013 - Gradient methods for minimizing composite functions:/Volumes/T6/Zotero Library/storage/6LKMMPQJ/Nesterov - 2013 - Gradient methods for minimizing composite functions.pdf:application/pdf}
}

@book{nesterov_lectures_2018,
  series    = {Springer {Optimization} and {Its} {Applications}},
  title     = {Lectures on {Convex} {Optimization}},
  volume    = {137},
  isbn      = {978-3-319-91577-7 978-3-319-91578-4},
  url       = {http://link.springer.com/10.1007/978-3-319-91578-4},
  urldate   = {2023-10-11},
  publisher = {Springer International Publishing},
  author    = {Nesterov, Yurii},
  year      = {2018},
  keywords  = {Optimization, Fast Gradient Methods, Algorithmic Complexity, Interior-Point Methods, Numerical Optimization, Optimization in Relative Scale, Self-Concordant Functions, Smoothing Technique},
  file      = {Nesterov - 2018 - Lectures on Convex Optimization.pdf:/Volumes/T6/Zotero Library/storage/HSCCPYL9/Nesterov - 2018 - Lectures on Convex Optimization.pdf:application/pdf}
}

@article{nesterov_method_1983,
  title    = {A method for solving the convex programming problem with convergence rate {O}(1/k{\textasciicircum}2)},
  url      = {https://www.semanticscholar.org/paper/A-method-for-solving-the-convex-programming-problem-Nesterov/8d3a318b62d2e970122da35b2a2e70a5d12cc16f},
  abstract = {Semantic Scholar extracted view of "A method for solving the convex programming problem with convergence rate O(1/k{\textasciicircum}2)" by Y. Nesterov},
  urldate  = {2024-10-10},
  journal  = {Proceedings of the USSR Academy of Sciences},
  author   = {Nesterov, Y.},
  year     = {1983}
}

@article{nesterov_universal_2015,
  title    = {Universal gradient methods for convex optimization problems},
  volume   = {152},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-014-0790-0},
  doi      = {10.1007/s10107-014-0790-0},
  abstract = {In this paper, we present new methods for black-box convex minimization. They do not need to know in advance the actual level of smoothness of the objective function. Their only essential input parameter is the required accuracy of the solution. At the same time, for each particular problem class they automatically ensure the best possible rate of convergence. We confirm our theoretical results by encouraging numerical experiments, which demonstrate that the fast rate of convergence, typical for the smooth optimization problems, sometimes can be achieved even on nonsmooth problem instances.},
  language = {en},
  number   = {1},
  urldate  = {2025-01-09},
  journal  = {Mathematical Programming},
  author   = {Nesterov, Yu},
  month    = aug,
  year     = {2015},
  keywords = {68Q25, 90C25, 90C47, Black-box methods, Complexity bounds, Convex optimization, Optimal methods, Weakly smooth functions},
  pages    = {381--404},
  file     = {Universal gradient methods for convex optimization problems:/Volumes/T6/Zotero Library/storage/STCVAD58/Nesterov - 2015 - Universal gradient methods for convex optimization problems.pdf:application/pdf}
}

@article{noel_nesterovs_nodate,
  title    = {Nesterov's method for convex optimization},
  volume   = {65},
  url      = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/epdf/10.1137/21M1390037},
  doi      = {10.1137/21M1390037},
  abstract = {While Nesterov's algorithm for computing the minimum of a convex function is now over forty years old, it is rarely presented in texts for a first course in optimization. This is unfortunate since for many problems this algorithm is superior to the ubiquitous steepest descent algorithm, and it is equally simple to implement. This article presents an elementary analysis of Nesterov's algorithm that parallels that of steepest descent. It is envisioned that this presentation of Nesterov's algorithm could easily be covered in a few lectures following the introductory material on convex functions and steepest descent included in every course on optimization.},
  language = {en},
  number   = {2},
  urldate  = {2023-10-09},
  journal  = {SIAM Review},
  author   = {Noel, Walkington},
  pages    = {539--562},
  file     = {Nesterov's Method for Convex Optimization.pdf:/Volumes/T6/Zotero Library/storage/BZC6TFHX/Nesterov's Method for Convex Optimization.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/M8MDL6E3/21M1390037.html:text/html}
}

@inproceedings{paquette_catalyst_2018,
  title     = {Catalyst for gradient-based nonconvex optimization},
  url       = {https://proceedings.mlr.press/v84/paquette18a.html},
  abstract  = {We introduce a generic scheme to solve nonconvex optimization problems using gradient-based algorithms originally designed for minimizing convex functions. Even though these methods may originally require convexity to operate, the proposed approach allows one to use them without assuming any knowledge about the convexity of the objective. In general, the scheme is guaranteed to produce a stationary point with a worst-case efficiency typical of first-order methods, and when the objective turns out to be convex, it automatically accelerates in the sense of Nesterov and achieves near-optimal convergence rate in function values. We conclude the paper by showing promising experimental results obtained by applying our approach to incremental algorithms such as SVRG and SAGA for sparse matrix factorization and for learning neural networks.},
  language  = {en},
  urldate   = {2024-10-02},
  booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
  publisher = {PMLR},
  author    = {Paquette, Courtney and Lin, Hongzhou and Drusvyatskiy, Dmitriy and Mairal, Julien and Harchaoui, Zaid},
  month     = mar,
  year      = {2018},
  pages     = {613--622},
  file      = {Paquette et al. - 2018 - Catalyst Acceleration for Gradient-Based Non-Convex Optimization.pdf:/Volumes/T6/Zotero Library/storage/N4YDZMYK/Paquette et al. - 2018 - Catalyst Acceleration for Gradient-Based Non-Convex Optimization.pdf:application/pdf}
}

@article{polyak_minimization_1969,
  series   = {{USSR} {Computational} {Mathematics} and {Mathematical} {Physics}},
  title    = {Minimization of unsmooth functionals},
  issn     = {0041-5553},
  url      = {https://www.researchgate.net/publication/238270077_Minimization_of_Unsmooth_Functionals},
  abstract = {PDF {\textbar} WE consider the minimization of a convex, but but necessarily differentiable, functional in a convex set of Hilbert space. The minimization method... {\textbar} Find, read and cite all the research you need on ResearchGate},
  language = {en},
  urldate  = {2024-12-04},
  author   = {Polyak, B.T.},
  month    = dec,
  year     = {1969},
  file     = {Polyak - 2024 - Minimization of unsmooth functionals.pdf:/Volumes/T6/Zotero Library/storage/EAAGQFFM/Polyak - 2024 - Minimization of unsmooth functionals.pdf:application/pdf}
}

@unpublished{recht_optimization_nodate,
  title    = {Optimization for {Modern} {Data} {Analysis}},
  url      = {https://people.eecs.berkeley.edu/~brecht/opt4ml_book/},
  abstract = {This book explores theory and algorithms for nonlinear optimization with a particular focus on problems that arise in machine learning and data analysis. The text balances worst-case analysis against implementation concerns and aims to highlight the core theoretical tools that provide reasonable guidelines to optimization practice.
              This book covers material appropriate for a quarter-length course on optimization aimed at graduate students in computer science, industrial engineering, electrical engineering, and related fields.},
  urldate  = {2023-11-28},
  author   = {Recht, Benjamin and Wright, Stephen},
  keywords = {Coordinate descent, Data Science, Fast Gradient Methods, Optimization, Stochastic Gradient},
  file     = {O4MD_01_Introduction.pdf:/Volumes/T6/Zotero Library/storage/J7QQQNQW/O4MD_01_Introduction.pdf:application/pdf;O4MD_02_Foundations.pdf:/Volumes/T6/Zotero Library/storage/QL7MJRAH/O4MD_02_Foundations.pdf:application/pdf;O4MD_03_Descent_Methods.pdf:/Volumes/T6/Zotero Library/storage/ZQVWX65C/O4MD_03_Descent_Methods.pdf:application/pdf;O4MD_04_Momentum.pdf:/Volumes/T6/Zotero Library/storage/7IQ3LXHY/O4MD_04_Momentum.pdf:application/pdf;O4MD_05_SGM.pdf:/Volumes/T6/Zotero Library/storage/8H8VP8MJ/O4MD_05_SGM.pdf:application/pdf;O4MD_06_Coordinate_Descent.pdf:/Volumes/T6/Zotero Library/storage/A975I6XN/O4MD_06_Coordinate_Descent.pdf:application/pdf;O4MD_07_Constrained_Opt.pdf:/Volumes/T6/Zotero Library/storage/A44XNIAD/O4MD_07_Constrained_Opt.pdf:application/pdf;O4MD_08_Subgradients.pdf:/Volumes/T6/Zotero Library/storage/RKY44LD9/O4MD_08_Subgradients.pdf:application/pdf;O4MD_09_Nonsmooth.pdf:/Volumes/T6/Zotero Library/storage/G67CC56T/O4MD_09_Nonsmooth.pdf:application/pdf;people.eecs.berkeley.edu/~brecht/opt4ml_book/:/Volumes/T6/Zotero Library/storage/CAVSC9YL/opt4ml_book.html:text/html}
}

@article{rockafellar_augmented_1976,
  title    = {Augmented lagrangians and applications of the proximal point algorithm in convex programming},
  volume   = {1},
  issn     = {0364-765X},
  url      = {https://www.jstor.org/stable/3689277},
  abstract = {The theory of the proximal point algorithm for maximal monotone operators is applied to three algorithms for solving convex programs, one of which has not previously been formulated. Rate-of-convergence results for the "method of multipliers," of the strong sort already known, are derived in a generalized form relevant also to problems beyond the compass of the standard second-order conditions for optimality. The new algorithm, the "proximal method of multipliers," is shown to have much the same convergence properties, but with some potential advantages.},
  number   = {2},
  urldate  = {2024-09-22},
  journal  = {Mathematics of Operations Research},
  author   = {Rockafellar, R. T.},
  year     = {1976},
  pages    = {97--116},
  file     = {Rockafellar - 1976 - Augmented Lagrangians and Applications of the Proximal Point Algorithm in Convex Programming:/Volumes/T6/Zotero Library/storage/295EN3CR/Rockafellar - 1976 - Augmented Lagrangians and Applications of the Proximal Point Algorithm in Convex Programming.pdf:application/pdf}
}

@book{rockafellar_convex_1997,
  address   = {Princeton, NJ},
  edition   = {10. print. and 1. paperb. print},
  series    = {Princeton {Landmarks} in mathematics and physics},
  title     = {Convex {Analysis}},
  isbn      = {978-0-691-01586-6 978-0-691-08069-7},
  language  = {eng},
  publisher = {Princeton Univ. Press},
  author    = {Rockafellar, Ralph Tyrrell},
  year      = {1997},
  keywords  = {Convex Analysis},
  file      = {Rockafellar - 1997 - Convex analysis.pdf:/Volumes/T6/Zotero Library/storage/LLST9GW6/Rockafellar - 1997 - Convex analysis.pdf:application/pdf}
}

@article{rockafellar_monotone_1976,
  title    = {Monotone operators and the proximal point algorithm},
  volume   = {14},
  issn     = {0363-0129, 1095-7138},
  url      = {http://epubs.siam.org/doi/10.1137/0314056},
  doi      = {10.1137/0314056},
  language = {en},
  number   = {5},
  urldate  = {2023-11-06},
  journal  = {SIAM Journal on Control and Optimization},
  author   = {Rockafellar, R. Tyrrell},
  month    = aug,
  year     = {1976},
  pages    = {877--898},
  file     = {Monotone Operators and the Proximal Point Algorithm.pdf:/Volumes/T6/Zotero Library/storage/5L82ZWY4/Monotone Operators and the Proximal Point Algorithm.pdf:application/pdf}
}

@book{ryu_large-scale_2022,
  title      = {Large-scale {Convex} {Optimization}: {Algorithms} \& {Analyses} via {Monotone} {Operators}},
  isbn       = {978-1-009-16085-8},
  shorttitle = {Large-{Scale} {Convex} {Optimization}},
  url        = {https://large-scale-book.mathopt.com/},
  abstract   = {Starting from where a first course in convex optimization leaves off, this text presents a unified analysis of first-order optimization methods – including parallel-distributed algorithms – through the abstraction of monotone operators. With the increased computational power and availability of big data over the past decade, applied disciplines have demanded that larger and larger optimization problems be solved. This text covers the first-order convex optimization methods that are uniquely effective at solving these large-scale optimization problems. Readers will have the opportunity to construct and analyze many well-known classical and modern algorithms using monotone operators, and walk away with a solid understanding of the diverse optimization algorithms. Graduate students and researchers in mathematical optimization, operations research, electrical engineering, statistics, and computer science will appreciate this concise introduction to the theory of convex optimization algorithms.},
  urldate    = {2024-01-22},
  publisher  = {Cambridge University Press},
  author     = {Ryu, Ernest K. and Yin, Wotao},
  year       = {2022},
  file       = {Ryu and Yin - 2022 - Large-Scale Convex Optimization Algorithms & Analyses via Monotone Operators.pdf:/Volumes/T6/Zotero Library/storage/JPZLJBL8/Ryu and Yin - 2022 - Large-Scale Convex Optimization Algorithms & Analyses via Monotone Operators.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/PNYCF4FI/2A7F8E7428BFA4EDB8AFACA11AB97E4C.html:text/html}
}

@misc{schmidt_convergence_2011,
  title     = {Convergence rates of inexact proximal-gradient methods for convex optimization},
  url       = {http://arxiv.org/abs/1109.2415},
  abstract  = {We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates.Using these rates, we perform as well as or better than a carefully chosen fixed error level on a set of structured sparsity problems.},
  urldate   = {2024-10-23},
  publisher = {arXiv},
  author    = {Schmidt, Mark and Roux, Nicolas Le and Bach, Francis},
  month     = dec,
  year      = {2011},
  note      = {arXiv:1109.2415},
  keywords  = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
  file      = {Full Text PDF:/Volumes/T6/Zotero Library/storage/MY8DHKCE/Schmidt et al. - 2011 - Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization.pdf:application/pdf;Snapshot:/Volumes/T6/Zotero Library/storage/23BW6UYQ/1109.html:text/html}
}

@inproceedings{schmidt_convergence_2011-1,
  title     = {Convergence rates of inexact proximal-gradient methods for convex optimization},
  volume    = {24},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2011/hash/8f7d807e1f53eff5f9efbe5cb81090fb-Abstract.html},
  abstract  = {We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the second term. We show that the basic proximal-gradient method, the basic proximal-gradient method with a strong convexity assumption, and the accelerated proximal-gradient method achieve the same convergence rates as in the error-free case, provided the errors decrease at an appropriate rate.  Our experimental results on a structured sparsity problem indicate that sequences of errors with these appealing theoretical properties can lead to practical performance improvements.},
  urldate   = {2024-10-23},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Schmidt, Mark and Roux, Nicolas and Bach, Francis},
  year      = {2011},
  file      = {Full Text PDF:/Volumes/T6/Zotero Library/storage/AT4TDV9A/Schmidt et al. - 2011 - Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization.pdf:application/pdf}
}

@article{schmidt_minimizing_2017,
  title    = {Minimizing finite sums with the stochastic average gradient},
  volume   = {162},
  issn     = {1436-4646},
  url      = {https://doi.org/10.1007/s10107-016-1030-6},
  doi      = {10.1007/s10107-016-1030-6},
  abstract = {We analyze the stochastic average gradient (SAG) method for optimizing the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method’s iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods. The convergence rate is improved from \$\$O(1/{\textbackslash}sqrt\{k\})\$\$to O(1 / k) in general, and when the sum is strongly-convex the convergence rate is improved from the sub-linear O(1 / k) to a linear convergence rate of the form \$\$O({\textbackslash}rho {\textasciicircum}k)\$\$for \$\${\textbackslash}rho {\textless} 1\$\$. Further, in many cases the convergence rate of the new method is also faster than black-box deterministic gradient methods, in terms of the number of gradient evaluations. This extends our earlier work Le Roux et al. (Adv Neural Inf Process Syst, 2012), which only lead to a faster rate for well-conditioned strongly-convex problems. Numerical experiments indicate that the new algorithm often dramatically outperforms existing SG and deterministic gradient methods, and that the performance may be further improved through the use of non-uniform sampling strategies.},
  language = {en},
  number   = {1},
  urldate  = {2025-01-07},
  journal  = {Mathematical Programming},
  author   = {Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  month    = mar,
  year     = {2017},
  keywords = {68Q25, 90C25, 90C30, First-order methods, Convex optimization, 62L20, 90C15, 65K05, 90C06, Convergence Rates, Stochastic gradient methods},
  pages    = {83--112},
  file     = {Full Text PDF:/Volumes/T6/Zotero Library/storage/UDU2RNRN/Schmidt et al. - 2017 - Minimizing finite sums with the stochastic average gradient.pdf:application/pdf}
}

@article{su_differential_2016,
  title      = {A differential equation for modeling nesterov's accelerated gradient method: {Theory} and {Insights}},
  volume     = {17},
  issn       = {1533-7928},
  shorttitle = {A {Differential} {Equation} for {Modeling} {Nesterov}'s {Accelerated} {Gradient} {Method}},
  url        = {http://jmlr.org/papers/v17/15-084.html},
  abstract   = {We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.},
  language   = {en},
  number     = {153},
  urldate    = {2023-10-09},
  journal    = {Journal of Machine Learning Research},
  author     = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
  year       = {2016},
  keywords   = {ODEs, dynamical system},
  pages      = {1--43},
  file       = {Full Text PDF:/Volumes/T6/Zotero Library/storage/JMSC3D9R/Su et al. - 2016 - A Differential Equation for Modeling Nesterov's Accelerated Gradient Method Theory and Insights.pdf:application/pdf;Su et al. - 2015 - A differential equation for modeling nesterov's ac.pdf:/Volumes/T6/Zotero Library/storage/ANWIF5NT/Su et al. - 2015 - A differential equation for modeling nesterov's ac.pdf:application/pdf}
}

@article{tibshirani_regression_1996,
  title    = {Regression shrinkage and selection via the {Lasso}},
  volume   = {58},
  issn     = {0035-9246},
  url      = {https://www.jstor.org/stable/2346178},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
  number   = {1},
  urldate  = {2024-03-29},
  journal  = {Journal of the Royal Statistical Society. Series B (Methodological)},
  author   = {Tibshirani, Robert},
  year     = {1996},
  pages    = {267--288},
  file     = {Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:/Volumes/T6/Zotero Library/storage/K4KSLBP7/Tibshirani - 1996 - Regression Shrinkage and Selection via the Lasso.pdf:application/pdf}
}

@article{villa_accelerated_2013,
  title     = {Accelerated and inexact forward-backward algorithms},
  volume    = {23},
  copyright = {© 2013, Society for Industrial and Applied Mathematics},
  issn      = {10526234},
  url       = {https://www.proquest.com/docview/1418210204/abstract/AF88FFBCD234AADPQ/1},
  doi       = {10.1137/110844805},
  abstract  = {We propose a convergence analysis of accelerated forward-backward splitting methods for composite function minimization, when the proximity operator is not available in closed form, and can only be computed up to a certain precision. We prove that the \$1/k{\textasciicircum}2\$ convergence rate for the function values can be achieved if the admissible errors are of a certain type and satisfy a sufficiently fast decay condition. Our analysis is based on the machinery of estimate sequences first introduced by Nesterov for the study of accelerated gradient descent algorithms. Furthermore, we give a global complexity analysis, taking into account the cost of computing admissible approximations of the proximal point. An experimental analysis is also presented. [PUBLICATION ABSTRACT]},
  language  = {English},
  number    = {3},
  urldate   = {2024-06-27},
  journal   = {SIAM Journal on Optimization},
  author    = {Villa, Silvia and Salzo, Saverio and Baldassarre, Luca and Verri, Alessandro},
  year      = {2013},
  keywords  = {Mathematics, Algorithms, Approximation, Laboratories, Machine learning},
  pages     = {1607--1633},
  file      = {Villa et al. - 2013 - Accelerated and Inexact Forward-Backward Algorithm.pdf:/Volumes/T6/Zotero Library/storage/K3MNNGVQ/Villa et al. - 2013 - Accelerated and Inexact Forward-Backward Algorithm.pdf:application/pdf}
}

@article{xiao_proximal_2014,
  title    = {A proximal stochastic gradient method with progressive variance reduction},
  volume   = {24},
  issn     = {1052-6234},
  url      = {https://epubs.siam.org/doi/10.1137/140961791},
  doi      = {10.1137/140961791},
  abstract = {We consider the problem of minimizing the sum of two convex functions: one is smooth and given by a gradient oracle, and the other is separable over blocks of coordinates and has a simple known structure over each block. We develop an accelerated randomized proximal coordinate gradient (APCG) method for minimizing such convex composite functions. For strongly convex functions, our method achieves faster linear convergence rates than existing randomized proximal coordinate gradient methods. Without strong convexity, our method enjoys accelerated sublinear convergence rates. We show how to apply the APCG method to solve the regularized empirical risk minimization (ERM) problem and devise efficient implementations that avoid full-dimensional vector operations. For ill-conditioned ERM problems, our method obtains improved convergence rates than the state-of-the-art stochastic dual coordinate ascent method.},
  number   = {4},
  urldate  = {2024-10-11},
  journal  = {SIAM Journal on Optimization},
  author   = {Xiao, Lin and Zhang, Tong},
  month    = jan,
  year     = {2014},
  pages    = {2057--2075},
  file     = {Full Text PDF:/Volumes/T6/Zotero Library/storage/T8I47JRF/Xiao and Zhang - 2014 - A Proximal Stochastic Gradient Method with Progressive Variance Reduction.pdf:application/pdf}
}

@article{yang_bregman_2022,
  title      = {Bregman proximal point algorithm revisited: {A} new inexact version and its inertial variant},
  volume     = {32},
  issn       = {1052-6234},
  shorttitle = {Bregman {Proximal} {Point} {Algorithm} {Revisited}},
  url        = {https://epubs-siam-org.eu1.proxy.openathens.net/doi/10.1137/20M1360748},
  doi        = {10.1137/20M1360748},
  abstract   = {We study a general convex optimization problem, which covers various classic problems in different areas and particularly includes many optimal transport related problems arising in recent years. To solve this problem, we revisit the classic Bregman proximal point algorithm (BPPA) and introduce a new inexact stopping condition for solving the subproblems, which can circumvent the underlying feasibility difficulty often appearing in existing inexact conditions when the problem has a complex feasible set. Our inexact condition also covers several existing inexact conditions as special cases and hence makes our inexact BPPA (iBPPA) more flexible to fit different scenarios in practice. As an application to the standard optimal transport (OT) problem, our iBPPA with the entropic proximal term can bypass some numerical instability issues that usually plague the popular Sinkhorn's algorithm in the OT community, since our iBPPA does not require the proximal parameter to be very small for obtaining an accurate approximate solution. The iteration complexity of 
                𝑂
                (
                1
                /
                𝑘
                )
                and the convergence of the sequence are also established for our iBPPA under some mild conditions. Moreover, inspired by Nesterov's acceleration technique, we develop an inertial variant of our iBPPA, denoted by V-iBPPA, and establish the iteration complexity of 
                𝑂
                (
                1
                /
                𝑘
                𝜆
                )
                , where 
                𝜆
                ≥
                1
                is a quadrangle scaling exponent of the kernel function. In particular, when the proximal parameter is a constant and the kernel function is strongly convex with Lipschitz continuous gradient (hence 
                𝜆
                =
                2
                ), our V-iBPPA achieves a faster rate of 
                𝑂
                (
                1
                /
                𝑘
                2
                )
                just as existing accelerated inexact proximal point algorithms. Some preliminary numerical experiments for solving the standard OT problem are conducted to show the convergence behaviors of our iBPPA and V-iBPPA under different inexactness settings. The experiments also empirically verify the potential of our V-iBPPA for improving the convergence speed.
                Keywords
                proximal point algorithm
                Bregman distance
                inexact condition
                Nesterov's acceleration
                optimal transport
                MSC codes
                90C46
                49K35
                90C30
                65K05},
  number     = {3},
  urldate    = {2024-12-24},
  journal    = {SIAM Journal on Optimization},
  author     = {Yang, Lei and Toh, Kim-Chuan},
  month      = sep,
  year       = {2022},
  pages      = {1523--1554},
  file       = {Bregman Proximal Point Algorithm Revisited A New Inexact Version and Its Inertial Variant:/Volumes/T6/Zotero Library/storage/MX3QYZ76/Yang and Toh - 2022 - Bregman Proximal Point Algorithm Revisited A New Inexact Version and Its Inertial Variant.pdf:application/pdf}
}

@book{ying_modern_2021,
  series    = {{MOS}-{SIAM} {Series} on {Optimization}},
  title     = {Modern {Nonconvex} {Nondifferentiable} {Optimization}},
  volume    = {1},
  isbn      = {978-1-61197-674-8},
  url       = {https://epubs.siam.org/doi/book/10.1137/1.9781611976748},
  language  = {en},
  number    = {29},
  urldate   = {2024-11-20},
  publisher = {MOS-SIAM},
  author    = {Ying, Cui and Jong-Shi, Pang},
  year      = {2021},
  file      = {1:/Volumes/T6/Zotero Library/storage/AU7MC9JA/1.9781611976748.ch1.pdf:application/pdf;2:/Volumes/T6/Zotero Library/storage/R3ISLTZ7/1.9781611976748.ch2 (1).pdf:application/pdf;3:/Volumes/T6/Zotero Library/storage/TD25IPT9/1.9781611976748.ch3.pdf:application/pdf;4:/Volumes/T6/Zotero Library/storage/59CJTJPL/1.9781611976748.ch4.pdf:application/pdf;5:/Volumes/T6/Zotero Library/storage/YD7IDPZ8/1.9781611976748.ch5.pdf:application/pdf;6:/Volumes/T6/Zotero Library/storage/JQ5TVRB5/1.9781611976748.ch6.pdf:application/pdf;7:/Volumes/T6/Zotero Library/storage/AFCH4MZY/1.9781611976748.ch7.pdf:application/pdf;8:/Volumes/T6/Zotero Library/storage/HF8Q4DUQ/1.9781611976748.ch8.pdf:application/pdf;9:/Volumes/T6/Zotero Library/storage/GW8V85A5/1.9781611976748.ch9.pdf:application/pdf;10:/Volumes/T6/Zotero Library/storage/M63NSY62/1.9781611976748.ch10.pdf:application/pdf;11:/Volumes/T6/Zotero Library/storage/DHI9VZVP/1.9781611976748.ch11.pdf:application/pdf;Bib, Idx, glossary:/Volumes/T6/Zotero Library/storage/NUZ5WXJ4/Ba and Pang - 2022 - Exact Penalization of Generalized Nash Equilibrium Problems.pdf:application/pdf;Modern Nonconvex Nondifferentiable Optimization.pdf:/Volumes/T6/Zotero Library/storage/FMCJICH7/Ying and Jong-Shi - 2021 - Modern Nonconvex Nondifferentiable Optimization.pdf:application/pdf}
}
