\documentclass[12pt]{article}

% \input{presets/wang_full.tex}
\usepackage{ifthen}\newboolean{draftmode}\setboolean{draftmode}{true}
\input{presets/wang/input.tex}
\input{presets/misc.tex}
\input{presets/julia_lstlisting.tex}

% NOTATIONS FOR THIS PAPER 
\DeclareMathOperator{\dist}{\mathop{dist}}
\DeclareMathOperator{\rng}{\mathop{rng}}



\title{{
    \fontfamily{ptm}\selectfont 
    Error Bound Can Give Near Optimal Convergence Rate for Inexact Accelerated Proximal Gradient Method
    }
}

\author{
    Author 1 Name, Author 2 Name
    \thanks{
        Subject type, Some Department of Some University, Location of the University,
        Country. E-mail: \texttt{author.namee@university.edu}.
    }
}

\begin{document}

% TITLE, ABSTRACT ==============================================================
\date{\today}
\maketitle
\todoinline{This paper is currently in draft mode. Check source to change options. }
\begin{abstract} 
    \noindent
    This is still a draft. \cite{zhang_robust_2022}. 
\end{abstract}
\noindent{\bfseries 2010 Mathematics Subject Classification:}
Primary 47H05, 52A41, 90C25; Secondary 15A09, 26A51, 26B25, 26E60, 47H09, 47A63.
\noindent{\bfseries Keywords:}

\section{Introduction}
    \textbf{Notations.}
    Let $g: \RR^n \rightarrow\overline \RR$, we denote $g^\star$ to be the Fenchel conjugate. 
    $I: \RR^n \rightarrow \RR^n$ denotes the identity operator.
    For a multivalued mapping $T: \RR^n \rightarrow 2^{\RR^n}$, $\gra T$ denotes the graph of the operator, defined as $\{(x, y)\in \RR^n \times \RR^n : y \in Tx\}$. 
    $\Pi_S$ denotes the projection onto a set $S \subseteq \RR^n$. 
    \subsection{Preliminaries}
        \begin{definition}[$\epsilon$-subgradient]\label{def:esp-subgrad}
            Let $g: \RR^n \rightarrow \overline \RR$ be proper, lsc. 
            Let $\epsilon \ge 0$. 
            Then the $\epsilon$-subgradient of $g$ at some $\bar x \in \dom g$ is given by: 
            $$
            \begin{aligned}
                \partial g_\epsilon(\bar  x) := 
                \left\lbrace
                    v \in \RR^n \left| \; 
                        \langle v, x - \bar  x\rangle \le 
                        g(x) - g(\bar x) + \epsilon \;\forall x \in \RR^n
                    \right. 
                \right\rbrace.
            \end{aligned}
            $$
            When $\bar x \not \in \dom g$, it has $\partial g_\epsilon(\bar x) = \emptyset$. 
        \end{definition}
        \begin{remark}
            $\partial_\epsilon g$ is a multivalued operator and, it's not monotone, unless $\epsilon = 0$, which makes it equivalent to Fenchel subgradient $\partial g$. 
        \end{remark}
        If we assume lsc, proper and convex $g$, we will now introduce results in the literatures that we will use. 
        \begin{fact}[$\epsilon$-Fenchel inequality]\label{fact:esp-fenchel-ineq}
            Let $\epsilon \ge 0$, then:
            \begin{align*}
                x^* \in \partial_\epsilon f(\bar x)\iff f^\star(x^*) + f(\bar x) \le \langle x^*, \bar x\rangle + \epsilon \implies \bar x \in \partial_\epsilon f^\star(x^*).
            \end{align*}
            They are all equivalent if $f^{\star\star}(\bar x) = f(\bar x)$. 
        \end{fact}
        \begin{remark}
            The above fact is taken from Zalinascu \cite[Theorem 2.4.2]{zalinescu_convex_2002}. 
        \end{remark}
        We will now define inexact proximal point based on $\epsilon$-subgradient
        \begin{definition}[inexact proximal point]\label{def:inxt-pp}
            For all $x \in \RR^n, \epsilon \ge 0, \lambda > 0$, $\tilde x$ is an inexact evaluation of proximal point at $x$, if and only if it satisfies: 
            \begin{align*}
                \lambda^{-1}(x - \tilde x) \in \partial_{\epsilon} g(\tilde x). 
            \end{align*}
            We denote it by $\tilde x \approx_\epsilon \hprox_{\lambda g}(x)$. 
        \end{definition}
        \begin{remark}
            This definition is nothing new, for example see Villa et al. \cite[Definition 2.1]{villa_accelerated_2013}
        \end{remark}
        \begin{fact}[the resolvant identity]\label{fact:resv-identity}
            Let $T: \RR^n \rightarrow 2^{\RR^n}$, then it has: 
            \begin{align*}
                (I + T)^{-1} = (I - (I + T^{-1})^{-1}).
            \end{align*}
        \end{fact}
        \begin{theorem}[inexact Moreau decomposition]\label{thm:inxt-moreau-decomp}
            Let $g: \RR^n \rightarrow \overline \RR$ be a closed, convex and proper function. 
            It has the equivalence
            \begin{align*}
                \tilde y \approx_\epsilon \hprox_{\lambda^{-1}g^\star}(\lambda^{-1}y)
                \iff 
                y - \lambda \tilde y \approx_\epsilon \hprox_{\lambda g}(y). 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Consider $\tilde y \approx_\epsilon \hprox_{\lambda^{-1}g^\star}(\lambda^{-1}y)$, then it has: 
            \begin{align*}
                & 
                \tilde y 
                \in (I + \lambda^{-1}\partial_\epsilon g^\star)^{-1}(\lambda^{-1}y)
                \\
                \iff &
                (\lambda^{-1}y, \tilde y)\in 
                \gra(I + \lambda^{-1}\partial_\epsilon g^\star)^{-1}
                \\
                \underset{(1)}{\iff} &
                (\lambda^{-1}y, \tilde y)\in 
                \gra(I - (I + \partial_\epsilon g\circ(\lambda I))^{-1})
                \\
                \iff &
                (\lambda^{-1}y, \lambda^{-1}y - \tilde y)\in 
                \gra(I + \partial_\epsilon g\circ(\lambda I))^{-1}
                \\
                \iff &
                (\lambda^{-1}y - \tilde y, \lambda^{-1}y)\in 
                \gra(I + \partial_\epsilon g\circ(\lambda I))
                \\
                \iff &
                (y - \lambda\tilde y, \lambda^{-1}y)\in 
                \gra(\lambda^{-1}I + \partial_\epsilon g)
                \\
                \iff &
                (y - \lambda\tilde y, y)\in 
                \gra(I + \lambda\partial_\epsilon g)
                \\
                \iff& 
                y - \lambda \tilde y \in 
                (I + \lambda \partial_\epsilon g)^{-1}y
                \\
                \iff& 
                y - \lambda \tilde y \approx_\epsilon \hprox_{\lambda g}(y). 
            \end{align*}
            At (1) we can use Fact \ref{fact:resv-identity}, and it has $(\lambda^{-1}\partial_\epsilon g^\star)^{-1} = \partial_\epsilon g\circ(\lambda I)$ by Fact \ref{fact:esp-fenchel-ineq} and the assumption that $g$ is closed, convex and proper. 
        \end{proof}
    \subsection{Inexact proximal gradient inequality}
        \begin{assumption}[for inexact proximal gradient]\label{ass:for-inxt-pg-ineq}
            The assumption is about $(F, f, g, L)$. 
            We assume that 
            \begin{enumerate}[nosep]
                \item $f: \RR^n \rightarrow \RR$ is a convex, $L$ Lipschitz smooth function (i.e: $\nabla f$ is $L$ a Lipschitz continuous mapping). 
                \item $g: \RR^n \rightarrow \overline\RR$ is a convex, proper, and lsc function which we do not have its exact proximal operator. 
                \item The over all objective is $F = f + g$. 
            \end{enumerate}
        \end{assumption}
        No, we develop the theory based on the use of epsilon subgradient as in Definition \ref{def:esp-subgrad}. 
        Let $\rho > 0$, the exact proximal gradient operator defined for $(f, g, L)$ satisfying Assumption \ref{ass:for-inxt-pg-ineq} has
        \begin{align*}
            T_{\rho}(x) &= \argmin_{z\in\RR^n}\left\lbrace g(z) + \langle \nabla f(x), z\rangle + \frac{\rho}{2}\Vert z - x\Vert^2 \right\rbrace
            \\
            &= \hprox_{\rho^{-1} g}\left(x - \rho^{-1}\nabla f(x)\right). 
        \end{align*}
        The following definition extends the proximal gradient operator to the inexact case using the concept of $\epsilon$-subgradient as given by Definition \ref{def:esp-subgrad}. 
        \begin{definition}[inexact proximal gradient]\label{def:inxt-pg}
            Let $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}. 
            Let $\epsilon \ge 0, \rho > 0$. 
            Then, $\tilde x \approx_\epsilon T_\rho(x)$ is an inexact proximal gradient if it satisfies variational inequality: 
            \begin{align*}
                \mathbf 0 \in \nabla f(x) + \rho(x - \tilde x) + \partial_{\epsilon} g(\tilde x). 
            \end{align*}
        \end{definition}
        \begin{remark}
            We assumed that we can get exact evaluation of $\nabla f$ at any points $x \in \RR^n$. 
        \end{remark}
        \begin{lemma}[other representations of inexact proximal gradient]\;\label{lemma:other-repr-inxt-pg}\\
            Let $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, $\epsilon \ge 0, \rho > 0$, then for all $\tilde x \approx_\epsilon T_\rho(x)$, it has the following equivalent representations: 
            \begin{align*}
                & (x - \rho^{-1}\nabla f(x)) - \tilde x 
                \in \rho^{-1} \partial_\epsilon g(\tilde x)
                \\
                \iff 
                & \tilde x \in (I + \rho^{-1}\partial_\epsilon g(\tilde x))^{-1}
                (x - \rho^{-1}\nabla f(x))
                \\
                \iff 
                & x \approx_\epsilon \hprox_{\rho^{-1} g}
                \left(x - \rho^{-1}\nabla f(x)\right)
            \end{align*}
        \end{lemma}
        \begin{proof}
            It's direct. 
        \end{proof}
        \begin{theorem}[inexact over-regularized proximal gradient inequality]\;\label{thm:inxt-pg-ineq}\\
            Let $(F, f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, $\epsilon \ge 0, B \ge 0, \rho > 0$. 
            Consider $\tilde x \approx_\epsilon T_{B + \rho}(x)$. 
            Denote $F = f + g$. 
            If in addition, $\tilde x, B$ satisfies the line search condition $D_f(\tilde x, x) \le B/2\Vert x - \tilde x\Vert^2$, then it has $\forall z \in \RR^n$: 
            \begin{align*}
                - \epsilon &\le 
                F(z) - F(\tilde x)
                + \frac{B + \rho}{2}\Vert x - z\Vert^2
                - \frac{B + \rho}{2}\Vert z - \tilde x\Vert^2
                - \frac{\rho}{2}\Vert \tilde x - x\Vert^2. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            By Definition \ref{def:inxt-pg} write the variational inequality that describes $\tilde x \approx_\epsilon T_B(x)$, and the definition of epsilon subgradient (Definition \ref{def:esp-subgrad}) it has for all $z \in \RR^n$: 
            \begin{align*}
                - \epsilon &\le 
                g(z) - g(\tilde x) - \langle (B + \rho)(\tilde x - x) - \nabla f(x), z - \tilde x\rangle
                \\
                &= 
                g(z) - g(\tilde x) 
                - (B + \rho)\langle \tilde x - x, z - \tilde x\rangle
                + \langle \nabla f(x), z - \tilde x\rangle
                \\
                &\underset{(1)}{\le} 
                g(z) + f(z) - g(\tilde x) - f(\tilde x)
                - (B + \rho)\langle \tilde x - x, z - \tilde x\rangle
                - D_f(z, x) + D_f(\tilde x, x)
                \\
                &\underset{(2)}{\le} 
                F(z) - F(\tilde x)
                - (B + \rho)\langle \tilde x - x, z - \tilde x\rangle
                + \frac{B}{2}\Vert \tilde x - x\Vert^2
                \\
                &=
                F(z) - F(\tilde x) + \frac{B + \rho}{2}\left(
                    \Vert x - z\Vert^2
                    - \Vert \tilde x - x\Vert^2
                    - \Vert z - \tilde x\Vert^2
                \right)
                + \frac{B}{2}\Vert \tilde x - x\Vert^2
                \\
                &= 
                F(z) - F(\tilde x)
                + \frac{B + \rho}{2}\Vert x - z\Vert^2
                - \frac{B + \rho}{2}\Vert z - \tilde x\Vert^2
                - \frac{\rho}{2}\Vert \tilde x - x\Vert^2. 
            \end{align*}
            At (1), we used considered the following: 
            \begin{align*}
                \langle \nabla f(x), z - x\rangle &= \langle \nabla f(x), z - x + x - \tilde x\rangle
                \\
                &= \langle \nabla f(x), z - x\rangle + \langle \nabla f(x), x - \tilde x\rangle
                \\
                &= -D_f(z, x) + f(z) - f(x) + D_f(\tilde x, x) - f(\tilde x) + f(x)
                \\
                &= -D_f(z, x) + f(z) + D_f(\tilde x, x) - f(\tilde x). 
            \end{align*}
            At (2), we used the fact that $f$ is convex hence $- D_f(z, x) \le 0$ always, and in the statement hypothesis we assumed that $B$ has $D_f(\tilde x, x) \le B/2\Vert \tilde x - x\Vert^2$. 
            We also used $F = f + g$. 
        \end{proof}
        \begin{remark}
            When $\epsilon = 0, \rho = 0$, this reduces to proximal gradient inequality in the exact case. 
            In this inequality, observe that the parameter $\epsilon$ controls the inexactness of the proximal gradient evaluation. 
            More specifically, $\epsilon_k$ controls the absolute perturbations of the proximal gradient inequality compared to its exact counterpart. 
            $\rho$ on the other hand, it is the over-relaxation of proximal gradient operator, and it compensates the perturbations caused by $\approx_\epsilon$ relative to the term $\Vert \tilde x - x\Vert^2$. 
        \end{remark}
    \subsection{Optimizing an inexact proximal point problem}\label{sec:optz-inxt-pp-problem}
        In this section we will present the optimization problem that obtains a $\tilde x$ such that $\tilde x \approx_\epsilon \hprox_{\lambda g}(z)$. 
        Eventually we want to evaluate $T_{\rho}(x)$ of some $F = f + g$ inexactly using Lemma \ref{lemma:other-repr-inxt-pg}. 
        To do that one would need to evaluate $\hprox_{\rho^{-1}g}$ inexactly which is defined in Definition \ref{def:inxt-pp}. 
        \par
        Most of these results that will follow are from the literature. 
        To start, we must assume the following about a function $g: \RR^n \rightarrow \overline \RR$, with $g$ closed, convex and proper. 
        \begin{assumption}[linear composite of convex nonsmooth function]\;\label{ass:for-inxt-prox}\\
            This assumption is about $(g, \omega, A, K_\omega, K_{\omega^\star})$. 
            Let $m \in \N, n \in \RR^n$, we assume that 
            \begin{enumerate}[nosep]
                \item $A\in \RR^{m \times n}$ is a matrix. 
                \item $\omega: \RR^n \rightarrow \overline \RR$ is a closed and convex function such that it has exact proximal operator $\hprox_{\lambda\omega}$ and, with known conjugate $\omega^\star$ is known. In addition assume that $\dom \omega = \RR^n$ so that 
                \item $g: \RR^m \rightarrow \RR := \omega(Ax)$ and by previous item it satisfies constraint qualification $\rng A \cap \reli\dom \omega \neq\emptyset$. 
                \item $\omega$ is $K_\omega$ Lipschitz continuous, and $\omega^\star$ is $K_{\omega^\star}$ Lipschitz continuous and in addition, $\dom \omega^\star$ is a compact set. As an immediate consequence, the operator $\partial \omega$ is a bounded on $\RR^n$. 
            \end{enumerate}
        \end{assumption}
        Now, we are ready to discuss how to choose $\tilde x \approx_\epsilon \hprox_{\lambda g}(x)$. 
        Fix $y \in \RR^n, \lambda > 0$, we are ultimately interested in minimizing: 
        \begin{align}\label{eqn:primal-pp}
            \Phi_\lambda(u) &:= \omega(Au) + \frac{1}{2\lambda} \Vert u - y\Vert^2
        \end{align}
        Observe that $\rng A \cap \reli\dom g \neq\emptyset$ in Assumption \ref{ass:for-inxt-prox} shows $g$ is also closed convex and proper. 
        The function $\Phi_\lambda$ is coersive due to its quadratic term and hence it must admit a minimizer \cite[Theorem 1.9]{rockafellar_variational_1998}. 
        Recall the following famous theoretical result in the convex programming literature that we had adapted into our context. 
        \begin{fact}[Fenchel Rockafellar Duality {\cite[Proposition 15.22]{bauschke_convex_2017}}]\;\label{fact:fn-rck-duality}\\
            Let $f:\RR^n \rightarrow \overline \RR$ be closed convex and proper, $g: \RR^m \rightarrow \overline \RR$, $A \in \RR^{m \times n}$. 
            If ${\mathbf 0 \in \inte(\dom g - A \dom f)}$, then 
            \begin{align*}
                \inf_{u\in \RR^n}\left\lbrace
                    f(u) + g(Au)
                \right\rbrace 
                + \min_{v \in \RR^m}\left\lbrace
                    f^\star\circ(-A^\top) + g^\star(v)
                \right\rbrace = 0. 
            \end{align*}
        \end{fact}
        \begin{remark}
            The theorem is not exactly the same as what is claimed in the original text, since we are in a finite dimensional setting.
            To see the original theorem cited in the finite dimension, we need the space $\mathcal H = \RR^n$ and then use \cite[Proposition 6.12]{bauschke_convex_2017}. 
        \end{remark}
        In our context, we are interested in the dual of the proximal problem $\Phi_\lambda$ which makes $f = u \mapsto \frac{1}{2\lambda}\Vert u - y\Vert^2$, and $g = \omega$, and it has $f^\star(v) = \frac{1}{2\lambda}\Vert \lambda v + y\Vert^2 - \frac{1}{2\lambda}\Vert y\Vert^2$ (see Apendix \ref{lemma:chore1}). 
        Consequently, $[f^\star \circ (- A^\top)](v) = \frac{1}{2\lambda}\Vert -\lambda A^\top v  + y\Vert^2 - \frac{1}{2\lambda}\Vert y\Vert^2$. 
        And therefore, $\Phi_\lambda$ admits Fenchel Rockafellar dual (or simply the dual) objective in $\RR^m$: 
        \begin{align}\label{eqn:dual-pp}
            \Psi_\lambda(v) &:= f^\star\circ(-A^\top) + g^\star(v) 
            = \frac{1}{2\lambda}\Vert \lambda A^\top v - y\Vert^2
            + \omega^\star(v) - \frac{1}{2\lambda}\Vert y\Vert^2. 
        \end{align}
        We define the duality gap
        \begin{align}\label{eqn:duality-gap-pp}
            \mathbf G_\lambda(u, v) &:= \Phi_\lambda(u) + \Psi_\lambda(v). 
        \end{align}
        Note that in this case the smooth part is quadratic and, $\dom f = \RR^n$, hence it translates to ${\mathbf 0 \in \inte(\dom g - A \dom f) = \inte(\dom g - \rng A)}$. 
        This will hold because of $\rng A \cap \reli\dom g \neq\emptyset$ in Assumption \ref{ass:for-inxt-prox}.
        Therefore strong duality holds and it exists $(\hat u, \hat v)$ such that we have the following: 
        \begin{align*}
            \mathbf G_\lambda(\hat u, \hat v) = 0 = \min_{u} \Phi_\lambda(u) + \min_v \Psi_\lambda(v)
        \end{align*}
        The following theorem quantifies a sufficient conditions for $\tilde x \approx_\epsilon \hprox_{\lambda g}(x)$. 
        The theorem below is from \cite[Proposition 2.2]{villa_accelerated_2013}. 
        \begin{theorem}[primal translate to dual {\cite[Proposition 2.2]{villa_accelerated_2013}}]\label{thm:primal-dual-trans}
            Let $(g, \omega, A)$ satisfies assumption \ref{ass:for-inxt-prox}, $\epsilon \ge 0$, then 
            \begin{align*}
                \left(
                    \forall z \approx_\epsilon \hprox_{\lambda g}(y) 
                \right)(\exists v \in \dom \omega^\star): z = y - \lambda A^\top v. 
            \end{align*}
        \end{theorem}
        This theorem that follows is from Villa et al. \cite[Proposition 2.3]{villa_accelerated_2013}. 
        \begin{theorem}[duality gap of inexact proximal problem {\cite[Proposition 2.3]{villa_accelerated_2013}}]\;\label{thm:dlty-gap-inxt-pp}\\
            Let $(g, \omega, A)$ satisfies Assumption \ref{ass:for-inxt-prox}, for all $\epsilon \ge 0$, $v \in \RR^n$ consider the following conditions: 
            \begin{enumerate}[nosep]
                \item $\mathbf G_\lambda(y - \lambda A^\top v, v) \le \epsilon$. 
                \item $A^\top v \approx_\epsilon \hprox_{\lambda^{-1}g^\star}(\lambda^{-1}y)$. 
                \item $y - \lambda A^\top v \approx_{\epsilon} \hprox_{\lambda g}(y)$. 
            \end{enumerate}
            They have $(a)\implies (b) \iff (c)$. 
            If in addition $\omega^\star(v) = g^\star(A^\top v)$, then all three conditions are equivalent. 
        \end{theorem}
        \begin{proof}
            The proof of $(a) \implies (b)$, and the case when $(a)\iff (b)$, we refer readers to Villa et al. \cite[ Proposition 2.3]{villa_accelerated_2013}, and to show $(b) \iff (c)$ use Theorem \ref{thm:inxt-moreau-decomp}. 
        \end{proof}
        \par
        The following theorem is enhenced from Villa et al. \cite[Theorem 5.1]{villa_accelerated_2013}. 
        % \begin{fact}[minimizing dual of the proximal problem {\cite[Theorem 5.1]{villa_accelerated_2013}}]\;\label{fact:minimizing-dual-pp}\\
        %     Let $\bar v$ be a solution of $\Psi_\lambda$. 
        %     Suppose that $(v_j)_{j \ge 0}$ is a minimizing sequence for $\Psi_\lambda$. 
        %     Let $z_j = y - \lambda A^\top v_j$, and $\bar z = y - \lambda A^\top \bar v$. 
        %     Then the following are true: 
        %     \begin{enumerate}[nosep]
        %         \item The sequence $z_j \rightarrow \bar z$ as well. 
        %         \item There exists a constant $K$ that depends on the sequence $z_j$, and $\Phi_\lambda$ such that for all $j \ge 0$ it has the inequality: 
        %         \begin{align*}
        %             \Phi_{\lambda}(z_j) - \Phi_\lambda(\bar z) 
        %             &\le K \Vert z_j - \bar z\Vert 
        %             \le K \sqrt{2\lambda}(\Psi_\lambda(v_j) - \Psi_\lambda(\bar v))^{1/2}. 
        %         \end{align*}
        %     \end{enumerate}
        % \end{fact}
        % We remark that the above fact translates any algorithm that optimizes the dual problem $\Psi_\lambda$ into optimizing the duality gap $\mathbf G(z_j, v_j)$ by this inequality. 
        % \par
        % With the proof back in Villa et al. \cite[Theorem 5.1]{villa_accelerated_2013}, pay close attention to the constant $L_\Phi$ exists by the virtue of $\Psi$ being convex, and the sequence $v_j \rightarrow \bar v$ which allows the sequence $z_j \rightarrow \bar z_j$ to be bounded inside the domain of $\Phi_\lambda$. 
        % By convexity, $\Phi_\lambda$ is locally Lipschitz on $\reli \dom \Phi_\lambda$, by convergence of $z_j$ the sequence is in a compact set in $\reli\dom \Phi_\lambda$ hence a Lipschitz bound with constant $K$ exists. 
        % \par
        % For this reason, the number of iterations of the inner loop required to achieve $\mathbf G(z_j, v_j) < \epsilon$ for a given $\epsilon$ is related to the convergence rate of the algorithms used to optimize $\Psi_\lambda(v_j)$. 
        % With the theorem derived above, and using Theorem \ref{thm:dlty-gap-inxt-pp} it implies that any algorithm which can optimize function value $\Psi_\lambda$ will produce iterates sufficient to achieve $\approx_\epsilon \hprox_{\lambda g}(y)$. 
        \todoinline{
            Un nouvel candidat ce que remplace le denière théorem suivant. 
        }
        
        \begin{theorem}[minimizing the dual of the proximal problem]\;\label{thm:minimizing-dual-pp}\\
            Assume that we have $(g, \omega, A)$ given by Assumption \ref{ass:for-inxt-prox}. 
            Let the $\Phi_\lambda$ be given by \eqref{eqn:primal-pp}, and dual $\Psi_\lambda$ by \eqref{eqn:dual-pp}. 
            Let $\bar v$ be a minimizer of $\Psi_\lambda$ and suppose that sequence $(v_j)_{j\ge 0}$ minimizes dual $\Psi_\lambda$. 
            Then, the followings are true: 
            \begin{enumerate}[nosep]
                \item\label{thm:minimizing-dual-pp:item1} If $\bar v$ is an minimizer of dual $\Psi_\lambda$, then $\bar z = y - \lambda A^\top \bar v$ is a minimizer of primal $\Phi_\lambda$. 
                \item\label{thm:minimizing-dual-pp:item2} Let $z_j = y - \lambda A^\top v_j$, it has $\Psi_\lambda(v_j) - \Psi_\lambda(\bar v) \ge \frac{1}{2 \lambda} \Vert z_j - \bar z\Vert^2$, and therefore it has $z_j \rightarrow \bar z$. 
                \item\label{thm:minimizing-dual-pp:item3} And, by our assumption, the primal optimality gap is bounded by dual for all $v \in \RR^m$ by the inequality: 
                \begin{align*}
                    & \Phi_\lambda(z_j) - \Phi_\lambda(\bar z) 
                    \\
                    &\le 
                    \sqrt{2\lambda(\Psi_\lambda(v_j) - \Psi_\lambda(\bar v))}\left(
                        K_\omega \Vert A\Vert 
                        + \lambda^{-1}\Vert z_j - y\Vert 
                        + \frac{1}{2\lambda}\sqrt{2\lambda(\Psi_\lambda(v_j) - \Psi_\lambda(\bar v))}
                    \right).
                \end{align*}
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            There are two intermediate results to prior to proving the items in the theorem. 
            In preparations, for all $v \in \RR^m$, it's obvious that we have: 
            \begin{align}\label{eqn:thm:minimizing-dual-pp:pitem1}
                \begin{split}
                    & \frac{1}{2\lambda}\left\Vert \lambda A^\top v - y\right\Vert^2
                    - \frac{1}{2\lambda}\left\Vert \lambda A^\top \bar v - y\right\Vert^2 
                    + \langle A \bar z, v - \bar v\rangle 
                    \\
                    &= 
                    \frac{1}{2\lambda}
                    \left\Vert 
                        \lambda A^\top v - \lambda A^\top \bar v  + \lambda A^\top \bar v - y
                    \right\Vert^2
                    - \frac{1}{2\lambda}\left\Vert \lambda A^\top \bar v - y\right\Vert^2 
                    + \langle A \bar z, v - \bar v\rangle 
                    \\
                    &= 
                    \frac{1}{2\lambda} \left\Vert \lambda A^\top(v - \bar v)\right\Vert^2
                    + \frac{1}{\lambda}\left\langle \lambda A^\top(v - \bar v), \lambda A^\top \bar v - y \right\rangle
                    + \left\langle \bar z, A^\top(v - \bar v)\right\rangle 
                    \\
                    &\underset{\text{(1)}}{=} 
                    \frac{1}{2\lambda} \left\Vert \lambda A^\top(v - \bar v)\right\Vert^2
                    - \frac{1}{\lambda}\left\langle \lambda A^\top(v - \bar v), \bar z \right\rangle
                    + \left\langle \bar z, A^\top(v - \bar v)\right\rangle 
                    \\
                    &= \frac{1}{2\lambda} \left\Vert \lambda A^\top(v - \bar v)\right\Vert^2. 
                \end{split}
            \end{align}
            At (1), we assumed for the theorem statement that $\bar z = y - \lambda A^\top \bar v$. 
            By the optimality of $\bar v$ on the dual problem, we have the following: 
            \begin{align}\label{eqn:thm:minimizing-dual-pp:pitem2}
                \begin{split}
                    & \mathbf 0 \in 
                    A \left(\lambda A^\top \bar v - y\right) + \partial \omega^\star(\bar v)
                    \\
                    \iff &
                    A\bar z \in \partial \omega^\star(\bar v)
                    \\
                    \iff &
                    \omega^\star(v) - \omega^\star(\bar v) 
                    \ge \langle A \bar z, v_n - \bar v\rangle. 
                \end{split}
            \end{align}
            \par
            We will now prove \ref{thm:minimizing-dual-pp:item1}, because $A \bar z \in \partial \omega^\star (\bar v) \iff \partial \omega(A \bar z) \ni \bar v$, we can derive that $\bar z = y - \lambda A^\top \bar v$ is primal optimal because 
            \begin{align*}
                y - \bar z &= \lambda A^\top \bar v \in \lambda A^\top\partial \omega (A \bar z). 
            \end{align*}
            Re-arranging yields: $\mathbf 0 \in \bar z - y + \lambda A^\top \partial \omega (A \bar z) = \partial \Phi_\lambda(\bar z)$. 
            \par
            Now we will prove \ref{thm:minimizing-dual-pp:item2}. 
            Recall that $z_j = y - \lambda A^\top v_j$, therefore it has: 
            \begin{align*}
                \Psi_\lambda(v_j) - \Psi_\lambda(\bar v)
                &= \frac{1}{2\lambda}\left\Vert
                    \lambda A^\top v_j - y
                \right\Vert^2 
                - \frac{1}{2\lambda}\left\Vert
                    \lambda A^\top \bar v - y
                \right\Vert^2
                + \omega^\star(v_j) - \omega^\star(\bar v)
                \\
                &\underset{(2)}{\ge}
                \frac{1}{2\lambda}\left\Vert
                    \lambda A^\top v_j - y
                \right\Vert^2 
                - \frac{1}{2\lambda}\left\Vert
                    \lambda A^\top \bar v - y
                \right\Vert^2
                + \langle A\bar z, v_j - \bar v\rangle
                \\
                &\underset{(3)}{=} \frac{1}{2\lambda}\Vert \lambda A^\top(v_j - \bar v)\Vert^2
                \\
                &\underset{(4)}{=} \frac{1}{2\lambda}\Vert z_j - \bar z\Vert^2. 
            \end{align*}
            At (2) we used \eqref{eqn:thm:minimizing-dual-pp:pitem2}, at (3) we used \eqref{eqn:thm:minimizing-dual-pp:pitem1}. 
            At (4), we again $\bar z = y - \lambda A^\top \bar v$. 
            We have $\bar z$ being the optimal solution of primal $\Phi_\lambda$. 
            Now, if we set $v = v_j$, and then: 
            \begin{align*}
                0 = \lim_{j \rightarrow \infty} \Psi_\lambda(v_j) - \Psi_{\lambda} (\bar v) \ge \lim_{j \rightarrow \infty}\Vert z_j - \bar z \Vert^2. 
            \end{align*}
            Implying that $z_j \rightarrow \bar z$ also. 
            \par
            We will now prove \ref{thm:minimizing-dual-pp:item3}. 
            We have for all $z \in \RR^n$, the following: 
            {\allowdisplaybreaks
            \begin{align*}
                & \Phi_\lambda(z_j) - \Phi_\lambda(\bar z) 
                \\
                &= \omega(Az_j) - \omega(A \bar z)
                + \frac{1}{2\lambda}(\Vert z_j - y \Vert^2 - \Vert \bar z - y\Vert^2)
                \\
                &\le 
                K_\omega\Vert A\Vert \Vert z_j - \bar z\Vert 
                + \frac{1}{2\lambda}\left(
                    \Vert z_j - y\Vert + \Vert \bar z_j - y\Vert
                \right)\left(
                    \Vert z_j - y\Vert - \Vert \bar z - y\Vert
                \right)
                \\
                &\le
                K_\omega\Vert A\Vert \Vert z_j - \bar z\Vert 
                + \frac{1}{2\lambda}\left(
                    \Vert z_j - y\Vert + \Vert \bar z - y\Vert
                \right)\Vert z_j - \bar z\Vert
                \\
                &\le 
                K_\omega\Vert A\Vert \Vert z_j - \bar z\Vert 
                + \frac{1}{2\lambda}\left(
                    \Vert z_j - \bar z\Vert + 2\Vert \bar z - y\Vert
                \right)\Vert z_j - \bar z\Vert
                \\
                &= 
                \Vert z_j - \bar z\Vert
                \left(
                    K_\omega\Vert A\Vert
                    + \lambda^{-1}\Vert \bar z - y\Vert
                    + \frac{\Vert z_j - \bar z\Vert}{2\lambda}
                \right)
                \\ &\underset{\text{\ref{thm:minimizing-dual-pp:item2}}}\le 
                \sqrt{2\lambda\left(\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)\right)}
                \left(
                    K_\omega\Vert A\Vert
                    + \lambda^{-1}\Vert \bar z - y\Vert
                    + \frac{\sqrt{2\lambda}}{2\lambda}\sqrt{\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)}
                \right)
                \\
                &\underset{(5)}\le 
                \sqrt{2\lambda\left(\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)\right)}
                \left(
                    K_\omega\Vert A\Vert
                    + \max_{z \in \partial g(\bar z)} \Vert z\Vert
                    + \frac{\sqrt{2\lambda}}{2\lambda}\sqrt{\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)}
                \right)
                \\
                &\underset{(6)}= 
                \sqrt{2\lambda\left(\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)\right)}
                \left(
                    K_\omega\Vert A\Vert
                    + K_\omega
                    + \frac{\sqrt{2\lambda}}{2\lambda}\sqrt{\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)}
                \right). 
            \end{align*}
            }
            At (5), recall the primal minimizer $\mathbf 0 \in \partial \Phi_\lambda(\bar z) = (I + \lambda\partial g)^{-1}(y)$ which means $\lambda^{-1}(y - z) \in \partial g(\bar z)$, which implies that $\lambda^{-1}\Vert y - z \Vert \le \max_{z \in \partial g}\Vert z\Vert$. 
            At (6) we used the assumption that $\omega$ is $K_\omega$ Lipschitz continuous stated in Assumption \ref{ass:for-inxt-prox}, and we invoke Lemma \ref{lemma:lipz-cnvx-fxn}. 
        \end{proof}


    \subsection{Literature reviews}

    \subsection{Our contributions}

\section{The inexact accelerated proximal gradient with controlled errors}
    In this section, we present an accelerated algorithm with controlled error using Definition \ref{def:inxt-pg}, and show that it can have a convergence rate under certain error conditions. 
    \begin{definition}[our inexact accelerated proximal gradient]\;\label{def:inxt-apg}\\
        Suppose that $(F, f, g, L)$ and, sequences $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$ satisfies the following
        \begin{enumerate}[nosep]
            \item $(\alpha_k)_{k \ge 0}$ is a sequence such that $\alpha \in (0, 1]$ for all $k \ge 0$. 
            \item $(B_k)_{k \ge 0}$ has $B_k > 0\; \forall k$, it characterizes any potential line search, back tracking routine. 
            \item $(\rho_k)_{k \ge 0}$ be a sequence such that $\rho_k \ge 0$, characterizing the over-relaxation of the proximal gradient operator. 
            \item $(\epsilon_k)_{k \ge 0}$ has $\epsilon_k > 0$ for all $k \ge 0$, it characterizes the errors of inexact proximal evaluation.
            \item $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, and let $F = f + g$. 
        \end{enumerate}
        Denote $L_k = B_k + \rho_k$ for short. 
        Given any initial condition $v_{-1}, x_{-1} \in \RR^n$, the algorithm generates the sequences $(y_k, x_k, v_k)_{k \ge 0}$ such that they satisfy for all $k \ge 0$: 
        \begin{align}
            & y_k = \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}, \label{def:inxt-apg:yk}
            \\
            & x_k \approx_{\epsilon_k} T_{L_k}(y_k), \label{def:inxt-apg:xk}
            \\
            & D_f(x_k, y_k) \le \frac{B_k}{2}\Vert x_k - y_k\Vert^2, 
            \\
            & v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}). \label{def:inxt-apg:vk}
        \end{align}
    \end{definition}
    \begin{lemma}[inexact accelerated proximal gradient preparation stage I]\; \label{lemma:inxt-apg-cnvg-prep1}\\
        Let $(F, f, g, L)$, and $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, be given by Definition \ref{def:inxt-apg}. 
        Denote $L_k = B_k + \rho_k$. 
        Then, for any $\bar x \in \RR^n$, the sequences $(y_k, x_k, v_k)_{k \ge 0}$ generated satisfy for all $k \ge 1$ the inequality: 
        \begin{align*}
            & \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k
            \\
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            \\ &
            + \max\left(
                    1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
                \right)\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
        \end{align*}
        When, $k = 1$ it instead has: 
        \begin{align*}
            & \frac{\rho_0}{2}\Vert x_0 - y_0\Vert^2 - \epsilon_0 
            \\
            &\le 
            (1 - \alpha_0)(F(x_{-1}) - F(\bar x)) + F(\bar x) - F(x_0) 
            + \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_{-1}\Vert^2
            - \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_0\Vert^2. 
        \end{align*}
    \end{lemma}
    \begin{proof}
        Two intermediate results are in order before we can prove the inequality. 
        Define $z_k := \alpha_k \bar x + (1 - \alpha_k)x_{k - 1}$ for short. 
        It has for all $k \ge 1$ the equality: 
        \begin{align}\tag{a}\label{eqn:inxt-apg-cnvg-prep1-a}\begin{split}
            z_k - x_k &= 
            \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - x_k
            \\
            &= \alpha_kx^+ + (x_{k - 1} - x_k) - \alpha_kx_{k - 1}
            \\
            &= \alpha_k \bar x - \alpha_k v_k. 
        \end{split}\end{align}
        It also has for all $k \ge 1$ the equality: 
        \begin{align}\tag{b}\label{eqn:inxt-apg-cnvg-prep1-b}\begin{split}
            z_k - y_k &= 
            \alpha_k \bar x + (1 - \alpha_k)x_{k - 1} - y_k
            \\
            &= \alpha_k \bar x - \alpha_k v_{k - 1}. 
        \end{split}\end{align}
        Let's denote $L_k = B_k + \rho_k$ for short. 
        Recall that $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, if we choose $x = y_k$ so $\tilde x = x_k \approx_{\epsilon_k} T_{L_k}(y_k)$, and set $z = z_k, \epsilon = \epsilon_k$ then Theorem \ref{thm:inxt-pg-ineq} has: 
        \begin{align*}
            & \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k
            \\
            &\le
            F(z_k) - F(x_k) + \frac{L_k}{2}\Vert y_k - z_k\Vert^2 - \frac{L_k}{2}\Vert z_k - x_k\Vert^2
            \\
            &\underset{(1)}{\le} \alpha_k F(\bar x) + (1 - \alpha_k)F(x_{k - 1}) - F(x_k)
            + \frac{L_k}{2}\Vert y_k - z_k\Vert^2 - \frac{L_k}{2}\Vert z_k - x_k\Vert^2
            \\
            &\underset{(2)}{=} 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_{k}\Vert^2 
            \\
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            \\ &
            + \max\left(
                    1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
                \right)\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
        \end{align*}
        At (1) we used the fact that $F = f + g$ hence $F$ is convex. 
        At (2) we used \eqref{eqn:inxt-apg-cnvg-prep1-a}, \eqref{eqn:inxt-apg-cnvg-prep1-b}. 
        Finally, if $k = 0$, then take the RHS of $\underset{(1)}{=}$ then:
        \begin{align*}
            & \frac{\rho_0}{2}\Vert x_0 - y_0\Vert^2 - \epsilon_0 
            \\
            &\le 
            (1 - \alpha_0)(F(x_{-1}) - F(\bar x)) + F(\bar x) - F(x_0) 
            + \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_{-1}\Vert^2
            - \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_0\Vert^2. 
        \end{align*}
    \end{proof}
    \par
    The following assumption encapsulate assumptions on the errors such that a near optimal convergence rate is still attainable by an algorithm that satisfies Definition \ref{def:inxt-apg}. 
    \begin{assumption}[valid error schedule]\label{ass:valid-err-schedule}
        The following assumption is about an algorithm satisfying Definition \ref{def:inxt-apg}, its parameters $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$ in relation to its iterates $(y_k, x_k, v_k)_{k\ge 0}$ and, some additional parameters $(\beta_k)_{k\ge 0}, \mathcal E_0$ and $p$. 
        Let 
        \begin{enumerate}[nosep]
            \item $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}, (F, f, g, L)$ and $(y_k, x_k, v_k)_{k\ge 0}$ be given by Definition \ref{def:inxt-apg}. 
            \item $\mathcal E_0 \ge 0$ be arbitrary;
            \item the sequence $(\beta_k)_{k\ge 0}$ be defined as $\beta_k := \prod_{i = 1}^{k} \max\left(1 - \alpha_i, \frac{\alpha_i^2L_i}{\alpha_{i - 1}^2L_{i - 1}}\right)$ for all $k \ge 1$, with the base case being $\beta_0 = 1$; 
            \item $p \ge 1$ is some constant which will bound the error $\epsilon_k$ relative to $\rho_k\Vert x_k - y_k\Vert^2, \beta_k$ and, $k$. 
        \end{enumerate}
        In addition, we assume that the error parameter $\epsilon_k \ge 0$ and over-relaxation parameter $\rho_k$, iterates $x_k, y_k$ and $\beta_k$ together satisfies for all $k \ge 0$ the relations:
        \begin{align*}
            \frac{- \mathcal E_0\beta_k}{k^p} &\le 
            \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k. 
        \end{align*}
        
    \end{assumption}
    \par
    The following proposition is a prototype of the convergence rate together with the error schedule that delivers convergence of algorithms satisfying Definition \ref{def:inxt-apg}. 
    \begin{proposition}[convergence with valid error schedule]\;\label{prop:inxt-apg-cnvg-generic}\\
        Let $(F, f, g, L)$, $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, $(\beta_k)_{k\ge 0}, \mathcal E_0, p$ as assumed in Assumption \ref{ass:valid-err-schedule}. 
        Fix any $\bar x \in \RR^n$ for all $k \ge 0$ and assume that $\alpha_0 = 1$. 
        Then for the iterates generated $(y_k, x_k, v_k)_{k \ge 0}$ by the algorithm, for all $k \ge 0$ they will satisfy: 
        \begin{align*}
            & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
            \le 
            \beta_k \left(
                \frac{L_0}{2}\Vert \bar x - v_{-1}\Vert^2
                + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{align*}
    \end{proposition}
    \begin{proof}
        Consider results from Lemma \ref{lemma:inxt-apg-cnvg-prep1} has $\forall k \ge 1$: 
        \begin{align*}
            & \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k
            \\
            &\le 
            (1 - \alpha_k)(F(x_{k - 1}) - F(\bar x)) + F(\bar x) - F(x_k) 
            \\ &
            + \max\left(
                    1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
                \right)\frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
            \\
            &\le \max\left(
                1 - \alpha_k, \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2L_{k - 1}}
            \right)\left(
                F(x_{k - 1}) - F(\bar x)
                + \frac{\alpha_{k - 1}^2L_{k - 1}}{2}\Vert \bar x - v_{k - 1}\Vert^2 
            \right)
            \\&\quad 
                + F(\bar x) - F(x_k) - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
        \end{align*}
        For notation brevity, we introduce $\beta_k, \Lambda_k$: 
        \begin{align*}
            \beta_0 &= 1, 
            \\
            \beta_k &:= \prod_{i = 1}^{k} \max\left(
                1 - \alpha_i, \frac{\alpha_i^2L_i}{\alpha_{i - 1}^2L_{i - 1}}
            \right),
            \\
            \Lambda_k &:= 
            - F(\bar x) + F(x_k) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2. 
        \end{align*}
        Now, suppose that in addition there is a non-negative sequence $(\mathcal  E_k)_{k \ge 0}$ such that 
        \begin{enumerate}[nosep]
            \item For all $k \ge 0$, it has $\frac{-\mathcal E_k}{k^p} \le \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k$ where $p \ge 1$, 
            \item For all $k \ge 1$, it has $\mathcal E_k = \frac{\beta_k}{\beta_{k - 1}}\mathcal E_{k - 1}$, with $\mathcal E_0 \ge 0$. 
        \end{enumerate}
        These conditions are equivalent to the assumption that $\frac{- \mathcal E_0\beta_k}{k^p} \le \frac{\rho_k}{2}\Vert x_k - y_k\Vert^2 - \epsilon_k$ (which was stated in Assumption \ref{ass:valid-err-schedule}). 
        One can show that by unrolling recurrence on $\mathcal E_k$. 
        Then \eqref{ineq:inxt-apg-cnvg-generic-pitem-1} implies $\forall k \ge 1$: 
        \begin{align}\label{ineq:inxt-apg-cnvg-generic-pitem-1}
            \frac{- \mathcal E_k}{k^p} &\le 
            \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} - \Lambda_k
            \iff 
            \Lambda_k \le 
            \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} + \frac{\mathcal E_k}{k^p}. 
        \end{align}
        Now, we show the convergence of $\Lambda_k$, using the relations of $\mathcal E_k, \Lambda_k, \beta_k$ above. 
        \begin{align*}\begin{split}
            \Lambda_k &\le 
            \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} + \frac{\mathcal E_k}{k^p}
            \\
            &\le \frac{\beta_k}{\beta_{k - 1}}\Lambda_{k - 1} 
            + \frac{\beta_k}{\beta_{k -1}}\frac{\mathcal E_{k - 1}}{k^p}
            \\
            &= 
            \frac{\beta_k}{\beta_{k - 1}}
            \left(
                \Lambda_{k - 1} + \frac{\mathcal E_{k - 1}}{k^p}
            \right)
            \\
            &\le 
            \frac{\beta_k}{\beta_{k - 1}}
            \left(
                \frac{\beta_{k - 1}}{\beta_{k - 2}}\Lambda_{k - 2}
                + \frac{\mathcal E_{k - 1}}{(k - 1)^p}
                + \frac{\mathcal E_{k - 1}}{k^p}
            \right)
            \\
            &= 
            \frac{\beta_k}{\beta_{k - 2}}
            \left(
                \Lambda_{k - 2}
                + \frac{\mathcal E_{k - 2}}{(k - 1)^p}
                + \frac{\mathcal E_{k - 2}}{k^p}
            \right)
            \\
            & ...
            \\
            &\le \frac{\beta_k}{\beta_1}\left(
                \Lambda_1 + \mathcal E_1\sum_{n = 2}^{k} \frac{1}{n^p}
            \right)
            \\
            &\le \frac{\beta_k}{\beta_1}\left(
                \frac{\beta_1}{\beta_0}\Lambda_0 
                + \mathcal E_1\sum_{n = 1}^{k} \frac{1}{n^p}
            \right)
            \\
            &= \frac{\beta_k}{\beta_0}\left(
                \Lambda_0 
                + \mathcal E_0\sum_{n = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{split}\end{align*}
        Therefore, it points to the following inequality: 
        \begin{align*}
            & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
            \\
            &\le 
            \beta_k \left(
                F(x_0) - F(\bar x) + \frac{\alpha_0^2L_0}{2}\Vert \bar x - v_0\Vert^2
                + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{align*}
        Finally, when $\alpha_0 = 1$, then the results from \ref{lemma:inxt-apg-cnvg-prep1} with $k = 0$ simplifies the above inequality and give: 
        \begin{align*}
            & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
            \le 
            \beta_k \left(
                \frac{L_0}{2}\Vert \bar x - v_{-1}\Vert^2
                + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
            \right). 
        \end{align*}
    \end{proof}
    \par 
    Now, it only remains to determine the sequence $\alpha_k$ to derive a type of convergence rate for the algorithm because from the above theorem, we have the convergence rate $\beta_k$ and, the error parameters $\epsilon_k, \rho_k$ both controlled by the sequence $(\alpha_k)_{k \ge 0}$. 
    
    \subsection{Convergence results of the outer loop}
        This section will give specific instances of the error control sequence $(\epsilon_k)_{k \ge 0}, (\rho_k)_{k \ge0}$ and, momentum sequence $(\alpha_k)_{k \ge0}$ such that an optimal convergence rate of $\mathcal O(1/k^2)$ can be achieved. 
        \begin{assumption}[the optimal momentum sequence]\;\label{ass:opt-mmntm-seq}\\
            Keeping everything assumed in Assumption \ref{ass:valid-err-schedule} about $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, $(F, f, g, L)$, $(y_k, x_k, v_k)_{k \ge 0}$, $(\beta_k)_{k \ge 0}$, $\mathcal E_0$ and $p$. 
            We assume in addition that the sequence $(\alpha_k)_{k \ge 0}$ satisfies for all $k \ge 0$ the equality: $(1 - \alpha_k) = \alpha_{k}^2L_k\alpha_{k - 1}^{-2}L_{k - 1}^{-1}$ and, $p > 1$. 
        \end{assumption}
        \begin{lemma}[the optimal momentum sequence is indeed valid and optimal]\;\label{lemma:opt-mmntm-seq}\\
            Let $(\alpha_k)_{k \ge 0}, (\beta_k)_{k \ge 0}$ be given by Assumption \ref{ass:opt-mmntm-seq}. 
            If we choose $\alpha_0 \in (0, 1]\;$ then for all $k \ge 1$ it has: 
            \begin{align}\label{eqn:lemma:opt-mmntm-seq-result1}
                \alpha_k &=
                \frac{L_{k - 1}}{2L_k}\left(
                    - \alpha_{k - 1}^2 + \left(
                        \alpha_{k - 1}^4 + 4 \alpha_{k - 1}\frac{L_k}{L_{k - 1}}
                    \right)^{1/2}
                \right) \in (0, 1)
            \end{align}
            For the sequence $(\beta_k)_{k \ge 0}$ it has $\forall k \ge 1$
            \begin{align}\label{ineq:lemma:opt-mmntm-seq-result2}
                \left(
                    1 + \alpha_0\sqrt{L_0}\sum_{i = 1}^{k}\sqrt{L_i^{-1}}
                \right)^{-2}
                \hspace{-1em}\le 
                \beta_k = \frac{\alpha_k^2L_k}{\alpha_0^2L_0}
                \le 
                \left(
                    1 + \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k}\sqrt{L_i^{-1}}
                \right)^{-2}. 
            \end{align}
        \end{lemma}
        \begin{proof}
            Firstly, we will show \eqref{eqn:lemma:opt-mmntm-seq-result1}. 
            We will prove using induction.
            Fix any $k \ge 1$. 
            Assume inductively that $\alpha_{k - 1} \in (0, 1]$. 
            We can solve for $\alpha_k$ using the recursive equality $(1 - \alpha_k) = \alpha_{k}^2L_k\alpha_{k - 1}^{-2}L_{k - 1}^{-1}$ from Assumption \ref{ass:opt-mmntm-seq}.
            Writing $\alpha_{k - 1}$ as $\alpha$, and $L_k/L_{k - 1}$ as $q$.
            Then, we solve for $\alpha_{k}$, the quadratic equation always admits one root that is strictly positive which is given as: 
            \begin{align*}
                \alpha_k &= \frac{1}{2}\left(
                    - \frac{\alpha^2}{q} + \sqrt{
                        \frac{\alpha^4}{q^2} + \frac{4\alpha^2}{q}
                    }
                \right)
                \\
                &= \frac{\alpha^2}{2q}\left(
                    - 1 + \sqrt{1 + \frac{4q}{\alpha^2}}
                \right)
                \\
                &\underset{(1)}{<}
                \frac{\alpha^2}{2q}\left(
                    -1 + 1 + \frac{2q}{\alpha^2}
                \right)
                \\
                &= 1
            \end{align*}
            At (1) we completed a square in the radical, and we used the assumption $\alpha_k > 0$ and, $L_k > 0, L_{k - 1} > 0$ because we had $B_k > 0$, therefore the following chain of inequality holds: 
            \begin{align*}
                1 + \frac{4q}{\alpha^2} &= 
                1 + \frac{4q}{\alpha^2} + \frac{4q^2}{\alpha^4} - \frac{4\alpha^2}{\alpha^2}
                \\
                &= \left(
                    1 + \frac{2q}{\alpha^2}
                \right)^2 - \frac{4q^2}{\alpha^4} 
                \\
                &< \left(
                    1 + \frac{2q}{\alpha^2}  
                \right)^2. 
            \end{align*}
            To see that $\alpha_k > 0$, recall the same fact that $L_k > 0$, and the inductive hypothesis $\alpha_{k - 1} \in (0, 1]$ then $4q/\alpha^2 > 0$ so obviously $\alpha_k = \frac{\alpha^2}{2q}\left(-1 + \sqrt{1 + 4q/\alpha^2}\right) > 0$ because the quantity inside the radical is strictly larger than $1$. 
            Therefore, inductively it holds that $\alpha_k \in (0, 1)$ too. 
            \par
            We will now show \eqref{ineq:lemma:opt-mmntm-seq-result2}. 
            From the assumption that $(\alpha_k)_{k \ge 0}$ has $(1 - \alpha_k) = \alpha_k^2L_k \alpha_{k - 1}^{-2}L_{k - 1}^{-1}$ for all $k \ge 0$ and, the definition of $\beta_k$, it yields the following equalities: 
            \begin{align*}
                \beta_k = \prod_{i = 1}^k \max\left(
                    1 - \alpha_i, \frac{\alpha_i^2L}{\alpha_{i - 1}^2L_{i - 1}}
                \right) 
                = \prod_{i = 1}^k(1 - \alpha_i) 
                = \prod_{i = 1}^k \frac{\alpha_i^2L}{\alpha_0^2L_0} = \frac{\alpha_k^{2}L_k}{\alpha_0^2L_0}. 
            \end{align*}
            With the above relation, and the definitions of the sequences $(\alpha_k)_{k \ge 0}, (\beta_k)_{k\ge 0}$ it satisfies for all $k \ge 1$ the properties: 
            \begin{enumerate}[nosep]
                \item[(a)] $\beta_k$ is monotone decreasing and $\beta_k > 0$ for all $k \ge 0$ because $\beta_k = \prod_{i = 1}^{k} (1 - \alpha_i)$ and, $\alpha_k \in (0, 1]$. 
                \item[(b)] It has the equalities $\beta_k/\beta_{k - 1} = (1 - \alpha_k) = \frac{\alpha_k^2L_k}{\alpha_{k - 1}^2 L_{k - 1}}$ for all $k \ge 1$. 
            \end{enumerate}
            Using the above observations, we can show the chain of equalities $\alpha_k^{2} = (1 - \beta_k/\beta_{k - 1})^2 = \beta_kL_0\alpha_0^2L_k^{-1}$ for all $k \ge 0$. 
            This is true by first considering the relations $\prod_{i = 1}^k(1 - \alpha_i) = \beta_k$: 
            \begin{align}\label{eqn:opt-mmntm-seq-pitem1}\begin{split}
                (1 - \alpha_k) &= \beta_k/\beta_{k - 1}
                \\
                \iff 
                \alpha_k &= 1 - \beta_k / \beta_{k - 1}
                \\
                \implies 
                \alpha_k^2 &= (1 - \beta_k / \beta_{k - 1})^2. 
            \end{split}\end{align}
            Next, the recursive relation of $(\alpha_k)_{k \ge 0}$ gives
            \begin{align}\label{eqn:opt-mmntm-seq-pitem2}\begin{split}
                \alpha_k^2&= (1 - \alpha_k)\alpha_{k - 1}^2L_{k - 1}L_k^{-1}
                \\
                &= 
                (1 - \alpha_k)
                \left(
                    \frac{\alpha_{k - 1}^2L_{k - 1}}{\alpha_0^2L_0}
                \right)
                \frac{\alpha_0^2L_0}{L_k}
                \\
                &= 
                (\beta_k\beta_{k - 1}^{-1})\left(
                    \beta_{k - 1}
                \right)L_0\alpha_0^2L_k^{-1}
                \\
                &= \beta_kL_0\alpha_0^2L_k^{-1}.         
            \end{split}\end{align}
            Combining \eqref{eqn:opt-mmntm-seq-pitem1}, \eqref{eqn:opt-mmntm-seq-pitem2} and the fact that $\beta_k > 0\;\forall k \ge 0$, it would mean for all $i \ge 1$ it has: 
            \begin{align*}
                L_0 \alpha_0^2 L_i^{-1} &= 
                \beta_i^{-1}\left(
                    1 - \frac{\beta_k}{\beta_{k - 1}}
                \right)^2
                \\
                &= 
                \beta_i \left(
                    \beta_i^{-1} - \beta_{i - 1}^{-1}
                \right)^2
                \\
                &=
                \beta_i \left(
                    \beta_i^{-1/2} - \beta_{i - 1}^{-1/2}
                \right)^2
                \left(
                    \beta_i^{-1/2} + \beta_{i - 1}^{-1/2}
                \right)^2
                \\
                &= \left(
                    \beta_i^{-1/2} - \beta_{i - 1}^{-1/2}
                \right)^2\left(
                    1 + \beta_i^{1/2}\beta_{i - 1}^{-1/2}
                \right)^2. 
            \end{align*}
            Since $\beta_i$ is monotone decreasing, it has $0 < \beta_i^{1/2}\beta_{i - 1}^{-1/2} \le 1$, this gives: 
            \begin{align*}
                \beta_i^{-1/2} - \beta_{i - 1}^{-1/2} 
                &\le \alpha_0\sqrt{\frac{L_0}{L_i}} 
                \le 2 \left(
                    \beta_i^{-1/2} - \beta_{i - 1}^{-1/2}
                \right).
            \end{align*}
            Performing a telescoping sum for $i = 1, 2, \ldots, k$, use the fact that $\beta_0 = 1$ will yield the desired results after some algebraic manipulations. 
        \end{proof}
        \begin{remark}
        \end{remark}
        \begin{proposition}[$\mathcal O(1/k^2)$ outer loop function value convergence]\;\label{prop:opt-cnvg-outr-loop}\\
            Let $(f, g, L)$, $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, $(\beta_k)_{k\ge 0}, \mathcal E_0, p$ be given by Assumption \ref{ass:opt-mmntm-seq}. 
            Assume in addition that
            \begin{enumerate}[nosep]
                \item there exists $\bar x \in \RR^n$ that is a minimizer of $F = f + g$;
                \item the sequence $L_k := B_k + \rho_k$ is bounded, and there exists an $L_{\max}$ such that for all $k \ge 0$ it has $L_{\max} \ge \max_{k\ge i\ge 0} L_i$. 
            \end{enumerate}
            Then, it has $\forall k \ge 0$: 
            \begin{align*}
                & F(x_k) - F(\bar x) + \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
                \le 
                \left(
                    1 + \frac{k\alpha_0\sqrt{L_0}}{2\sqrt{L_{\max}}}
                \right)^{-2}\left(
                    \frac{L_0}{2}\Vert \bar x - v_{-1}\Vert^2
                    + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
                \right). 
            \end{align*}
            Since, $p > 0$ the sum is convergent and hence the above inequality claims an overall convergence rate $\mathcal O(1/k^2)$. 
        \end{proposition}
        \begin{proof}
            We use the results from Lemma \ref{lemma:opt-mmntm-seq} because of the same assumption on $(\alpha_k)_{k 
            \ge 0}$, then using the fact that $L_k$ is bounded: 
            \begin{align*}
                \beta_k &\le 
                \left(
                    1 + \frac{\alpha_0\sqrt{L_0}}{2}\sum_{i = 1}^{k} \sqrt{L_i^{-1}}
                \right)^{-2}
                \hspace{-0.7em}\le 
                \left(
                    1 + \frac{k\alpha_0\sqrt{L_0}}{2\sqrt{L_{\max}}}
                \right)^{-2}. 
            \end{align*}
            Then, apply Theorem \ref{prop:inxt-apg-cnvg-generic} to obtain the desired results.
        \end{proof}
        \begin{remark}
            In this remark, we assert the fact that all assumptions made in the theorem can be satisfied on practice, and we will also bring attention to the current, and future roles played by some parameters used in the inexact algorithm. 
            \par
            Pay attention that $\alpha_k$ had been determined in the above theorem (as seemed in Lemma \ref{lemma:opt-mmntm-seq}), and $(B_k)_{k \le 0}$ is reserved for potential line search routine, the only parameter left undetermined in Definition \ref{def:inxt-apg} is the over-relaxation sequence $(\rho_k)_{k \ge 0}$.
            Since we only need the entire sequence $L_k = \rho_k + B_k$ to be bounded above, we give the freedom to the practitioners to choose $(\rho_k)_{k \ge 0}$. 
            However, this sequence $\rho_k$ is not useless because it counters $\epsilon_k$ in the proximal gradient inequality, this has huge impact in the earlier stage (the first few iterations) of the algorithm $\Vert x_k - y_k\Vert$ is large. 
            Of course, the parameter $\mathcal E_0$ is also free to choose. 
            \par
            Finally, observe that from the above proof, in case when $p = 1$, the convergence rate would be $\mathcal O(\log(k)/k^2)$. 
        \end{remark}
        In the next subsection, we will show that under the assumption of the above theorem, there exists an error sequence $\epsilon_k$ such that it can never approach $0$ at an arbitrarily fast rate. 
    \subsection{The fastest rate of which the error schedule can shrink}
        To have overall convergence claim of the algorithm, it's necessary to prevent the error schedule $(\epsilon_k)_{k \ge 0}$ from crashing into zero too quickly. 
        Following Assumption \ref{ass:valid-err-schedule}, in this section, we will provide the lower bound results for $\epsilon_k$ in Theorem \ref{prop:opt-cnvg-outr-loop} to show that in the worst case it cannot approach zero arbitrarily fast, if we choose the largest possible $\epsilon_k$ using $\beta_k$. 
        \begin{lemma}[error schedule lower bound]\;\label{lemma:err-schedule-lbnd}\\
            Let, $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, $(\beta_k)_{k\ge 0}, \mathcal E_0, p$ be given by Assumption \ref{ass:opt-mmntm-seq}, $L_k := \rho_k + B_k$. 
            Let $(\epsilon_k)_{k \ge 0}$ be given by $\epsilon_k := \frac{\mathcal E_0 \beta_k}{k^p} + \rho_k \frac{\Vert x_k - y_k\Vert^2}{2}$, then it will be a valid error sequence and so that it satisfies the assumption. 
            If in addition, there exists $L_{\min}$ such that for all $k\ge 0$ such that it has $L_{\min} \le \min_{1 \le i \le k}L_i$ and, we assume $\mathcal E > 0$, then $\epsilon_k$ admits the non-trivial lower bound: 
            \begin{align*}
                \epsilon_k \ge \frac{\mathcal E_0}{k^p}\left(
                    1 + k \alpha_0 \sqrt{L_0} \sqrt{L_{\min}^{-1}}
                \right)^{-2} = \mathcal O(k^{-2-p}). 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Recall Assumption \ref{ass:valid-err-schedule}, the largest valid error schedule is $\epsilon_k = \frac{\mathcal E_0 \beta_k}{k^p} + \rho_k \frac{\Vert x_k - y_k\Vert^2}{2}$. 
            Then it has 
            \begin{align*}
                \epsilon_k &\ge \frac{\mathcal E_0 \beta_k}{k^p}
                \\
                &\underset{(1)}{\ge} 
                \left(
                    1 + \alpha_0\sqrt{L_0}\sum_{i = 1}^{k}\sqrt{L_i^{-1}}
                \right)^{-2} 
                \frac{\mathcal E_0}{k^p}
                \\
                &\underset{(2)}{\ge}
                \frac{\mathcal E_0}{k^p}
                \left(
                    1 + k\alpha_0\sqrt{L_0}\sqrt{L_{\min}^{-1}}
                \right)^{-2} 
                \\
                &= \mathcal O(k^{-2-p}). 
            \end{align*}
            At (1), we used Lemma \ref{lemma:opt-mmntm-seq}. 
            At (2), we used that $L_{\min} \le L_i$ for all $i = 0, 1, 2, \ldots$. 
        \end{proof}
    \subsection{The gradient mapping also shrinks}
        In this section, we show one extremely favorable properties and its assumptions such that, successive iterates $v_k, v_{k - 1}$ converges to zero as $k \rightarrow \infty$, and hence the norm of the gradient mapping converges at a rate of $\mathcal O(1/k)$. 
        This property will be crucial for analyzing the total complexity for the algorithm because it is the termination criteria. 
        \begin{proposition}[iterates $v_k$ and gradient mapping]\;\label{prop:vk-gm}\\
            Let $(F, f, g, L)$, $(\alpha_k, B_k, \rho_k, \epsilon_k)_{k \ge 0}$, $(\beta_k)_{k\ge 0}, \mathcal E_0, p$ as assumed in Assumption \ref{ass:valid-err-schedule}. 
            Assume there exists $\bar x \in \RR^n$ which is the minimizer of $F$ and, we set $\alpha_0 = 1$. 
            Then for the iterates generated $(y_k, x_k, v_k)_{k \ge 0}$ by the algorithm, for all $k \ge 0$ all followings are true. 
            \begin{enumerate}[nosep]
                \item\label{prop:vk-gm:result1} it has $v_k - v_{k - 1} = \alpha_k^{-1}(x_k - y_k) = \alpha_k^{-1}L_k^{-1} \mathcal G_{L_k}(y_k)$. 
                \item\label{prop:vk-gm:result2} as a consequence of the former, we have the following bound: 
                $$
                    \Vert \bar x - v_k\Vert \le \Vert \bar x - v_{- 1}\Vert
                    + \left(
                        \frac{2 \mathcal E_0}{L_0} 
                        \sum_{n = 1}^{k} \frac{1}{n^p}
                    \right)^{1/2}. 
                $$
            \end{enumerate}
        \end{proposition}
        \begin{proof}
            \textbf{Our first result follows. }
            It has $y_k = \alpha_k v_{k - 1} + (1 - \alpha_k)x_{k - 1}$ from \eqref{def:inxt-apg:yk}, and $v_k = x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1})$ from \eqref{def:inxt-apg:vk}. 
            Using this information we have 
            {\allowdisplaybreaks
            \begin{align*}
                v_k - v_{k - 1} &=
                x_{k - 1} + \alpha_k^{-1}(x_k - x_{k - 1}) 
                - \alpha_k^{-1}(y_k - (1 - \alpha_k)x_{k - 1})
                \\
                &= (1 - \alpha_k^{-1})x_{k - 1} + \alpha_k^{-1}x_k - \alpha_k^{-1}y_k + (\alpha_k^{-1} - 1)x_{k - 1}
                \\
                &= \alpha_k^{-1}(x_k - y_k) = \alpha_k^{-1}L_k^{-1}L_k(x_k - y_k) = \alpha_k^{-1}L_k^{-1}\mathcal G_{L_k}(y_k). 
            \end{align*}
            }
            \textbf{Our second result follows. }
            The assumption for Proposition \ref{prop:inxt-apg-cnvg-generic} are satisfied here hence: 
            \begin{align*}
                0 &\le \beta_k\left(
                    \frac{L_0}{2}\Vert \bar x - v_{k - 1}\Vert^2
                    + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
                \right) 
                - \frac{\alpha_k^2L_k}{2}\Vert \bar x - v_k\Vert^2
                - (F(x_k) - F(\bar x))
                \\
                \underset{(1)}\implies
                0 &\le 
                \frac{L_0}{2}\Vert \bar x - v_{k - 1}\Vert^2
                + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
                - \frac{\alpha_k^2L_k}{2\beta_k}\Vert \bar x - v_k\Vert^2
                \\
                &\underset{(2)}= 
                \frac{L_0}{2}\Vert \bar x - v_{k - 1}\Vert^2
                + \mathcal E_0 \sum_{n = 1}^{k} \frac{1}{n^p}
                - \frac{L_0}{2}\Vert \bar x - v_k\Vert^2
                \\
                \iff 
                0 &\le \Vert \bar x - v_{-1}\Vert - \Vert \bar x - v_k\Vert
                + \left(
                    \frac{2 \mathcal E_0}{L_0}\sum_{n = 1}^{k}\frac{1}{n^p}
                \right)^{1/2}. 
            \end{align*}
            At (1), we assumed $\bar x$ is the minimizer so $- F(x_k) + F(\bar x) < 0$ and so we replaced it with a zero, and since $\beta_k > 0$ always we can divided on both side of the inequality without changing the sign. 
            At (2), we used $\beta_k = \frac{\alpha_k^2 L_k}{\alpha_0^2 L_0}$ from \eqref{ineq:lemma:opt-mmntm-seq-result2}, since we assumed $\alpha_0 = 1$ here it has $\beta_k = \alpha_k^2L_k/L_0$ hence the coefficient $\frac{\alpha_k^2L_k}{2 \beta_k} = \frac{L_0}{2}$. 
        \end{proof}
        \par
        Now, we will show the convergence rate of the normed gradient mapping is $\mathcal O(1/k)$ for the best chosen sequence. 
        The theorem that follows will acomplish that 
        \begin{theorem}[$\mathcal O(1/k)$ convergence of the gradient mapping]
            \todoinline{
                N'obliez pas de finir cette partie.
            }
        \end{theorem}
        \begin{proof}
            
        \end{proof}

        
\section{Linear convergence for the proximal problem in the inner loop}
    In this section, we continue the discussion from Section \ref{sec:optz-inxt-pp-problem}. 
    As an important reminder, we will fix the vector $y \in \RR^n$, which is in the inexact proximal problem as a constant in this entire section. 
    \par
    The inner loop is another algorithm that evaluates $x_k \approx_\epsilon T_{(B_k + \rho_k)}(y_k)$ for a given value of $\epsilon, B + \rho$ and at the point $y_k$. 
    Let's assume that the outer loop iteration $k$ is fixed throughout this entire section to simplify our discussion since, in this section we will only focus on the convergence rate of the inner loop for one specific iteration of the outer loop. 
    \par
    Let $\lambda = (B_k + \rho_k)^{-1}$, the algorithm needs to resolve the following equivalent inexact proximal point problem: 
    \begin{align*}
        x_k \approx_\epsilon \hprox_{\lambda g}(y_k - \lambda \nabla f(y_k)). 
    \end{align*}
    Unfortunately recall that $g = \omega \circ A$ in the context of the outer loop hence it's impossible to directly evaluate the proximal operator of $g$ and hence we optimize the function $\Phi_\lambda$ as given by \eqref{eqn:primal-pp}. 
    \par
    We will show that there exists an algorithm generating the sequences $z_j, v_j$ such that $\mathbf G_\lambda (z_j, v_j)$ converges linearly if $\Psi_\lambda$ satisfies the error bound conditions. 
    Using results available in the literature, we will characterize the exact scenarios of $\omega \circ A$ when this is possible to achieve. 
    To start, the following assumption is the general error bound condition of a convex with additive composite structure. 
    \begin{definition}[the exact proximal gradient and gradient mapping]\;\label{def:pg-and-gm}\\
        Let $F = f + g$ where $f:\RR^n \rightarrow \RR$ is a differentiable function and, $g: \RR^n \rightarrow \overline \RR$ is closed convex and proper. 
        For all $\tau > 0$, we define the exact proximal gradient mapping and, gradient mapping: 
        \begin{enumerate}[nosep]
            \item The proximal gradient operator $T_{\tau}(x) := \hprox_{\tau^{-1}g}(x - \tau^{-1}\nabla f(x))$,
            \item and the gradient mapping operator $\mathcal G_\tau(x) := \tau (x - T_\tau(x))$. 
        \end{enumerate}
    \end{definition}
    \begin{definition}[the error bound condition]\;\label{def:err-bnd}\\
        Under the same assumptions in Definition \ref{def:pg-and-gm}, the function $F = f + g$ satisfies the error bound condition if there exists $\gamma > 0$ such that 
        \begin{align}\label{ineq:pg-eb}
            \Vert \mathcal G_\tau(x)\Vert \ge \gamma\dist(x|S). 
        \end{align}
    \end{definition}
    \begin{assumption}[problem with proximal gradient and error bound]\;\label{ass:pg-eb}\\
        The following assumption is about $(F, f, g, L, S, \gamma)$. 
        Assume that
        \begin{enumerate}[nosep]
            \item $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, 
            \item let $\tau > 0$ be the step size inverse, let $T_{\tau}$ be the proximal operator of $f + g$ as given by $T_{\tau}(x) := \hprox_{\tau^{-1}g}(x - \tau^{-1}\nabla f(x))$, 
            \item $S= \argmin_{x}{f(x) + g(x)} \neq \emptyset$, 
            \item the objective function is given by $F = f + g$ and, it satisfis error bound (Definition \ref{def:err-bnd}). 
        \end{enumerate} 
    \end{assumption}
    \begin{definition}[the proximal gradient method]\;\label{def:ista}\\
        Suppose that $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}. 
        Let $\tau \ge L$, and $x_0 \in \RR^n$. 
        Then an algorithm is a proximal gradient method if it generates iterates $(x_k)_{k \ge 0}$ such that they satisfy for all $k \ge 1$: 
        \begin{align*}
            x_{k + 1} = \hprox_{\tau^{-1} g}\left(x_k + \tau^{-1}\nabla f(x_k)\right). 
        \end{align*}
    \end{definition}
    \subsection{Error bound and linear convergence of ISTA}
        The following theorem characterizes linear convergence of the proximal gradient method under gradient mapping error bound condition. 
        \begin{theorem}[linear convergence under gradient mapping error bound]\;\label{thm:lin-cnvg-ista-eb}\\
            Assume that $(F, f, g, L, S, \gamma)$ is given by Assumption \ref{ass:pg-eb}. 
            Under this assumption, the iterates $(x_k)_{k \ge 0}$ given by Definition \ref{def:ista} satisfies for all $k \ge 0, \bar x \in S$ and $\tau \ge L$: 
            \begin{align*}
                F(x_{k + 1}) - F(\bar x)
                &\le 
                \left(
                    1 - \frac{\gamma}{2\tau}
                \right)(F(x_k) - F(\bar x)), \text{ where } \frac{\gamma}{2\tau} \in (0, 1)
            \end{align*}
            Hence, the algorithm generates $F(x_k) - F(\bar x)\le \mathcal O((1 - \gamma/(2\tau))^k)$. 
        \end{theorem}
        \begin{proof}
            Two important immediate results will be presented first.
            Consider the proximal gradient inequality from Theorem \ref{thm:inxt-pg-ineq}, but with $\rho = 0, \epsilon = 0, B = \tau$, then for all $x$ such that $\Vert \mathcal G_\tau(x)\Vert > 0$ it has for $\tilde x = T_{\tau}(x), z \in \RR^n$ the inequality 
            \begin{align*}
                F(\tilde x) - F(z) 
                &\le 
                \frac{\tau}{2}\Vert x - z\Vert^2 - \frac{\tau}{2}\Vert z - \tilde x\Vert^2
                \\
                &=  
                - \frac{\tau}{2}\Vert x - \tilde x\Vert^2
                + \tau\langle x - z, x - \tilde x\rangle
                \\
                &=  - \frac{1}{2\tau}\Vert \mathcal G_\tau(x) \Vert^2
                + \langle x - z, \mathcal G_\tau(x)\rangle
                \\
                &\le  - \frac{1}{2\tau}\Vert \mathcal G_\tau(x)\Vert^2 
                + \Vert x - z\Vert \Vert \mathcal G_\tau(x)\Vert
                \\
                &=
                \Vert \mathcal G_\tau(x)\Vert^2\left(
                    \frac{\Vert x - z \Vert}{\Vert \mathcal G_\tau(x)\Vert} - \frac{1}{2\tau}
                \right). 
            \end{align*}
            Now, for all $z = \bar x \in S$, from Assumption \ref{ass:lin-cnvg-for-pp} $\exists \gamma > 0$ such that: 
            \begin{align*}
                \frac{\Vert x - z \Vert}{\Vert \mathcal G_\tau(x)\Vert}
                \le 
                \frac{\Vert x - z \Vert}{\gamma \dist(x | S)} \le \frac{1}{\gamma}. 
            \end{align*}
            Hence, for all $\bar x \in S$ it has 
            \begin{align}\label{ineq:lin-cnvg-ista-eb-pitem1}
                0\le F(\tilde x) - F(\bar x)&\le 
                \Vert \mathcal G_\tau(x)\Vert^2\left(
                    \frac{1}{\gamma} - \frac{1}{2\tau}
                \right). 
            \end{align}
            Obviously it has $\gamma^{-1} - (1/2)\tau^{-1} > 0$. 
            When $z = x$, we have the inequality: 
            \begin{align}\label{ineq:lin-cnvg-ista-eb-pitem2}
                F(\tilde x) - F(x) &\le - \frac{1}{2\tau}\Vert \mathcal G_\tau(x)\Vert^2. 
            \end{align}
            To derive the linear convergence, we use \eqref{ineq:lin-cnvg-ista-eb-pitem1} with $x = x_k, \tilde x = x_{k + 1}$:
            {\allowdisplaybreaks
            \begin{align*}
                0 &\le 
                \Vert \mathcal G_\tau(x_k)\Vert^2\left(
                    \frac{1}{\gamma} - \frac{1}{2\tau} 
                \right)
                - F(x_{k + 1}) + F(\bar x)
                \\
                &= 
                \frac{1}{2\tau}\Vert \mathcal G_\tau(x_k)\Vert^2\left(
                    \frac{2\tau}{\gamma} - 1
                \right)
                - F(x_{k + 1}) + F(\bar x)
                \\
                &\underset{(1)}{\le}
                \left(
                    \frac{2\tau}{\gamma} - 1
                \right)
                \left(
                    F(x_k) - F(x_{k + 1})
                \right)
                - F(x_{k + 1}) + F(\bar x)
                \\
                &= 
                \left(
                    \frac{2\tau}{\gamma} - 1
                \right)
                \left(
                    F(x_k) - F(\bar x) + F(\bar x) - F(x_{k + 1})
                \right)
                - F(x_{k + 1}) + F(\bar x)
                \\
                &= \frac{2\tau}{\gamma}(F(\bar x) - F(x_{k + 1}))
                + \left(
                    \frac{2\tau}{\gamma} - 1
                \right)(F(x_k) - F(\bar x)). 
            \end{align*}
            }
            At (1) we used \eqref{ineq:lin-cnvg-ista-eb-pitem2}. 
            Multiple both side by $\frac{\gamma}{2\tau}$ then we are done. 
        \end{proof}
    \subsection{Conditions for linear convergence of the proximal problem}\label{sec:conds-lin-cnvg-pp}
        In this section, we will focus on the sufficient characterization of the proximal problem which allows proximal gradient method to achieve linear convergence rate. 
        The following assumption characterizes a set of sufficient conditions of the proximal problem such that linear convergence rate of applying ISTA to dual proximal objective $\Psi_\lambda$ can be achieved. 
        \begin{assumption}[conditions for linear convergence of proximal problem]\;\label{ass:lin-cnvg-for-pp}\\
            This assumption is about $(g, \omega, A, y, \Phi_\lambda,\Psi_\lambda,\gamma_\lambda)$. 
            Here are the assumptions
            \begin{enumerate}[nosep]
                \item\label{ass:lin-cnvg-for-pp:item1} $y \in \RR^n$ is the vector of which the proximal problem is anchored at, fixed it to be an arbitrary vector. 
                \item\label{ass:lin-cnvg-for-pp:item2} Assume $(g, \omega, A)$ satisfies Assumption \ref{ass:for-inxt-prox}. The primal objective $\Phi_\lambda$ is given by \eqref{eqn:primal-pp}, and dual $\Psi_\lambda$ by \eqref{eqn:dual-pp}. This means if we let $h(v) := \frac{1}{2\lambda}\Vert \lambda A^\top v - y\Vert^2 - \frac{1}{2\lambda}\Vert y\Vert^2$, then $\Psi_\lambda(v) = h(v) + \omega^\star(v)$. 
                \item\label{ass:lin-cnvg-for-pp:item3} Next, assume $\Psi_\lambda = h + \omega^\star$ satisfies error bound condition (Assumption \ref{ass:pg-eb}) with $\Psi_\lambda = F$ and, $\gamma = \gamma_\lambda$ and $f = h$. Note that we can do this because $h$ is quadratic hence obviously Lipschitz continuous and Lipschitz smooth. 
                % \item\label{ass:lin-cnvg-for-pp:item4} Assume that there exists the constant $C_\Psi^\lambda < \infty$ such that 
                % $$
                %     C_\Psi^\lambda = \sup_{v\in \RR^m} \Psi_\lambda(v) - \inf_{v\in \RR^m}\Psi_\lambda(v). 
                % $$
                % \item\label{ass:lin-cnvg-for-pp:item5} Assume that $\omega$ is a $K_\omega$ Lipschitz continuous function on $\dom \omega$. 
            \end{enumerate}
        \end{assumption}
        \par
        The following definition specifies the algorithm that can achieve linear convergence rate with the assumptions above. 
        \begin{definition}[the ISTA inner loop algorithm]\;\label{def:ista-inner-lp}\\
            Let $\lambda > 0, \epsilon > 0$, and $(g, \omega, A, y, \Phi_\lambda,\Psi_\lambda,\gamma_\lambda)$ satisfies Assumption \ref{ass:lin-cnvg-for-pp}. 
            \begin{enumerate}[nosep]
                \item Let $v_0\in\dom \omega^\star$ be a feasible initial guess of $\Psi_{\lambda}$, and let $\tau \ge \lambda\Vert AA^\top\Vert$ be the inverse step size. 
                \item Define $z_0 = y - \lambda A^\top v_0$ and smooth part of $\Psi_\lambda$ as $h := v \mapsto \frac{1}{2\lambda}\Vert \lambda A^\top v - y\Vert^2$. 
            \end{enumerate}
            The algorithm that solves the proximal problem generates the primal dual sequences $(z_j, v_j)$ such that for all $j = 0, 1, 2, \cdots$, they satisfy: 
            \begin{align*}
                v_{j + 1} &= \hprox_{\tau^{-1} \omega^\star}\left(
                    v_{j} - \tau^{-1}\nabla h(v_j)
                \right), 
                \\
                z_{j + 1} &= y - \lambda A^\top v_{j + 1}. 
            \end{align*}
            Terminates if $\mathbf G_\lambda(z_j, v_j) \le \epsilon$ where $\mathbf G_\lambda$ is given by \eqref{eqn:duality-gap-pp}, then the result we want is $z_j$. 
        \end{definition}
        \begin{remark}
            The value of $\mathbf G_\lambda(z_j, v_j)$ is easy to compute, it only requires access to matrix $A, A^\top$, and the function $\omega$. 
            In case when the proximal operator for $\omega^\star$ is nontrivial, we can use the Moreau identity and the proximal operator of $\omega$ instead. 
            The gradient $\nabla f(v)$ is easy to compute, and it is: $AA^\top v - Ay$.
        \end{remark}
        \par
        The following propositions precisely show that the linear convergence is achievable when Assumption \ref{ass:lin-cnvg-for-pp} holds. 
        \begin{proposition}[sufficient conditions of linear convergence of the inner loop]\;\label{prop:inn-loop-lin-cnvg}\\
            Let the parameters $(g, \omega, A, y, \Phi_\lambda, \Psi_\lambda, \gamma_\lambda)$ of a proximal problem satisfy Assumption \ref{ass:lin-cnvg-for-pp}. 
            Let $\tau$, $v_0, \epsilon > 0$ be given by Definition \ref{def:ista-inner-lp} along with iterates $(z_j, v_j)_{j \ge 0}$. 
            Let $\bar v$ be a minimizer of $\Psi_\lambda$, then the followings are true: 
            \begin{enumerate}[nosep]
                \item\label{prop:inn-loop-lin-cnvg:item1} Let $\bar v$ be minimizer of $\Psi_\lambda$, the sequence $\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)$ converges linearly to zero. 
                \item\label{prop:inn-loop-lin-cnvg:item2} There exists constants $K_\Psi^\lambda$ such that $\Psi_\lambda$ is $K_\Psi^\lambda$ Lipschitz continuous on $\dom \Psi_\lambda$, as a consequence there exists constant $C_\Psi^\lambda$ such that, $\sup_{v \in \RR^m}\Phi_\lambda(v) - \Phi_\lambda(\bar v) \le K_\Psi^\lambda\sup_{v \in \dom\Psi_\lambda}\Vert v - \bar v \Vert = C_\Psi^\lambda < \infty$. 
                \item\label{prop:inn-loop-lin-cnvg:item3} The duality gap has a linear convergence rate by the inequality: 
                $$
                    \mathbf G_\lambda(z_j, v_j) 
                    \le
                    \left(
                        1 - \frac{\gamma_\lambda}{2\tau}
                    \right)^{j/2}
                    C_\lambda
                    \text{ where: } 
                    C_\lambda = \sqrt{2\lambda C_\Psi^\lambda}
                    \left(
                        K_\omega\Vert A\Vert
                        + K_\omega
                        + \frac{\sqrt{2\lambda C_\Psi^\lambda}}{\lambda}
                    \right). 
                $$
                \item\label{prop:inn-loop-lin-cnvg:item4} If $\mathbf G_\lambda(z_j, v_j) \le \epsilon$, then $z_j \approx_\epsilon \hprox_{\lambda g}(y)$ and the number of iterations sufficient to achieve the accuracy would be 
                \begin{align*}
                    j \ge 
                    \left\lceil\frac{
                        2\ln \left(
                            \frac{\epsilon}{C_\lambda}
                        \right)
                    }{
                        \ln\left(
                            1 - \frac{\gamma_\lambda}{2\tau}
                        \right)
                    }\right\rceil. 
                \end{align*}
            \end{enumerate}
        \end{proposition}
        \begin{proof}
            To start, we prove \ref{prop:inn-loop-lin-cnvg:item1}. 
            Recall that it has $\Psi_\lambda = h + \omega^\star$ with $h$ being Lipschitz smooth and, $\omega^\star$ closed convex proper from item \ref{ass:lin-cnvg-for-pp:item2} in Assumption \ref{ass:lin-cnvg-for-pp}. 
            In addition $\Psi_\lambda$ also satisfies error bound in item \ref{ass:lin-cnvg-for-pp:item3} of Assumption \ref{ass:lin-cnvg-for-pp} with $\gamma = \gamma_\lambda$. 
            Finally, $(z_j, v_j)_{j \ge 0}$ given by Definition \ref{def:ista-inner-lp} has $\tau \ge \lambda\Vert A^\top A\Vert$ so it's essntially Definition \ref{def:ista} with $x_j = v_j$. 
            Therefore results from Theorem \ref{thm:lin-cnvg-ista-eb} applies with $F = \Psi_\lambda, x_j= v_j$ and it has: 
            \begin{align}\label{ineq:inn-loop-lin-cnvg-pitem1}
                \Psi_\lambda(v_j) - \Psi_\lambda(\bar v) &\le 
                \left(
                    1 - \frac{\gamma_\lambda}{2\tau}
                \right)^j
                \left(
                    \Psi_\lambda(v_0) - \Psi_\lambda(\bar v)
                \right). 
            \end{align}
            \par
            Next, we show \ref{prop:inn-loop-lin-cnvg:item2}. 
            The smooth part of $\Phi_\lambda$ is the quadratic $h(v) = \frac{1}{2\lambda}\Vert \lambda A^\top v - y\Vert^2 - \frac{1}{2\lambda}\Vert y\Vert^2$, it's locally Lipschitz on the compact the domain of $\dom\Psi_\lambda = \dom \omega^\star$ is compact by Assumption \ref{ass:for-inxt-prox}, hence the smooth part is Lipschitz on $\dom \omega^\star$. 
            In addition, $\omega^\star$ is assumed to be Lipschitz on $\dom \omega^\star$, therefore the entire function $\Psi$ is Lipschitz on $\omega^\star$, we denote the Lipschitz constant by $K_\Psi^\lambda$. 
            Because $\omega^\star$ is bounded, therefore there exists constant $C_\Psi^\lambda$ such that.
            \begin{align*}
                \sup_{v \in \RR^m}\Phi_\lambda(v) - \Phi_\lambda(\bar v) \le K_\Psi^\lambda\sup_{v \in \dom\Psi_\lambda}\Vert v - \bar v \Vert = C_\Psi^\lambda < \infty. 
            \end{align*}
            \par
            To see the duality gap, Theorem \ref*{thm:minimizing-dual-pp}\ref{thm:minimizing-dual-pp:item3} applies because $v_j$ by \eqref{ineq:inn-loop-lin-cnvg-pitem1} provides us the sequence $(v_j)_{j \ge 0}$ such $\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)$ converges. 
            {\small
            \begin{align*}
                & \mathbf G_\lambda(z_j, v_j) 
                \\
                &= \Phi_\lambda(z_j) - \Phi_\lambda(\bar z) + 0
                \\
                &= \Phi_\lambda(z_j) - \Phi_\lambda(\bar z) + \Psi_\lambda(v_j) - \Psi_\lambda(\bar v)
                \\
                &\le 
                \sqrt{2\lambda\left(\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)\right)}
                \left(
                    K_\omega\Vert A\Vert
                    + K_\omega
                    + \frac{\sqrt{2\lambda}}{2\lambda}\sqrt{\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)}
                \right)
                \\&\quad 
                    + \Psi_\lambda(v_j) - \Psi_\lambda(\bar v)
                \\
                &=
                \sqrt{2\lambda(\Psi_\lambda(v_j) - \Psi_\lambda(\bar v))}\left(
                    K_\omega\Vert A\Vert
                    + K_\omega
                    + \frac{\sqrt{2\lambda}}{\lambda}\sqrt{\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)}
                \right) 
                \\
                &\underset{(1)}{\le} 
                \left(
                    1 - \frac{\gamma_\lambda}{2\tau}
                \right)^{j/2}\sqrt{2\lambda(\Psi_\lambda(v_0) - \Psi_\lambda(\bar v))}
                \left(
                    K_\omega\Vert A\Vert
                    + K_\omega
                    + \frac{\sqrt{2\lambda}}{\lambda}\sqrt{\Psi_\lambda(v_j) - \Psi_\lambda(\bar v)}
                \right) 
                \\
                &\underset{(2)}{\le} 
                \left(
                    1 - \frac{\gamma_\lambda}{2\tau}
                \right)^{j/2}\sqrt{2\lambda C_\Psi^\lambda}
                \left(
                    K_\omega\Vert A\Vert
                    + K_\omega
                    + \frac{\sqrt{2\lambda C_\Psi^\lambda}}{\lambda}
                \right). 
            \end{align*}
            }
            At (1) we used \eqref{ineq:inn-loop-lin-cnvg-pitem1}. 
            At (2) we used \ref{prop:inn-loop-lin-cnvg:item2}. 
            To see \ref{prop:inn-loop-lin-cnvg:item4}, use Theorem \ref{thm:dlty-gap-inxt-pp} and the rest is just some algebra because $\gamma/(2\tau) \le 1$ from \ref{thm:lin-cnvg-ista-eb}. 
        \end{proof}
        \begin{remark}
            In practice, use the proximal operator of $\omega^\star$ to choose a feasible $v_0$, equivalently we can translate any primal feasible solution into a dual feasible solution using Theorem \ref{thm:primal-dual-trans}. 
            \par
            As a prelude, to have a global convergence rate it requires an upper bound of the initial optimally gap of $\Psi_\lambda$ of for all iterations of the outer loop. 
            Suppose for each outer iteration $k$, the inner loop is initialized to start on $v_0^{(k)}$, then there must be an upper bound for all $\Psi_\lambda\left(v_0^{(k)}\right) - \Psi_\lambda(\bar v)$. 
        \end{remark}
    \subsection{Concrete examples where we can have linear convergence}
        Continuing our discussion from the previous section, the goal of this section is to show that there exists specific type of $\omega$ that appears in applications and satisfies the error bound condition for the proximal problem $\Phi_\lambda$, and hence the results from the previous section are applicable and relevant to practical applications. 
        \par
        Our major results are stated in Proposition \ref{prop:1nrm-prox-problem}. 
        \begin{definition}[smooth quasi strongly convex {\cite[Definition 1]{necoara_linear_2019}}]\;\label{def:q-scnvx}\\
            Let $f:\RR^n \rightarrow \RR$ be convex, differentiable and, $L$ lipschitz smooth. 
            Let $X \subseteq \RR^n$ and suppose that the set of minimizers $X^+=\argmin_{x \in X} f(x) \neq \emptyset$ exists and, denote $f^+$ to be the minimum of $f$ on $X$. 
            It is smooth quasi strongly convex (SQSC) on the set $X \subseteq \RR^n$ if $\exists \kappa > 0$ such that $\forall x \in X$, let $\bar x = \Pi_{X^+} x$, it satisfies
            \begin{align*}
                0 &\le f^+ - f(x) - \langle \nabla f(x), \bar x - x\rangle 
                - \frac{\kappa}{2}\Vert x - \bar x\Vert^2 \quad \left(\forall x \in X\right). 
            \end{align*}
        \end{definition}
        \begin{definition}[quadratic growth condition]\;\label{def:q-grwth}\\
            Let $F = f + g$ where $f: \RR^n \rightarrow \RR$ be convex differentiable and, $g: \RR^n \rightarrow \overline \RR$ be closed convex and, proper. 
            Assume in addition that the set of minimizer $S := \argmin_{x\in \RR^n} F \neq \emptyset$, denote $F^+$ to be the minimum, then it satisfies the quadratic growth conditions if there exists $\kappa > 0$ such that
            \begin{align*}
                (\forall x \in \RR^n)\; F(x) \ge F^+ + \frac{\kappa}{2}\dist^2(x | S).
            \end{align*}
        \end{definition}
        Obviously, 
        The following theorem shows that under the standard framework of smooth nonsmooth additive compsitive objective function, the quadratic growth condition implies error bound.
        \begin{theorem}[quadratic growth implies error bound]\;\label{thm:qfg-means-eb}\\
            Suppose $(f, g, L)$ satisfies Assumption \ref{ass:for-inxt-pg-ineq}, let $F = f + g$. 
            In addition, let's assume that
            \begin{enumerate}[nosep, label=(\roman*)]
                \item the set of minimizers $S = \argmin_{x} F\neq \emptyset$, 
                \item the function $F$ satisfies quadratic growth (Definition \ref{def:q-grwth}) with some parameter $\kappa > 0$. 
            \end{enumerate}
            Then, by choosing any $\tau \ge L$, the function $F$ also satisfies error bound (Definition \ref{def:err-bnd}) with: 
            \begin{align*}
                \gamma = \frac{\tau\kappa}{\kappa + \tau + \sqrt{\tau(\kappa + \tau)}}. 
            \end{align*}
        \end{theorem}
        \begin{proof}
            Denote $x^+ = T_\tau(x)$, $\bar x = \Pi_{S}x$, and $\bar x ^+ = \Pi_{S} x^+$. 
            We use the exact proximal gradient inequality by applying Theorem \ref{thm:inxt-pg-ineq} with $\epsilon = 0, \rho = 0$ and, $z = \bar x^+$ (we can apply this because $\tau \ge L$) which gives: 
            \begin{align*}
                0 &\ge F(x^+) - F(\bar x^+) - \tau \langle \bar x^+ - x^+, x^+ - x\rangle - \frac{\tau}{2}\Vert x - x^+\Vert^2
                \\
                &\underset{(1)}{\ge} \frac{\kappa}{2}\Vert x^+ - \bar x^+\Vert^2
                - \Vert \tau(\bar x^+ - x^+)\Vert \Vert x^+ - x\Vert - \frac{1}{2\tau }\Vert \tau(x - x^+)\Vert^2
                \\
                &= \frac{\kappa}{2}\left\Vert
                    x^+ - \bar x^+
                \right\Vert^2 
                - \frac{\tau}{2}\left(
                    2\left\Vert
                        \bar x^+ - x^+
                    \right\Vert \left\Vert
                        x^+ - x
                    \right\Vert
                    + \Vert x - x^+\Vert^2
                \right)
                \\
                &\underset{(2)}{=}
                \frac{\kappa + \tau}{2}\left\Vert x^+ - \bar x^+ \right\Vert^2 
                - \frac{\tau}{2}(
                    \Vert \bar x^+ - x^+\Vert + \Vert x^+ - x\Vert
                )^2
                \\
                &\underset{(3)}{\ge}
                \frac{\kappa + \tau}{2}\left(
                    \Vert x - \bar x\Vert - \Vert x - x^+\Vert
                \right)^2
                - \frac{\tau}{2}(\Vert x - \bar x\Vert)^2
                \\
                \iff 
                0 &\ge
                \sqrt{\kappa + \tau}\left(
                    \Vert x - \bar x\Vert - \Vert x - x^+\Vert
                \right)
                - \sqrt{\tau}(\Vert x - \bar x\Vert)
                \\
                &= (\sqrt{\kappa + \tau} - \sqrt{\tau})\Vert x - \bar x \Vert
                - \sqrt{\kappa + \tau} \Vert x - x^+\Vert. 
            \end{align*}
            At (1), we used quadratic growth assumption and substitute the inequality of Definition \ref{def:q-grwth}, and the Cauchy inequality. 
            At (2), consider substituting $a = \bar x^+ - x^+, b = x^+ - x$, so the second terms with the parenthesis has inside $2\Vert a\Vert\Vert b\Vert + \Vert b\Vert^2 = (\Vert a\Vert + \Vert b\Vert)^2 - \Vert a\Vert^2$. 
            At (3), we used the properties of projecting onto the set set of minimizer $S$, which gives:
            \begin{align*}
                & \Vert x - \bar x\Vert 
                \underset{(4)}{\le}
                \Vert x - \bar x^+\Vert 
                \le
                 \Vert x - x^+\Vert + \Vert x^+ - \bar x^+\Vert
                \\
                \implies
                & \Vert x - \bar x\Vert - \Vert x - x^+\Vert
                \le \Vert x^+ - \bar x^+\Vert
            \end{align*}
            At (4) we used the fact that $\bar x = \Pi_S(x)$ is closer to $S$ than the point $\bar x^+$. 
            Finally, re-arranging the results it should yield the following inequality: 
            \begin{align*}
                \Vert \mathcal G_\tau(x)\Vert 
                &= \Vert \tau(x - x^+)\Vert 
                \\
                &\ge \tau\left(
                    \frac{\sqrt{\kappa + \tau} - \sqrt{\tau}}{\sqrt{\kappa + \tau}}
                \right)\Vert x - \bar x\Vert
                \\
                &= \frac{\tau\kappa}{\kappa + \tau + \sqrt{\tau(\kappa + \tau)}}
                \Vert x - \bar x\Vert. 
            \end{align*}
            The above is error bound conditions as defined in Definition \ref{def:err-bnd}, with $\gamma = \frac{\tau\kappa}{\kappa + \tau + \sqrt{\tau(\kappa + \tau)}}$. 
        \end{proof}
        \begin{remark}
            This theorem is not a new result, and the converse of the statement remainds true and is not hard to show. 
            We adapted claim and the proof here so the notations stays consistent and to make it self contained. 
        \end{remark}
        \begin{fact}[quasi strongly convex implies quadratic growth {\cite[Theorem 4]{necoara_linear_2019}}]\;\label{fact:qscnvx-q-growth}\\
            Let $f, X, \kappa$ be given by Definiton \ref{def:q-scnvx}. 
            Then the function $F = f + \delta_X$ satisfies Definition \ref{def:q-grwth} (quadratic growth) with the same $\kappa$.
        \end{fact}
        Paraphrased in Necoara et al. \cite{necoara_linear_2019} is the following classic fact on Hoffman error bound. 
        \begin{fact}[Hoffman error bound]\label{fact:hoffm-eb}
            Consider a nonempty polyhedral set $P = \{x \in \RR^n:Ax = b, Cx \le d\}$ defined via some $A \in \RR^{p \times n}, C \in \RR^{m\times n}, b \in \RR^m, d \in \RR^m$. 
            Then there exists a constant $\theta > 0$ which only depends on $A, C$ such that: 
            \begin{align*}
                (\forall x \in \RR^n) \quad \dist(x | P) 
                \le \theta \dist\left(
                    (Ax - b, Cx - d) \;|\; 
                    \{\mathbf 0\}\times \RR^m_-
                \right). 
            \end{align*}
        \end{fact}
        \begin{remark}
            In the literature, the exact, the smallest value of $\theta$ is a big topic. 
            It has something do with the angle and geometric between a cone, or an affine space of two convex sets. 
            Here, we will only state its existence without giving it a precise expression. 
        \end{remark}
        \begin{fact}[a type of smooth quasi strongly convex function {\cite[Theorem 8]{necoara_linear_2019}}]\;\label{fact:polyhedral-qscnvx-fxn}\\
            Consider any $C \in \RR^{m\times n}, d \in \RR^m$ such that it defines nonempty polyhedral set $X = \{x : Cx \le d\}$. 
            Let $h$ be $\sigma > 0$ strongly convex and $L_g$ Lipschitz smooth, consider $f = h\circ A$ where $A \in \RR^{p \times n}$. 
            Then
            \begin{enumerate}[nosep]
                \item The set of minimizer $X^+$ is nonempty and it's a polyhedral which its definition involves matrices $C, A$. 
                \item The function $f$ is quasi strongly convex with $\kappa = \sigma/\theta^2, L_f = L_h \Vert A\Vert^2$ where $\theta$ is the Hoffman constant as given by Fact \ref{fact:hoffm-eb} of the polyhedral set $X^+$.
            \end{enumerate}
        \end{fact}
        Now, we give our first major results of this section. 
        The proposition that follows characterizes a precise case where Assumption \ref{ass:lin-cnvg-for-pp} is true, and it's a case widely available in applications. 
        \begin{proposition}[1-norm problem has a linear convergence rate]\;\label{prop:1nrm-prox-problem}\\
            Let $(g, \omega, A)$ satisfies Assumption \ref{ass:for-inxt-prox}. 
            In addition, if $\omega = \Vert \cdot\Vert_1$, then the function $\Psi_\lambda, \Phi_\lambda$ as given by \eqref{eqn:primal-pp}, \eqref{eqn:dual-pp} satisfies Assumption \ref{ass:lin-cnvg-for-pp} and progressively it can be breakdown into: 
            \begin{enumerate}[nosep]
                \item\label{prop:1nrm-prox-problem:result1} 
                $\Psi$ is quasi strongly convex, with $\kappa =\lambda/\theta^2$ where $\theta$ is a Hoffman constant of polyhedral set $X^+ = \argmin_{x\in \RR^n}\Psi_\lambda$, and Lipschitz smoothness constant $L_\Psi = \lambda \Vert A^\top\Vert^2$. 
                \item\label{prop:1nrm-prox-problem:result2} 
                $\Psi$ satisfies quadratic growth condition (Definition \ref{def:q-grwth}), with the same constant $\kappa$ from its quasi strong convexity. 
                \item\label{prop:1nrm-prox-problem:result3} 
                $\Psi$ satisfies the error bound condition (Definition \ref{def:err-bnd}) for all step size $\tau \ge L_\Psi$. 
                And the error bound constant has 
                \begin{align*}
                    \gamma = \frac{\frac{\lambda}{\theta^2}}{\frac{\lambda}{\tau\theta^2} + 1 + \sqrt{\frac{\lambda}{\theta^2 \tau} + 1}}. 
                \end{align*}
                \item\label{prop:1nrm-prox-problem:result4} The primal, and dual statisfies Assumption \ref{ass:lin-cnvg-for-pp}, hence Proposition \ref{prop:inn-loop-lin-cnvg} applies and, a linear convergence rate is possible. 
            \end{enumerate}
            And they have the relation $(i) \implies (ii)\implies (iii)$. 
        \end{proposition}
        \begin{proof}
            We first show \ref{prop:1nrm-prox-problem:result1}. 
            Recall $\Psi_\lambda(v) = \frac{1}{2\lambda}\Vert \lambda A^\top v - y\Vert^2 + \omega^\star(v) - \frac{1}{2\lambda}\Vert y\Vert^2$ where $\omega^\star = \delta(z | \{x : \Vert x\Vert_1 \le 1\})$ so it's a quadratic function with a polyhedral constraint. 
            Observe that that $\Psi_\lambda(v) = h(A^\top v) + \delta_{X}$, and here it has $h = z \mapsto (\lambda/2)\left\Vert z - \lambda^{-1}y\right\Vert^2$ so $h$ is a $\lambda$ strongly convex and smooth function, and $X = \{x : \Vert x\Vert_1 \le 1\}$ which can be represented by a polyhedral set characterized by linear inequalities. 
            Therefore, Fact \ref{fact:polyhedral-qscnvx-fxn} applies, and $\Psi_\lambda$ is quasi strongly convex with $\kappa = \lambda/\theta^2, L_\Phi = \lambda\Vert A^\top\Vert^2$. 
            \par
            To see \ref{prop:1nrm-prox-problem:result2}, use Fact \ref{fact:qscnvx-q-growth}. 
            To see \ref{prop:1nrm-prox-problem:result3}, use Theorem \ref{thm:qfg-means-eb}. 
            To see \ref{prop:1nrm-prox-problem:result4}, we already have most of the results from the assumption and \ref{prop:1nrm-prox-problem:result3}, and it remains to verify that $\Phi_\lambda$ is Lipschitz continuous on $\dom(\omega \circ A)$ with is true because $\Phi_\lambda = \Vert Au\Vert_1 + \frac{1}{2\lambda}\Vert u - y\Vert^2$, it's the sum of a quadratic and a Lipschitz continuous function $\Vert Au\Vert_1$. 
        \end{proof}
        \begin{remark}
            It is very difficult to obtain a lower estimate for $\gamma$ in practice. 
            
        \end{remark}
        \begin{definition}[piecewise linear-quadratic function {\cite[Definition 10.20]{rockafellar_variational_1998}}]\;\\
            A function $f: \RR^n \rightarrow \overline \RR$ is piecewise linear-quadratic when $\dom f$ is the union of finitely many piecewise polyhedral set, such that on each polyhedral partition of $f$ there exists $\alpha \in \RR, a \in \RR^n$, $C\in \RR^{n \times n}$ as a symmetric matrix is where $f$ has the representation $x \rightarrow \langle x, Cx\rangle + \langle a, x\rangle + c$. 
        \end{definition}
        The following proposition characterizes another case where error bound conditions of problem holds. 
        \begin{example}[inner loop linear convergence rate examples]\label{example:inn-lp-lin-cnvg}
            
        \end{example}



\section{Total complexity of the algorithm}
    This section puts results regarding the total complexity of the proposed inexact proximal gradient algorithm. 
    \subsection{Inner loop complexity}
        We remind that the parameters in the inner loop will change depending on the outer loop's iteration. 
        To discuss the convergence we unfortunately have to re-introduce the proximal problem in the context of the accelerated proximal gradient method of the outer loop. 
        \par
        Let $(F, f, g, L)$ and sequence $(\alpha_k, B_k, \rho_k,\epsilon_k)_{k \ge0}$ satisfies Definition \ref{def:inxt-apg}. 
        Fix any $k \ge 0$ to be the iteration counter of the outer loop. 
        Let $(g, \omega, A)$ satisfies Assumption \ref{ass:for-inxt-prox}. 
        The inner loop is an algorithm that solves the inexact proximal gradient problem $x \approx_{\epsilon_k} T_{B_k + \rho_k}(y_k)$ which is equivalent to evaluating: 
        \begin{align*}
            x_k \approx_{\epsilon_k} \hprox_{L_k^{-1} g}(y_k - L_k^{-1}\nabla f(y_k)).
        \end{align*}
        Where, $L_k := B_k + \rho_k$. 
        Let $\lambda^{(k)} := L_k^{-1}, \tilde y_k := y_k - L_k^{-1}\nabla f(y_k)$, the proximal problem boils down to optimizing the following function $\Phi_\lambda^{(k)}$ as defined by: 
        \begin{align}\label{eqn:primal-pp-k}
            \Phi_\lambda^{(k)}(u) &:= \frac{1}{2\lambda^{(k)}} \Vert u - \tilde y_k \Vert^2 + \omega(A u). 
        \end{align}
        And its dual which is 
        \begin{align}\label{eqn:dual-pp-k}
            \Psi_\lambda^{(k)}(v) &:=
            \frac{1}{2\lambda^{(k)}}
            \left\Vert \lambda^{(k)}A^\top v - \tilde y_k
            \right\Vert^2
            + \omega^\star(v) 
            - \frac{1}{2\lambda^{(k)}}\Vert \tilde y_k\Vert^2. 
        \end{align}
        The primal dual gap is $\mathbf G_\lambda^{(k)}(u, v) = \Phi_\lambda^{(k)}(u) + \Psi_\lambda^{(k)}(v)$. 
        The above are identical to proximal problem defined in Section \ref{sec:optz-inxt-pp-problem} except for the introduction of iteration counter $k$ from the outer loop, and we had specified $\tilde y_k$ in relation to the outer loop. 
        Finally, the inner loop algorithm is responsible for optimizing the optimality gap, it satisfies that $\mathbf G_\lambda^{(k)}(u, v) \le \epsilon_k$ and all results from Section \ref{sec:conds-lin-cnvg-pp} applies. 
        \par
        The following assumption specifies conditions where the complexity of the inner using ISTA can be globally bounded for all iteration of the accelerated proximal gradient algorithm of the outer loop independent of the initial guess $v_0^{(k)}$, $\lambda^{(k)}$. 
        \begin{assumption}[globally bounded inner loop complexity]\;\label{ass:inn-cmplx}\\
            For any integer $k \ge 0$, this assumption is about parameters of the proximal problem $\left(g, \omega, A, \tilde y_k, \Phi_\lambda^{(k)}, \Psi_\lambda^{(k)}, L^{\lambda, (k)}_{\Phi} ,\gamma_\lambda^{(k)}\right)$ introduced at the beginning of this section, iterates $\left(z_j^{(k)}, v_j^{(k)}\right)_{j\ge0}$, the primal dual solutions $\left(\bar z^{(k)}, \bar v^{(k)}\right)$ and additional constants $\lambda^{\min}, \lambda^{\max}, \gamma^{\min}$. 
            The assumptions now follow. 
            \begin{enumerate}[nosep]
                \item\label{ass:inn-cmplx:item1} As specified in this section, the parameter $\lambda^{(k)} = (B_k + \rho_k)^{-1}$ are from the outer loop. We denote $L_k = B_k + \rho_k$ for short. We assume in addition, there exists $\lambda^{\min}, \lambda^{\max}$ such that: 
                $$
                    - \infty < \lambda^{\min} \le \inf_{k \in \N \cup \{0\}} \lambda^{(k)} \le \sup_{k \in \N \cup \{0\}} \lambda^{(k)} \le \lambda^{\max} < \infty. 
                $$ 
                \item\label{ass:inn-cmplx:item2} There exists the smallest error bound constant $\gamma^{\min}$ such that it lower bound all the constants for the proximal problem, i.e: $\exists \gamma_{\min}:\;0 < \gamma^{\min} \le \inf_{k \in \N \cup \{0\}} \gamma^{(k)}_\lambda$. 
                \item\label{ass:inn-cmplx:item3} $\tilde y_k, \Phi_\lambda^{(k)}, \Psi_\lambda^{(k)}, L^{\lambda, (k)}_{\Phi} ,\gamma_\lambda^{(k)}$ all satisfies error bound conditions (Assumption \ref{ass:lin-cnvg-for-pp}) with $y = \tilde y_k, \Phi_\lambda = \Phi_\lambda^{(k)}, \Psi_\lambda = \Psi_\lambda^{(k)}, L_\Phi^\lambda = L^{\lambda, (k)}_{\Phi}$ and $\gamma_\lambda = \gamma_\lambda^{(k)}$. 
                \item\label{ass:inn-cmplx:item4} The iterates of the inner loop $\left(z_j^{(k)}, v_j^{(k)}\right)_{k \ge 0}$ are produced by an ISTA algorithm satisfying Definition \ref{def:ista-inner-lp}. We choose the same stepsize $\tau = 1/\Vert A^T A\Vert$ for all iteration of ISTA. 
            \end{enumerate}
        \end{assumption}
        \par
        Here, we discuss shorter which assumption are easy to satisfies, and which are more demanding. 
        Item \ref{ass:inn-cmplx:item1} is easy to satisfies since popular line search and backtracking method ensures that $B_k$ will be bounded above and blow, and $\rho_k$ is entirely to the choice of the practitioner so any bounded sequence works. 
        Item \ref{ass:inn-cmplx:item2} requires some extra argument but one example of $\omega$ remains feasible. 
        Observe that it is trivially true if the domain of $\Psi_\lambda^{(k)}$ is always bounded because initial guess $v_0^{(k)} \in \text{dom}\Psi_\lambda^{(k)}$, which is equivalent to $\omega^\star$ has a bounded domain which can be satisfied if $\omega = \Vert \cdot\Vert_1$.
        In addition, the results of Proposition \ref{prop:1nrm-prox-problem}\ref{prop:1nrm-prox-problem:result3}, \ref{prop:1nrm-prox-problem}\ref{prop:1nrm-prox-problem:result4}, item \ref{ass:inn-cmplx:item3} can be satisfied because $\gamma^{(k)}_\lambda$ only depends $\lambda^{(k)}$ it would be bounded from \ref{ass:inn-cmplx:item1}. 
        \todoinline{
            N'obliez pas se faire ici. 
        }
        \par
        The proposition that follows characterize the bare minimum requirements so that the inner loop's complexity depends only on required accuracies $\epsilon$, and parameter $\lambda$ for all initial guess of the outer loop. 
        \begin{proposition}[inner loop complexity can be bounded globally]\;\label{prop:inner-lp-cmplx}\\
            Let $\left(g, \omega, A, \tilde y_k, \Phi_\lambda^{(k)}, \Psi_\lambda^{(k)}, L^{\lambda, (k)}_{\Phi} ,\gamma_\lambda^{(k)}\right)$, $\left(z_j^{(k)}, v_{j}^{(k)}\right)_{j \ge 0}$, $\left(\bar z^{(k)}, \bar v^{(k)}\right)$ and additional parameters $\lambda^{\min}, \lambda^{\max}, \gamma^{\min}, C_0$ such that they satisfy Assumption \ref{ass:inn-cmplx}. 
            Then, the total number of inner loop iteration sufficient to achieve $\mathbf G_\lambda^{(k)}\left(z_j^{(k)}, v_j^{(k)}\right) \le \epsilon_k$, at each iteration $k$ denoted by $J^{(k)}_\epsilon$ can be upper bounded by: 

        \end{proposition}
        \begin{proof}
            To prove the above theorem, we use the results from Proposition \ref{prop:inn-loop-lin-cnvg} into the context of the iterations in the outer loop. 
           
        \end{proof}
    \subsection{Overall complexity}
        We derive the overall complexity in this section. 
        Before we start, it's necessary to discuss the best initial guess vector $v^{(k)}_0$ of the inner loop so that item \ref{ass:inn-cmplx:item2} in Assumption \ref{ass:inn-cmplx} can be satisfied for as many objective functions as possible. 

\appendix
    \section{Super boring chores}
        Super boring stuff that I just want to skip but somehow necessary. 
        \begin{lemma}[That conjugate for the dual of proximal problem]\;\label{lemma:chore1}\\
            Let $f:\RR^n \rightarrow \overline \RR:= \frac{1}{2\lambda}\Vert u - v\Vert^2$, then its conjugate is given by
            \begin{align*}
                f^\star(v) = \frac{1}{2\lambda}\Vert \lambda v + y\Vert^2 - \frac{1}{2\lambda}\Vert y\Vert^2. 
            \end{align*}
        \end{lemma}
        \begin{proof}
            Recall the following properties for any closed, proper convex function $f: \RR^n \rightarrow \overline \RR$. 
            Let $a \in \RR^n$ be any vector, let $\alpha \in \RR, c \in\RR$ be some scalar, then we have these three properties of conjugating convex funtion. 
            \begin{enumerate}[nosep]
                \item $(\alpha f)^\star = \alpha f^\star \circ (\alpha^{-1}I)$. 
                \item $(f + c)^\star(y) = f^\star(y) - c$. 
                \item $\left(x \mapsto f(x) + \langle x, y\rangle\right)^\star(y) = f^\star(y - a)$. 
            \end{enumerate}
            From here we have: 
            \begin{align*}
                f^\star(v) 
                &= 
                \left(
                   u \mapsto \lambda^{-1}\left(\frac{1}{2}\Vert u\Vert^2 - \langle u, y\rangle\right) 
                    + \frac{1}{2\lambda} \Vert y\Vert^2
                \right)^\star(v)
                \\
                &= \left(
                    u \mapsto \lambda^{-1}\left(\frac{1}{2}\Vert u\Vert^2 - \langle u, y\rangle\right) 
                \right)^\star(v)
                - \frac{1}{2\lambda} \Vert y\Vert^2
                \\
                &= 
                \left[
                    \lambda^{-1}\left(
                    u \mapsto \left(\frac{1}{2}\Vert u\Vert^2 - \langle u, y\rangle\right) 
                \right)^\star\circ(\lambda I)
                \right](v)
                - \frac{1}{2\lambda} \Vert y\Vert^2
                \\
                &= \left[
                    \lambda^{-1}\left(
                    u \mapsto \left(
                        \frac{\Vert \cdot\Vert^2}{2}
                    \right)^\star(u + y)
                \right)\circ(\lambda I)
                \right](v)
                - \frac{1}{2\lambda} \Vert y\Vert^2
                \\
                &= \left[
                    \lambda^{-1}\left(
                        u \mapsto \frac{\Vert u + y\Vert^2}{2}
                    \right)\circ(\lambda I)
                \right](v)
                - \frac{1}{2\lambda} \Vert y\Vert^2
                \\
                &= \lambda^{-1}\left(
                    \frac{1}{2}\Vert \lambda v + y\Vert^2
                \right) 
                - \frac{1}{2\lambda} \Vert y\Vert^2. 
            \end{align*}
        \end{proof}
        \begin{lemma}[Lipschitz constant of convex function]\;\label{lemma:lipz-cnvx-fxn}
            Let $f: \RR^n \rightarrow \overline \RR$ be a closed, convex, proper function. 
            Let $\partial f$ be its convex subgradient. 
            Then, 
            \begin{enumerate}[nosep]
                \item for all $x\in \RR^n, y\in \RR^n$ it has: $|f(x) - f(y)| \le \sup_{x\in \dom \partial f}\ (\partial f(x) \;|\; \mathbf 0)\Vert y - x\Vert$. 
                \item If in addition, the function is $K_f$ Lipschitz continuous globally on $\RR^n$, then: $(\forall y \in \RR^n)(\forall v_y \in \partial f(y)):\; K_f \ge \Vert v_y\Vert$. 
            \end{enumerate}
        \end{lemma}
        \begin{proof}
            The proof is direct. 
            Let $x, y \in \RR^n$ be arbitrary.
            Choose $v_x \in \partial f(y)$ and $v_y \in \partial f(y)$ such that $\Vert v_x\Vert = \dist (\partial f(x) | \mathbf 0), \Vert v_y\Vert = \dist (\partial f(y) | y)$ which is possible because $\partial f(x)$ is closed for all $x \in \dom \partial f$. 
            Therefore, we have the following. 
            \begin{align*}
                |f(x) - f(y) | &\le 
                \max(f(x) - f(y), f(y) - f(x))
                \\
                &\underset{(1)}\le \max(-\langle v_x, y - x\rangle, - \langle v_y, x - y\rangle)
                \\
                &\le \max(\Vert v_x\Vert, \Vert v_y\Vert)\Vert y - x\Vert
                \\
                &\le \left(
                    \sup_{x \in \dom \partial f} \dist(\partial f(x) \;|\; \mathbf 0)
                \right)\Vert y - x\Vert. 
            \end{align*}
            At (1), we used the fact that $f(x) - f(y) \le - \langle v_x, y - x\rangle$ and, $f(y) - f(x) \le - \langle v_y, x - y\rangle$ by convex subgradient inequality. 
            \par
            For the second results it's direct, choose any $y \in \dom f = \RR^n, v_y \in \partial f(y)$, then there exists some $x \in \RR^n$: 
            \begin{align*}
                0&\le \frac{f(x) - f(y)}{\Vert x - y\Vert} 
                - \frac{\langle v_y, x - y\rangle}{\Vert x - y\Vert}
                \\
                &\le 
                K_f - \left\langle 
                    v_y, \frac{x - y}{\Vert x - y\Vert}
                \right\rangle
                \\
                &= K_f - \Vert v_y\Vert. 
            \end{align*}
            AT (1), consider chosing any $x$ such that $\frac{x - y}{\Vert x - y\Vert} = \frac{v_y}{\Vert v_y\Vert}$, this is possible because the domain of $f$ is $\RR^n$. 
        \end{proof}
        
\bibliographystyle{siam}
\bibliography{references/refs.bib}
\end{document}