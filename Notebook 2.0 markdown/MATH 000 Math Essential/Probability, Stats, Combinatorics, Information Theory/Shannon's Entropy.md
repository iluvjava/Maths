We need this for machine learning. 
[Expectations of Random Variables without Sigma Algebra](Expectations%20of%20Random%20Variables%20without%20Sigma%20Algebra.md). Here is the wiki [here](https://www.wikiwand.com/en/Entropy_(information_theory))

---
### **Intro**

The Shannon's Entropy is a measure on the amount of bits needed to represents the information from a given probability distribution of a random variable. 

Remember this: 

> A rare events convey more information than a common event. 

Given a discrete random variable $X$, taking $n$ possible outcomes with nonzero possibility, the entropy is measured by: 

$$
\begin{aligned}
    H(X) &= \sum_{i = 1}^{n}
        \mathbb{P}\left(X = i\right)\log\left(
            \frac{1}{\mathbb{P}\left(X = i\right)}
        \right)
    \\
    H(X) &= \mathbb{E}_{x\sim P}\left[
            \log_2\left(
                \frac{1}{P(x)}
            \right)
        \right]
\end{aligned}
$$

And one can extend it to also define the conditional entropy of something, let $X, Y$ be 2 random variable jointly generated by the PDF $p(x, y)$, then: 

$$
\begin{aligned}
    H(X|Y) &= - \sum_{i,j}^{}
        p(x_i, y_j) \log\left(
            \frac{p(x_i, y_j)}{p(y_j)}
        \right)
    \\
    H(X, Y) &= 
    \sum_{i,j}^{}
        p(x_i, y_j)\log\left(
            \frac{1}{p(x_i, y_j)}
        \right)
\end{aligned}
$$

Given $Y$, how random is the variable $X$. 

**Remarks**

The continuous version of the entropy exists and it defined by a continuous random variables, which could also give negative values sometimes. see the this [wiki](https://en.wikipedia.org/wiki/Differential_entropy) for more info *differential entropy*. 

**FAQ:**

> What is this a measure of? 

The measures the expected value of "surprises". 

> Why is there a log? 

Shannon's idea is that, for a set of $N$ elements, one would need another set of symbols of $\log_2(N)$ to represents all the element, without loss, without compression. 

> What is information? 

Information is: 

$$
I(x) = -\log_2(f(x))
$$

**Observations**

1. The natural log of $\frac{1}{p(x)}$ is the element of "surprise".
2. If all outcomes are equally likely to happen, the entropy is maximized, this gives us a distributions that is the least exciting. 
3. Entropy is a non-negative value, probability of an even from a finite set means that it's less than one, then $-\log$ of that will only make it positive. 

---
### **Legit Interpretations**

What is "Surprise"? 

**Claim**: 

> Information is the reciprocal of the probability. Equivalently, the amount of reduced uncertainly is the inverse of the probability.  

Consider outcomes with 2 cases: $1, 0$, with probability of $\frac{1}{4}, \frac{3}{4}$. Each event can be measured by the common factor, in this case it's $\frac{1}{4}$. If $1$ is true, then by knowing $\frac{1}{4}$, we obtained that $\frac{3}{4}$ didn't happen, a total of 1. The ratio between everything we know and the outcome that happened is $(\frac{1}{4} + \frac{3}{4})\div \frac{1}{4}$, which is $4$. 


**Claim:** 

> The entropy is a measure on the the best average length of the message needed to encode all the possible outcomes given a distribution. 

Remember from huffman encoding that, we can encode all the symbols from a set using its frequencies appeared in a distribution. In a greedy way where the symbols that appear more often get a shorter message to encode it, and for a message that appears less, we give a longer length. 

- "the length of the message" is $\log_2$. 
- "for each of the outcome is": $-\log(f(x))$.  

Suppose that 3 messages need to be send, with a distribution of $\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8}$. Then the message will be: $0, 10, 110, 111$ for each of these outcome. Take note that, if I put them into the $\log_2\left(\frac{1}{f(x)}\right)$ then we actually had the right thing. 

On this case, the measure of $\log(f(x)^{-1})$ gives us the depth of we need to go on the huffman tree before we getting to the code for that particular given message. 

---
### **Properties of the Shannon's Entropy**




---
### **Decision Tree and Information Gain**

One of the major applications for the entropy measure is the information gain when branching a [Decision Tree](../../AMATH%20582%20Data%20Science/Decision%20Tree.md) learning model. 

> When breaching, we choose a splitting criteria such that the information gain is maximized. The information gain over a splitting criterion is measured as the entropy of the parent node minus the average entropy on the children nodes. 

$$
\text{IG}(k) = H(X) - \frac{H(X|X > k) + H(X| X< k)}{2}
$$

And this is how information gain is relevant to Shannon's entropy. 


