[Lanczos Iteration](../AMATH%20584%20Numerical%20Linear%20Algebra/Hessenberg%20Form/Lanczos%20Iteration.md)
[Ritz Vectors from Lanczos](Ritz%20Vectors%20from%20Lanczos.md)


----
### **Intro**

The orthogonal vectors generated by the Lanczos Algorithm loses orthogonality systematically, proposed by Paige at first, and we are interested in the theory behind the orthogonality of the Lanczos Vector. The proof of the theorem is largely in reference to the Book: \<Applied Numerical Linear Algebra\> by James W.Demmel, in chapter 7. To state the theorem, define the following quantities: 

**Basic Quantities**: 

$$
\begin{aligned}
    T_k &:: \text{Tridiagonal at step k of Lanczos}
    \\
    Q_k &:: \text{Orthogonal matrix at step k of Lanczos}
    \\
    V_k = [v_1\;  v_2\; \cdots \; v_k] &:: \text{Eigen Matrix for } T_k
    \\
    \theta_i &:: \text{the eigenvalues for }v_i, \text{ Ritz Value}
    \\
    \Lambda_k &:: \text{Eigenvalues Matrix for }T_k
    \\
    F &::\text{The floats error matrix from Lanczos Factorizations}
    \\
    \epsilon &:: \text{The machine Epsilon}
\end{aligned}
$$

$\beta_k, \alpha_k$ are the coefficients inside of the Lanczos Tridiaognal Matrix. $Q_k^T T_k Q_k$ will approximate the matrix $A$. Consider the Exact Lanczos Recurrence: 

$$
\begin{aligned}
    AQ_k &= Q_kT_k + \beta_kq_{k + 1}\xi_k^Tv_i^{(k)}
    \\
    AQ_k v_i^{(k)} &= Q_k\theta_iv_i^{(k)} + \beta_k q_{k + 1}(v_i^{(k)})_k
    \\
    \text{Let: } Q_k v_i^{(k)} &= y_i^{(k)}
    \\
    Ay_i^{(k)} &= \theta_i y_{i}^{(k)} + \beta_k q_{k + 1}(v_i^{(k)})_k
\end{aligned}
$$

When, $q_{k + 1}$, representing the case when the Krylov subspace is now invariant, the ritz vector $v_i$ is the eigenvector of the matrix $A$. However, our interest is in about Floating Point Arithmetic. 


----
### **Paige's Theorem**

> $$
> \begin{aligned}
>     (y_i^{(k)})^T q_{k + 1} &= \frac{\mathcal{O}(\epsilon \Vert A\Vert)}
>     {
>         \beta_k |(v_i)_k|
>     }
> \end{aligned}
> $$


The projection of the new Lanczos Vector onto the ith ritz vector is inversely proportional to $\beta_k (v_i)_k$, where the multiplier is determined by $\mathcal{O}(\epsilon \Vert A\Vert)$. 


**Proof**

For simplicity, we ignore all the subscript goes under $Q_k, q_{k + 1}, T_k, F_k, v_i^{(k)}, y_i^{(k)}$. Starting with the Lanczos recurrences under floating point arithmetic: 

$$
\begin{aligned}
    AQ &= AT + \beta q\xi_k^T + F
    \\
    Q^TAQ &= Q^TQT + Q^T\beta q\xi_k^T + Q^TF
    \\
    Q^TAQ &= (Q^TQT + Q^T\beta q\xi_k^T + Q^TF)^T
    \\
    &= T^TQ^TQ  + \beta\xi_k q^T Q + F^TQ
\end{aligned} \tag{1}
$$


The third line is obtained by the fact that $Q^TAQ$ is symmetric. Takes the difference between the second and the third line in (1), we have: 

$$
\mathbf{0} = (Q^TQT - T^TQ^TQ) + \beta(Q^Tq\xi_k^T - \xi_k q^TQ) + (Q^TF - F^TQ)\tag{3}
$$


Here, we make the approximation that $\langle q_i, q_j\rangle = 0$ when $|i - j| \le 2$ throughout the rest of the derivation, in theory, it should be $\mathcal{O}(\epsilon)$, but we ignore error of orthgonalizing the vector $q_{j + 1}$ against the vector $q_j, q_{j - 1}$ because it's small enough and doesn't change the final result.  

As a result we obtained the factorization $Q^TQ = I + C^T + C$ where the matrix $C$ is lower triangular with diagonal and sub-diagonals being all zeros, representing the Lanczos vectors losing orthogonality. Which means that $C^T + C$ is a matrix with zeros on the tridiagonal parts and all other entries are the floating point errors from Lanczos. 

Simplifying expression (3): 

$$
\begin{aligned}
    Q^TQT - T^TQ^TQ &= (I + C^T + C)T -T^T (I + C^T + C)
    \\
    &= T + C^TT + CT - T^T - T^TC^T - T^TC
    \\
    &= (CT - TC) + (C^TT - TC^T)
\end{aligned}\tag{5}
$$

**Reader Please Observe**

$CT$ is strictly lower triangular, $TC$ is strictly lower triangular as well. This is true because a tridiagonal matrix only encodes interations between adjacent rows/columns, and $C$ has zeros on it's tridiagonal parts. ~~therefore, $TC, CT$ gives a matrix that is strictly lower diagonal. Therefore, expression (5) can be partitioned into strictly lower and upper triangular matrices.~~

**Reader Please Consider**

Consider $\xi q^TQ$ after adding back the subscript we have: $\xi_k q_{k + 1}^TQ_k$, observe that $\xi_k q_{k + 1}^T$ is a matrix whose last row is $q_{k + 1}^T$. And because of the way that $q_{k + 1}$ is orthogonalized, the last 2 elements of the last row of $\xi_k q_{k + 1}Q_k$ is zero, which is strictly lower triangular. <u>Hence it's contributing to the strictly lower triangular parts of the equation</u>.

**Reader Please Snap out and re-consider**


$$
\begin{aligned}
    \mathbf{0} &= (Q^TQT - T^TQ^TQ) + \beta(Q^Tq\xi_k^T - \xi_k q^TQ) + (Q^TF - F^TQ)
    \\
    \mathbf{0}&= 
    -\underbrace{(CT - TC)}_{\text{strict tril}} + \underbrace{(C^TT - TC^T)}_{\text{strict triu}} + \beta(\underbrace{Q^Tq\xi_k^T}_{\text{strict triu}} - \underbrace{\xi_k q^TQ}_{\text{strict tril}}) - (Q^TF - F^TQ)
    \\\implies
    \mathbf{0} &= (CT - TC) - \beta \xi_k q^TQ + \underbrace{\text{tril}(Q^TF - F^TQ)}_{=:L}
\end{aligned}\tag{6}
$$

From the first to second line, we made substitution from (5). From the second line to the third line, we take $\text{triu}$ on both side of the equation. Now consider multiplying by $v^T(\bullet)v$ for each terms in the expression and we have: 

$$
\begin{aligned}
    v^T(CT - TC)v &= v^TCTv - v^TTCv
    \\
    &= 
    v^TC\theta v - \theta v^TCv
    \\
    &= 0
    \\
    \implies 
    v^T\beta\xi_k q^TQv &= v^TLv
    \\
    (\beta(v)_k)(q^TQv) &= v^T Lv
    \\
    \beta(v)_kq^Ty &= v^TLv
\end{aligned}\tag{7}
$$

Observe that: 

$$
\begin{aligned}
    |v^TLv| \le \Vert L\Vert &= \mathcal{O}(\Vert Q^TF - F^TQ\Vert)
    \\
    \mathcal{O}(\Vert F\Vert) &= \mathcal{O}(\epsilon \Vert A\Vert)
\end{aligned}\tag{8}
$$


I don't know what references support the argument above in (7), but it feels right. (**#CITATION_NEEDED**) 

$$
\begin{aligned}
    \beta (v)_k &= \frac{\mathcal{O}(\epsilon \Vert A\Vert)}{\beta(v)_k}
    \\
    (Q_kv_i^{(k)})^Tq_{k + 1} &= \frac{\mathcal{O}(\epsilon \Vert A\Vert)}{\beta(v_i)k}
    \\
    (y_i^{(k)})^Tq_{k + 1} &= \frac{\mathcal{O}(\epsilon \Vert A\Vert)}{\beta(v_i)k}
\end{aligned}\tag{9}
$$

Which completes the proof for the claim. 

---
### **Significant of the Theorem**

It's later, experimented in the papers for the selective orthogonalizations of the Lanczos Algorithm that we see the projection onto the Ritz Vector marks the lost of orthogonality of Lanczos Vectors. 

The Lanczos algorithm forgets that it explored into the direction of well-converged ritz vectors, regenerating $q$ into the same directions, causing $q$ to lose orthogonality. A paper titled \<The Lanczos Algorithm With Selective Orthogonalization\> compact the issues with selective re-orthogonalization and various other tricks to keep track of both, the lost of orthogonality of lanczos vectors, and the converged ritz vectors. 

This is also significant because it uses converged ritz vector to improve stability of the Lanczos. 

---
### **Comments**

[Cauchy Interlace Theorem](../AMATH%20584%20Numerical%20Linear%20Algebra/Matrix%20Theory/Cauchy%20Interlace%20Theorem.md), stated that the eigenvalues of $T_{k}$  are strictly in between $T_{k + 1}$, which implies that, the eigenvalues found for the $T_{k}$ matrix will be changing monotonically, they will have to start somewhere in the middle of the spectrum, and then gradually move to the exterior of the spectrums. 