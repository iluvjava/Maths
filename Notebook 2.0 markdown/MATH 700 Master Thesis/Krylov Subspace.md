Krylov Subspace is used in a lot of places: 
* Eigenvectors related stuff
* Solutions for the system. 

[Minimal Polynomial](Minimal%20Polynomial.md)

---
### **Intro**

This is the Krylov Subspace initialized by $b$: 

$$
\mathcal{K}_k(A|b) = \text{span}( b, Ab, A^2b, \cdots A^{k - 1}b)
$$

**Claim 0:**
> $$
> \forall v: \mathcal{K}_1(A|v)  \subseteq  \mathcal{K}_2(A|v)  \subseteq \mathcal{K}_3(A|v)  \cdots 
> $$

It's trivial to justify claim 0. 


**Claim 1**

> A matrix satisfies it's own characteristic equation 
> $$p(A) = \mathbf{0}$$ 
> The polynomial outputs a matrix. This is called the Cayley-Hamilton Theorem. 

**Full proof is not given, but this is true for diagonalizable matrices.** 

Let the eigen decomposition(Jordan Decomposition) for matrix $A$ to be $X\Lambda X^{-1}$

$$
\begin{aligned}
    p(A) &= \sum_{i=0}^{n}c_iA^i
    \\
    p(A) &= \sum_{i=0}^{n}c_i(X\Lambda X^{-1})^i
    \\
    p(A) &= \sum_{i=0}^{n}c_iX\Lambda^iX^{-1}
    \\
    p(A) &= X \left(\sum_{i = 0}^{n}
        c_i\Lambda^{i}
    \right)X^{-1}
\end{aligned}
$$

Consider $\Lambda$ to be the Jordan Decomposition matrix for the matrix, when it's diagonalizable, $p(\lambda_i) = 0$, and from above expression, putting $\Lambda$ into the characteristic polynomial will set it equals to zero. **Note**: $c_0 = (-1)^n|A|$; the constant term for the characteristic polynomial, this quantity tells us whether the matrix is going to be invertible or not. 


**Consequence of Cayley Hamilton**

$$
\begin{aligned}
    \mathbf{0} &= 
        c_0I_n + c_1A + c_2A^2 \cdots c_nA^n
    \\
    -c_0I_n &=  
    c_1A + c_2A^2 \cdots c_nA^n
    \\
    -I_n &= \frac{c_1}{c_0}A + \frac{c_2}{c_0}A^2 \cdots \frac{c_n}{c_0}A^n
    \\\implies
    -A^{- 1} &= \frac{c_1}{c_0}I_n + \frac{c_2}{c_0}A \cdots \frac{c_n}{c_0}A^{n - 1}
    \\\implies
    -b &= \frac{c_1}{c_0}Ab + \frac{c_2}{c_0}A^2b \cdots \frac{c_n}{c_0}A^nb
\end{aligned}
$$

The inverse of $A$ can be expressed with the matrix polynomials of max degree $n$, using the coefficients from the characteristic polynomials. This implies that for any vector $b$, there exists $v$ such that $\mathcal K_{k}(A|v)$ such that the vector is in that spanned subspace. If we know that $A$ is invertible. This is known as the: **Cayleys Hamilton's Theorem**.

---
### **Reighly Quotient and Invariant Subspace**
 
# TODO: Not sure if it's important enough. 

---
### **Properties of Krylov Subspace**

**Claim 1: Existence of Matrix Polynomial**

> Every element inside of krylov subspace generated by matrix $A$, and an initial veoctr $v$ can be represented as a polynomial of matrix $A$ multiplied by the vector $v$ and vice versa. 
> $$
> \begin{aligned}
>     & \forall x \in \mathcal K_k(A|v) \;\exists\; w: p_k(A|w)v = x
> \end{aligned}
> $$

We use $p_k(A|w)$ to denotes a matrix polynomial with coefficients $w$, where $w$ is a vector. No proof this is trivial. Take note that, we can change the field of where the scalar $w$ is coming from, but for discussion below, $\mathbb R, \mathbb C$  doesn't matter and won't change the results. 

$$
p_k(A|w)v = \sum_{j = 0}^{k - 1}w_jA^jv
$$

The most important porperty of the subspace is the idea of grade denoted as $\text{grade}(A|v)$, indicating when the Krylov Subspace of $A$ wrt to $v$ *becomes invariant* when the grade of the subspace is reached and it kept its invariance for all subsequent subspaces. To show this idea, we consider the following 3 statements about Krylov Subspace which we will proceed to prove. 


**Statement (1)**: 
> $$\exists 1 \le k \le m + 1: \mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v)$$
> There exists an natural number between $1$ and $m+ 1$ such that, the successive krylov subspace span the same space asthe previous one. 


**Statement (2)**: 
> $$
>     \exists ! k \text{ s.t: }\mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v) \implies 
>     \mathcal K_k(A|v) \text{ is Lin Ind} \wedge \mathcal K_{k + 1}(A|v) \text{ is Lin Dep}. 
> $$
> There eixsts uniquely such $k$ where the immediate next krylov subspace is linear dependent.  


**Statement (3)**: 
> $$
> \mathcal K_k(A|v) \text{ Lin Dep} \implies \mathcal K_{k + 1}(A|v) = \mathcal K_k(A|v)$$
> if the $k$ krylov subspace is linear dependent, then it stops expanding and the successive krylov subspace spans the same space. 

**Theorem**: Existence of The Grade of the Subspace
> Let $k$ be the minumum number when the krylov subspace stops expanding, then all successive krylov subspace spand the same space. $\mathcal K_k(A|v) = \mathcal K_{k + j}(A|v) \;\forall j \ge 0$. The number $k$ is regarded as the grade of krylov subspace wrt to v denoted using $\text{grade}(A|v)$. 

**Proof of Statement (1)**

For notational simplicity, $\mathcal K_k$ now denotes $\mathcal K_k(A|v)$. Let's start the considerations from the definition of the Krylov Subspace: 

$$
\begin{aligned}
    \forall\; k: \mathcal K_k \subseteq \mathcal K_{k + 1}\implies \text{dim}(\mathcal K_{k})\le \text{dim}(\mathcal K_{k + 1})
    \\
    \mathcal K_{k + 1}\setminus \mathcal K_k = \text{span}(A^{k}v) 
    \\
    \implies \dim(\mathcal K_{k + 1}) - \dim(\mathcal K_k) \le 1
\end{aligned}
$$

Therefore, the dimension of the successive krylov subspace forms a sequence of positive integer that is monotonically increasing. By the Cayley's Hamilton's theorem, the sequence is bounded by $m$, since there are $m + 1$ terms, it must be the case that at least 2 of the krylov subspace has the same dimension (And the earliest such occurance will exist), implying the the fact that the new added vector from $k$ to $k + 1$ is in the span of the previous subspace. 

**Proof of Statement (3)**

The direction $\mathcal K_k(A|v) \subseteq \mathcal K_k(A|v)$ is trivial. Assuming that $\mathcal K_{k + 1}(A|v)$ is linear dependence, we wish to prove that $\mathcal K_{k + 1}(A|v)\subseteq \mathcal K(A|v)$. 

Consider: 

$$
\begin{aligned}
    & \mathcal K_{k + 1}(A|v) \text{ is Lin dependent} 
    \\
    & \implies \exists w^{(k)}: A^kv  = p_k(A|w^{(k)})v
    \\
    & x \in \mathcal K_{k + 1}(A|v)\iff \exists w^{(k + 1)}: p_{k + 1}(A|w^{(k + 1)}) v = x
    \\
    & x = w^{(k + 1)} A^kv + \sum_{j = 0}^{k - 1}w^{(k + 1)}_jA^jv
    \\
    & x = w^{(k + 1)}_k p_k(A|w^{(k)})v + \sum_{j = 0}^{k - 1}w^{(k + 1)}_jA^jv
    \\
    & x \in \mathcal K_k(A|v)
\end{aligned}
$$

For notations, we used $w^{(k)}, w^{(k + 1)}$ to represents the vector containing all coefficients for the polynomial and their $i$ element is denoted as $w^{(k)}_i$. From the last line, we proved that for all $x$ in $\mathcal K_{k + 1}(A|v)$, it's must also be in $\mathcal K(A|v)$. The frist line is using the fact that $\mathcal K_{k + 1}(A|v)$ is linear dependent, giving us an polynomial for the term $A^kv$. The next line is saying that for any element in $\mathcal K_{k + 1}(A|v)$ there exists a matrix polynomial representing $x$. Doingsome algebra, we reduced the polynomial of max degree $k$ into degree $k - 1$, proving that $x$ must also be in $\mathcal K_k(A|v)$.


**Proof of Statement (2):**

Statement (2) is saying that $k$ is unique, and it makes $\mathcal K_k(A|v)$ is linear independent and the next $\mathcal K_{k + 1}(A|v)$ is linear dependent. Statement (1) asserts the existence of $k$, and statement (3) stated that $\mathcal K{k + j}(A|v)$ are linearly dependent for all $j\ge 0$. Statement (2) is a direct results of statement (1), (2). 

---
### **Consequences of these Properties**

**Invariance Subspace**: 

The krylov subspace is invariant after a certain number, more importantly observe that $A \mathcal K_k(A|v) \subseteq \mathcal K_{k + 1}(A|v)$ and if $\mathcal K_k = \mathcal K_{k + 1}$, then the subspace $\mathcal K$ becomes an invariant of the linear operator $A$. The minimum such $k$ is also important because it can be used to determine the terminations conditions of some algorithm. 

**Polynomial Interpolations, Shifting and Eigen Decomposition**


Further consequence of the Krylov Subspace concerns with the shifting of the spectrum of linear opeartor $A$ and its eigen decomposition if we assume that the matrix $A$ is diagonalizable. The claims goes by: 

$$
\begin{aligned}
    \forall w: p_k(A|w)v \in \mathcal K_k(A|v)
\end{aligned}
$$

All polynomial is in the Krylov Subspace, consider: 

$$
\begin{aligned}
    &\hspace{0.5em} p_k(A - \alpha I|w)v
    \\
    &= \sum_{j = 0}^{k - 1} w_jX(\Lambda - \alpha I)^jX^{-1}v
    \\
    &= X
    \left(
        \sum_{j = 0}^{k - 1} w_j(\Lambda - \alpha I)^j
    \right)
    X^{-1}v
    \\
    &= X
    \left(
        \sum_{j = 0}^{k - 1} w_j'\Lambda^j
    \right)
    X^{-1}v
    \\
    &\in \mathcal K(A|v)
\end{aligned}
$$

Shifting it will not change the span, however, it might reduce the grade of the krylov subpace, as it happens sometimes when $\alpha$ is one of the eigenvalue of the matrix $A$. For analysis one would be very interesting in consider the eigen decomposition of matrix $A$, which will be useful for the termination of various Krylov Subspace Based Method. 

**Minimal Polynomial**

Please also observe that, since krylov subspace is equivalent to matrix polynomial, the absolute limit of the grade of a given Krylov Subspace for matrix $A$ is less than the degree of that Minimal Polynomial for that matrix. 

**The Grade**

Observe that, the number of distinct eigenvalues of matrix $A$ and the number of zeros in $X^{-1}v$ elements determines the grade of a Krylov Subspace Generated by $A$ given $v$. Useful for framing the terminations conditions for Krylov Subspace Methods. 


---
### **The Grade and Minimal Polynomial and the Non-zero Constants**


> Let $k$ be the grade of Krylov Subspace $A$ initialized with $v$, then exists $p_{k}(A|w)v = x$ for all $x$ in the subspace  with $w\neq \mathbf 0$, and it must be the case that $w_0\neq 0$.

For contradiction suppose otherwise that such a polynomial exists then: 

$$
\begin{aligned}
    \exists w &\neq \mathbf 0 : p_{k}(A|w)v = \mathbf 0 
    \\
    \implies& w_0v + \sum_{j = 1}^{k - 1} w_jA^jv = \mathbf 0
    \\
    \mathbf 0 &=\sum_{j = 1}^{k - 1} w_jA^jv
    \\
    \mathbf 0 &= A\sum_{j = 0}^{k - 2} w_{j + 1}A^jv
    \\
    \implies \sum_{j = 0}^{k - 2} w_{j + 1}A^jv &= \mathbf 0 
\end{aligned}
$$
From the second line to the third, I susbstitute $w_0 = 0$ for contradiction. On the last line, it suggested that $k$ is not the smallest, and $k - 1$ might be the grade, contradicting the assumption that $k$ is the grade of the Krylov Subspace. Therefore, $w_0 \neq 0 $. 