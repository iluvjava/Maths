[[CG Convergence Statmemt of Results]]


---

### **Intro**

We wish to prove the convergence rate listed in the prereq of this file, which is the convergence rate of the Conjugate Gradient under the Exact Arithmetic Assumptions. 

The final objective we wish to prove is that the convergence bound for the CG method is given as: 

> $$
> \frac{\Vert e^{(k)}\Vert_A^2}{\Vert e^{(0)}\Vert_A^2}
> \le 2 \left(
>         \frac{\sqrt{\kappa} + 1}{\sqrt{\kappa} - 1}
>     \right)^k
> $$

This is also listed as **Theorem 3.1.1** in professor's Greenbaums work: \<Iterative Method for linear System\>. 

---
### **The Chebyshev Polynomial**

This polynomial has the following definition: 

$$
T_k(x) = \min_{p(x)\in \mathcal{P}_k}\max_{x\in [-1, 1]}|p(x)|
$$

It's the polynomial that crosses the real line $k$ times with minimal amount of wiggles.

Take this for granted. This polynomial also appeared under the following context for my notebook: 

[[Spectral Method 2]]

[[Sturm Liouville Theory]]

[[Chebyshev via FFT]]

---
### **Adjusting the Cheb Polynomial**

Firstly, we are going to make the claim that: 

> $$
> \frac{\Vert e^{(k)}\Vert_A}{\Vert e^{(0)}\Vert_A} \le 
> \min_{p_k: p_k(0) = 1}\max_{x\in [\lambda_{\text{min}}, \lambda_{\text{max}}]} |p_k(x)|
> $$

Let's start with how the Conjugate Gradient works: 

$$
\begin{aligned}
    x^{(k)} &\in x^{(0)} + \text{span}\{A^jr^{(0)}\}_{j = 0}^{k - 1}
    \\
    x^{(k)} &= \mathcal{K}(r^{(0)})w + x^{(0)}
\end{aligned}
$$

**Proof**

Where the vetor $w$ is just some generic vector that is weighting the vectors from the Krolov Subspace Generated by the initial residual. 

The CG algorithm asserts that: 
$$
\begin{aligned}
    & \min_{x^{(k)}} \Vert e^{(k)}\Vert_A^2 
    \\
    =&  
    \min_{x^{(k)}} \Vert x^{(k)}- x^{*}\Vert_A^2
    \\
    =& 
    \min_{w} \left\Vert
        \mathcal{K}_k(r^{(0)})w + x^{(0)} - x^*
    \right\Vert
    \\
    =&
    \min_w \Vert p_k(A)e^{(0)}\Vert_A^2
\end{aligned}
$$

Observe that, the notation, $p_k(A)$ is denoting the weighted krylov subspace, which is a order $k - 1$ polynomial of the initial residual by operator $A$, and observe the fact that $p_k(0) = 1$, so that the first step of the iteration of CG is defined as minimization on $e^{(0)}$.

To Bound the quantity by considering an Eigen Decomposition of the PSD matrix $A$, giving us: 

$$
\begin{aligned}
    & \min_{w} \Vert Qp_k(\Lambda)Q^H\Vert_A^2
    \\
    = & \min_{w} \Vert p_k(\Lambda)e^{(0)}\Vert_A^2
    \\
    =& \min_{w} \left\Vert
        p_k(\Lambda)A^{1/2}e^{(0)}
    \right\Vert_2^2
    \\
    \le &
    \min_{w} \Vert p_k(\Lambda)\Vert_2^2 \Vert e^{(0)}\Vert_A^2
    \\
    
    \frac{\Vert e^{(k)}\Vert_A^2}{
        \Vert e^{(0)}\Vert_A^2
    }\le& \min_w \Vert p_k(\Lambda)\Vert_2^2
    \\
    \le &
    \min_w\max_{i = 1\cdots n} |p_k(\lambda_i)|^2
    \\
    \le & 
    \min_w\max_{x\in [\lambda_{\min}, \lambda_{\max}]}|p_k(x)|^2
\end{aligned}
$$

The claim is proved. 

Next, we wish to show the claim that, we can adjust the Chebyshev Polynomial in the following way to establish the bound for that minmax polynomial of the relative error measured under the energynorm. 

> $$
> p_k(x) = \frac{T_k(\varphi(x))}{T_k(\varphi(0))}
> \quad \varphi(x) = \frac{2x - \lambda_1 - \lambda_n}{\lambda_n - \lambda_1}
> $$

Where $T_k$ is the $k^{th}$ degree Chebyshev polynomial of the first kind, and we use $\lambda_1, \lambda_n$ to denote the quantity $\lambda_{\min}, \lambda_{max}$ for short. 

First, verify that $p_k(0) = 1$, which is easy. 

Next, observe the fact that $\varphi$ remaps the interval from $[\lambda_1, \lambda_n]$ to the interval $[-1, 1]$, so that polynomial is adapted to the convex hull of the eigenvalues of matrix $A$. 

Next, also observe that: 

$$
\left|
\frac{T_k(\varphi(x))}{T_k(\varphi(0))}
\right|
\le 
\left|
    \frac{1}{T_k(\varphi(0))}
\right|
$$

Because $T_k(x)$ is bounded by one. 

**Next Objective would be to find:** $T_k(\varphi(0))$. Placing a bound to the polynomial ultimately. 

Firstly, observe that $\varphi(0) \not\in [\lambda_1, \lambda_n]$, because all Eigenvalues are larger than zero, therefore it's out of the range of the Cheb and we need to find the actual value of that. 

**Finding** $\varphi(0)$. 

$$
\begin{aligned}
    T_k(x) &= \cosh(k\text{ arccosh}(z)) \quad \forall |z| \ge 1
    \\
    \implies
    T_k(\cosh(\zeta)) &= \cosh(k\zeta) \quad z = \cosh(\zeta)
\end{aligned}
$$

To need to match the form of the expression $T_k(\varphi(0))$ with the expression of the form $T_k(\cosh(\zeta))$. Observe that: 

$$
\varphi(0) = \cosh(\zeta) = \cosh(\ln(y)) \quad \ln(y) = \zeta
$$

Recall that $\cosh(x) = (\exp(-x) + \exp(x))/2$, so in this case: 

$$
\varphi(0) = (y + y^{-1})/2
$$

From the definition of $\varphi$ we can also obtain: 

$$
\varphi(x) = \frac{-\lambda_1 - \lambda_n}{\lambda_n - \lambda_1}
$$

Simplifying: 

$$
\begin{aligned}
    &\frac{-\lambda_n/\lambda_1 - 1}{\lambda_n/\lambda_1 - 1}
    \\ 
    =& - 
    \frac{\lambda_n/\lambda_1 + 1}{\lambda_n/\lambda_1 - 1} 
    \\
    \implies \varphi(0) =& 
    -\frac{\kappa + 1}{\kappa - 1}
\end{aligned}
$$

Therefore we seek for the solution of the equation: 

$$
-\frac{\kappa + 1}{\kappa - 1} = 
\frac{1}{2}(y + y^{-1})
$$

Spoiler, the solution is: 

$$
y = \frac{\sqrt{\kappa}\pm 1}{\sqrt{\kappa}\mp 1}
$$

Justification In the Appendix of the page, because it's just algebra. For now observe that the 2 solutions of the quadratic is the riciprical of each other, and it doesn't matter if we have 2 soltuion, they won't be affecting the final solution of the system.

Therefore, we can figure out the value of $T_k(\varphi(0))$ as the following: 

$$
\begin{aligned}
    & \frac{1}{2}(y + y^{-1})
    \\
    &= \cosh(\ln(y)) 
    \\
    &= \varphi(0)
    \\
    \implies 
    T_k(\varphi(0)) &= 
    T_k(\cosh(\ln(y)))
    \\
    &= (y^k + y^{-k})/2
\end{aligned}
$$

**Objective Accomplished. $\varphi(x)$ is found** 

Then, substituting the value of $y$, and invert the quantity we have: 

$$
\begin{aligned}
    & \frac{1}{T_k(\varphi(0))}
    \\
    &= 2(y^k + y^{-k})^{-1}
    \\
    &= 
    2\left(
        \left(
            \frac{\sqrt{\kappa}\pm 1}{\sqrt{\kappa}\mp 1}
        \right)^{k} + 
        \left(
            \frac{\sqrt{\kappa}\pm 1}{\sqrt{\kappa}\mp 1}
        \right)^{-k}
    \right)^{-1}
    \\
    & \le 2 \left(
        \frac{\sqrt{\kappa} + 1}{\sqrt{\kappa} - 1}
    \right)^k
\end{aligned}
$$

Because, the second quantity in the Denominator is always larger than zero and gets smaller as the value of $k$ incrases, setting it to zero and take the riciprical of the fraction, and setting it to a positive quantity only, then we have it. 


---
### **Big Eigen Outlier Convergence Bound**

We can place a better bound on the convergence rate when the largest eigenvalues are noticably larger than all the other eigenvalues which are clustere around the origin. 

Eigenvalues of the matrix $A$ are like: 

$$
p_k(z) = 
\frac
{
    T_{k-1}\left(
        \frac{2z - \lambda_{n-1} - \lambda_1}{\lambda_{n-1} -\lambda_n}
    \right)
}{
    T_{k-1}\left(
        \frac{
            -\lambda_{n-1} - \lambda_1
        }
        {
            \lambda_{n-1} - \lambda_1
        }
    \right)
}\frac{\lambda_n - z}{\lambda_n}
$$

Where the line extra linear polynomial $(\lambda_n - z)/\lambda_n$ will interpolote the point $p_1(x) = 1, p_1(\lambda_n)=0$.

Now we wish to observe these following fact about this polynomial: 

$$
\begin{aligned}
    &\frac{\lambda_n - z}{\lambda_n} \in [0, 1]
    \quad \forall z \in [\lambda_1, \lambda_n]
    \\
    & \frac{\lambda_n - z}{\lambda_n} <<1 \quad  \forall z \in 
    [\lambda_1, \lambda_{n-1}]
\end{aligned}
$$

And in that sense, we will be able to make an upper bound for this: 

$$
p_k(x) \le 
\left|
    \hat{T}_{[\lambda_1, \lambda_{n-1}]}
    (\varphi(x))
\right|
\le 
\frac{1}{
\left|
    T_{k-1}\left(
        \frac{-\lambda_{n-1} - \lambda_1}{
            \lambda_{n-1} - \lambda_1
        }
    \right)
\right|
}
$$

Where $\hat{T}$ denotes the perturbed Cheb on the interval of $[\lambda_1, \lambda_{n-1}]$.

Now, we make use the theorem 3.1.1 which we just proved above to bound the quantity on the denominator, giving us: 

$$
\begin{aligned}
    & \left|
        T_{k-1}\left(
            \frac{
                -\lambda_{n-1} - \lambda_1
            }
            {\lambda_{n-1}- \lambda_1}
        \right)
    \right|
    \\ 
    = & 
    \frac{1}{2}(y^{k - 1} + y^{-(k - 1)})
    \quad \text{ where: } y = \frac{\sqrt{\kappa_{n - 1}} + 1}{\sqrt{\kappa_{n - 1}} - 1}, \kappa_{n - 1} = \frac{\lambda_{n - 1}}{\lambda_1}
\end{aligned}
$$

Read please observe that this is exactly the same expression as what we did for theorem 3.1.1, except for the fact that the condition number is paramaterzied by the first $n - 1$ eigenvalues of the matrix. The tighter bound would be given as: 

$$
\frac{\Vert e_k\Vert_A^2}{\Vert e_0\Vert_A^2} \le 
2\left(
    \frac{\sqrt{\kappa_{n - 1}} - 1}{\sqrt{\kappa_{n - 1}} + 1}
\right)^{k - 1}
$$

Under the extreme case, the convergence is reached in 2 step of the CG iteration. 

`