[[Krylov Subspace]]
[[Krylov Matrix QR and Arnoldi Iterations]]
[[Hessenberg Transform with Arnoldi Iterations]]

---
### **Intro**

The GMRes methods is based on the idea of Arnoldi Iterations, and it uses the Krylov Subspace to minimize the current residual. It's more expensive than the Conjugate Gradient algorithm, but it's worth it. 

Major Idea
> Minimizing the projection into the orthogonal subspace generated by the arnoldi iteration. 

### **Comments and References**

The first part comes from myself, the second parts are from Yousaf Saad's sparse linear system textbook. 

---
### **From the Arnoldi Iterations**

Suppose that the arnoldi iterations is initialized with the vector $r_0$ at the start and it generates the following quantities to assist: 

$Q_k \in \mathbb{C}^{n\times k}$:: An orthogonal matrix with $q_1 = r_0/\Vert r_0\Vert$, spanning the krylov subspace. 

$H_k \in \mathbb{C}^{k\times k}$:: An upper hessenberg matrix. 

And the following recurrence relations are satisifed: 

$$
AQ_k = Q_k H_k + h_{k + 1, k}q_{k + 1}\xi_k^T
$$

Where $\xi_i$ is used to denote the ith canonical basis vector. 

**Objective of GMRes**: 

> $$
> \min_{x\in x_0 + \mathcal{K}_k(r_0)} \Vert r_k\Vert^2
> $$

Which implies that: 

$$
x_k = x_0 + Q_k y
$$

Minimizing the Squared 2 norm of the residual vector. 

And therefore, we can say: 

$$
\begin{aligned}
    & \min_{y} \Vert b - Ax_k\Vert^2
    \\
    &= 
    \min_{y} \Vert r_0 - AQ_k y\Vert^2
    \\
    &= 
    \min_{y} \Vert 
        r_0 - (Q_kH_k + h_{k+1, k}q_{k + 1}\xi_k^T)y
    \Vert^2
    \\
    &= 
    \min_{y} \left\Vert 
       \underbrace{ \Vert r_0\Vert q_1}_{=\beta q_1}
        - 
        Q_{k + 1}
        \begin{bmatrix}
            H_k \\ h_{k + 1, k}\xi_k^T
        \end{bmatrix}
    \right\Vert^2
    \\
    &= 
    \min_{y}
    \left\Vert
        \beta q_1 - Q_{k + 1}\widetilde{H}_k
    \right\Vert^2
    \\
    &= 
    \min_{y}
    \left\Vert
        Q_{k + 1}\xi_1\beta - Q_{k + 1}\widetilde{H}_ky
    \right\Vert^2
    \\
    &= 
    \min_{y} \left\Vert
        \beta\xi_1 - \widetilde{H}_ky
    \right\Vert
\end{aligned}
$$

Where, the matrix $\widetilde{H}_k$ is $\mathbb{C}^{k + 1, k}$, the matrix should be full rank and the last row should have $h_{k + 1, k}\neq 0$, if not, the algorithm should terminates before $k$. 

To minimizes the quantity, we need to seek for a solution for $y$, therefore $k + 1$ equations, but there are only $k$ variables an over determined system, and there are usually no solution, which means that, the residual will not be zero as long as $h_{k + 1, k}\neq 0$. 

NEXT, we can also go the extra mile and solve for the vector $y$, obtaining a closed form from it. 


---
### **QR Factorizations on Hessenberg**

Exactly what is in the matrix will come later, here, we just want to reason with the shapes and type of matrices that are involving in solving the problems at hands. 

Consider matrix $F_k$, where $F_k = \Omega_k\Omega_{k -1}\cdots \Omega_2\Omega_1$, where $\Omega_i$ are unitary matrices that reduces the Hessenberg form one by one into a diagonal matrix. Now suppose that $F_k$ is inductively kept. 

Now, suppose that $R_k$ is an upper triangular matrix, so that $R_k = F_k \widetilde{H}_k$. 

**We now have enough for the size of the Residual**:

$$
\begin{aligned}
    & \min_{y} \Vert \beta \xi_1 - \widetilde{H}_k y\Vert^2
    \\
    &= \min_{y} \Vert 
        F_k\beta \xi_1 - R_k y
    \Vert^2
    \\
    &= \min_{y} \Vert 
        \beta (F_k)_{:, 1} - R_k y
    \Vert^2
    \\
    &= \beta(F_k)_{k+1, 1}
\end{aligned}
$$

We can finely put the matrix $F_k$ into the norm at the second line without changing the value of the norm because we assumed $F_k$ to be an unitary transform. 

And that is the theory. 


---
### **Plane Rotation Triangularizations**

The plane rotation matrix eliminates a vector with 2 elements. Consider the following expression: 

For each column of the matrix $\widetilde{H}_k$, we wish to reduce the last element into a zero via an unitary transform, the base case is with a vector of 2 elements, and the matrix is given by: 

$$
\begin{bmatrix}
    c & s 
    \\
    -s^* & c
\end{bmatrix}
\begin{bmatrix}
    d \\ h
\end{bmatrix}
$$

Where if $d \neq 0$ then $c = |d|/\sqrt{|d|^2 + |h|^2}$, and $s = (ch/d)^*$, else $c = 0$ and $s = 1$. 

It's not hard to convince that the second element of the matrix vector multiplications results in: 

$$
-s^*d + ch = -(ch/d)d - ch
$$

And finally, oberve that the determinant of the matrix is one, which is a quick verification: 

$$
c^2 + |s|^2 = \frac{|d|^2}{|d|^2 + |h|^2} + c \frac{|h|^2}{|d|^2} = 1
$$

To perform the rotation to eliminate the sub-diagonal of the matrix $\widetilde{H}_k$, we consider the $n\times n$ rotations matrices adapted to eliminating the sub-diagonal of Hessenberg of size $n \times n$: 

$$
\Omega_{k}^{[n]} = \begin{bmatrix}
    I_{k -1}& &  
    \\
    & \begin{matrix}
        c & s \\ -s^* & c
    \end{matrix} &  
    \\
    & & I_{n - k - 1}  
\end{bmatrix}
$$

When $\Omega_{k}^{[k + 1]}$ is applied to the matrix $\widetilde{H}_k$, it reduces the $k^{th}$ column's last element to zero. Therefore, the matrix $F_k$ would be given as: 

$$
F_k = \Omega^{[k + 1]}_k\Omega^{[k + 1]}_{k - 1}\cdots 
\Omega^{[k + 1]}_2\Omega^{[k + 1]}_1
$$

Then we may assert that $F_k \widetilde{H}_k = R_k$, where $R_k$ is a diagonal matrix. 

We have identify the unitary transformation matrix that can reduce the tall upper Hessenberg to triangular. 

---
### **Algorithm and Implementations**

The triangular solve during the minimization process is performed at the last stage of the iterations, during the Arnoldi iterations, the algorithm only keep track of the Residual 2Norm for progress of the convergence. 

The accumulative plane rotation of $\Omega_i^{[k + 1]}$ are done via applying the Unitary matrix $F_k$ to each new column of the $H$ matrix generated from the Arnoldi Iterations, and to the vector $\beta \xi_1$ as well. 

**Algorithm**:

$$
\begin{aligned}
    & \text{Given: }x_0, \text{ Compute: }r_0 = b - Ax_0
    \\
    & \text{Initialize Subroutine Arnoldi with: } q_1 = r_0/\Vert r_0\Vert
    \\
    & \text{Initialize: }\xi = \xi_1 \in \mathbb{R}^{2\times 1}
    \\
    & \text{For: }i = 1, \cdots , k + 1: 
    \\
    & \hspace{1.1em} 
    \begin{aligned}
        & \text{Compute: }q_{k + 1}, h_{i, k} = (\tilde{H}_{k})_{i, k} \text{ Using Arnoldi Subroutine}
        \\
        & \text{For } i = 1, \cdots, k - 1
        \\
        &\hspace{1.1em} 
        \begin{aligned}
            &\begin{bmatrix}
                (\tilde{H}_k)_{i, k} \\ (\widetilde{H}_k)_{i + 1, k}
            \end{bmatrix}\leftarrow 
            \begin{bmatrix}
                c_i & s_i 
                \\
                -s_i^{*} & c_i
            \end{bmatrix}
            \begin{bmatrix}
                (\widetilde{H}_k)_{i, k}
                \\
                (\widetilde{H}_k)_{i + 1, k}
            \end{bmatrix}
        \end{aligned}
        \\
        &\text{Endfor}
        \\&
        \begin{bmatrix}
            (\xi)_k \\ (\xi)_{k + 1}
        \end{bmatrix}\leftarrow 
        \begin{bmatrix}
            c_k & s_k \\ -s_k^{*} & c_k
        \end{bmatrix}\begin{bmatrix}
            (\xi)_k \\ 0
        \end{bmatrix}
        \\&
        (\widetilde{H}_k)_{k, k} \leftarrow c_k \widetilde{H}_{k, k} + 
        s_k (\widetilde{H}_k)_{k + 1, k}
        \\& 
        \widetilde{H}_{k + 1, k} \leftarrow 0
        \\& \text{If } \beta|(\xi)_{k + 1}| \text{ small enough then: }
        \\& \hspace{1.1em} \text{Solve: } (\widetilde{H}_{k})_{:k, :k} = \beta \xi
        \\& \hspace{1.1em} \text{Compute: } x_k = x_0 + Q_k y_k
    \end{aligned}
    \\ &\text{EndFor}
\end{aligned}
$$

That the GMRes algorithm. To make it more practical, consider restarting the algorithm with previous residuals when the memory got full, if the matrix is huge, you will ran out of memory very quickly. 



---
### **The Matrix Polynomial of the GMRes**

$x_k$ is in the affine subspace $x_0 + \mathcal{K}_k(A, r_0)$, which can be written as: $x_k = x_0 + \mathcal{K}_k(A, r_0)w$, where $w$ is a weight vector that add weights to the columns of krylov subspace matrix. In that way, we have the objective of minimizing $\Vert r_k\Vert$ written as:  

$$
\begin{aligned}
    &\hspace{1.1em}\min_{w} \left\Vert
        b - Ax_k
    \right\Vert_2^2
    \\
    &= \min_{w}\left\Vert
         b - A(x_0 + \mathcal{K}_k(A, r_0))w
    \right\Vert_2^2
    \\
    &= 
    \min_{w}\left\Vert
        r_0 + \mathcal{K}_k(A, Ar_0)w
    \right\Vert_2^2
    \\
    &= 
    \min_{p(k): p(0) = 1}
    \left\Vert
        p_k(A) r_0
    \right\Vert_2^2 
    \\
    & \le 
    \min_{p(k): p(0) = 1}
    \Vert p_k(A)\Vert_2^2 \Vert r_0\Vert^2_2
\end{aligned}
$$

Here we take note tha, $\exists\; w: \mathcal{K}_k(A, x)w = p_{k -1}(A)x$, which is an alternative way of representing the krylov subspace. 


---
### **GMRes for Hermitian Matrices**: MinRes



