Krylov Subspace is used in a lot of places: 
* Eigenvectors related stuff
* Solutions for the system. 

[[Minmal Polynomial]]

---
### **Intro**

This is the Krylov Subspace initialized by $b$: 

$$
\mathcal{K}_k(A|b) = \text{span}( b, Ab, A^2b, \cdots A^{k - 1}b)
$$

**Claim 0:**
> $$
> \forall v: \mathcal{K}_1(A|v)  \subseteq  \mathcal{K}_2(A|v)  \subseteq \mathcal{K}_3(A|v)  \cdots 
> $$

It's trivial to justify claim 0. 


**Claim 1**

> A matrix satisfies it's own characteristic equation 
> $$p(A) = \mathbf{0}$$ 
> The polynomial outputs a matrix. This is called the Cayley-Hamilton Theorem. 

**Full proof is not given, but this is true for diagonalizable matrices.** 

Let the eigen decomposition(Jordan Decomposition) for matrix $A$ to be $X\Lambda X^{-1}$

$$
\begin{aligned}
    p(A) &= \sum_{i=0}^{n}c_iA^i
    \\
    p(A) &= \sum_{i=0}^{n}c_i(X\Lambda X^{-1})^i
    \\
    p(A) &= \sum_{i=0}^{n}c_iX\Lambda^iX^{-1}
    \\
    p(A) &= X \left(\sum_{i = 0}^{n}
        c_i\Lambda^{i}
    \right)X^{-1}
\end{aligned}
$$

Consider $\Lambda$ to be the Jordan Decomposition matrix for the matrix, when it's diagonalizable, $p(\lambda_i) = 0$, and from above expression, putting $\Lambda$ into the characteristic polynomial will set it equals to zero. **Note**: $c_0 = (-1)^n|A|$; the constant term for the characteristic polynomial, this quantity tells us whether the matrix is going to be invertible or not. 


**Consequence of Cayley Hamilton**

$$
\begin{aligned}
    \mathbf{0} &= 
        c_0I_n + c_1A + c_2A^2 \cdots c_nA^n
    \\
    -c_0I_n &=  
    c_1A + c_2A^2 \cdots c_nA^n
    \\
    -I_n &= \frac{c_1}{c_0}A + \frac{c_2}{c_0}A^2 \cdots \frac{c_n}{c_0}A^n
    \\\implies
    -A^{- 1} &= \frac{c_1}{c_0}I_n + \frac{c_2}{c_0}A \cdots \frac{c_n}{c_0}A^{n - 1}
    \\\implies
    -b &= \frac{c_1}{c_0}Ab + \frac{c_2}{c_0}A^2b \cdots \frac{c_n}{c_0}A^nb
\end{aligned}
$$

The inverse of $A$ can be expressed with the matrix polynomials of max degree $n$, using the coefficients from the characteristic polynomials. This implies that for any vector $b$, there exists $v$ such that $\mathcal K_{k}(A|v)$ such that the vector is in that spanned subspace. If we know that $A$ is invertible. This is known as the: **Cayleys Hamilton's Theorem**.

---
### **Reighly Quotient and Invariant Subspace**
 
# TODO: Not sure if it's important enough. 

---
### **Properties of Krylov Subspace**

**Claim 1: Existence of Matrix Polynomial**

> Every element inside of krylov subspace generated by matrix $A$, and an initial veoctr $v$ can be represented as a polynomial of matrix $A$ multiplied by the vector $v$ and vice versa. 
> $$
> \begin{aligned}
>     & \forall x \in \mathcal K_j(A|v) \;\exists\; w: p_k(A|w)v = x
> \end{aligned}
> $$

We use $p_k(A|w)$ to denotes a matrix polynomial with coefficients $w$, where $w$ is a vector. No proof this is trivial. Take note that, we can change the field of where the scalar $w$ is coming from, but for discussion below, $\mathbb R, \mathbb C$  doesn't matter and won't change the results. 

$$
p_k(A|w)v = \sum_{j = 0}^{k - 1}w_jA^jv
$$

The most important porperty of the subspace is the idea of grade denoted as $\text{grade}(A|v)$, indicating when the Krylov Subspace of $A$ wrt to $v$ stops expanding after a certain size. To show this idea, we consider the following 3 statements about Krylov Subspace which we will proceed to prove. 


**Statement (1)**: 
> $$\exists 1 \le k \le m + 1: \mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v)$$
> There exists an natural number between $1$ and $m+ 1$ such that, the successive krylov subspace span the same space asthe previous one. 


**Statement (2)**: 
> $$
>     \exists \min k \text{ s.t: }\mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v) \implies 
>     \mathcal K_k(A|v) \text{ is Lin Ind} \wedge \mathcal K_{k + 1}(A|v) \text{ is Lin Dep}. 
> $$
> There eixsts a minimum such $k$ where the immediate next krylov subspace is linear dependent. 


**Statement (3)**: 
> $$\mathcal K_k(A|v) \text{ Lin Dep} \implies \mathcal K_{k + 1}(A|v) = \mathcal K_k(A|v)$$
> if the $k$ krylov subspace is linear dependent, then it stops expanding and the successive krylov subspace spans the same space. 

**Theorem**: Existence of The Grade of the Subspace
> Let $k$ be the minumum number when the krylov subspace stops expanding, then all successive krylov subspace spand the same space. $\mathcal K_k(A|v) = \mathcal K_{k + j}(A|v) \;\forall j \ge 0$. The number $k$ is regarded as the grade of krylov subspace wrt to v denoted using $\text{grade}(A|v)$. 

**Proof of Statement (1)**

For notational simplicity, $\mathcal K_k$ now denotes $\mathcal K_k(A|v)$. Let's start the considerations from the definition of the Krylov Subspace: 

$$
\begin{aligned}
    \forall\; k: \mathcal K_k \subseteq \mathcal K_{k + 1}\implies \text{dim}(\mathcal K_{k})\le \text{dim}(\mathcal K_{k + 1})
    \\
    \mathcal K_{k + 1}\setminus \mathcal K_k = \text{span}(A^{k}v) 
    \\
    \implies \dim(\mathcal K_{k + 1}) - \dim(\mathcal K_k) \le 1
\end{aligned}
$$

Therefore, the dimension of the successive krylov subspace forms a sequence of positive integer that is monotonically increasing. By the Cayley's Hamilton's theorem, the sequence is bounded by $m$, since there are $m + 1$ terms, it must be the case that at least 2 of the krylov subspace has the same dimension (And the earliest such occurance will exist), implying the the fact that the new added vector from $k$ to $k + 1$ is in the span of the previous subspace. 

**Proof of Statement (3)**

Consider: 

$$
\begin{aligned}
    & \mathcal K_k \text{Lin Dep}
    \\
    \implies & \exists w_k \neq \mathbf 0 : p_k(A|w^+_k)v = \mathbf 0
    \\
    \implies & Ap_k(A|w_k^+)v = \mathbf 0
    \\
    & p_{k + 1}(A| [0 \; (w_k^+)^T]) = \mathbf 0
    \\
    & \mathcal K_{k + 1} \text{ is Lin Dep}
\end{aligned}
$$

The recurrence of multplying by $A$ allows the krylov subspace to grow and the new bigger subspace will contain the previous one. Therefore inheriting the linear dependence, we use the idea of matrix polynomial for the proof. 

**Proof of Statement (2):**

Assuming that statement (1) and (3) is true. Statement (1) implies the existence of the smallest such $k$. For contradiction, we only have one case to assume, that is $\mathcal K_k$ and $\mathcal K_{k + 1}$ are linear dependence. Then $\mathcal K_k$ is either Linear Dependence, or Independence. 

If $\mathcal K_{k - 1}$ is linear dependence, then by (3) $\mathcal K_{k - 1} = \mathcal K_k$, hence $k$ is not the minimum. Else assume $\mathcal K_{k - 1}$ is linear independence, however $\mathcal K_k$ is linear dependence, and $\mathcal K_k \setminus \mathcal K_{k - 1} = \text{span}(A^{k - 1}v)$; therefore, $A^{k -1}v$ is in the span of $\mathcal K_{k -1}$, hence $\mathcal K_{k -1} = \mathcal K_k$, contradicing again that $k$ is the minimum such $k$. 

---
### **Consequences of these Properties**

**Invariance Subspace**: 

The krylov subspace is invariant after a certain number, more importantly observe that $A \mathcal K_k(A|v) \subseteq \mathcal K_{k + 1}(A|v)$ and if $\mathcal K_k = \mathcal K_{k + 1}$, then the subspace $\mathcal K$ becomes an invariant of the linear operator $A$. The minimum such $k$ is also important because it can be used to determine the terminations conditions of some algorithm. 

**Polynomial Interpolations, Shifting and Eigen Decomposition**


Further consequence of the Krylov Subspace concerns with the shifting of the spectrum of linear opeartor $A$ and its eigen decomposition if we assume that the matrix $A$ is diagonalizable. The claims goes by: 

$$
\begin{aligned}
    \forall w: p_k(A|w)v \in \mathcal K_k(A|v)
\end{aligned}
$$

All polynomial is in the Krylov Subspace, consider: 

$$
\begin{aligned}
    &\hspace{0.5em} p_k(A - \alpha I|w)v
    \\
    &= \sum_{j = 0}^{k - 1} w_jX(\Lambda - \alpha I)^jX^{-1}v
    \\
    &= X
    \left(
        \sum_{j = 0}^{k - 1} w_j(\Lambda - \alpha I)^j
    \right)
    X^{-1}v
    \\
    &= X
    \left(
        \sum_{j = 0}^{k - 1} w_j'\Lambda^j
    \right)
    X^{-1}v
    \\
    &\in \mathcal K(A|v)
\end{aligned}
$$

Shifting it will not change the span, however, it might reduce the grade of the krylov subpace, as it happens sometimes when $\alpha$ is one of the eigenvalue of the matrix $A$. For analysis one would be very interesting in consider the eigen decomposition of matrix $A$, which will be useful for the termination of various Krylov Subspace Based Method. 

**Minimal Polynomial**

Please also observe that, since krylov subspace is equivalent to matrix polynomial, the absolute limit of the grade of a given Krylov Subspace for matrix $A$ is less than the degree of that Minimal Polynomial for that matrix. 

**The Grade**

Observe that, the number of distinct eigenvalues of matrix $A$ and the number of zeros in $X^{-1}v$ elements determines the grade of a Krylov Subspace Generated by $A$ given $v$. Useful for framing the terminations conditions for Krylov Subspace Methods. 
