Krylov Subspace is used in a lot of places: 
* Eigenvectors related stuff
* Solutions for the system. 

---
### **Intro**

This is the Krylov Subspace initialized by $b$: 

$$
\mathcal{K}_k(A|b) = \langle b, Ab, A^2b, \cdots A^{k - 1}b \rangle = \text{span}(\{A^jb\}_{j = 0}^{k - 1})
$$

**Claim 0:**
> $$
> \langle \mathcal{K}_1 \rangle \subseteq  \langle \mathcal{K}_2 \rangle \subseteq \langle \mathcal{K}_3 \rangle \cdots 
> $$

It's trivial to justify claim 0. 

**Claim 1:**

> If a matrix is invertible, then it can be represented via vectors in the Krylov Subspace. 

Suppose we have $A$ as a square matrix that is invertible. Then: 

**Characteristic Polynomial** 

$$
p(\lambda) = \sum_{i = 0}^{n}
    c_0 \lambda^i = |A - \lambda I|
$$

**Claim 1.1**

> A matrix satisfies it's own characteristic equation 
> $$p(A) = \mathbf{0}$$ 
> The polynomial outputs a matrix. This is called the Cayley-Hamilton Theorem. 

**Full proof is not given, but this is true for diagonalizable matrices.** 

Let the eigen decomposition(Jordan Decomposition) for matrix $A$ to be $X\Lambda X^{-1}$

$$
\begin{aligned}
    p(A) &= \sum_{i=0}^{n}c_iA^i
    \\
    p(A) &= \sum_{i=0}^{n}c_i(X\Lambda X^{-1})^i
    \\
    p(A) &= \sum_{i=0}^{n}c_iX\Lambda^iX^{-1}
    \\
    p(A) &= X \left(\sum_{i = 0}^{n}
        c_i\Lambda^{i}
    \right)X^{-1}
\end{aligned}
$$

Consider $\Lambda$ to be the Jordan Decomposition matrix for the matrix. 

**Note**: $c_0 = (-1)^n|A|$, this quantity tells us whether the matrix is going to be invertible or not. 


---
### **Computationally Relevant to Inverse of the Matrix**

$$
\begin{aligned}
    \mathbf{0} &= 
        c_0I_n + c_1A + c_2A^2 \cdots c_nA^n
    \\
    -c_0I_n &=  
    c_1A + c_2A^2 \cdots c_nA^n
    \\
    -I_n &= \frac{c_1}{c_0}A + \frac{c_2}{c_0}A^2 \cdots \frac{c_n}{c_0}A^n
    \\\implies
    -A^{- 1} &= \frac{c_1}{c_0}I_n + \frac{c_2}{c_0}A \cdots \frac{c_n}{c_0}A^{n - 1}
    \\\implies
    -b &= \frac{c_1}{c_0}Ab + \frac{c_2}{c_0}A^2b \cdots \frac{c_n}{c_0}A^nb
\end{aligned}
$$

The inverse of $A$ can be expressed using another matrix polynomials, using the coefficients from the characteristic polynomials. Any vector, can be spanned by the Krylov Subspace, if we know that $A$ is invertible.  This is known as the: **Cayleys Hamilton's Theorem**.

---
### **Reighly Quotient and Invariant Subspace**




---
### **Properties of the Krylov Subspace**

Some of these properties are discussed by Yousaf Saad, in his sparse linear system book, others are my own proof and ideas. Here, we will paraphrase some of the results for quick references. Let's consider the linear operator $A$ and initial vector $x_0$ that generates the Krylov Subspace: $\mathcal K_k(A|x_0)$. Then, if there exists some minimum $k \le n$ such that $\mathcal K_{k}(A|x_0) = \mathcal K_{k + 1}(A|x_0)$, then it will have to be the case that $\mathcal K_k(A|x_0) = \mathcal K_{k + j}(A|x_0)\;\forall j \le 0$. And that $k$, is the *minimum grade* of the krylov subspace, denote that as $\text{grade}(A|v)$. In addition, it will be the case that $\mathcal K_{k + j}(A|v)$ is an linear dependent basis for all $j\ge 0$. 

**Claim 1: Existence of Matrix Polynomial**

> Every element inside of krylov subspace generated by matrix $A$, and an initial veoctr $v$ can be represented as a polynomial of matrix $A$ multiplied by the vector $v$. 
> $$
> \begin{aligned}
>     & \forall x \in \mathcal K_j(A|v) \;\exists\; w: p_k(A|w)v = x
> \end{aligned}
> $$

Notation wise, we use $p_k(A|w)$ to denotes a matrix polynomial with coefficients $w$, where $w$ is a vector. No proof this is trivial 

**Claim 2: The Grade of a Subspace**

> There exists minimum $k$ such that: $\mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v)$, and $k\le m + 1$, In addition to that, we have $\mathcal K_{k} (A|v) = \mathcal K_{k + j}(A|v) \; \forall 0 \le j$ and consequently there exists a matrix polynomial of degree $k +1$ such that $p_{k + 1}(A|w)v = \mathbf 0$.

**Proof**: 

$\text{dim}(\mathcal K_{k}(A|v))\le \text{dim}(\mathcal K_{k + 1}(A|v))$, observe trivially that $\mathcal K_{j}(A|b) \subseteq \text{ran}(A) \le n$, therefore the dimension of $\mathcal K_k(A|v)$ is a strictly increasing sequence of integer and it's upper bounded by $m$ and lower bounded by $1$. If the sequence has $m + 1$ term, then it must repeat, and consequently it has a a first term $j$ where: $\mathcal K_k(A|v) = \mathcal K_{k + 1}(A|v)$. 

To prove the second part, observe that if $\mathcal{K}_k(A|v) = \mathcal{K}_{k+ 1}(A|v)$ then $\mathcal{K}(A|v)$ is linear dependent, and for any $\mathcal{K}_{j'}(A|v)$ that is linear dependent, $\mathcal K_{j'}(A|v) = \mathcal K_{j' + 1}(A|v)$, let $j' = k$ then it falls through inductively that $\mathcal K_{k + j}(A|v) = \mathcal K_{k}(A|v)\;\forall\; 0\le j$, this is true because $\mathcal K_k(A|v)\subseteq \mathcal K_{k + j}(A|v)$, finally, we consider the the last part of the claim which can be proved by: 

$$
\begin{aligned}
    & 
    \mathcal K_k(A|v) = \mathcal K_{k+1}(A|v)  \quad \text{k is minimal such quantity}
    \\
    \implies &
    A^kv \in \mathcal K(A|v)
    \\
    \iff &
    \exists ! w_k^+ \in \mathbb R^k 
    : 
    p_k(A|w_k^+) v = A^kv
    \\
    \implies 
    & p_k(A|w_k^+)v - A^kv = \mathbf 0
    \\
    \implies & \exists w_{k + 1}^+ : p_{k + 1}(A| w^+_{k + 1}) = \mathbf 0
    \\
    \text{where: }& w^+_{k + 1} \in \text{span}([(w_k^+)^T \; - 1])
\end{aligned}
$$

Therefore, if the minimum value of $k$ that makes the Krylov subapce linear independent, then it implies that there exists a minimal polynomial of $A\in \mathbb R^n$ wrt to the vector $v$, and the degree is $k + 1$. From the Cayley Hamilton theorem, we know that, for all vectors of a linear matrix $A$, the degree minimal polynomial doesn't exceed $n + 1$. 

**Grade Of $A$ wrt to v**

> We define the grade of the matrix $A$ wrt to vector $v$ is the minimal $k$ such that $\mathcal K_{k + 1}(A|v) = \mathcal K_{k}(A|v)$. 



