<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>&midast;&midast;Intro&midast;&midast;</title>
        <style>
</style>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 12px;
                line-height: 1;
            }
        </style>
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        
        <script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
        
    </head>
    <body class="vscode-body vscode-light">
        <p>[[Distributed Computing]] Is the context for this file.</p>
<p>Open MPI Documentation: <a href="https://www.open-mpi.org/doc/">here</a></p>
<hr>
<h3 id="intro"><strong>Intro</strong></h3>
<p>Different processes or computer posses the same program solving the same problem.</p>
<p>Each communicate using MPI, about their assigned ID and passing information to each other.</p>
<p>Let's repeat this:</p>
<p><strong>SPMD</strong> Single Program Multiple Data. Each process has the same program but the data they are dealing with is different.</p>
<p><strong>Node</strong>: A single Computing Unit, could be a process, or a computer.</p>
<p><strong>DRPDDD</strong>: Distinguished replicated process, distributed data.</p>
<p>MPI ID distinguishes the process, data are distributed via MPI, and one of the programs, that one that distributed the data and collect and combine the data must identify itself and do the job accordingly.</p>
<p>MPI is using a network interface to pass messages between different process. We cant put data directly into the memory of another process.</p>
<hr>
<h3 id="mpi-example"><strong>MPI Example</strong></h3>
<p>Here we use the example of evaluating: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mfrac><mn>4</mn><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>1</mn></mrow></mfrac><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\int_{0}^{1}\frac{4}{x^2 + 1}dx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4123390000000002em;vertical-align:-0.403331em;"></span><span class="mop"><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0005599999999999772em;">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0090080000000001em;"><span style="top:-2.34418em;margin-left:-0.19445em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span><span style="top:-3.2579000000000002em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.35582em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463142857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathdefault">d</span><span class="mord mathdefault">x</span></span></span></span></p>
<pre><code class="language-cpp"><div><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc, <span class="hljs-keyword">char</span>* argv[])</span> 
</span>{
    <span class="hljs-keyword">size_t</span> intervals = <span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>;
    <span class="hljs-comment">// MPI declare current thread as a member. </span>
    MPI::Init();
    
    <span class="hljs-keyword">int</span> myrank = MPI::COMM_WORLD.Get_rank();
    <span class="hljs-keyword">int</span> mysize = MPI::COMM_WORLD.Get_size();
    
    <span class="hljs-comment">//Thread 0 get input from user. </span>
    <span class="hljs-keyword">if</span> (<span class="hljs-number">0</span> == myrank) 
    {
        <span class="hljs-keyword">if</span> (argc &gt;= <span class="hljs-number">2</span>) intervals = <span class="hljs-built_in">std</span>::atol(argv[<span class="hljs-number">1</span>]);
    }
    
    <span class="hljs-comment">// Broadcast a variable</span>
    MPI::COMM_WORLD.Bcast(&amp;intervals, <span class="hljs-number">1</span>, MPI::UNSIGNED_LONG, <span class="hljs-number">0</span>);
    
    <span class="hljs-comment">// Assign tasks </span>
    <span class="hljs-keyword">size_t</span> blocksize = intervals / mysize;
    <span class="hljs-keyword">size_t</span> begin = blocksize * myrank;
    <span class="hljs-keyword">size_t</span> end = blocksize * (myrank + <span class="hljs-number">1</span>);

    <span class="hljs-keyword">double</span> h = <span class="hljs-number">1.0</span> / ((<span class="hljs-keyword">double</span>)intervals);
    <span class="hljs-keyword">double</span> pi = <span class="hljs-number">0.0</span>;

    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = begin; i &lt; end; ++i) 
    {
        pi += <span class="hljs-number">4.0</span> / (<span class="hljs-number">1.0</span> + (i * h * i * h));
    }

    <span class="hljs-comment">// MPI storage reduce on a variable. </span>
    MPI::COMM_WORLD.Reduce(&amp;mypi, &amp;pi, <span class="hljs-number">1</span>, MPI::DOUBLE, MPI::SUM, <span class="hljs-number">0</span>);
    
    <span class="hljs-keyword">if</span> (<span class="hljs-number">0</span> == myrank) 
    {
        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">&quot;pi is approximately &quot;</span> &lt;&lt; pi &lt;&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">endl</span>;
    }
    
    MPI::Finalize();
    
    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;
}
</div></code></pre>
<p><code>include&lt;mpi.h&gt;</code> is needed for the program.</p>
<p><code>MPI::COMM_WORLD</code> defines a communicator that exists after the declaration of <code>MPI::Init()</code>.
* This must be declared for all processes that is using MPI.</p>
<p><code>MPI::COMM_WORLD.Get_rank()</code> returns a number that can be used to uniquely identify the current running process.</p>
<p><code>MPI:COMM_WORLD.Get_size()</code> returns a number that is the total number of running process invoked by MPI at the start.</p>
<p><code>MPI:COMM_WORLD.Bcast()</code> Take a reference on a variable and set all the variables in different programs to be the same.</p>
<blockquote>
<p>All processes reaches the Bcast function and they pass a reference of <code>interval</code> and then the variable got modified by MPI. Root of the communicator controls what value is set, in the above example, the last parameter is <code>0</code> which indicates that communcators for this MIP Bcast. See <a href="https://www.open-mpi.org/doc/v3.1/man3/MPI_Bcast.3.php">documentation here</a> for more on <code>Bcast</code>.</p>
</blockquote>
<p><code>MPI::COMM_WORLD.Reduce</code>: This is a send buffer from all the node other than the root note.</p>
<blockquote>
<p>When called, the last parameter of the function notifies it which node the results should be reducing to, in this case it's process 0. See <a href="https://www.open-mpi.org/doc/v4.0/man3/MPI_Reduce.3.php">here</a> for more details about the parameters for the function.</p>
</blockquote>
<p><code>MPI::COMM_WORLD.Allreduce</code>: Reduce a variable and then sync to all processes.</p>
<blockquote>
<p>If, somehow the reduced results need to be used for computations, we will definitely use this for computations.</p>
</blockquote>
<p><strong>MPI Sends and Receives</strong></p>
<p>Let's see an example that makes use of <code>MPI::COMM_WORLD.Send</code> and <code>.Recv</code>. It's called a pingpong because 2 processes are exchanging some kind of trivial information.</p>
<pre><code class="language-cpp"><div><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> 
</span>{
    <span class="hljs-comment">// we are using MPI</span>
    MPI::Init();
    
    <span class="hljs-comment">// setup</span>
    <span class="hljs-keyword">int</span> myrank = MPI::COMM_WORLD.Get_rank();
    <span class="hljs-keyword">int</span> mysize = MPI::COMM_WORLD.Get_size();
    <span class="hljs-keyword">int</span> ballsent = <span class="hljs-number">42</span>, ballreceived = <span class="hljs-number">0</span>;
    
    MPI::COMM_WORLD.Send(&amp;ballsent, <span class="hljs-number">1</span>, MPI::INT, <span class="hljs-number">1</span>, <span class="hljs-number">321</span>);<span class="hljs-comment">//(1)</span>
    MPI::COMM_WORLD.Recv(&amp;ballsent, <span class="hljs-number">1</span>, MPI::INT, <span class="hljs-number">0</span>, <span class="hljs-number">321</span>); <span class="hljs-comment">//(2)</span>
    MPI::COMM_WORLD.Send(&amp;ballreceived, <span class="hljs-number">1</span>, MPI::INT, <span class="hljs-number">0</span>, <span class="hljs-number">321</span>);<span class="hljs-comment">//(3)</span>
    MPI::COMM_WORLD.Recv(&amp;ballreceived, <span class="hljs-number">1</span>, MPI::INT, <span class="hljs-number">1</span>, <span class="hljs-number">321</span>);<span class="hljs-comment">//(4)</span>
    
    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">&quot;Received &quot;</span> &lt;&lt; ballreceived &lt;&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">endl</span>;
    MPI::Finalize();
    
    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;
}
</div></code></pre>
<p><strong>There is a big mistake here</strong>, There are 2 running processes executing the program, and both of them are sending and receiving. Both tries to execute <code>(1), (3)</code> in this case.</p>
<p>Process 1 will send and Process 2 will send, and then both will try to receive.</p>
<p><strong>Question</strong>: In the previous one, we used <code>Bcast</code>, how does that one work?</p>
<p><code>Bcast</code> is not specifying sender and receiveers because itself is syncing and all the processes will make a function call on <code>Bcast</code>. The last argument in the function call for <code>Bcast</code> sepcifies where the root of the data is comming from. Usually process node with a rank of 0.</p>
<p>A working Send and receive between 2 processes:</p>
<pre><code class="language-cpp"><div>main() {
    MPI::Init();
    <span class="hljs-keyword">int</span> myrank = MPI::COMM_WORLD.Get_rank();
    <span class="hljs-keyword">int</span> mysize = MPI::COMM_WORLD.Get_size();
    <span class="hljs-keyword">int</span> ballsent = <span class="hljs-number">42</span>, ballreceived = <span class="hljs-number">0</span>;

    <span class="hljs-comment">// send the ball only if we are compute node 0    </span>
    <span class="hljs-keyword">if</span> (<span class="hljs-number">0</span> == myrank) 
    {
        MPI::COMM_WORLD.Send(&amp;ballsent, <span class="hljs-number">1</span>, MPI::INT, <span class="hljs-number">1</span>, <span class="hljs-number">321</span>);
        MPI::COMM_WORLD.Recv(&amp;ballreceived, <span class="hljs-number">1</span>, MPI::INT, <span class="hljs-number">1</span>, <span class="hljs-number">321</span>);
        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">&quot;Received &quot;</span> &lt;&lt; ballreceived &lt;&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">endl</span>;
    }
    
    <span class="hljs-comment">// Receive from compute node 1 and send it back. </span>
    <span class="hljs-keyword">if</span> (<span class="hljs-number">1</span> == myrank) 
    {
        MPI::COMM_WORLD.Recv(&amp;ballreceived, <span class="hljs-number">1</span>, MPI::INT, <span class="hljs-number">0</span>, <span class="hljs-number">321</span>);
        MPI::COMM_WORLD.Send(&amp;ballsent, <span class="hljs-number">1</span>, MPI::INT, <span class="hljs-number">0</span>, <span class="hljs-number">321</span>);
    }
    
    MPI::Finalize();
    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;
}
</div></code></pre>
<p>This version works.</p>
<p><strong>Question</strong>:</p>
<p><code>send, recv</code> are just library functions, what is the state of the program when the call of the function returns????? What does it mean that the process of sending a buffer has ended?</p>
<p><strong>Answers</strong>:</p>
<p>In the case of <code>COMM_WORLD.Send</code>, returned meaning that <strong>the buffer is safe to reuse</strong>. For small message, this might block, or not. <strong>We don't know</strong>.</p>
<p>&quot;Safe to re-reuse&quot;: This means that the data has been buffered into some place, or it's during transit and we can modify that without worrying about potential problems.</p>
<p>In the case of <code>COMM_WORLD.Recv</code>, Blocked until the message is receive. We must get the data before advancing from this function call.</p>
<hr>
<h3 id="compiling-and-running"><strong>Compiling and Running</strong></h3>
<p>We need MPI to wrap the compiling and execution of C++ when running the code.</p>
<pre><code class="language-cpp"><div>$ mpic++ hello.cpp
$ mpirun -np <span class="hljs-number">4</span> ./a.out
</div></code></pre>
<p>We need the MPI to compile and invoke the system, we can't just compile and run it directly using C++.</p>
<hr>
<h3 id="mpi-deadlock"><strong>MPI Deadlock</strong></h3>
<p>Process 1 has  o <code>send, receive</code> and Process 2 has <code>send, receive</code></p>
<p>They send and receive from each other.</p>
<p>MPI has ambiguity.</p>
<p>When <code>MPI::COMM_WORLD.send</code> blocks when we can use the buffer. However, it's possible that the send will halt until the message is received by the receiver.</p>
<p>Here is how it goes:</p>
<p>Process 1 sends and halt until thread 2 recieve, process 2 sends and halt until Process 1 receive, so both of them are waiting for each other to receive while and halt on the send method. This is a ceadlock.</p>
<p>For large message, MPI will synchronize sync, and it's buffering when the data are exchanged between each of the process.</p>
<p><strong>Question:</strong></p>
<p>Why no buffering when we had small messages?</p>
<p><strong>Answer:</strong></p>
<p>It saves system resources</p>
<p><strong>Question:</strong></p>
<p>Ok, why not just send and receive on process 1 and then receive and send on process 2? This can prevent deadlock</p>
<p><strong>Answer:</strong></p>
<p>This is good and doable, but to some extend, it it's not following the paradigm of SPMD. In a sense that we need to more ways to distinguish the order of sending and receiving for each of the process, and that means more complex code and serilization between the message exchange. Impacting the speed of the algorithms.</p>
<hr>
<h3 id="exchanging-message-in-a-smart-way"><strong>Exchanging Message in a Smart Way</strong></h3>
<p>There are non-blocking send and receive in MPI:</p>
<pre><code class="language-cpp"><div><span class="hljs-function">Request <span class="hljs-title">Comm::Isend</span><span class="hljs-params">(cost <span class="hljs-keyword">void</span>* buf, <span class="hljs-keyword">int</span> count, <span class="hljs-keyword">const</span> Datatype&amp; datatype, <span class="hljs-keyword">int</span> dest, <span class="hljs-keyword">int</span> tag)</span> <span class="hljs-keyword">const</span>

Request <span class="hljs-title">Comm::Irecv</span><span class="hljs-params">(<span class="hljs-keyword">void</span>* buf, <span class="hljs-keyword">int</span> count, <span class="hljs-keyword">const</span> Datatype&amp; datatype, <span class="hljs-keyword">int</span> source, <span class="hljs-keyword">int</span> tag)</span> <span class="hljs-keyword">const</span>
</span></div></code></pre>
<p>The request is then returned, the entity is similar to what we had for promise in javascript, and to wait for the request, we use wait_all, see <a href="https://www.open-mpi.org/doc/v4.0/man3/MPI_Waitall.3.php">wait all</a> in open MPI documentation.</p>
<p>Give it an array of request.</p>
<p><strong>Scatter</strong></p>
<pre><code class="language-cpp"><div><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MPI_Scatter</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">void</span> *sendbuf, <span class="hljs-keyword">int</span> sendcount, MPI_Datatype sendtype,
    <span class="hljs-keyword">void</span> *recvbuf, <span class="hljs-keyword">int</span> recvcount, MPI_Datatype recvtype, <span class="hljs-keyword">int</span> root,
    MPI_Comm comm)</span>
</span></div></code></pre>
<p>Process <code>root </code> scatters its data to all other processes. <code>sendbuf</code> provided by <code>root</code>, <code>recvbuf</code> provided by none <code>root</code>. <code>sendbuf</code> is conserved, because of <code>const</code>.</p>
<p><strong>Simultaneous Send and Receive</strong></p>
<pre><code class="language-cpp"><div><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MPI_Sendrecv</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">void</span> *sendbuf, <span class="hljs-keyword">int</span> sendcount, MPI_Datatype sendtype,
    <span class="hljs-keyword">int</span> dest, <span class="hljs-keyword">int</span> sendtag, <span class="hljs-keyword">void</span> *recvbuf, <span class="hljs-keyword">int</span> recvcount,
    MPI_Datatype recvtype, <span class="hljs-keyword">int</span> source, <span class="hljs-keyword">int</span> recvtag,
    MPI_Comm comm, MPI_Status *status)</span>
</span></div></code></pre>
<hr>
<h3 id="misc-all-about-mpi"><strong>MISC: All About MPI</strong></h3>
<p>MPI Constants: see <a href="https://www.mpi-forum.org/docs/mpi-2.2/mpi22-report/node375.htm">here</a> on the forum. It's for MPI 2.2</p>

    </body>
    </html>