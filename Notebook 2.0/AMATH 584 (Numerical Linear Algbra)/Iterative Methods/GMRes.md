[[Krylov Subspace]], [[Krylov Matrix QR and Arnoldi Iterations]], [[Hessenberg Transform with Arnoldi Iterations]]

---
### **Intro**

The GMRes methods is based on the idea of Arnoldi Iterations, and it uses the Krylov Subspace to minimize the current residual. It's more expensive than the Conjugate Gradient algorithm, but it's worth it. 

Major Idea
> Minimizing the projection into the orthogonal subspace generated by the arnoldi iteration. 

### **Comments and References**

The first part comes from myself, the second parts are from Yousaf Saad's sparse linear system textbook. 

---
### **From the Arnoldi Iterations**

Suppose that the arnoldi iterations is initialized with the vector $r_0$ at the start and it generates the following quantities to assist: 

$Q_k \in \mathbb{C}^{n\times k}$:: An orthogonal matrix with $q_1 = r_0/\Vert r_0\Vert$, spanning the krylov subspace. 

$H_k \in \mathbb{C}^{k\times k}$:: An upper hessenberg matrix. 

And the following recurrence relations are satisifed: 

$$
AQ_k = Q_k H_k + h_{k + 1, k}q_{k + 1}\xi_k^T
$$

Where $\xi_i$ is used to denote the ith canonical basis vector. 

**Objective of GMRes**: 

> $$
> \min_{x\in x_0 + \mathcal{K}_k(r_0)} \Vert r_k\Vert^2
> $$

Which implies that: 

$$
x_k = x_0 + Q_k y
$$

Minimizing the Squared 2 norm of the residual vector. 

And therefore, we can say: 

$$
\begin{aligned}
    & \min_{y} \Vert b - Ax_k\Vert^2
    \\
    &= 
    \min_{y} \Vert r_0 - AQ_k y\Vert^2
    \\
    &= 
    \min_{y} \Vert 
        r_0 - (Q_kH_k + h_{k+1, k}q_{k + 1}\xi_k^T)y
    \Vert^2
    \\
    &= 
    \min_{y} \left\Vert 
       \underbrace{ \Vert r_0\Vert q_1}_{=\beta q_1}
        - 
        \begin{bmatrix}
            Q_k H_k \\ h_{k + 1, k}\xi_k^T
        \end{bmatrix}
    \right\Vert^2
    \\
    &= 
    \min_{y}
    \left\Vert
        \beta q_1 - Q_{k + 1}\widetilde{H}_k
    \right\Vert^2
    \\
    &= 
    \min_{y}
    \left\Vert
        Q_{k + 1}\xi_1\beta - Q_{k + 1}\widetilde{H}_ky
    \right\Vert^2
    \\
    &= 
    \min_{y} \left\Vert
        \beta\xi_1 - \widetilde{H}_ky
    \right\Vert
\end{aligned}
$$

Where, the matrix $\widetilde{H}_k$ is $\mathbb{C}^{k + 1, k}$, the matrix should be full rank and the last row should have $h_{k + 1, k}\neq 0$, if not, the algorithm should terminates before $k$. 

To minimizes the quantity, we need to seek for a solution for $y$, therefore $k + 1$ equations, but there are only $k$ variables an over determined system, and there are usually no solution, which means that, the residual will not be zero as long as $h_{k + 1, k}\neq 0$. 

---
### **QR Factorizations on Hessenberg**

Exactly what is in the matrix will come later, here, we just want to reason with the shapes and type of matrices that are involving in solving the problems at hands. 

Consider matrix $F_k$, where $F_k = \Omega_k\Omega_{k -1}\cdots \Omega_2\Omega_1$, where $\Omega_i$ are unitary matrices that reduces the Hessenberg form one by one into a diagonal matrix. Now suppose that $F_k$ is inductively kept. 

Now, suppose that $R_k$ is an upper triangular matrix, so that $R_k = F_k \widetilde{H}_k$. 

**We now have enough for the size of the Residual**:

$$
\begin{aligned}
    & \min_{y} \Vert \beta \xi_1 - \widetilde{H}_k y\Vert^2
    \\
    &= \min_{y} \Vert 
        F_k\beta \xi_1 - R_k y
    \Vert^2
    \\
    &= \min_{y} \Vert 
        \beta (F_k)_{:, 1} - R_k y
    \Vert^2
    \\
    &= \beta(F_k)_{k+1, 1}
\end{aligned}
$$

And that is the theory. 


