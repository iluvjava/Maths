[[Krylov Subspace]], [[Krylov Matrix QR and Arnoldi Iterations]], [[Hessenberg Transform with Arnoldi Iterations]]

---
### **Intro**

The GMRes methods is based on the idea of Arnoldi Iterations, and it uses the Krylov Subspace to minimize the current residual. It's more expensive than the Conjugate Gradient algorithm, but it's worth it. 

Major Idea
> Minimizing the projection into the orthogonal subspace generated by the arnoldi iteration. 

### **Comments and References**

The first part comes from myself, the second parts are from Yousaf Saad's sparse linear system textbook. 

---
### **From the Arnoldi Iterations**

Suppose that the arnoldi iterations is initialized with the vector $r_0$ at the start and it generates the following quantities to assist: 

$Q_k \in \mathbb{C}^{n\times k}$:: An orthogonal matrix with $q_1 = r_0/\Vert r_0\Vert$, spanning the krylov subspace. 

$H_k \in \mathbb{C}^{k\times k}$:: An upper hessenberg matrix. 

And the following recurrence relations are satisifed: 

$$
AQ_k = Q_k H_k + h_{k + 1, k}q_{k + 1}\xi_k^T
$$

Where $\xi_i$ is used to denote the ith canonical basis vector. 

**Objective of GMRes**: 

> $$
> \min_{x\in x_0 + \mathcal{K}_k(r_0)} \Vert r_k\Vert^2
> $$

Which implies that: 

$$
x_k = x_0 + Q_k y
$$

Minimizing the Squared 2 norm of the residual vector. 

And therefore, we can say: 

$$
\begin{aligned}
    & \min_{y} \Vert b - Ax_k\Vert^2
    \\
    &= 
    \min_{y} \Vert r_0 - AQ_k y\Vert^2
    \\
    &= 
    \min_{y} \Vert 
        r_0 - (Q_kH_k + h_{k+1, k}q_{k + 1}\xi_k^T)y
    \Vert^2
    \\
    &= 
    \min_{y} \left\Vert 
       \underbrace{ \Vert r_0\Vert q_1}_{=\beta q_1}
        - 
        \begin{bmatrix}
            Q_k H_k \\ h_{k + 1, k}\xi_k^T
        \end{bmatrix}
    \right\Vert^2
    \\
    &= 
    \min_{y}
    \left\Vert
        \beta q_1 - Q_{k + 1}\widetilde{H}_k
    \right\Vert^2
    \\
    &= 
    \min_{y}
    \left\Vert
        Q_{k + 1}\xi_1\beta - Q_{k + 1}\widetilde{H}_ky
    \right\Vert^2
    \\
    &= 
    \min_{y} \left\Vert
        \beta\xi_1 - \widetilde{H}_ky
    \right\Vert
\end{aligned}
$$

Where, the matrix $\widetilde{H}_k$ is $\mathbb{C}^{k + 1, k}$, the matrix should be full rank and the last row should have $h_{k + 1, k}\neq 0$, if not, the algorithm should terminates before $k$. 

To minimizes the quantity, we need to seek for a solution for $y$, therefore $k + 1$ equations, but there are only $k$ variables an over determined system, and there are usually no solution, which means that, the residual will not be zero as long as $h_{k + 1, k}\neq 0$. 

---
### **QR Factorizations on Hessenberg**

Exactly what is in the matrix will come later, here, we just want to reason with the shapes and type of matrices that are involving in solving the problems at hands. 

Consider matrix $F_k$, where $F_k = \Omega_k\Omega_{k -1}\cdots \Omega_2\Omega_1$, where $\Omega_i$ are unitary matrices that reduces the Hessenberg form one by one into a diagonal matrix. Now suppose that $F_k$ is inductively kept. 

Now, suppose that $R_k$ is an upper triangular matrix, so that $R_k = F_k \widetilde{H}_k$. 

**We now have enough for the size of the Residual**:

$$
\begin{aligned}
    & \min_{y} \Vert \beta \xi_1 - \widetilde{H}_k y\Vert^2
    \\
    &= \min_{y} \Vert 
        F_k\beta \xi_1 - R_k y
    \Vert^2
    \\
    &= \min_{y} \Vert 
        \beta (F_k)_{:, 1} - R_k y
    \Vert^2
    \\
    &= \beta(F_k)_{k+1, 1}
\end{aligned}
$$

We can finely put the matrix $F_k$ into the norm at the second line without changing the value of the norm because we assumed $F_k$ to be an unitary transform. 

And that is the theory. 


---
### **Plane Rotation Triangularizations**

The plane rotation matrix eliminates a vector with 2 elements. Consider the following expression: 

For each column of the matrix $\widetilde{H}_k$, we wish to reduce the last element into a zero via an unitary transform, the base case is with a vector of 2 elements, and the matrix is given by: 

$$
\begin{bmatrix}
    c & s 
    \\
    -s^* & c
\end{bmatrix}
\begin{bmatrix}
    d \\ h
\end{bmatrix}
$$

Where if $d \neq 0$ then $c = |d|/\sqrt{|d|^2 + |h|^2}$, and $s = (ch/d)^*$, else $c = 0$ and $s = 1$. 

It's not hard to convince that the second element of the matrix vector multiplications results in: 

$$
-s^*d + ch = -(ch/d)d - ch
$$

And finally, oberve that the determinant of the matrix is one, which is a quick verification: 

$$
c^2 + |s|^2 = \frac{|d|^2}{|d|^2 + |h|^2} + c \frac{|h|^2}{|d|^2} = 1
$$

To perform the rotation to eliminate the sub-diagonal of the matrix $\widetilde{H}_k$, we consider the $n\times n$ rotations matrices adapted to eliminating the sub-diagonal of Hessenberg of size $n \times n$: 

$$
\Omega_{k}^{[n]} = \begin{bmatrix}
    I_{k -1}& &  
    \\
    & \begin{matrix}
        c & s \\ -s^* & c
    \end{matrix} &  
    \\
    & & I_{n - k - 1}  
\end{bmatrix}
$$

When $\Omega_{k}^{[k + 1]}$ is applied to the matrix $\widetilde{H}_k$, it reduces the $k^{th}$ column's last element to zero. Therefore, the matrix $F_k$ would be given as: 

$$
F_k = \Omega^{[k + 1]}_k\Omega^{[k + 1]}_{k - 1}\cdots 
\Omega^{[k + 1]}_2\Omega^{[k + 1]}_1
$$

Then we may assert that $F_k \widetilde{H}_k = R_k$, where $R_k$ is a diagonal matrix. 

We have identify the unitary transformation matrix that can reduce the tall upper Hessenberg to triangular. 

---
### **Algorithm and Implementations**