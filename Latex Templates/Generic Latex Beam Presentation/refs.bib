@book{book:first_order_opt,
    address = {Philadelphia, PA},
    author = {Beck, Amir},
    doi = {10.1137/1.9781611974997},
    eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611974997},
    publisher = {Society for Industrial and Applied Mathematics},
    title = {First-Order Methods in Optimization},
    url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974997},
    year = {2017},
    Bdsk-Url-1 = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974997},
    Bdsk-Url-2 = {https://doi.org/10.1137/1.9781611974997}
}


@book{book:ml_prob_murphy,
    author = "Kevin P. Murphy",
    title = "Probabilistic Machine Learning: An introduction",
    publisher = "MIT Press",
    year = "2022",
    url = "probml.ai",
    pages = "223--225", 
    numpages = "1098"
}


@article{paper:FISTA,
    abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude. },
    author = {Beck, Amir and Teboulle, Marc},
    doi = {10.1137/080716542},
    eprint = {https://doi.org/10.1137/080716542},
    journal = {SIAM Journal on Imaging Sciences},
    number = {1},
    pages = {183--202},
    title = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
    url = {https://doi.org/10.1137/080716542},
    volume = {2},
    year = {2009},
    Bdsk-Url-1 = {https://doi.org/10.1137/080716542}
}


@article{paper:bloyd,
    author = {Parikh, Neal and Boyd, Stephen},
    title = {Proximal Algorithms},
    year = {2014},
    issue_date = {January 2014},
    publisher = {Now Publishers Inc.},
    address = {Hanover, MA, USA},
    volume = {1},
    number = {3},
    issn = {2167-3888},
    url = {https://doi.org/10.1561/2400000003},
    doi = {10.1561/2400000003},
    abstract = {This monograph is about a class of optimization algorithms called proximal algorithms. Much like Newton's method is a standard tool for solving unconstrained smooth optimization problems of modest size, proximal algorithms can be viewed as an analogous tool for nonsmooth, constrained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical algorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed-form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpretations of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.},
    journal = {Found. Trends Optim.},
    month = {jan},
    pages = {127â€“239},
    numpages = {113}
}

@article{paper:lassos,
    ISSN = {00359246},
    URL = {http://www.jstor.org/stable/2346178},
    abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
    author = {Robert Tibshirani},
    journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
    number = {1},
    pages = {267--288},
    publisher = {[Royal Statistical Society, Wiley]},
    title = {Regression Shrinkage and Selection via the Lasso},
    urldate = {2022-11-15},
    volume = {58},
    year = {1996}
}


@paper{paper:lip_grad_cvx_equiv,
  doi = {10.48550/ARXIV.1803.06573},
  url = {https://arxiv.org/abs/1803.06573},
  author = {Zhou, Xingyu},
  keywords = {Optimization and Control (math.OC), FOS: Mathematics, FOS: Mathematics},
  title = {On the Fenchel Duality between Strong Convexity and Lipschitz Continuous Gradient},
  publisher = {arXiv},
  year = {2018},  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{bezanson2012julia,
  title={Julia: A fast dynamic language for technical computing},
  author={Bezanson, Jeff and Karpinski, Stefan and Shah, Viral B and Edelman, Alan},
  journal={arXiv preprint arXiv:1209.5145},
  year={2012}
}

